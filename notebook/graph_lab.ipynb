{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T16:41:44.406328Z",
     "start_time": "2022-05-20T16:41:44.387344Z"
    }
   },
   "outputs": [],
   "source": [
    "# env : pixlake\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T16:41:44.545373Z",
     "start_time": "2022-05-20T16:41:44.408019Z"
    }
   },
   "outputs": [],
   "source": [
    "# make you auto compeletion faster\n",
    "# https://stackoverflow.com/questions/40536560/ipython-and-jupyter-autocomplete-not-working\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T16:41:44.652319Z",
     "start_time": "2022-05-20T16:41:44.547097Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have pyspark version :  ['spark-2.3', 'spark-3.1.2-bin-hadoop2.7', 'spark-3.0', 'spark-3.0.1-bin-hadoop2.7', 'spark-2.3.4-bin-hadoop2.7', 'spark-2.4.7-bin-hadoop2.7', 'spark-2.4', 'spark-3.1']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "def get_workstation_spark_path(where_are_you : str) -> str:\n",
    "    if where_are_you == 'titan':\n",
    "        return '/home/data/ryanchao2012/lib'\n",
    "    elif where_are_you == 'thor':\n",
    "        return '/opt/spark/versions'\n",
    "    else:\n",
    "        raise ValueError(\"wrong work station name\")\n",
    "\n",
    "spark_path = get_workstation_spark_path('thor')\n",
    "\n",
    "print('You have pyspark version : ', os.listdir(spark_path))\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "# spark-2.3, spark-2.4\n",
    "# os.environ['SPARK_HOME'] = os.path.join(spark_path,'spark-2.3')\n",
    "\n",
    "# use spakr 3.1\n",
    "\n",
    "os.environ['SPARK_HOME'] = '/home/joetsai/download/spark-3.1.2-bin-hadoop3.2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T16:41:44.761771Z",
     "start_time": "2022-05-20T16:41:44.654497Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/joetsai/download/spark-3.1.2-bin-hadoop3.2'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ['SPARK_HOME']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T16:41:45.126263Z",
     "start_time": "2022-05-20T16:41:44.764030Z"
    }
   },
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession as Session\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark import SparkConf as Conf\n",
    "from pyspark.sql import functions as F, Window as W, types as T\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "C = F.col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T16:41:45.135062Z",
     "start_time": "2022-05-20T16:41:45.127434Z"
    }
   },
   "outputs": [],
   "source": [
    "# Not supported on Maven\n",
    "# https://mvnrepository.com/artifact/graphframes/graphframes?repo=spark-packages\n",
    "conf = (Conf()\n",
    "    .set('spark.sql.sources.partitionOverwriteMode', 'dynamic')\n",
    "    .set('spark.driver.memory', '10g')\n",
    "    .set('spark.driver.maxResultSize', '1g')\n",
    "#     .set('spark.jars.packages','graphframes:graphframes:0.6.0-spark3.1-s_2.11')\n",
    "   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T16:41:48.573484Z",
     "start_time": "2022-05-20T16:41:45.136016Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/21 00:41:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = (Session\n",
    "     .builder\n",
    "     .appName('graph-lab')\n",
    "     .master('local[10]')\n",
    "     .config(conf=conf)\n",
    "     .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T16:41:48.592340Z",
     "start_time": "2022-05-20T16:41:48.575787Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://thor:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[10]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>graph-lab</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f2f3d485c40>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T16:41:48.741840Z",
     "start_time": "2022-05-20T16:41:48.593647Z"
    }
   },
   "outputs": [],
   "source": [
    "# from graphframes import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# types of algorithm\n",
    "\n",
    "* 路徑查找(Path Finding), 漫步 (Walk)\n",
    "    * 最短跳躍數 / 加權最小的穿越路徑\n",
    "    * 平均最短路徑\n",
    "    \n",
    "* 中心性 (Centrality)\n",
    "    * 網路中的哪些節點更重要?\n",
    "    * 快速傳播資訊的能力 / 快速橋接不同群體的能力\n",
    "    \n",
    "* 社群檢測(Community Detection)\n",
    "    * 圖的子結構 (Clustering)\n",
    "    * 子圖如何吸引 or 排斥其他節點"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Path Finding & Walk\n",
    "\n",
    "## Beam Search\n",
    "\n",
    "* 從特定的 `節點` 透過 `邊` 向外擴展，如果 `節點` 的外連邊過多，需要選出 `TopN` 個邊\n",
    "* when `TopN` = 1 --> Greedy Approach\n",
    "\n",
    "<img src='../assets/bs_1.png'></img>\n",
    "\n",
    "https://www.debugger.wiki/article/html/163584216127749\n",
    "\n",
    "\n",
    "### Use Case\n",
    "\n",
    "* Node : keyword\n",
    "* edges : article shared between keywords\n",
    "* wieght : n articles\n",
    "* hypothesis : article will collect slightly different intent(keyword)\n",
    "* use beam search to demonstrate (visulize) how intent spread from given input\n",
    "    * e.g. 幫寶適 --> 尿布 比較\n",
    "    *               --> 幫寶適 好市多\n",
    "    *               --> ...\n",
    "\n",
    "## prune graph to tree (huristic)\n",
    "\n",
    "* Node : entity (from wiki)\n",
    "* edges : A entity belongs to B entity\n",
    "* use prune graph to tree to demonstrate (visulize) how the taxonomy tree expand from given input\n",
    "    * e.g. 演員 --> 各國演員 --> 台灣演員 --> 台灣女演員 ...\n",
    "                        --> 美國演員 --> ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Community Detection\n",
    "## Connected Components\n",
    "\n",
    "<img src='../assets/cc_1.png'></img>\n",
    "\n",
    "https://en.wikipedia.org/wiki/Component_(graph_theory)\n",
    "\n",
    "\n",
    "* 可以視為某種降維\n",
    "* 找出 Connected Component 的演算法，有時又被稱為 Union Find algo\n",
    "\n",
    "### Use case\n",
    "\n",
    "1. Node - Article\n",
    "2. Edge - Visit By a user-session\n",
    "3. Hypothesis - each user-sesson is the same intent (意圖)\n",
    "4. Use Connected Components to `cluster the articles(content)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T16:41:52.106329Z",
     "start_time": "2022-05-20T16:41:48.744464Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|session_id|article_id|\n",
      "+----------+----------+\n",
      "|        s1|  [a1, a2]|\n",
      "|        s2|  [a2, a3]|\n",
      "|        s3|  [a2, a4]|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "g = [\n",
    "    ('s1',['a1','a2']),\n",
    "    ('s2',['a2','a3']),\n",
    "    ('s3',['a2','a4'])\n",
    "]\n",
    "\n",
    "# 'a1', 'a2', 'a3', 'a4' is a connected component (The content is similar)\n",
    "\n",
    "edges_sdf = spark.createDataFrame(g,['session_id','article_id'])\n",
    "edges_sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T16:41:57.516495Z",
     "start_time": "2022-05-20T16:41:57.499262Z"
    }
   },
   "outputs": [],
   "source": [
    "def find_connected_component(edges_sdf : DataFrame) -> DataFrame:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strongly Connected Component\n",
    "\n",
    "\n",
    "<img src='../assets/scc_1.png'></img>\n",
    "\n",
    "https://zh.wikipedia.org/zh-tw/%E5%BC%BA%E8%BF%9E%E9%80%9A%E5%88%86%E9%87%8F\n",
    "\n",
    "* 非常適合用於分析群組中的類似行為，偏好\n",
    "* 僅適用於有向圖中\n",
    "\n",
    "### Use Case\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pixlake_spark3",
   "language": "python",
   "name": "pixlake_spark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "241.484px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
