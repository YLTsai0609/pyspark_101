{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-04T16:39:12.021573Z",
     "start_time": "2021-10-04T16:39:12.015660Z"
    }
   },
   "outputs": [],
   "source": [
    "# total : 58 problem and solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-04T16:39:12.375314Z",
     "start_time": "2021-10-04T16:39:12.355507Z"
    }
   },
   "outputs": [],
   "source": [
    "# env : pixlake\n",
    "# we focuing on pyspark dataframe processing\n",
    "# documentation https://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-04T16:39:12.573970Z",
     "start_time": "2021-10-04T16:39:12.555180Z"
    }
   },
   "outputs": [],
   "source": [
    "# make you auto compeletion faster\n",
    "# https://stackoverflow.com/questions/40536560/ipython-and-jupyter-autocomplete-not-working\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-04T16:39:12.893877Z",
     "start_time": "2021-10-04T16:39:12.795381Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have pyspark version :  ['spark-2.3', 'spark-3.1.2-bin-hadoop2.7', 'spark-3.0', 'spark-3.0.1-bin-hadoop2.7', 'spark-2.3.4-bin-hadoop2.7', 'spark-2.4.7-bin-hadoop2.7', 'spark-2.4', 'spark-3.1']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "def get_workstation_spark_path(where_are_you : str) -> str:\n",
    "    if where_are_you == 'titan':\n",
    "        return '/home/data/ryanchao2012/lib'\n",
    "    elif where_are_you == 'thor':\n",
    "        return '/opt/spark/versions'\n",
    "    else:\n",
    "        raise ValueError(\"wrong work station name\")\n",
    "\n",
    "spark_path = get_workstation_spark_path('thor')\n",
    "\n",
    "print('You have pyspark version : ', os.listdir(spark_path))\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "# spark-2.3, spark-2.4\n",
    "# os.environ['SPARK_HOME'] = os.path.join(spark_path,'spark-2.3')\n",
    "\n",
    "# use spakr 3.1\n",
    "\n",
    "os.environ['SPARK_HOME'] = '/home/joetsai/download/spark-3.1.2-bin-hadoop3.2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-04T16:39:13.166855Z",
     "start_time": "2021-10-04T16:39:13.148305Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/joetsai/download/spark-3.1.2-bin-hadoop3.2'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ['SPARK_HOME']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-04T16:39:16.938144Z",
     "start_time": "2021-10-04T16:39:13.864079Z"
    }
   },
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession as Session\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark import SparkConf as Conf\n",
    "from pyspark.sql import functions as F, Window as W, types as T\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "C = F.col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-04T16:39:16.952211Z",
     "start_time": "2021-10-04T16:39:16.940875Z"
    }
   },
   "outputs": [],
   "source": [
    "conf = (Conf()\n",
    "    .set('spark.sql.sources.partitionOverwriteMode', 'dynamic')\n",
    "    .set('spark.driver.memory', '4g')\n",
    "    .set('spark.driver.maxResultSize', '1g')\n",
    "   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-04T16:39:28.347742Z",
     "start_time": "2021-10-04T16:39:16.953817Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/10/05 00:39:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = (Session\n",
    "     .builder\n",
    "     .appName('pyspark-challenge')\n",
    "     .master('local[2]')\n",
    "     .config(conf=conf)\n",
    "     .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-04T16:39:28.367485Z",
     "start_time": "2021-10-04T16:39:28.351306Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://thor:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[2]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-challenge</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f9403d93b20>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating DataFrame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-04T16:40:20.484489Z",
     "start_time": "2021-10-04T16:40:17.825951Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Copy a dataframe\n",
    "\n",
    "# https://stackoverflow.com/questions/52287553/how-to-create-a-copy-of-a-dataframe-in-pyspark\n",
    "\n",
    "x = spark.createDataFrame([[1,2], [3,4]], ['a', 'b'])\n",
    "\n",
    "x2 = x.alias('x2')\n",
    "\n",
    "id(x) != id(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-25T05:44:30.115623Z",
     "start_time": "2021-08-25T05:44:29.512434Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_SUCCESS', 'category=美味食記', '._SUCCESS.crc', 'category=國內旅遊']\n",
      "\n",
      "['date=20160302', 'date=20160301']\n",
      "\n",
      "['date=20160301']\n",
      "\n",
      "['.part-00001-aa7530fb-b7d4-4bb8-9d78-fb931a5bc3af.c000.snappy.parquet.crc', 'part-00001-aa7530fb-b7d4-4bb8-9d78-fb931a5bc3af.c000.snappy.parquet']\n",
      "+-------+--------+--------+\n",
      "|  title|category|    date|\n",
      "+-------+--------+--------+\n",
      "|title_1|美味食記|20160301|\n",
      "|title_2|國內旅遊|20160301|\n",
      "+-------+--------+--------+\n",
      "\n",
      "+-------+--------+--------+\n",
      "|  title|category|    date|\n",
      "+-------+--------+--------+\n",
      "|title_3|美味食記|20160302|\n",
      "|title_1|美味食記|20160301|\n",
      "+-------+--------+--------+\n",
      "\n",
      "+-------+--------+--------+\n",
      "|  title|category|    date|\n",
      "+-------+--------+--------+\n",
      "|title_1|美味食記|20160301|\n",
      "+-------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1 hive partitioning\n",
    "# https://sparkbyexamples.com/apache-hive/hive-partitions-explained-with-examples/\n",
    "\n",
    "# split the parger table into serval smaller parts based on \n",
    "# one or multiple columns (partitioning key, for example, date, state, ...)\n",
    "\n",
    "# similar to table partitioning available in SQL server or any other RDBMS database\n",
    "\n",
    "\n",
    "# example\n",
    "\n",
    "# article_meta = spark.sparkContext.parallelize(\n",
    "#     [\n",
    "#         (\"美味食記\",\"20160301\", \"title_1\"), \n",
    "#         (\"國內旅遊\",\"20160301\", \"title_2\"), \n",
    "#         (\"美味食記\",\"20160302\", \"title_3\"), \n",
    "#     ]\n",
    "# )\n",
    "# article_meta_sdf = spark.createDataFrame(\n",
    "#     article_meta,\n",
    "#     ['category','date','title']\n",
    "# )\n",
    "\n",
    "# article_meta_sdf.write.parquet(\n",
    "#     'output/article_meta.parquet',\n",
    "#     partitionBy=['category','date']\n",
    "# )\n",
    "\n",
    "print(\n",
    "    os.listdir('output/article_meta.parquet'),\n",
    "    os.listdir('output/article_meta.parquet/category=美味食記'),\n",
    "    os.listdir('output/article_meta.parquet/category=國內旅遊'),\n",
    "    os.listdir('output/article_meta.parquet/category=美味食記/date=20160302'),\n",
    "    sep='\\n\\n'\n",
    ")\n",
    "\n",
    "# Read Case I\n",
    "(\n",
    "    spark.read.parquet('output/article_meta.parquet')\n",
    "    .where(C(\"date\") == '20160301')\n",
    ").show()\n",
    "\n",
    "# Read Case II\n",
    "\n",
    "(\n",
    "    spark.read.parquet('output/article_meta.parquet')\n",
    "    .where(C(\"category\") == '美味食記')\n",
    ").show()\n",
    "\n",
    "# Read Case III\n",
    "(\n",
    "    spark.read.parquet('output/article_meta.parquet')\n",
    "    .where(C(\"category\") == '美味食記')\n",
    "    .where(C(\"date\") == '20160301')\n",
    ").show()\n",
    "\n",
    "\n",
    "# pros\n",
    "\n",
    "# Fast accessed to the data\n",
    "# Provides the ability to perform an operation on a smaller dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pixlake_spark3",
   "language": "python",
   "name": "pixlake_spark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "241.484px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
