{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-20T05:14:18.967977Z",
     "start_time": "2021-06-20T05:14:18.929963Z"
    }
   },
   "outputs": [],
   "source": [
    "# env : pixlake\n",
    "# we focuing on pyspark dataframe processing\n",
    "# documentation https://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-20T05:14:19.259069Z",
     "start_time": "2021-06-20T05:14:18.970071Z"
    }
   },
   "outputs": [],
   "source": [
    "# make you auto compeletion faster\n",
    "# https://stackoverflow.com/questions/40536560/ipython-and-jupyter-autocomplete-not-working\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-20T05:14:19.415984Z",
     "start_time": "2021-06-20T05:14:19.262128Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have pyspark version :  ['spark-2.3', 'spark-3.0', 'spark-3.0.1-bin-hadoop2.7', 'spark-2.3.4-bin-hadoop2.7', 'spark-2.4.7-bin-hadoop2.7', 'spark-2.4']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from os.path import join as PJ\n",
    "def get_workstation_spark_path(where_are_you : str) -> str:\n",
    "    if where_are_you == 'titan':\n",
    "        return '/home/data/ryanchao2012/lib'\n",
    "    elif where_are_you == 'thor':\n",
    "        return '/opt/spark/versions'\n",
    "    else:\n",
    "        raise ValueError(\"wrong work station name\")\n",
    "\n",
    "spark_path = get_workstation_spark_path('thor')\n",
    "\n",
    "print('You have pyspark version : ', os.listdir(spark_path))\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "# spark-2.3, spark-2.4\n",
    "os.environ['SPARK_HOME'] = os.path.join(spark_path,'spark-2.3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-20T05:14:19.471717Z",
     "start_time": "2021-06-20T05:14:19.418943Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/opt/spark/versions/spark-2.3'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ['SPARK_HOME']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-20T05:14:22.437379Z",
     "start_time": "2021-06-20T05:14:19.473933Z"
    }
   },
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession as Session\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark import SparkConf as Conf\n",
    "from pyspark.sql import functions as F, Window as W, types as T\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "C = F.col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-20T05:14:22.452625Z",
     "start_time": "2021-06-20T05:14:22.439898Z"
    }
   },
   "outputs": [],
   "source": [
    "conf = (Conf()\n",
    "    .set('spark.sql.sources.partitionOverwriteMode', 'dynamic')\n",
    "    .set('spark.driver.memory', '4g')\n",
    "    .set('spark.driver.maxResultSize', '1g')\n",
    "   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-20T05:14:31.887126Z",
     "start_time": "2021-06-20T05:14:22.454141Z"
    }
   },
   "outputs": [],
   "source": [
    "spark = (Session\n",
    "     .builder\n",
    "     .appName('pyspark-challenge-nlp')\n",
    "     .master('local[2]')\n",
    "     .config(conf=conf)\n",
    "     .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-20T05:14:31.905868Z",
     "start_time": "2021-06-20T05:14:31.889937Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://thor:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[2]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-challenge-nlp</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f2bfae00890>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CounterVectorizer | Hashing TF\n",
    "\n",
    "https://towardsdatascience.com/countvectorizer-hashingtf-e66f169e2d4e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-20T05:14:36.706100Z",
     "start_time": "2021-06-20T05:14:31.908402Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "|id |words               |\n",
      "+---+--------------------+\n",
      "|0  |[PYTHON, HIVE, HIVE]|\n",
      "|1  |[JAVA, JAVA, SQL]   |\n",
      "+---+--------------------+\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (0, \"PYTHON HIVE HIVE\".split(\" \")),\n",
    "    (1, \"JAVA JAVA SQL\".split(\" \")),\n",
    "]\n",
    "\n",
    "cols = [\"id\",\"words\"]\n",
    "\n",
    "df = spark.createDataFrame(data, cols)\n",
    "df.show(truncate=False)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-20T05:14:37.574955Z",
     "start_time": "2021-06-20T05:14:36.709616Z"
    }
   },
   "outputs": [],
   "source": [
    "## CounterVectorlizerModel\n",
    "\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "# CountVectorizer??\n",
    "\n",
    "# minTF=1.0,\n",
    "# minDF=1.0,\n",
    "# vocabSize=262144,\n",
    "# binary=False,\n",
    "# inputCol=None,\n",
    "# outputCol=None,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-20T05:14:39.247762Z",
     "start_time": "2021-06-20T05:14:37.577018Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-------------------+\n",
      "|id |words               |features           |\n",
      "+---+--------------------+-------------------+\n",
      "|0  |[PYTHON, HIVE, HIVE]|(4,[0,3],[2.0,1.0])|\n",
      "|1  |[JAVA, JAVA, SQL]   |(4,[1,2],[2.0,1.0])|\n",
      "+---+--------------------+-------------------+\n",
      "\n",
      "you can check the vocabulary :  ['HIVE', 'JAVA', 'PYTHON', 'SQL']\n",
      "the order per row (follow the counts)\n",
      "0 : {'PYTHON', 'HIVE'}\n",
      "1 : {'JAVA', 'SQL'}\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(inputCol=\"words\",outputCol=\"features\")\n",
    "\n",
    "model = cv.fit(df)\n",
    "\n",
    "res = model.transform(df)\n",
    "\n",
    "res.show(truncate=False)\n",
    "\n",
    "print('you can check the vocabulary : ',sorted(model.vocabulary))\n",
    "print('the order per row (follow the counts)')\n",
    "\n",
    "for row in df.rdd.toLocalIterator():\n",
    "    print(row.id, ':', set(row.words), sep=' ')\n",
    "    \n",
    "# Term    Freq Index\n",
    "# HIVE     2     0\n",
    "# JAVA     2     1\n",
    "# PYTHON      1     2\n",
    "# SQL   1     3\n",
    "\n",
    "# (4,             [1, 2], [2.0, 1.0])\n",
    "# (vector_legnth, vector index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-20T05:14:39.260726Z",
     "start_time": "2021-06-20T05:14:39.249890Z"
    }
   },
   "outputs": [],
   "source": [
    "# sorted(model.vocabulary)\n",
    "# model.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-20T05:14:39.404689Z",
     "start_time": "2021-06-20T05:14:39.262319Z"
    }
   },
   "outputs": [],
   "source": [
    "# dir(cv)\n",
    "# dir(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-20T05:14:39.495668Z",
     "start_time": "2021-06-20T05:14:39.407137Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-20T05:14:39.907014Z",
     "start_time": "2021-06-20T05:14:39.497696Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+----------------------------------+\n",
      "|id |words               |features                          |\n",
      "+---+--------------------+----------------------------------+\n",
      "|0  |[PYTHON, HIVE, HIVE]|(262144,[129668,134160],[2.0,1.0])|\n",
      "|1  |[JAVA, JAVA, SQL]   |(262144,[53343,167238],[2.0,1.0]) |\n",
      "+---+--------------------+----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ht = HashingTF(inputCol=\"words\",outputCol=\"features\")\n",
    "\n",
    "ht_res = ht.transform(df)\n",
    "\n",
    "ht_res.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFIDF\n",
    "\n",
    "https://spark.apache.org/docs/2.3.4/api/python/pyspark.ml.html?highlight=idf#pyspark.ml.feature.IDF\n",
    "\n",
    "$t$ : term\n",
    "\n",
    "$d$ : document\n",
    "\n",
    "$D$ : corpus\n",
    "\n",
    "$TF(t, d)$ - Term frequency : number of times that term $t$ appears in document $d$\n",
    "\n",
    "$DF(t, D) : $ - document frequency : number of documents that contains term $t$\n",
    "\n",
    "$IDF(t, D) = log \\frac{|D| + 1}{DF(t, D) + 1}$ - numerical measure of how much information a term $t$ provides in corpus $D$\n",
    "\n",
    "* a term appears in all documents, $IDF(t, D) = 0$\n",
    "\n",
    "* smoothing term is applied to avoid dividibng by zero for terms outside the corpus(unseen testing set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy Sample I\n",
    "\n",
    "https://runawayhorse001.github.io/LearningApacheSpark/pyspark.pdf\n",
    "\n",
    "page 93"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-20T05:16:04.189861Z",
     "start_time": "2021-06-20T05:16:04.172178Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.feature import IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-20T05:43:11.404386Z",
     "start_time": "2021-06-20T05:43:11.243820Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- doc_id: long (nullable = true)\n",
      " |-- document: string (nullable = true)\n",
      "\n",
      "+------+-------------------------+\n",
      "|doc_id|document                 |\n",
      "+------+-------------------------+\n",
      "|0     |Python python Spark Spark|\n",
      "|1     |Python SQL               |\n",
      "+------+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentence = [\n",
    "    (0, \"Python python Spark Spark\"),\n",
    "    (1, \"Python SQL\")\n",
    "]\n",
    "\n",
    "col = [\"doc_id\",\"document\"]\n",
    "\n",
    "df = spark.createDataFrame(sentence, col)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the idf of each terms by hand\n",
    "\n",
    "$t$ : (normalized) term : $\\text{{python, spark, sql}}$\n",
    "\n",
    "$d_{i}$ : document : $d_1, d_2$\n",
    "\n",
    "$D$ : copus = ${d_1, d_2}$\n",
    "\n",
    "----------------------------------------------------------------\n",
    "\n",
    "$DF(\\text{python}, D) = 2$\n",
    "\n",
    "$DF(\\text{spark}, D) = 1$\n",
    "\n",
    "$DF(\\text{sql}, D) = 1$\n",
    "\n",
    "$DF(\\text{python}, D) = log_{2} \\frac{2+1}{2+1} = 0$ - `python` appears in every document\n",
    "\n",
    "$DF(\\text{spark}, D) = log_{2} \\frac{2+1}{1+1} = log_{2} \\frac{3}{2}$ - 0.405\n",
    "\n",
    "$DF(\\text{sql}, D) = log_{2} \\frac{2+1}{1+1} = log_{2} \\frac{3}{2}$ - 0.405\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-20T05:43:14.325116Z",
     "start_time": "2021-06-20T05:43:14.307034Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4054651081081644"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.log(3/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-20T05:43:19.422189Z",
     "start_time": "2021-06-20T05:43:19.154228Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "# It's wrong, we need idf only\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"document\",\n",
    "                      outputCol=\"term\")\n",
    "\n",
    "vectorizer = CountVectorizer(inputCol=\"term\",\n",
    "                             outputCol=\"raw_features\")\n",
    "\n",
    "idf = IDF(inputCol=\"raw_features\",outputCol=\"features\")\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, vectorizer, idf])\n",
    "\n",
    "model = pipeline.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-20T05:43:20.600705Z",
     "start_time": "2021-06-20T05:43:20.416447Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------------------------------\n",
      " doc_id       | 0                                  \n",
      " document     | Python python Spark Spark          \n",
      " term         | [python, python, spark, spark]     \n",
      " raw_features | (3,[0,1],[2.0,2.0])                \n",
      " features     | (3,[0,1],[0.0,0.8109302162163288]) \n",
      "-RECORD 1------------------------------------------\n",
      " doc_id       | 1                                  \n",
      " document     | Python SQL                         \n",
      " term         | [python, sql]                      \n",
      " raw_features | (3,[0,2],[1.0,1.0])                \n",
      " features     | (3,[0,2],[0.0,0.4054651081081644]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.transform(df).show(vertical=True, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IDF Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-20T05:44:43.519978Z",
     "start_time": "2021-06-20T05:44:43.366700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------------+\n",
      "|doc_id|document                 |\n",
      "+------+-------------------------+\n",
      "|0     |Python python Spark Spark|\n",
      "|1     |Python SQL               |\n",
      "+------+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-20T05:52:32.017754Z",
     "start_time": "2021-06-20T05:52:29.906615Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_term dataframe : \n",
      "+------+-------------------------+------+\n",
      "|doc_id|document                 |term  |\n",
      "+------+-------------------------+------+\n",
      "|0     |Python python Spark Spark|python|\n",
      "|0     |Python python Spark Spark|python|\n",
      "|0     |Python python Spark Spark|spark |\n",
      "|0     |Python python Spark Spark|spark |\n",
      "|1     |Python SQL               |python|\n",
      "|1     |Python SQL               |sql   |\n",
      "+------+-------------------------+------+\n",
      "\n",
      "n document :  2\n",
      "+------+--------------------------+-----+------------------+\n",
      "|  term|document_frequency_by_term|n_doc|               idf|\n",
      "+------+--------------------------+-----+------------------+\n",
      "|   sql|                         1|    2|0.5849625007211562|\n",
      "| spark|                         1|    2|0.5849625007211562|\n",
      "|python|                         2|    2|               0.0|\n",
      "+------+--------------------------+-----+------------------+\n",
      "\n",
      "-RECORD 0-----------------------------------------------\n",
      " term                       | sql                       \n",
      " doc_id                     | 1                         \n",
      " document                   | Python SQL                \n",
      " document_frequency_by_term | 1                         \n",
      " idf                        | 0.5849625007211562        \n",
      "-RECORD 1-----------------------------------------------\n",
      " term                       | spark                     \n",
      " doc_id                     | 0                         \n",
      " document                   | Python python Spark Spark \n",
      " document_frequency_by_term | 1                         \n",
      " idf                        | 0.5849625007211562        \n",
      "-RECORD 2-----------------------------------------------\n",
      " term                       | spark                     \n",
      " doc_id                     | 0                         \n",
      " document                   | Python python Spark Spark \n",
      " document_frequency_by_term | 1                         \n",
      " idf                        | 0.5849625007211562        \n",
      "-RECORD 3-----------------------------------------------\n",
      " term                       | python                    \n",
      " doc_id                     | 0                         \n",
      " document                   | Python python Spark Spark \n",
      " document_frequency_by_term | 2                         \n",
      " idf                        | 0.0                       \n",
      "-RECORD 4-----------------------------------------------\n",
      " term                       | python                    \n",
      " doc_id                     | 0                         \n",
      " document                   | Python python Spark Spark \n",
      " document_frequency_by_term | 2                         \n",
      " idf                        | 0.0                       \n",
      "-RECORD 5-----------------------------------------------\n",
      " term                       | python                    \n",
      " doc_id                     | 1                         \n",
      " document                   | Python SQL                \n",
      " document_frequency_by_term | 2                         \n",
      " idf                        | 0.0                       \n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Pipeline(stages=[tokenizer])\n",
    "\n",
    "model = tokenizer.fit(df)\n",
    "\n",
    "doc_term_sdf = (\n",
    "    model.transform(df)\n",
    "    .withColumn('term',F.explode_outer('term'))\n",
    ")\n",
    "\n",
    "print(\"doc_term dataframe : \")\n",
    "\n",
    "doc_term_sdf.show(truncate=False)\n",
    "\n",
    "def compute_idf(doc_term_sdf : DataFrame,\n",
    "                doc_id_col : str,\n",
    "                term_col : str,\n",
    "                verbose : bool = True\n",
    "               ):\n",
    "    \"\"\"\n",
    "    Input : should be term level dataframe \n",
    "            with document content and doc_id\n",
    "    \n",
    "    +------+-------------------------+\n",
    "    |doc_id|document                 |\n",
    "    +------+-------------------------+\n",
    "    |0     |Python python Spark Spark|\n",
    "    |1     |Python SQL               |\n",
    "    +------+-------------------------+\n",
    "    \n",
    "    Out : term level idf values\n",
    "    \n",
    "    +------+--------------------------+-----+------------------+\n",
    "    |term  |document_frequency_by_term|n_doc|idf               |\n",
    "    +------+--------------------------+-----+------------------+\n",
    "    |sql   |1                         |2    |0.5849625007211562|\n",
    "    |spark |1                         |2    |0.5849625007211562|\n",
    "    |python|2                         |2    |0.0               |\n",
    "    +------+--------------------------+-----+------------------+\n",
    "    \n",
    "    TODO add formula\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    n_document = doc_term_sdf.select(doc_id_col).distinct().count()\n",
    "    \n",
    "    if verbose:\n",
    "        print('n document : ',n_document)\n",
    "    \n",
    "    idf : DataFrame = (\n",
    "    doc_term_sdf\n",
    "    .groupBy(term_col)\n",
    "    # well, id means document_id\n",
    "    .agg(F.countDistinct(doc_id_col).alias(\"document_frequency_by_term\"))\n",
    "    .withColumn('n_doc', F.lit(n_document).cast('integer'))\n",
    "    .withColumn(\"idf\",\n",
    "                F.log2(\n",
    "                    (C(\"n_doc\") + F.lit(1)) / (C(\"document_frequency_by_term\") + F.lit(1))\n",
    "                )\n",
    "               )\n",
    "    )\n",
    "    \n",
    "    return idf\n",
    "    \n",
    "    \n",
    "idf_sdf = compute_idf(doc_term_sdf, 'doc_id', 'term')\n",
    "idf_sdf.show()\n",
    "\n",
    "doc_term_idf_sdf = (\n",
    "    doc_term_sdf\n",
    "    .join(\n",
    "        idf_sdf.drop('n_doc'),\n",
    "    on='term',\n",
    "    how='left')\n",
    ")\n",
    "\n",
    "doc_term_idf_sdf.show(vertical=True,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy Sample III (PoI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-20T05:53:18.199046Z",
     "start_time": "2021-06-20T05:53:14.375557Z"
    }
   },
   "outputs": [],
   "source": [
    "from pixlake.tables.warehouse.log.mainstream.meta import poi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-20T05:53:41.802378Z",
     "start_time": "2021-06-20T05:53:18.201815Z"
    }
   },
   "outputs": [],
   "source": [
    "poi_sdf = ( poi\n",
    "            .load(daterange=[20210530, 20210530])\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-20T05:59:16.204300Z",
     "start_time": "2021-06-20T05:59:15.177377Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|name                  |\n",
      "+----------------------+\n",
      "|江蘇老趙刀切麵               |\n",
      "|石研室 石頭火鍋              |\n",
      "|嘉味仙麻油雞腿庫飯             |\n",
      "|Vivienne Westwood Cafe|\n",
      "|五鮮級平價鍋物               |\n",
      "+----------------------+\n",
      "\n",
      "root\n",
      " |-- doc_id: long (nullable = true)\n",
      " |-- document: string (nullable = true)\n",
      "\n",
      "+------+----------------------+\n",
      "|doc_id|document              |\n",
      "+------+----------------------+\n",
      "|0     |江蘇老趙刀切麵               |\n",
      "|1     |石研室 石頭火鍋              |\n",
      "|2     |石二鍋                   |\n",
      "|3     |肉多多火鍋                 |\n",
      "|4     |Vivienne Westwood Cafe|\n",
      "+------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    poi_sdf\n",
    "    .where(C(\"category_id\") == 0)\n",
    "#     .orderBy(C(\"name\"))\n",
    "    .limit(5)\n",
    "    .select(['name'])\n",
    ").show(truncate=False)\n",
    "\n",
    "\n",
    "sentence = [\n",
    "    (0, \"江蘇老趙刀切麵\"),\n",
    "    (1, \"石研室 石頭火鍋\"),\n",
    "    (2, \"石二鍋\"),\n",
    "    (3, \"肉多多火鍋\"),\n",
    "    (4, \"Vivienne Westwood Cafe\")\n",
    "]\n",
    "\n",
    "col = [\"doc_id\",\"document\"]\n",
    "\n",
    "df = spark.createDataFrame(sentence, col)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-20T05:59:17.555776Z",
     "start_time": "2021-06-20T05:59:17.336142Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"document\",\n",
    "                      outputCol=\"term\")\n",
    "\n",
    "vectorizer = CountVectorizer(inputCol=\"term\",\n",
    "                             outputCol=\"raw_features\")\n",
    "\n",
    "idf = IDF(inputCol=\"raw_features\",outputCol=\"features\")\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, vectorizer, idf])\n",
    "\n",
    "model = pipeline.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-20T05:59:18.257278Z",
     "start_time": "2021-06-20T05:59:18.096095Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- doc_id: long (nullable = true)\n",
      " |-- document: string (nullable = true)\n",
      " |-- term: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- raw_features: vector (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n",
      "-RECORD 0------------------------------------------------------------------------------\n",
      " doc_id       | 0                                                                      \n",
      " document     | 江蘇老趙刀切麵                                                                \n",
      " term         | [江蘇老趙刀切麵]                                                              \n",
      " raw_features | (8,[3],[1.0])                                                          \n",
      " features     | (8,[3],[1.0986122886681098])                                           \n",
      "-RECORD 1------------------------------------------------------------------------------\n",
      " doc_id       | 1                                                                      \n",
      " document     | 石研室 石頭火鍋                                                               \n",
      " term         | [石研室, 石頭火鍋]                                                            \n",
      " raw_features | (8,[2,6],[1.0,1.0])                                                    \n",
      " features     | (8,[2,6],[1.0986122886681098,1.0986122886681098])                      \n",
      "-RECORD 2------------------------------------------------------------------------------\n",
      " doc_id       | 2                                                                      \n",
      " document     | 石二鍋                                                                    \n",
      " term         | [石二鍋]                                                                  \n",
      " raw_features | (8,[4],[1.0])                                                          \n",
      " features     | (8,[4],[1.0986122886681098])                                           \n",
      "-RECORD 3------------------------------------------------------------------------------\n",
      " doc_id       | 3                                                                      \n",
      " document     | 肉多多火鍋                                                                  \n",
      " term         | [肉多多火鍋]                                                                \n",
      " raw_features | (8,[1],[1.0])                                                          \n",
      " features     | (8,[1],[1.0986122886681098])                                           \n",
      "-RECORD 4------------------------------------------------------------------------------\n",
      " doc_id       | 4                                                                      \n",
      " document     | Vivienne Westwood Cafe                                                 \n",
      " term         | [vivienne, westwood, cafe]                                             \n",
      " raw_features | (8,[0,5,7],[1.0,1.0,1.0])                                              \n",
      " features     | (8,[0,5,7],[1.0986122886681098,1.0986122886681098,1.0986122886681098]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = model.transform(df)\n",
    "\n",
    "res.printSchema()\n",
    "\n",
    "res.show(vertical=True, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## idf only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-20T05:59:21.468201Z",
     "start_time": "2021-06-20T05:59:21.326349Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------------------+\n",
      "|doc_id|document              |\n",
      "+------+----------------------+\n",
      "|0     |江蘇老趙刀切麵               |\n",
      "|1     |石研室 石頭火鍋              |\n",
      "|2     |石二鍋                   |\n",
      "|3     |肉多多火鍋                 |\n",
      "|4     |Vivienne Westwood Cafe|\n",
      "+------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pixlake.nlp.utils import unigram\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-20T06:02:08.459507Z",
     "start_time": "2021-06-20T06:02:06.957635Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+----+\n",
      "|doc_id|document|term|\n",
      "+------+--------+----+\n",
      "|0     |江蘇老趙刀切麵 |江   |\n",
      "|0     |江蘇老趙刀切麵 |蘇   |\n",
      "|0     |江蘇老趙刀切麵 |老   |\n",
      "|0     |江蘇老趙刀切麵 |趙   |\n",
      "|0     |江蘇老趙刀切麵 |刀   |\n",
      "|0     |江蘇老趙刀切麵 |切   |\n",
      "|0     |江蘇老趙刀切麵 |麵   |\n",
      "|1     |石研室 石頭火鍋|石   |\n",
      "|1     |石研室 石頭火鍋|研   |\n",
      "|1     |石研室 石頭火鍋|室   |\n",
      "|1     |石研室 石頭火鍋|石   |\n",
      "|1     |石研室 石頭火鍋|頭   |\n",
      "|1     |石研室 石頭火鍋|火   |\n",
      "|1     |石研室 石頭火鍋|鍋   |\n",
      "|2     |石二鍋     |石   |\n",
      "|2     |石二鍋     |二   |\n",
      "|2     |石二鍋     |鍋   |\n",
      "|3     |肉多多火鍋   |肉   |\n",
      "|3     |肉多多火鍋   |多   |\n",
      "|3     |肉多多火鍋   |多   |\n",
      "+------+--------+----+\n",
      "only showing top 20 rows\n",
      "\n",
      "n document :  5\n",
      "-RECORD 0--------------------------------------------\n",
      " term                       | 鍋                      \n",
      " doc_id                     | 1                      \n",
      " document                   | 石研室 石頭火鍋               \n",
      " document_frequency_by_term | 3                      \n",
      " idf                        | 0.5849625007211562     \n",
      "-RECORD 1--------------------------------------------\n",
      " term                       | 鍋                      \n",
      " doc_id                     | 2                      \n",
      " document                   | 石二鍋                    \n",
      " document_frequency_by_term | 3                      \n",
      " idf                        | 0.5849625007211562     \n",
      "-RECORD 2--------------------------------------------\n",
      " term                       | 鍋                      \n",
      " doc_id                     | 3                      \n",
      " document                   | 肉多多火鍋                  \n",
      " document_frequency_by_term | 3                      \n",
      " idf                        | 0.5849625007211562     \n",
      "-RECORD 3--------------------------------------------\n",
      " term                       | 石                      \n",
      " doc_id                     | 1                      \n",
      " document                   | 石研室 石頭火鍋               \n",
      " document_frequency_by_term | 2                      \n",
      " idf                        | 1.0                    \n",
      "-RECORD 4--------------------------------------------\n",
      " term                       | 石                      \n",
      " doc_id                     | 1                      \n",
      " document                   | 石研室 石頭火鍋               \n",
      " document_frequency_by_term | 2                      \n",
      " idf                        | 1.0                    \n",
      "-RECORD 5--------------------------------------------\n",
      " term                       | 石                      \n",
      " doc_id                     | 2                      \n",
      " document                   | 石二鍋                    \n",
      " document_frequency_by_term | 2                      \n",
      " idf                        | 1.0                    \n",
      "-RECORD 6--------------------------------------------\n",
      " term                       | 火                      \n",
      " doc_id                     | 1                      \n",
      " document                   | 石研室 石頭火鍋               \n",
      " document_frequency_by_term | 2                      \n",
      " idf                        | 1.0                    \n",
      "-RECORD 7--------------------------------------------\n",
      " term                       | 火                      \n",
      " doc_id                     | 3                      \n",
      " document                   | 肉多多火鍋                  \n",
      " document_frequency_by_term | 2                      \n",
      " idf                        | 1.0                    \n",
      "-RECORD 8--------------------------------------------\n",
      " term                       | 蘇                      \n",
      " doc_id                     | 0                      \n",
      " document                   | 江蘇老趙刀切麵                \n",
      " document_frequency_by_term | 1                      \n",
      " idf                        | 1.5849625007211563     \n",
      "-RECORD 9--------------------------------------------\n",
      " term                       | 多                      \n",
      " doc_id                     | 3                      \n",
      " document                   | 肉多多火鍋                  \n",
      " document_frequency_by_term | 1                      \n",
      " idf                        | 1.5849625007211563     \n",
      "-RECORD 10-------------------------------------------\n",
      " term                       | 多                      \n",
      " doc_id                     | 3                      \n",
      " document                   | 肉多多火鍋                  \n",
      " document_frequency_by_term | 1                      \n",
      " idf                        | 1.5849625007211563     \n",
      "-RECORD 11-------------------------------------------\n",
      " term                       | 二                      \n",
      " doc_id                     | 2                      \n",
      " document                   | 石二鍋                    \n",
      " document_frequency_by_term | 1                      \n",
      " idf                        | 1.5849625007211563     \n",
      "-RECORD 12-------------------------------------------\n",
      " term                       | 趙                      \n",
      " doc_id                     | 0                      \n",
      " document                   | 江蘇老趙刀切麵                \n",
      " document_frequency_by_term | 1                      \n",
      " idf                        | 1.5849625007211563     \n",
      "-RECORD 13-------------------------------------------\n",
      " term                       | 切                      \n",
      " doc_id                     | 0                      \n",
      " document                   | 江蘇老趙刀切麵                \n",
      " document_frequency_by_term | 1                      \n",
      " idf                        | 1.5849625007211563     \n",
      "-RECORD 14-------------------------------------------\n",
      " term                       | 老                      \n",
      " doc_id                     | 0                      \n",
      " document                   | 江蘇老趙刀切麵                \n",
      " document_frequency_by_term | 1                      \n",
      " idf                        | 1.5849625007211563     \n",
      "-RECORD 15-------------------------------------------\n",
      " term                       | 室                      \n",
      " doc_id                     | 1                      \n",
      " document                   | 石研室 石頭火鍋               \n",
      " document_frequency_by_term | 1                      \n",
      " idf                        | 1.5849625007211563     \n",
      "-RECORD 16-------------------------------------------\n",
      " term                       | Westwood               \n",
      " doc_id                     | 4                      \n",
      " document                   | Vivienne Westwood Cafe \n",
      " document_frequency_by_term | 1                      \n",
      " idf                        | 1.5849625007211563     \n",
      "-RECORD 17-------------------------------------------\n",
      " term                       | Vivienne               \n",
      " doc_id                     | 4                      \n",
      " document                   | Vivienne Westwood Cafe \n",
      " document_frequency_by_term | 1                      \n",
      " idf                        | 1.5849625007211563     \n",
      "-RECORD 18-------------------------------------------\n",
      " term                       | 肉                      \n",
      " doc_id                     | 3                      \n",
      " document                   | 肉多多火鍋                  \n",
      " document_frequency_by_term | 1                      \n",
      " idf                        | 1.5849625007211563     \n",
      "-RECORD 19-------------------------------------------\n",
      " term                       | 麵                      \n",
      " doc_id                     | 0                      \n",
      " document                   | 江蘇老趙刀切麵                \n",
      " document_frequency_by_term | 1                      \n",
      " idf                        | 1.5849625007211563     \n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "doc_term_sdf = (\n",
    "    df\n",
    "    .withColumn('term',F.udf(unigram,\n",
    "                             returnType=\"array<string>\")(\"document\", F.lit(True))\n",
    "               )\n",
    "    .withColumn('term',F.explode_outer('term'))\n",
    ")\n",
    "\n",
    "\n",
    "doc_term_sdf.show(truncate=False)\n",
    "\n",
    "\n",
    "    \n",
    "idf_sdf = compute_idf(doc_term_sdf, 'doc_id', 'term')\n",
    "\n",
    "doc_term_idf_sdf = (\n",
    "    doc_term_sdf\n",
    "    .join(\n",
    "        idf_sdf.drop('n_doc'),\n",
    "    on='term',\n",
    "    how='left')\n",
    ")\n",
    "\n",
    "(\n",
    "    doc_term_idf_sdf\n",
    "    .orderBy(\"idf\")\n",
    "#     .orderBy(\"doc_id\",'document')\n",
    ").show(vertical=True,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pixlake",
   "language": "python",
   "name": "pixlake"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
