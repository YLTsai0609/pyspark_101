{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T08:27:43.911941Z",
     "start_time": "2022-02-28T08:27:43.891313Z"
    }
   },
   "outputs": [],
   "source": [
    "# env : pixlake\n",
    "# we focuing on pyspark dataframe processing\n",
    "# documentation https://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T08:27:44.108210Z",
     "start_time": "2022-02-28T08:27:44.090619Z"
    }
   },
   "outputs": [],
   "source": [
    "# make you auto compeletion faster\n",
    "# https://stackoverflow.com/questions/40536560/ipython-and-jupyter-autocomplete-not-working\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T08:27:44.279910Z",
     "start_time": "2022-02-28T08:27:44.264126Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have pyspark version :  ['spark-2.3', 'spark-3.1.2-bin-hadoop2.7', 'spark-3.0', 'spark-3.0.1-bin-hadoop2.7', 'spark-2.3.4-bin-hadoop2.7', 'spark-2.4.7-bin-hadoop2.7', 'spark-2.4', 'spark-3.1']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "def get_workstation_spark_path(where_are_you : str) -> str:\n",
    "    if where_are_you == 'titan':\n",
    "        return '/home/data/ryanchao2012/lib'\n",
    "    elif where_are_you == 'thor':\n",
    "        return '/opt/spark/versions'\n",
    "    else:\n",
    "        raise ValueError(\"wrong work station name\")\n",
    "\n",
    "spark_path = get_workstation_spark_path('thor')\n",
    "\n",
    "print('You have pyspark version : ', os.listdir(spark_path))\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "# spark-2.3, spark-2.4\n",
    "# os.environ['SPARK_HOME'] = os.path.join(spark_path,'spark-2.3')\n",
    "\n",
    "# use spakr 3.1\n",
    "\n",
    "os.environ['SPARK_HOME'] = '/opt/spark/versions/spark-3.1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T08:27:46.160002Z",
     "start_time": "2022-02-28T08:27:46.141056Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/opt/spark/versions/spark-3.1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ['SPARK_HOME']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T08:27:47.772833Z",
     "start_time": "2022-02-28T08:27:47.480718Z"
    }
   },
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession as Session\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark import SparkConf as Conf\n",
    "from pyspark.sql import functions as F, Window as W, types as T\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "C = F.col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T08:27:48.658330Z",
     "start_time": "2022-02-28T08:27:48.641156Z"
    }
   },
   "outputs": [],
   "source": [
    "conf = (Conf()\n",
    "    .set('spark.sql.sources.partitionOverwriteMode', 'dynamic')\n",
    "    .set('spark.driver.memory', '4g')\n",
    "    .set('spark.driver.maxResultSize', '1g')\n",
    "   )\n",
    "\n",
    "spark = (Session\n",
    "     .builder\n",
    "     .appName('pyspark-challenge-nlp')\n",
    "     .master('local[2]')\n",
    "     .config(conf=conf)\n",
    "     .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T08:27:52.536569Z",
     "start_time": "2022-02-28T08:27:52.521393Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://thor:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[2]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-challenge-nlp</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7ffa61b965e0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CounterVectorizer | Hashing TF\n",
    "\n",
    "https://towardsdatascience.com/countvectorizer-hashingtf-e66f169e2d4e\n",
    "\n",
    "1. **input** : array of tokens\n",
    "2. **output** : vector (dense or sparse)\n",
    "\n",
    "e.g.\n",
    "\n",
    "```\n",
    " |-- words: array (nullable = true)\n",
    " |    |-- element: string (containsNull = true)\n",
    " |-- features: vector (nullable = true)\n",
    "```\n",
    "\n",
    "calculate via all the corpus(the word column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T08:28:05.066040Z",
     "start_time": "2022-02-28T08:28:01.621319Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "|id |words               |\n",
      "+---+--------------------+\n",
      "|0  |[PYTHON, HIVE, HIVE]|\n",
      "|1  |[JAVA, JAVA, SQL]   |\n",
      "+---+--------------------+\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (0, \"PYTHON HIVE HIVE\".split(\" \")),\n",
    "    (1, \"JAVA JAVA SQL\".split(\" \")),\n",
    "]\n",
    "\n",
    "cols = [\"id\",\"words\"]\n",
    "\n",
    "df = spark.createDataFrame(data, cols)\n",
    "df.show(truncate=False)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T08:28:05.137970Z",
     "start_time": "2022-02-28T08:28:05.069359Z"
    }
   },
   "outputs": [],
   "source": [
    "## CounterVectorlizerModel\n",
    "\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "# CountVectorizer??\n",
    "\n",
    "# minTF=1.0,\n",
    "# minDF=1.0,\n",
    "# vocabSize=262144,\n",
    "# binary=False,\n",
    "# inputCol=None,\n",
    "# outputCol=None,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T08:28:06.695143Z",
     "start_time": "2022-02-28T08:28:05.139479Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n",
      "+---+--------------------+-------------------+\n",
      "|id |words               |features           |\n",
      "+---+--------------------+-------------------+\n",
      "|0  |[PYTHON, HIVE, HIVE]|(4,[0,3],[2.0,1.0])|\n",
      "|1  |[JAVA, JAVA, SQL]   |(4,[1,2],[2.0,1.0])|\n",
      "+---+--------------------+-------------------+\n",
      "\n",
      "you can check the vocabulary :  ['HIVE', 'JAVA', 'PYTHON', 'SQL']\n",
      "the order per row (follow the counts)\n",
      "0 : {'PYTHON', 'HIVE'}\n",
      "1 : {'JAVA', 'SQL'}\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(inputCol=\"words\",outputCol=\"features\")\n",
    "\n",
    "model = cv.fit(df)\n",
    "\n",
    "res = model.transform(df)\n",
    "\n",
    "res.printSchema()\n",
    "\n",
    "res.show(truncate=False)\n",
    "\n",
    "print('you can check the vocabulary : ',sorted(model.vocabulary))\n",
    "print('the order per row (follow the counts)')\n",
    "\n",
    "for row in df.rdd.toLocalIterator():\n",
    "    print(row.id, ':', set(row.words), sep=' ')\n",
    "    \n",
    "# Term    Freq Index\n",
    "# HIVE     2     0\n",
    "# JAVA     2     1\n",
    "# PYTHON      1     2\n",
    "# SQL   1     3\n",
    "\n",
    "# (4,             [1, 2], [2.0, 1.0])\n",
    "# (vector_legnth, vector index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T08:28:06.712737Z",
     "start_time": "2022-02-28T08:28:06.698857Z"
    }
   },
   "outputs": [],
   "source": [
    "# sorted(model.vocabulary)\n",
    "# model.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T08:28:06.851504Z",
     "start_time": "2022-02-28T08:28:06.714077Z"
    }
   },
   "outputs": [],
   "source": [
    "# dir(cv)\n",
    "# dir(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T08:28:06.926460Z",
     "start_time": "2022-02-28T08:28:06.854002Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T08:28:07.285161Z",
     "start_time": "2022-02-28T08:28:06.928751Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n",
      "+---+--------------------+----------------------------------+\n",
      "|id |words               |features                          |\n",
      "+---+--------------------+----------------------------------+\n",
      "|0  |[PYTHON, HIVE, HIVE]|(262144,[129668,191247],[2.0,1.0])|\n",
      "|1  |[JAVA, JAVA, SQL]   |(262144,[53343,256570],[2.0,1.0]) |\n",
      "+---+--------------------+----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ht = HashingTF(inputCol=\"words\",outputCol=\"features\")\n",
    "\n",
    "ht_res = ht.transform(df)\n",
    "\n",
    "ht_res.printSchema()\n",
    "ht_res.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFIDF\n",
    "\n",
    "https://spark.apache.org/docs/2.3.4/api/python/pyspark.ml.html?highlight=idf#pyspark.ml.feature.IDF\n",
    "\n",
    "$t$ : term\n",
    "\n",
    "$d$ : document\n",
    "\n",
    "$D$ : corpus\n",
    "\n",
    "$TF(t, d)$ - Term frequency : number of times that term $t$ appears in document $d$\n",
    "\n",
    "$DF(t, D) : $ - document frequency : number of documents that contains term $t$\n",
    "\n",
    "$IDF(t, D) = log \\frac{|D| + 1}{DF(t, D) + 1}$ - numerical measure of how much information a term $t$ provides in corpus $D$\n",
    "\n",
    "* a term appears in all documents, $IDF(t, D) = 0$\n",
    "\n",
    "* smoothing term is applied to avoid dividibng by zero for terms outside the corpus(unseen testing set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy Sample I\n",
    "\n",
    "https://runawayhorse001.github.io/LearningApacheSpark/pyspark.pdf\n",
    "\n",
    "page 93"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T07:58:44.087603Z",
     "start_time": "2022-02-28T07:58:44.076510Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.feature import IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T07:58:44.464300Z",
     "start_time": "2022-02-28T07:58:44.089253Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- doc_id: long (nullable = true)\n",
      " |-- document: string (nullable = true)\n",
      "\n",
      "+------+-------------------------+\n",
      "|doc_id|document                 |\n",
      "+------+-------------------------+\n",
      "|0     |Python python Spark Spark|\n",
      "|1     |Python SQL               |\n",
      "+------+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentence = [\n",
    "    (0, \"Python python Spark Spark\"),\n",
    "    (1, \"Python SQL\")\n",
    "]\n",
    "\n",
    "col = [\"doc_id\",\"document\"]\n",
    "\n",
    "df = spark.createDataFrame(sentence, col)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the idf of each terms by hand\n",
    "\n",
    "$t$ : (normalized) term : $\\text{{python, spark, sql}}$\n",
    "\n",
    "$d_{i}$ : document : $d_1, d_2$\n",
    "\n",
    "$D$ : copus = ${d_1, d_2}$\n",
    "\n",
    "----------------------------------------------------------------\n",
    "\n",
    "$DF(\\text{python}, D) = 2$\n",
    "\n",
    "$DF(\\text{spark}, D) = 1$\n",
    "\n",
    "$DF(\\text{sql}, D) = 1$\n",
    "\n",
    "$DF(\\text{python}, D) = log_{2} \\frac{2+1}{2+1} = 0$ - `python` appears in every document\n",
    "\n",
    "$DF(\\text{spark}, D) = log_{2} \\frac{2+1}{1+1} = log_{2} \\frac{3}{2}$ - 0.405\n",
    "\n",
    "$DF(\\text{sql}, D) = log_{2} \\frac{2+1}{1+1} = log_{2} \\frac{3}{2}$ - 0.405\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T07:58:44.480727Z",
     "start_time": "2022-02-28T07:58:44.466739Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4054651081081644"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.log(3/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T07:58:45.353049Z",
     "start_time": "2022-02-28T07:58:44.482088Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "# It's wrong, we need idf only\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"document\",\n",
    "                      outputCol=\"term\")\n",
    "\n",
    "vectorizer = CountVectorizer(inputCol=\"term\",\n",
    "                             outputCol=\"raw_features\")\n",
    "\n",
    "idf = IDF(inputCol=\"raw_features\",outputCol=\"features\")\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, vectorizer, idf])\n",
    "\n",
    "model = pipeline.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T07:58:45.688719Z",
     "start_time": "2022-02-28T07:58:45.354562Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------------------------------\n",
      " doc_id       | 0                                  \n",
      " document     | Python python Spark Spark          \n",
      " term         | [python, python, spark, spark]     \n",
      " raw_features | (3,[0,1],[2.0,2.0])                \n",
      " features     | (3,[0,1],[0.0,0.8109302162163288]) \n",
      "-RECORD 1------------------------------------------\n",
      " doc_id       | 1                                  \n",
      " document     | Python SQL                         \n",
      " term         | [python, sql]                      \n",
      " raw_features | (3,[0,2],[1.0,1.0])                \n",
      " features     | (3,[0,2],[0.0,0.4054651081081644]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.transform(df).show(vertical=True, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IDF Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T07:58:45.862034Z",
     "start_time": "2022-02-28T07:58:45.690620Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------------+\n",
      "|doc_id|document                 |\n",
      "+------+-------------------------+\n",
      "|0     |Python python Spark Spark|\n",
      "|1     |Python SQL               |\n",
      "+------+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T07:58:52.545325Z",
     "start_time": "2022-02-28T07:58:45.864309Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_term dataframe : \n",
      "+------+-------------------------+------+\n",
      "|doc_id|document                 |term  |\n",
      "+------+-------------------------+------+\n",
      "|0     |Python python Spark Spark|python|\n",
      "|0     |Python python Spark Spark|python|\n",
      "|0     |Python python Spark Spark|spark |\n",
      "|0     |Python python Spark Spark|spark |\n",
      "|1     |Python SQL               |python|\n",
      "|1     |Python SQL               |sql   |\n",
      "+------+-------------------------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 26:==========================================>           (159 + 2) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n document :  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 29:==========================================>           (157 + 2) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------------+-----+------------------+\n",
      "|  term|document_frequency_by_term|n_doc|               idf|\n",
      "+------+--------------------------+-----+------------------+\n",
      "|   sql|                         1|    2|0.5849625007211562|\n",
      "| spark|                         1|    2|0.5849625007211562|\n",
      "|python|                         2|    2|               0.0|\n",
      "+------+--------------------------+-----+------------------+\n",
      "\n",
      "term_freq of df : \n",
      "+------+------+--------------------+--------------------------+------------------+---------+\n",
      "|  term|doc_id|            document|document_frequency_by_term|               idf|term_freq|\n",
      "+------+------+--------------------+--------------------------+------------------+---------+\n",
      "|   sql|     1|          Python SQL|                         1|0.5849625007211562|        1|\n",
      "| spark|     0|Python python Spa...|                         1|0.5849625007211562|        2|\n",
      "| spark|     0|Python python Spa...|                         1|0.5849625007211562|        2|\n",
      "|python|     0|Python python Spa...|                         2|               0.0|        2|\n",
      "|python|     0|Python python Spa...|                         2|               0.0|        2|\n",
      "|python|     1|          Python SQL|                         2|               0.0|        1|\n",
      "+------+------+--------------------+--------------------------+------------------+---------+\n",
      "\n",
      "document_freq of df : \n",
      "-RECORD 0-----------------------------------------------\n",
      " term                       | sql                       \n",
      " doc_id                     | 1                         \n",
      " document                   | Python SQL                \n",
      " document_frequency_by_term | 1                         \n",
      " idf                        | 0.5849625007211562        \n",
      "-RECORD 1-----------------------------------------------\n",
      " term                       | spark                     \n",
      " doc_id                     | 0                         \n",
      " document                   | Python python Spark Spark \n",
      " document_frequency_by_term | 1                         \n",
      " idf                        | 0.5849625007211562        \n",
      "-RECORD 2-----------------------------------------------\n",
      " term                       | spark                     \n",
      " doc_id                     | 0                         \n",
      " document                   | Python python Spark Spark \n",
      " document_frequency_by_term | 1                         \n",
      " idf                        | 0.5849625007211562        \n",
      "-RECORD 3-----------------------------------------------\n",
      " term                       | python                    \n",
      " doc_id                     | 0                         \n",
      " document                   | Python python Spark Spark \n",
      " document_frequency_by_term | 2                         \n",
      " idf                        | 0.0                       \n",
      "-RECORD 4-----------------------------------------------\n",
      " term                       | python                    \n",
      " doc_id                     | 0                         \n",
      " document                   | Python python Spark Spark \n",
      " document_frequency_by_term | 2                         \n",
      " idf                        | 0.0                       \n",
      "-RECORD 5-----------------------------------------------\n",
      " term                       | python                    \n",
      " doc_id                     | 1                         \n",
      " document                   | Python SQL                \n",
      " document_frequency_by_term | 2                         \n",
      " idf                        | 0.0                       \n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Pipeline(stages=[tokenizer])\n",
    "\n",
    "model = tokenizer.fit(df)\n",
    "\n",
    "doc_term_sdf = (\n",
    "    model.transform(df)\n",
    "    .withColumn('term',F.explode_outer('term'))\n",
    ")\n",
    "\n",
    "print(\"doc_term dataframe : \")\n",
    "\n",
    "doc_term_sdf.show(truncate=False)\n",
    "\n",
    "def compute_idf(doc_term_sdf : DataFrame,\n",
    "                doc_id_col : str,\n",
    "                term_col : str,\n",
    "                verbose : bool = True\n",
    "               ):\n",
    "    \"\"\"\n",
    "    Input : should be term level dataframe \n",
    "            with document content and doc_id\n",
    "    \n",
    "    +------+-------------------------+\n",
    "    |doc_id|document                 |\n",
    "    +------+-------------------------+\n",
    "    |0     |Python python Spark Spark|\n",
    "    |1     |Python SQL               |\n",
    "    +------+-------------------------+\n",
    "    \n",
    "    Out : term level idf values\n",
    "    \n",
    "    +------+--------------------------+-----+------------------+\n",
    "    |term  |document_frequency_by_term|n_doc|idf               |\n",
    "    +------+--------------------------+-----+------------------+\n",
    "    |sql   |1                         |2    |0.5849625007211562|\n",
    "    |spark |1                         |2    |0.5849625007211562|\n",
    "    |python|2                         |2    |0.0               |\n",
    "    +------+--------------------------+-----+------------------+\n",
    "    \n",
    "    TODO add formula\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    n_document = doc_term_sdf.select(doc_id_col).distinct().count()\n",
    "    \n",
    "    if verbose:\n",
    "        print('n document : ',n_document)\n",
    "    \n",
    "    idf : DataFrame = (\n",
    "    doc_term_sdf\n",
    "    .groupBy(term_col)\n",
    "    # well, id means document_id\n",
    "    .agg(F.countDistinct(doc_id_col).alias(\"document_frequency_by_term\"))\n",
    "    .withColumn('n_doc', F.lit(n_document).cast('integer'))\n",
    "    .withColumn(\"idf\",\n",
    "                F.log2(\n",
    "                    (C(\"n_doc\") + F.lit(1)) / (C(\"document_frequency_by_term\") + F.lit(1))\n",
    "                )\n",
    "               )\n",
    "    )\n",
    "    \n",
    "    return idf\n",
    "    \n",
    "    \n",
    "idf_sdf = compute_idf(doc_term_sdf, 'doc_id', 'term')\n",
    "idf_sdf.show()\n",
    "\n",
    "doc_term_idf_sdf = (\n",
    "    doc_term_sdf\n",
    "    .join(\n",
    "        idf_sdf.drop('n_doc'),\n",
    "    on='term',\n",
    "    how='left')\n",
    ")\n",
    "\n",
    "print('term_freq of df : ')\n",
    "(\n",
    "    doc_term_idf_sdf\n",
    "    .withColumn('term_freq',F.count('*').over(\n",
    "        W.partitionBy(C(\"doc_id\"),C(\"term\"))\n",
    "    ))\n",
    ").show()\n",
    "print('document_freq of df : ')\n",
    "\n",
    "doc_term_idf_sdf.show(vertical=True,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy Sample III (PoI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T07:58:53.178995Z",
     "start_time": "2022-02-28T07:58:52.914765Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- doc_id: long (nullable = true)\n",
      " |-- document: string (nullable = true)\n",
      "\n",
      "+------+----------------------+\n",
      "|doc_id|document              |\n",
      "+------+----------------------+\n",
      "|0     |江蘇老趙刀切麵        |\n",
      "|1     |石研室 石頭火鍋       |\n",
      "|2     |石二鍋                |\n",
      "|3     |肉多多火鍋            |\n",
      "|4     |Vivienne Westwood Cafe|\n",
      "+------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentence = [\n",
    "    (0, \"江蘇老趙刀切麵\"),\n",
    "    (1, \"石研室 石頭火鍋\"),\n",
    "    (2, \"石二鍋\"),\n",
    "    (3, \"肉多多火鍋\"),\n",
    "    (4, \"Vivienne Westwood Cafe\")\n",
    "]\n",
    "\n",
    "col = [\"doc_id\",\"document\"]\n",
    "\n",
    "df = spark.createDataFrame(sentence, col)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T07:58:53.472769Z",
     "start_time": "2022-02-28T07:58:53.181436Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"document\",\n",
    "                      outputCol=\"term\")\n",
    "\n",
    "vectorizer = CountVectorizer(inputCol=\"term\",\n",
    "                             outputCol=\"raw_features\")\n",
    "\n",
    "idf = IDF(inputCol=\"raw_features\",outputCol=\"features\")\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, vectorizer, idf])\n",
    "\n",
    "model = pipeline.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T07:58:53.715922Z",
     "start_time": "2022-02-28T07:58:53.474915Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- doc_id: long (nullable = true)\n",
      " |-- document: string (nullable = true)\n",
      " |-- term: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- raw_features: vector (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n",
      "-RECORD 0------------------------------------------------------------------------------\n",
      " doc_id       | 0                                                                      \n",
      " document     | 江蘇老趙刀切麵                                                         \n",
      " term         | [江蘇老趙刀切麵]                                                       \n",
      " raw_features | (8,[0],[1.0])                                                          \n",
      " features     | (8,[0],[1.0986122886681098])                                           \n",
      "-RECORD 1------------------------------------------------------------------------------\n",
      " doc_id       | 1                                                                      \n",
      " document     | 石研室 石頭火鍋                                                        \n",
      " term         | [石研室, 石頭火鍋]                                                     \n",
      " raw_features | (8,[3,7],[1.0,1.0])                                                    \n",
      " features     | (8,[3,7],[1.0986122886681098,1.0986122886681098])                      \n",
      "-RECORD 2------------------------------------------------------------------------------\n",
      " doc_id       | 2                                                                      \n",
      " document     | 石二鍋                                                                 \n",
      " term         | [石二鍋]                                                               \n",
      " raw_features | (8,[1],[1.0])                                                          \n",
      " features     | (8,[1],[1.0986122886681098])                                           \n",
      "-RECORD 3------------------------------------------------------------------------------\n",
      " doc_id       | 3                                                                      \n",
      " document     | 肉多多火鍋                                                             \n",
      " term         | [肉多多火鍋]                                                           \n",
      " raw_features | (8,[6],[1.0])                                                          \n",
      " features     | (8,[6],[1.0986122886681098])                                           \n",
      "-RECORD 4------------------------------------------------------------------------------\n",
      " doc_id       | 4                                                                      \n",
      " document     | Vivienne Westwood Cafe                                                 \n",
      " term         | [vivienne, westwood, cafe]                                             \n",
      " raw_features | (8,[2,4,5],[1.0,1.0,1.0])                                              \n",
      " features     | (8,[2,4,5],[1.0986122886681098,1.0986122886681098,1.0986122886681098]) \n",
      "\n",
      "vectorizer :  ['江蘇老趙刀切麵', '石二鍋', 'vivienne', '石頭火鍋', 'westwood', 'cafe', '肉多多火鍋', '石研室']\n"
     ]
    }
   ],
   "source": [
    "res = model.transform(df)\n",
    "\n",
    "res.printSchema()\n",
    "\n",
    "res.show(vertical=True, truncate=False)\n",
    "\n",
    "# dir(model)\n",
    "tokenizer , vectorizer, idf = model.stages\n",
    "print(\n",
    "    'vectorizer : ', \n",
    "    vectorizer.vocabulary,\n",
    "#     'idf model : ', \n",
    "#     dir(idf)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## idf only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T07:58:53.931633Z",
     "start_time": "2022-02-28T07:58:53.718297Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------------------+\n",
      "|doc_id|document              |\n",
      "+------+----------------------+\n",
      "|0     |江蘇老趙刀切麵        |\n",
      "|1     |石研室 石頭火鍋       |\n",
      "|2     |石二鍋                |\n",
      "|3     |肉多多火鍋            |\n",
      "|4     |Vivienne Westwood Cafe|\n",
      "+------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pixlake.nlp.utils import unigram\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T07:58:56.365455Z",
     "start_time": "2022-02-28T07:58:53.938317Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------+----+\n",
      "|doc_id|document       |term|\n",
      "+------+---------------+----+\n",
      "|0     |江蘇老趙刀切麵 |江  |\n",
      "|0     |江蘇老趙刀切麵 |蘇  |\n",
      "|0     |江蘇老趙刀切麵 |老  |\n",
      "|0     |江蘇老趙刀切麵 |趙  |\n",
      "|0     |江蘇老趙刀切麵 |刀  |\n",
      "|0     |江蘇老趙刀切麵 |切  |\n",
      "|0     |江蘇老趙刀切麵 |麵  |\n",
      "|1     |石研室 石頭火鍋|石  |\n",
      "|1     |石研室 石頭火鍋|研  |\n",
      "|1     |石研室 石頭火鍋|室  |\n",
      "|1     |石研室 石頭火鍋|石  |\n",
      "|1     |石研室 石頭火鍋|頭  |\n",
      "|1     |石研室 石頭火鍋|火  |\n",
      "|1     |石研室 石頭火鍋|鍋  |\n",
      "|2     |石二鍋         |石  |\n",
      "|2     |石二鍋         |二  |\n",
      "|2     |石二鍋         |鍋  |\n",
      "|3     |肉多多火鍋     |肉  |\n",
      "|3     |肉多多火鍋     |多  |\n",
      "|3     |肉多多火鍋     |多  |\n",
      "+------+---------------+----+\n",
      "only showing top 20 rows\n",
      "\n",
      "n document :  5\n",
      "-RECORD 0--------------------------------------------\n",
      " term                       | 鍋                     \n",
      " doc_id                     | 3                      \n",
      " document                   | 肉多多火鍋             \n",
      " document_frequency_by_term | 3                      \n",
      " idf                        | 0.5849625007211562     \n",
      "-RECORD 1--------------------------------------------\n",
      " term                       | 鍋                     \n",
      " doc_id                     | 2                      \n",
      " document                   | 石二鍋                 \n",
      " document_frequency_by_term | 3                      \n",
      " idf                        | 0.5849625007211562     \n",
      "-RECORD 2--------------------------------------------\n",
      " term                       | 鍋                     \n",
      " doc_id                     | 1                      \n",
      " document                   | 石研室 石頭火鍋        \n",
      " document_frequency_by_term | 3                      \n",
      " idf                        | 0.5849625007211562     \n",
      "-RECORD 3--------------------------------------------\n",
      " term                       | 石                     \n",
      " doc_id                     | 1                      \n",
      " document                   | 石研室 石頭火鍋        \n",
      " document_frequency_by_term | 2                      \n",
      " idf                        | 1.0                    \n",
      "-RECORD 4--------------------------------------------\n",
      " term                       | 石                     \n",
      " doc_id                     | 1                      \n",
      " document                   | 石研室 石頭火鍋        \n",
      " document_frequency_by_term | 2                      \n",
      " idf                        | 1.0                    \n",
      "-RECORD 5--------------------------------------------\n",
      " term                       | 石                     \n",
      " doc_id                     | 2                      \n",
      " document                   | 石二鍋                 \n",
      " document_frequency_by_term | 2                      \n",
      " idf                        | 1.0                    \n",
      "-RECORD 6--------------------------------------------\n",
      " term                       | 火                     \n",
      " doc_id                     | 1                      \n",
      " document                   | 石研室 石頭火鍋        \n",
      " document_frequency_by_term | 2                      \n",
      " idf                        | 1.0                    \n",
      "-RECORD 7--------------------------------------------\n",
      " term                       | 火                     \n",
      " doc_id                     | 3                      \n",
      " document                   | 肉多多火鍋             \n",
      " document_frequency_by_term | 2                      \n",
      " idf                        | 1.0                    \n",
      "-RECORD 8--------------------------------------------\n",
      " term                       | 蘇                     \n",
      " doc_id                     | 0                      \n",
      " document                   | 江蘇老趙刀切麵         \n",
      " document_frequency_by_term | 1                      \n",
      " idf                        | 1.584962500721156      \n",
      "-RECORD 9--------------------------------------------\n",
      " term                       | 多                     \n",
      " doc_id                     | 3                      \n",
      " document                   | 肉多多火鍋             \n",
      " document_frequency_by_term | 1                      \n",
      " idf                        | 1.584962500721156      \n",
      "-RECORD 10-------------------------------------------\n",
      " term                       | 多                     \n",
      " doc_id                     | 3                      \n",
      " document                   | 肉多多火鍋             \n",
      " document_frequency_by_term | 1                      \n",
      " idf                        | 1.584962500721156      \n",
      "-RECORD 11-------------------------------------------\n",
      " term                       | 二                     \n",
      " doc_id                     | 2                      \n",
      " document                   | 石二鍋                 \n",
      " document_frequency_by_term | 1                      \n",
      " idf                        | 1.584962500721156      \n",
      "-RECORD 12-------------------------------------------\n",
      " term                       | 趙                     \n",
      " doc_id                     | 0                      \n",
      " document                   | 江蘇老趙刀切麵         \n",
      " document_frequency_by_term | 1                      \n",
      " idf                        | 1.584962500721156      \n",
      "-RECORD 13-------------------------------------------\n",
      " term                       | Vivienne               \n",
      " doc_id                     | 4                      \n",
      " document                   | Vivienne Westwood Cafe \n",
      " document_frequency_by_term | 1                      \n",
      " idf                        | 1.584962500721156      \n",
      "-RECORD 14-------------------------------------------\n",
      " term                       | 老                     \n",
      " doc_id                     | 0                      \n",
      " document                   | 江蘇老趙刀切麵         \n",
      " document_frequency_by_term | 1                      \n",
      " idf                        | 1.584962500721156      \n",
      "-RECORD 15-------------------------------------------\n",
      " term                       | 室                     \n",
      " doc_id                     | 1                      \n",
      " document                   | 石研室 石頭火鍋        \n",
      " document_frequency_by_term | 1                      \n",
      " idf                        | 1.584962500721156      \n",
      "-RECORD 16-------------------------------------------\n",
      " term                       | Westwood               \n",
      " doc_id                     | 4                      \n",
      " document                   | Vivienne Westwood Cafe \n",
      " document_frequency_by_term | 1                      \n",
      " idf                        | 1.584962500721156      \n",
      "-RECORD 17-------------------------------------------\n",
      " term                       | 切                     \n",
      " doc_id                     | 0                      \n",
      " document                   | 江蘇老趙刀切麵         \n",
      " document_frequency_by_term | 1                      \n",
      " idf                        | 1.584962500721156      \n",
      "-RECORD 18-------------------------------------------\n",
      " term                       | 肉                     \n",
      " doc_id                     | 3                      \n",
      " document                   | 肉多多火鍋             \n",
      " document_frequency_by_term | 1                      \n",
      " idf                        | 1.584962500721156      \n",
      "-RECORD 19-------------------------------------------\n",
      " term                       | 麵                     \n",
      " doc_id                     | 0                      \n",
      " document                   | 江蘇老趙刀切麵         \n",
      " document_frequency_by_term | 1                      \n",
      " idf                        | 1.584962500721156      \n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 102:====================================================>(198 + 2) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "doc_term_sdf = (\n",
    "    df\n",
    "    .withColumn('term',F.udf(unigram,\n",
    "                             returnType=\"array<string>\")(\"document\", F.lit(True))\n",
    "               )\n",
    "    .withColumn('term',F.explode_outer('term'))\n",
    ")\n",
    "\n",
    "\n",
    "doc_term_sdf.show(truncate=False)\n",
    "\n",
    "\n",
    "    \n",
    "idf_sdf = compute_idf(doc_term_sdf, 'doc_id', 'term')\n",
    "\n",
    "doc_term_idf_sdf = (\n",
    "    doc_term_sdf\n",
    "    .join(\n",
    "        idf_sdf.drop('n_doc'),\n",
    "    on='term',\n",
    "    how='left')\n",
    ")\n",
    "\n",
    "(\n",
    "    doc_term_idf_sdf\n",
    "    .orderBy(\"idf\")\n",
    "#     .orderBy(\"doc_id\",'document')\n",
    ").show(vertical=True,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ngram\n",
    "\n",
    "* we use `unigram` in pixlake ( a simple tokenizer to split english word and chinese word, skip sapce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sklearn ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T07:58:56.636634Z",
     "start_time": "2022-02-28T07:58:56.367033Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# from pixlake.nlp.utils import ZH_PATTERNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T07:58:56.654883Z",
     "start_time": "2022-02-28T07:58:56.638132Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['is the first', 'the first document', 'this is the']\n",
      "[[1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "#     '江蘇老趙刀切麵',\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "vectorizer2 = CountVectorizer(\n",
    "    analyzer='word',\n",
    "     # you gonna need a tokenizer if you wanna dealing with chinese\n",
    "    ngram_range=(3, 3)\n",
    ")\n",
    "X2 = vectorizer2.fit_transform(corpus)\n",
    "print(vectorizer2.get_feature_names())\n",
    "print(X2.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pyspark NGram\n",
    "\n",
    "\n",
    "input : array of tokens\n",
    "output : array of Ngram(space seperated)\n",
    "\n",
    "\n",
    "e.g.\n",
    "\n",
    "```\n",
    "root\n",
    " |-- doc_id: long (nullable = true)\n",
    " |-- token: array (nullable = true)\n",
    " |    |-- element: string (containsNull = true)\n",
    " |-- bi-gram: array (nullable = true)\n",
    " |    |-- element: string (containsNull = false)\n",
    "```\n",
    "\n",
    "if we wanna build `NGram range`, union_dataframe is needed due to it's fux sized range\n",
    "\n",
    "```\n",
    "-RECORD 6----------------------------------------------------------------\n",
    " doc_id   | 6                                                            \n",
    " document | mo-mo-paradise                                               \n",
    " token    | [mo-mo-paradise]                                             \n",
    " bi-gram  | []\n",
    "```\n",
    "\n",
    "You gonna do some filtering each `i^th` gram\n",
    "\n",
    "* You can apply `sort` to hash the N-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T07:58:56.769960Z",
     "start_time": "2022-02-28T07:58:56.656138Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "喔是\n",
      "火鍋\n"
     ]
    }
   ],
   "source": [
    "def hash_test(text : str, joiner : str = '') -> str:\n",
    "    return joiner.join(sorted(text))\n",
    "\n",
    "print(hash_test('是喔'))\n",
    "print(hash_test('火鍋'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T07:58:57.259373Z",
     "start_time": "2022-02-28T07:58:56.772423Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- doc_id: long (nullable = true)\n",
      " |-- document: string (nullable = true)\n",
      " |-- token: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- bi-gram: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- bi-gram_token: string (nullable = true)\n",
      " |-- fingerprint: string (nullable = true)\n",
      "\n",
      "-RECORD 0------------------------------------------------------------------------------------------------\n",
      " doc_id        | 0                                                                                       \n",
      " document      | 江蘇老趙刀切麵                                                                          \n",
      " token         | [江, 蘇, 老, 趙, 刀, 切, 麵]                                                            \n",
      " bi-gram       | [江 蘇, 蘇 老, 老 趙, 趙 刀, 刀 切, 切 麵]                                              \n",
      " bi-gram_token | 江 蘇                                                                                   \n",
      " fingerprint   |  江蘇                                                                                   \n",
      "-RECORD 1------------------------------------------------------------------------------------------------\n",
      " doc_id        | 0                                                                                       \n",
      " document      | 江蘇老趙刀切麵                                                                          \n",
      " token         | [江, 蘇, 老, 趙, 刀, 切, 麵]                                                            \n",
      " bi-gram       | [江 蘇, 蘇 老, 老 趙, 趙 刀, 刀 切, 切 麵]                                              \n",
      " bi-gram_token | 蘇 老                                                                                   \n",
      " fingerprint   |  老蘇                                                                                   \n",
      "-RECORD 2------------------------------------------------------------------------------------------------\n",
      " doc_id        | 0                                                                                       \n",
      " document      | 江蘇老趙刀切麵                                                                          \n",
      " token         | [江, 蘇, 老, 趙, 刀, 切, 麵]                                                            \n",
      " bi-gram       | [江 蘇, 蘇 老, 老 趙, 趙 刀, 刀 切, 切 麵]                                              \n",
      " bi-gram_token | 老 趙                                                                                   \n",
      " fingerprint   |  老趙                                                                                   \n",
      "-RECORD 3------------------------------------------------------------------------------------------------\n",
      " doc_id        | 0                                                                                       \n",
      " document      | 江蘇老趙刀切麵                                                                          \n",
      " token         | [江, 蘇, 老, 趙, 刀, 切, 麵]                                                            \n",
      " bi-gram       | [江 蘇, 蘇 老, 老 趙, 趙 刀, 刀 切, 切 麵]                                              \n",
      " bi-gram_token | 趙 刀                                                                                   \n",
      " fingerprint   |  刀趙                                                                                   \n",
      "-RECORD 4------------------------------------------------------------------------------------------------\n",
      " doc_id        | 0                                                                                       \n",
      " document      | 江蘇老趙刀切麵                                                                          \n",
      " token         | [江, 蘇, 老, 趙, 刀, 切, 麵]                                                            \n",
      " bi-gram       | [江 蘇, 蘇 老, 老 趙, 趙 刀, 刀 切, 切 麵]                                              \n",
      " bi-gram_token | 刀 切                                                                                   \n",
      " fingerprint   |  刀切                                                                                   \n",
      "-RECORD 5------------------------------------------------------------------------------------------------\n",
      " doc_id        | 0                                                                                       \n",
      " document      | 江蘇老趙刀切麵                                                                          \n",
      " token         | [江, 蘇, 老, 趙, 刀, 切, 麵]                                                            \n",
      " bi-gram       | [江 蘇, 蘇 老, 老 趙, 趙 刀, 刀 切, 切 麵]                                              \n",
      " bi-gram_token | 切 麵                                                                                   \n",
      " fingerprint   |  切麵                                                                                   \n",
      "-RECORD 6------------------------------------------------------------------------------------------------\n",
      " doc_id        | 1                                                                                       \n",
      " document      | 石研室 石頭火鍋                                                                         \n",
      " token         | [石, 研, 室, 石, 頭, 火, 鍋]                                                            \n",
      " bi-gram       | [石 研, 研 室, 室 石, 石 頭, 頭 火, 火 鍋]                                              \n",
      " bi-gram_token | 石 研                                                                                   \n",
      " fingerprint   |  石研                                                                                   \n",
      "-RECORD 7------------------------------------------------------------------------------------------------\n",
      " doc_id        | 1                                                                                       \n",
      " document      | 石研室 石頭火鍋                                                                         \n",
      " token         | [石, 研, 室, 石, 頭, 火, 鍋]                                                            \n",
      " bi-gram       | [石 研, 研 室, 室 石, 石 頭, 頭 火, 火 鍋]                                              \n",
      " bi-gram_token | 研 室                                                                                   \n",
      " fingerprint   |  室研                                                                                   \n",
      "-RECORD 8------------------------------------------------------------------------------------------------\n",
      " doc_id        | 1                                                                                       \n",
      " document      | 石研室 石頭火鍋                                                                         \n",
      " token         | [石, 研, 室, 石, 頭, 火, 鍋]                                                            \n",
      " bi-gram       | [石 研, 研 室, 室 石, 石 頭, 頭 火, 火 鍋]                                              \n",
      " bi-gram_token | 室 石                                                                                   \n",
      " fingerprint   |  室石                                                                                   \n",
      "-RECORD 9------------------------------------------------------------------------------------------------\n",
      " doc_id        | 1                                                                                       \n",
      " document      | 石研室 石頭火鍋                                                                         \n",
      " token         | [石, 研, 室, 石, 頭, 火, 鍋]                                                            \n",
      " bi-gram       | [石 研, 研 室, 室 石, 石 頭, 頭 火, 火 鍋]                                              \n",
      " bi-gram_token | 石 頭                                                                                   \n",
      " fingerprint   |  石頭                                                                                   \n",
      "-RECORD 10-----------------------------------------------------------------------------------------------\n",
      " doc_id        | 1                                                                                       \n",
      " document      | 石研室 石頭火鍋                                                                         \n",
      " token         | [石, 研, 室, 石, 頭, 火, 鍋]                                                            \n",
      " bi-gram       | [石 研, 研 室, 室 石, 石 頭, 頭 火, 火 鍋]                                              \n",
      " bi-gram_token | 頭 火                                                                                   \n",
      " fingerprint   |  火頭                                                                                   \n",
      "-RECORD 11-----------------------------------------------------------------------------------------------\n",
      " doc_id        | 1                                                                                       \n",
      " document      | 石研室 石頭火鍋                                                                         \n",
      " token         | [石, 研, 室, 石, 頭, 火, 鍋]                                                            \n",
      " bi-gram       | [石 研, 研 室, 室 石, 石 頭, 頭 火, 火 鍋]                                              \n",
      " bi-gram_token | 火 鍋                                                                                   \n",
      " fingerprint   |  火鍋                                                                                   \n",
      "-RECORD 12-----------------------------------------------------------------------------------------------\n",
      " doc_id        | 2                                                                                       \n",
      " document      | 石二鍋                                                                                  \n",
      " token         | [石, 二, 鍋]                                                                            \n",
      " bi-gram       | [石 二, 二 鍋]                                                                          \n",
      " bi-gram_token | 石 二                                                                                   \n",
      " fingerprint   |  二石                                                                                   \n",
      "-RECORD 13-----------------------------------------------------------------------------------------------\n",
      " doc_id        | 2                                                                                       \n",
      " document      | 石二鍋                                                                                  \n",
      " token         | [石, 二, 鍋]                                                                            \n",
      " bi-gram       | [石 二, 二 鍋]                                                                          \n",
      " bi-gram_token | 二 鍋                                                                                   \n",
      " fingerprint   |  二鍋                                                                                   \n",
      "-RECORD 14-----------------------------------------------------------------------------------------------\n",
      " doc_id        | 3                                                                                       \n",
      " document      | しゃぶしゃぶ温野菜日本涮涮鍋專門店                                                      \n",
      " token         | [しゃぶしゃぶ, 温, 野, 菜, 日, 本, 涮, 涮, 鍋, 專, 門, 店]                              \n",
      " bi-gram       | [しゃぶしゃぶ 温, 温 野, 野 菜, 菜 日, 日 本, 本 涮, 涮 涮, 涮 鍋, 鍋 專, 專 門, 門 店] \n",
      " bi-gram_token | しゃぶしゃぶ 温                                                                         \n",
      " fingerprint   |  ししぶぶゃゃ温                                                                         \n",
      "-RECORD 15-----------------------------------------------------------------------------------------------\n",
      " doc_id        | 3                                                                                       \n",
      " document      | しゃぶしゃぶ温野菜日本涮涮鍋專門店                                                      \n",
      " token         | [しゃぶしゃぶ, 温, 野, 菜, 日, 本, 涮, 涮, 鍋, 專, 門, 店]                              \n",
      " bi-gram       | [しゃぶしゃぶ 温, 温 野, 野 菜, 菜 日, 日 本, 本 涮, 涮 涮, 涮 鍋, 鍋 專, 專 門, 門 店] \n",
      " bi-gram_token | 温 野                                                                                   \n",
      " fingerprint   |  温野                                                                                   \n",
      "-RECORD 16-----------------------------------------------------------------------------------------------\n",
      " doc_id        | 3                                                                                       \n",
      " document      | しゃぶしゃぶ温野菜日本涮涮鍋專門店                                                      \n",
      " token         | [しゃぶしゃぶ, 温, 野, 菜, 日, 本, 涮, 涮, 鍋, 專, 門, 店]                              \n",
      " bi-gram       | [しゃぶしゃぶ 温, 温 野, 野 菜, 菜 日, 日 本, 本 涮, 涮 涮, 涮 鍋, 鍋 專, 專 門, 門 店] \n",
      " bi-gram_token | 野 菜                                                                                   \n",
      " fingerprint   |  菜野                                                                                   \n",
      "-RECORD 17-----------------------------------------------------------------------------------------------\n",
      " doc_id        | 3                                                                                       \n",
      " document      | しゃぶしゃぶ温野菜日本涮涮鍋專門店                                                      \n",
      " token         | [しゃぶしゃぶ, 温, 野, 菜, 日, 本, 涮, 涮, 鍋, 專, 門, 店]                              \n",
      " bi-gram       | [しゃぶしゃぶ 温, 温 野, 野 菜, 菜 日, 日 本, 本 涮, 涮 涮, 涮 鍋, 鍋 專, 專 門, 門 店] \n",
      " bi-gram_token | 菜 日                                                                                   \n",
      " fingerprint   |  日菜                                                                                   \n",
      "-RECORD 18-----------------------------------------------------------------------------------------------\n",
      " doc_id        | 3                                                                                       \n",
      " document      | しゃぶしゃぶ温野菜日本涮涮鍋專門店                                                      \n",
      " token         | [しゃぶしゃぶ, 温, 野, 菜, 日, 本, 涮, 涮, 鍋, 專, 門, 店]                              \n",
      " bi-gram       | [しゃぶしゃぶ 温, 温 野, 野 菜, 菜 日, 日 本, 本 涮, 涮 涮, 涮 鍋, 鍋 專, 專 門, 門 店] \n",
      " bi-gram_token | 日 本                                                                                   \n",
      " fingerprint   |  日本                                                                                   \n",
      "-RECORD 19-----------------------------------------------------------------------------------------------\n",
      " doc_id        | 3                                                                                       \n",
      " document      | しゃぶしゃぶ温野菜日本涮涮鍋專門店                                                      \n",
      " token         | [しゃぶしゃぶ, 温, 野, 菜, 日, 本, 涮, 涮, 鍋, 專, 門, 店]                              \n",
      " bi-gram       | [しゃぶしゃぶ 温, 温 野, 野 菜, 菜 日, 日 本, 本 涮, 涮 涮, 涮 鍋, 鍋 專, 專 門, 門 店] \n",
      " bi-gram_token | 本 涮                                                                                   \n",
      " fingerprint   |  本涮                                                                                   \n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import NGram\n",
    "from pixlake.nlp.utils import fingerprint\n",
    "\n",
    "ngram = NGram(n=2, inputCol='token',outputCol='bi-gram')\n",
    "\n",
    "fivegram = NGram(n=5, inputCol='token',outputCol='5-gram')\n",
    "\n",
    "sentence = [\n",
    "    (0, \"江蘇老趙刀切麵\"),\n",
    "    (1, \"石研室 石頭火鍋\"),\n",
    "    (2, \"石二鍋\"),\n",
    "    (3, \"しゃぶしゃぶ温野菜日本涮涮鍋專門店\"),\n",
    "    (4, \"Vivienne Westwood Cafe\"),\n",
    "    (5, \"婧 shabu\"),\n",
    "    (6, 'mo-mo-paradise'),\n",
    "    (7, 'mo mo paradise')\n",
    "]\n",
    "\n",
    "\n",
    "col = [\"doc_id\",\"document\"]\n",
    "\n",
    "# space-separated by default\n",
    "# you can \"sort\" to performance hashing\n",
    "\n",
    "df = (\n",
    "    spark.createDataFrame(sentence, col)\n",
    "    .withColumn('token', F.udf(unigram,'array<string>')('document', F.lit(True)))\n",
    ")\n",
    "\n",
    "bigram_df = (\n",
    "    ngram.transform(df)\n",
    "    .withColumn('bi-gram_token',F.explode(\"bi-gram\"))\n",
    "#     .withColumn('fingerprint',F.udf(fingerprint, 'string')(C(\"bi-gram_token\")))\n",
    "    .withColumn('fingerprint',F.udf(hash_test,'string')(C(\"bi-gram_token\")))\n",
    ")\n",
    "bigram_df.printSchema()\n",
    "bigram_df.show(vertical=True,truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ngram with sparse vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T07:58:57.283280Z",
     "start_time": "2022-02-28T07:58:57.262049Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import NGram, CountVectorizer, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "\n",
    "def build_ngrams(inputCol=\"tokens\", n=3):\n",
    "\n",
    "    ngrams = [\n",
    "        NGram(n=i, inputCol=\"tokens\", outputCol=\"{0}_grams\".format(i))\n",
    "        for i in range(1, n + 1)\n",
    "    ]\n",
    "\n",
    "    vectorizers = [\n",
    "        CountVectorizer(inputCol=\"{0}_grams\".format(i),\n",
    "            outputCol=\"{0}_counts\".format(i))\n",
    "        for i in range(1, n + 1)\n",
    "    ]\n",
    "\n",
    "    assembler = [VectorAssembler(\n",
    "        inputCols=[\"{0}_counts\".format(i) for i in range(1, n + 1)],\n",
    "        outputCol=\"features\"\n",
    "    )]\n",
    "\n",
    "    return Pipeline(stages=ngrams + vectorizers + assembler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T07:58:58.542168Z",
     "start_time": "2022-02-28T07:58:57.284951Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " doc_id   | 0                                                                                                                                                                                                                                   \n",
      " document | 江蘇老趙刀切麵                                                                                                                                                                                                                      \n",
      " tokens   | [江, 蘇, 老, 趙, 刀, 切, 麵]                                                                                                                                                                                                        \n",
      " 1_grams  | [江, 蘇, 老, 趙, 刀, 切, 麵]                                                                                                                                                                                                        \n",
      " 2_grams  | [江 蘇, 蘇 老, 老 趙, 趙 刀, 刀 切, 切 麵]                                                                                                                                                                                          \n",
      " 3_grams  | [江 蘇 老, 蘇 老 趙, 老 趙 刀, 趙 刀 切, 刀 切 麵]                                                                                                                                                                                  \n",
      " 1_counts | (32,[4,15,18,19,22,25,31],[1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                                                                                                                                            \n",
      " 2_counts | (30,[1,2,3,11,16,18],[1.0,1.0,1.0,1.0,1.0,1.0])                                                                                                                                                                                     \n",
      " 3_counts | (23,[2,13,17,18,20],[1.0,1.0,1.0,1.0,1.0])                                                                                                                                                                                          \n",
      " features | (85,[4,15,18,19,22,25,31,33,34,35,43,48,50,64,75,79,80,82],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                                                               \n",
      "-RECORD 1---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " doc_id   | 1                                                                                                                                                                                                                                   \n",
      " document | 石研室 石頭火鍋                                                                                                                                                                                                                     \n",
      " tokens   | [石, 研, 室, 石, 頭, 火, 鍋]                                                                                                                                                                                                        \n",
      " 1_grams  | [石, 研, 室, 石, 頭, 火, 鍋]                                                                                                                                                                                                        \n",
      " 2_grams  | [石 研, 研 室, 室 石, 石 頭, 頭 火, 火 鍋]                                                                                                                                                                                          \n",
      " 3_grams  | [石 研 室, 研 室 石, 室 石 頭, 石 頭 火, 頭 火 鍋]                                                                                                                                                                                  \n",
      " 1_counts | (32,[0,1,8,16,21,29],[2.0,1.0,1.0,1.0,1.0,1.0])                                                                                                                                                                                     \n",
      " 2_counts | (30,[0,5,7,13,26,28],[1.0,1.0,1.0,1.0,1.0,1.0])                                                                                                                                                                                     \n",
      " 3_counts | (23,[4,7,10,11,19],[1.0,1.0,1.0,1.0,1.0])                                                                                                                                                                                           \n",
      " features | (85,[0,1,8,16,21,29,32,37,39,45,58,60,66,69,72,73,81],[2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                                                                        \n",
      "-RECORD 2---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " doc_id   | 2                                                                                                                                                                                                                                   \n",
      " document | 石二鍋                                                                                                                                                                                                                              \n",
      " tokens   | [石, 二, 鍋]                                                                                                                                                                                                                        \n",
      " 1_grams  | [石, 二, 鍋]                                                                                                                                                                                                                        \n",
      " 2_grams  | [石 二, 二 鍋]                                                                                                                                                                                                                      \n",
      " 3_grams  | [石 二 鍋]                                                                                                                                                                                                                          \n",
      " 1_counts | (32,[0,1,6],[1.0,1.0,1.0])                                                                                                                                                                                                          \n",
      " 2_counts | (30,[17,25],[1.0,1.0])                                                                                                                                                                                                              \n",
      " 3_counts | (23,[22],[1.0])                                                                                                                                                                                                                     \n",
      " features | (85,[0,1,6,49,57,84],[1.0,1.0,1.0,1.0,1.0,1.0])                                                                                                                                                                                     \n",
      "-RECORD 3---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " doc_id   | 3                                                                                                                                                                                                                                   \n",
      " document | しゃぶしゃぶ温野菜日本涮涮鍋專門店                                                                                                                                                                                                  \n",
      " tokens   | [しゃぶしゃぶ, 温, 野, 菜, 日, 本, 涮, 涮, 鍋, 專, 門, 店]                                                                                                                                                                          \n",
      " 1_grams  | [しゃぶしゃぶ, 温, 野, 菜, 日, 本, 涮, 涮, 鍋, 專, 門, 店]                                                                                                                                                                          \n",
      " 2_grams  | [しゃぶしゃぶ 温, 温 野, 野 菜, 菜 日, 日 本, 本 涮, 涮 涮, 涮 鍋, 鍋 專, 專 門, 門 店]                                                                                                                                             \n",
      " 3_grams  | [しゃぶしゃぶ 温 野, 温 野 菜, 野 菜 日, 菜 日 本, 日 本 涮, 本 涮 涮, 涮 涮 鍋, 涮 鍋 專, 鍋 專 門, 專 門 店]                                                                                                                      \n",
      " 1_counts | (32,[1,3,5,7,9,11,13,14,17,23,28],[1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                                                                                                                    \n",
      " 2_counts | (30,[4,8,9,12,15,19,20,21,22,27,29],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                                                                                                                  \n",
      " 3_counts | (23,[0,3,5,6,8,9,12,15,16,21],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                                                                                                                            \n",
      " features | (85,[1,3,5,7,9,11,13,14,17,23,28,36,40,41,44,47,51,52,53,54,59,61,62,65,67,68,70,71,74,77,78,83],[1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]) \n",
      "-RECORD 4---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " doc_id   | 4                                                                                                                                                                                                                                   \n",
      " document | Vivienne Westwood Cafe                                                                                                                                                                                                              \n",
      " tokens   | [Vivienne, Westwood, Cafe]                                                                                                                                                                                                          \n",
      " 1_grams  | [Vivienne, Westwood, Cafe]                                                                                                                                                                                                          \n",
      " 2_grams  | [Vivienne Westwood, Westwood Cafe]                                                                                                                                                                                                  \n",
      " 3_grams  | [Vivienne Westwood Cafe]                                                                                                                                                                                                            \n",
      " 1_counts | (32,[10,12,20],[1.0,1.0,1.0])                                                                                                                                                                                                       \n",
      " 2_counts | (30,[10,23],[1.0,1.0])                                                                                                                                                                                                              \n",
      " 3_counts | (23,[14],[1.0])                                                                                                                                                                                                                     \n",
      " features | (85,[10,12,20,42,55,76],[1.0,1.0,1.0,1.0,1.0,1.0])                                                                                                                                                                                  \n",
      "-RECORD 5---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " doc_id   | 5                                                                                                                                                                                                                                   \n",
      " document | 婧 shabu                                                                                                                                                                                                                            \n",
      " tokens   | [婧, shabu]                                                                                                                                                                                                                         \n",
      " 1_grams  | [婧, shabu]                                                                                                                                                                                                                         \n",
      " 2_grams  | [婧 shabu]                                                                                                                                                                                                                          \n",
      " 3_grams  | []                                                                                                                                                                                                                                  \n",
      " 1_counts | (32,[24,30],[1.0,1.0])                                                                                                                                                                                                              \n",
      " 2_counts | (30,[6],[1.0])                                                                                                                                                                                                                      \n",
      " 3_counts | (23,[],[])                                                                                                                                                                                                                          \n",
      " features | (85,[24,30,38],[1.0,1.0,1.0])                                                                                                                                                                                                       \n",
      "-RECORD 6---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " doc_id   | 6                                                                                                                                                                                                                                   \n",
      " document | mo-mo-paradise                                                                                                                                                                                                                      \n",
      " tokens   | [mo-mo-paradise]                                                                                                                                                                                                                    \n",
      " 1_grams  | [mo-mo-paradise]                                                                                                                                                                                                                    \n",
      " 2_grams  | []                                                                                                                                                                                                                                  \n",
      " 3_grams  | []                                                                                                                                                                                                                                  \n",
      " 1_counts | (32,[26],[1.0])                                                                                                                                                                                                                     \n",
      " 2_counts | (30,[],[])                                                                                                                                                                                                                          \n",
      " 3_counts | (23,[],[])                                                                                                                                                                                                                          \n",
      " features | (85,[26],[1.0])                                                                                                                                                                                                                     \n",
      "-RECORD 7---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " doc_id   | 7                                                                                                                                                                                                                                   \n",
      " document | mo mo paradise                                                                                                                                                                                                                      \n",
      " tokens   | [mo, mo, paradise]                                                                                                                                                                                                                  \n",
      " 1_grams  | [mo, mo, paradise]                                                                                                                                                                                                                  \n",
      " 2_grams  | [mo mo, mo paradise]                                                                                                                                                                                                                \n",
      " 3_grams  | [mo mo paradise]                                                                                                                                                                                                                    \n",
      " 1_counts | (32,[2,27],[2.0,1.0])                                                                                                                                                                                                               \n",
      " 2_counts | (30,[14,24],[1.0,1.0])                                                                                                                                                                                                              \n",
      " 3_counts | (23,[1],[1.0])                                                                                                                                                                                                                      \n",
      " features | (85,[2,27,46,56,63],[2.0,1.0,1.0,1.0,1.0])                                                                                                                                                                                          \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df = spark.createDataFrame([\n",
    "#   (1, [\"a\", \"b\", \"c\", \"d\"]),\n",
    "#   (2, [\"d\", \"e\", \"d\"])\n",
    "# ], (\"id\", \"tokens\"))\n",
    "\n",
    "# build_ngrams().fit(df).transform(df) \n",
    "\n",
    "\n",
    "sentence = [\n",
    "    (0, \"江蘇老趙刀切麵\"),\n",
    "    (1, \"石研室 石頭火鍋\"),\n",
    "    (2, \"石二鍋\"),\n",
    "    (3, \"しゃぶしゃぶ温野菜日本涮涮鍋專門店\"),\n",
    "    (4, \"Vivienne Westwood Cafe\"),\n",
    "    (5, \"婧 shabu\"),\n",
    "    (6, 'mo-mo-paradise'),\n",
    "    (7, 'mo mo paradise')\n",
    "]\n",
    "\n",
    "\n",
    "col = [\"doc_id\",\"document\"]\n",
    "\n",
    "# space-separated by default\n",
    "\n",
    "df = (\n",
    "    spark.createDataFrame(sentence, col)\n",
    "    .withColumn('tokens', F.udf(unigram,'array<string>')('document', F.lit(True)))\n",
    ")\n",
    "\n",
    "build_ngrams().fit(df).transform(df).show(vertical=True,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ngram by window func\n",
    "\n",
    "input : token - array of string\n",
    "output : Ngram - array of string\n",
    "\n",
    "```\n",
    "root\n",
    " |-- doc_id: long (nullable = true)\n",
    " |-- token: string (nullable = true)\n",
    " |-- bi-gram: array (nullable = true)\n",
    " |    |-- element: string (containsNull = true)\n",
    "```\n",
    "\n",
    "1. compare to `Pyspark NGram` - the NGram displayed at `row` level not `column` level\n",
    "\n",
    "It's fix size range, if you wanna implement NGram range, you still needs union_dataframe\n",
    "\n",
    "and perform some filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T07:58:59.384581Z",
     "start_time": "2022-02-28T07:58:58.544084Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- doc_id: long (nullable = true)\n",
      " |-- document: string (nullable = true)\n",
      "\n",
      "+------+----------------------------------+\n",
      "|doc_id|document                          |\n",
      "+------+----------------------------------+\n",
      "|0     |江蘇老趙刀切麵                    |\n",
      "|1     |石研室 石頭火鍋                   |\n",
      "|2     |石二鍋                            |\n",
      "|3     |しゃぶしゃぶ温野菜日本涮涮鍋專門店|\n",
      "|4     |Vivienne Westwood Cafe            |\n",
      "|5     |婧 shabu                          |\n",
      "|6     |mo-mo-paradise                    |\n",
      "|7     |mo mo paradise                    |\n",
      "+------+----------------------------------+\n",
      "\n",
      "root\n",
      " |-- doc_id: long (nullable = true)\n",
      " |-- document: string (nullable = true)\n",
      " |-- token_id: integer (nullable = true)\n",
      " |-- token: string (nullable = true)\n",
      " |-- bi-gram: array (nullable = false)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- length_of_bi-gram: integer (nullable = false)\n",
      "\n",
      "+------+----------------------------------+--------+--------------+------------------+-----------------+\n",
      "|doc_id|document                          |token_id|token         |bi-gram           |length_of_bi-gram|\n",
      "+------+----------------------------------+--------+--------------+------------------+-----------------+\n",
      "|0     |江蘇老趙刀切麵                    |0       |江            |[江, 蘇]          |2                |\n",
      "|0     |江蘇老趙刀切麵                    |1       |蘇            |[蘇, 老]          |2                |\n",
      "|0     |江蘇老趙刀切麵                    |2       |老            |[老, 趙]          |2                |\n",
      "|0     |江蘇老趙刀切麵                    |3       |趙            |[趙, 刀]          |2                |\n",
      "|0     |江蘇老趙刀切麵                    |4       |刀            |[刀, 切]          |2                |\n",
      "|0     |江蘇老趙刀切麵                    |5       |切            |[切, 麵]          |2                |\n",
      "|0     |江蘇老趙刀切麵                    |6       |麵            |[麵]              |1                |\n",
      "|7     |mo mo paradise                    |0       |mo            |[mo, mo]          |2                |\n",
      "|7     |mo mo paradise                    |1       |mo            |[mo, paradise]    |2                |\n",
      "|7     |mo mo paradise                    |2       |paradise      |[paradise]        |1                |\n",
      "|6     |mo-mo-paradise                    |0       |mo-mo-paradise|[mo-mo-paradise]  |1                |\n",
      "|5     |婧 shabu                          |0       |婧            |[婧, shabu]       |2                |\n",
      "|5     |婧 shabu                          |1       |shabu         |[shabu]           |1                |\n",
      "|1     |石研室 石頭火鍋                   |0       |石            |[石, 研]          |2                |\n",
      "|1     |石研室 石頭火鍋                   |1       |研            |[研, 室]          |2                |\n",
      "|1     |石研室 石頭火鍋                   |2       |室            |[室, 石]          |2                |\n",
      "|1     |石研室 石頭火鍋                   |3       |石            |[石, 頭]          |2                |\n",
      "|1     |石研室 石頭火鍋                   |4       |頭            |[頭, 火]          |2                |\n",
      "|1     |石研室 石頭火鍋                   |5       |火            |[火, 鍋]          |2                |\n",
      "|1     |石研室 石頭火鍋                   |6       |鍋            |[鍋]              |1                |\n",
      "|3     |しゃぶしゃぶ温野菜日本涮涮鍋專門店|0       |しゃぶしゃぶ  |[しゃぶしゃぶ, 温]|2                |\n",
      "|3     |しゃぶしゃぶ温野菜日本涮涮鍋專門店|1       |温            |[温, 野]          |2                |\n",
      "|3     |しゃぶしゃぶ温野菜日本涮涮鍋專門店|2       |野            |[野, 菜]          |2                |\n",
      "|3     |しゃぶしゃぶ温野菜日本涮涮鍋專門店|3       |菜            |[菜, 日]          |2                |\n",
      "|3     |しゃぶしゃぶ温野菜日本涮涮鍋專門店|4       |日            |[日, 本]          |2                |\n",
      "|3     |しゃぶしゃぶ温野菜日本涮涮鍋專門店|5       |本            |[本, 涮]          |2                |\n",
      "|3     |しゃぶしゃぶ温野菜日本涮涮鍋專門店|6       |涮            |[涮, 涮]          |2                |\n",
      "|3     |しゃぶしゃぶ温野菜日本涮涮鍋專門店|7       |涮            |[涮, 鍋]          |2                |\n",
      "|3     |しゃぶしゃぶ温野菜日本涮涮鍋專門店|8       |鍋            |[鍋, 專]          |2                |\n",
      "|3     |しゃぶしゃぶ温野菜日本涮涮鍋專門店|9       |專            |[專, 門]          |2                |\n",
      "+------+----------------------------------+--------+--------------+------------------+-----------------+\n",
      "only showing top 30 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentence = [\n",
    "    (0, \"江蘇老趙刀切麵\"),\n",
    "    (1, \"石研室 石頭火鍋\"),\n",
    "    (2, \"石二鍋\"),\n",
    "    (3, \"しゃぶしゃぶ温野菜日本涮涮鍋專門店\"),\n",
    "    (4, \"Vivienne Westwood Cafe\"),\n",
    "    (5, \"婧 shabu\"),\n",
    "    (6, 'mo-mo-paradise'),\n",
    "    (7, 'mo mo paradise')\n",
    "]\n",
    "\n",
    "col = [\"doc_id\",\"document\"]\n",
    "\n",
    "df = spark.createDataFrame(sentence, col)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n",
    "\n",
    "\n",
    "bigram_span_in_doc = (\n",
    "    W.partitionBy('doc_id')\n",
    "     .orderBy('token_id')\n",
    "#      .rowsBetween(0, 2) \n",
    "     .rowsBetween(0, 1) \n",
    "    \n",
    "    \n",
    "#     |2     |石二鍋     |0       |石    |石二鍋    | 4gram for 3 words, jusrt filter them =)\n",
    ")\n",
    "\n",
    "bigram_df = (\n",
    "    df\n",
    "    .withColumn('unigram', F.udf(unigram, \"array<string>\")(\"document\", F.lit(True)))\n",
    "    .select(\n",
    "        '*',\n",
    "        F.posexplode_outer('unigram').alias('token_id','token')\n",
    "    )\n",
    "    .drop('unigram')\n",
    "    .withColumn('bi-gram', F.collect_list(C(\"token\")).over(bigram_span_in_doc))\n",
    "    .withColumn('length_of_bi-gram', F.size(C(\"bi-gram\")))\n",
    ")\n",
    "#     .withColumn('bi-gram',F.concat_ws('',C(\"bi-gram\")))\n",
    "\n",
    "bigram_df.printSchema()\n",
    "bigram_df.show(n=30,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### creating ngram_groups "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T07:58:59.412949Z",
     "start_time": "2022-02-28T07:58:59.386552Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "from pixlake.etl.snippets import union_dataframes\n",
    "\n",
    "def build_ngram(\n",
    "                sdf : DataFrame,\n",
    "                doc_id_col : str,\n",
    "                document_col : str,\n",
    "                output_col : str ,\n",
    "                ngram_range : list):\n",
    "    \"\"\"\n",
    "    Example\n",
    "    \n",
    "        sentence = [\n",
    "        (0, \"江蘇老趙刀切麵\"),\n",
    "        (1, \"石研室 石頭火鍋\"),\n",
    "        (2, \"石二鍋\"),\n",
    "        (3, \"しゃぶしゃぶ温野菜日本涮涮鍋專門店\"),\n",
    "        (4, \"Vivienne Westwood Cafe\")\n",
    "        ]\n",
    "\n",
    "        col = [\"doc_id\",\"document\"]\n",
    "\n",
    "        df = spark.createDataFrame(sentence, col)\n",
    "        \n",
    "        +------+----------------------+\n",
    "        |doc_id|document              |\n",
    "        +------+----------------------+\n",
    "        |0     |江蘇老趙刀切麵               |\n",
    "        |1     |石研室 石頭火鍋              |\n",
    "        |2     |石二鍋                   |\n",
    "        |3     |しゃぶしゃぶ温野菜日本涮涮鍋專門店     |\n",
    "        |4     |Vivienne Westwood Cafe|\n",
    "        +------+----------------------+\n",
    "        \n",
    "        ngram_sdf = build_ngram(df,\n",
    "            doc_id_col='doc_id',\n",
    "            document_col='document',\n",
    "            output_col='ngram',\n",
    "            ngram_range = [2,3]\n",
    "           )\n",
    "\n",
    "        (\n",
    "            ngram_sdf\n",
    "            .select(\n",
    "                'doc_id',\n",
    "                'document',\n",
    "                C('ngram.n').alias('n'),\n",
    "                C('ngram.ngram').alias('ngram'),\n",
    "            )\n",
    "            .orderBy(C('doc_id'),C('n').desc())\n",
    "        ).show()\n",
    "        \n",
    "        +------+--------+---+---------+\n",
    "        |doc_id|document|  n|    ngram|\n",
    "        +------+--------+---+---------+\n",
    "        |     0| 江蘇老趙刀切麵|  3|[蘇, 老, 趙]|\n",
    "        |     0| 江蘇老趙刀切麵|  3|[刀, 切, 麵]|\n",
    "        |     0| 江蘇老趙刀切麵|  3|[趙, 刀, 切]|\n",
    "        |     0| 江蘇老趙刀切麵|  3|[老, 趙, 刀]|\n",
    "        |     0| 江蘇老趙刀切麵|  3|[江, 蘇, 老]|\n",
    "        |     0| 江蘇老趙刀切麵|  2|   [切, 麵]|\n",
    "        |     0| 江蘇老趙刀切麵|  2|   [江, 蘇]|\n",
    "        |     0| 江蘇老趙刀切麵|  2|   [老, 趙]|\n",
    "        |     0| 江蘇老趙刀切麵|  2|   [趙, 刀]|\n",
    "        |     0| 江蘇老趙刀切麵|  2|   [刀, 切]|\n",
    "        |     0| 江蘇老趙刀切麵|  2|   [蘇, 老]|\n",
    "        |     1|石研室 石頭火鍋|  3|[石, 頭, 火]|\n",
    "        |     1|石研室 石頭火鍋|  3|[研, 室, 石]|\n",
    "        |     1|石研室 石頭火鍋|  3|[室, 石, 頭]|\n",
    "        |     1|石研室 石頭火鍋|  3|[石, 研, 室]|\n",
    "        |     1|石研室 石頭火鍋|  3|[頭, 火, 鍋]|\n",
    "        |     1|石研室 石頭火鍋|  2|   [室, 石]|\n",
    "        |     1|石研室 石頭火鍋|  2|   [石, 頭]|\n",
    "        |     1|石研室 石頭火鍋|  2|   [火, 鍋]|\n",
    "        |     1|石研室 石頭火鍋|  2|   [頭, 火]|\n",
    "        +------+--------+---+---------+\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    ngram_start, ngram_end = ngram_range[0], ngram_range[1] # complement\n",
    "    print(f'ngram_start : {ngram_start}, ngram_end : {ngram_end}')\n",
    "    \n",
    "    unigram_sdf = (\n",
    "        sdf\n",
    "        .withColumn('unigram', F.udf(unigram, \"array<string>\")(document_col, F.lit(True)))\n",
    "        .withColumn('n', F.lit(1))\n",
    "        .select(\n",
    "            '*',\n",
    "            F.posexplode_outer('unigram').alias('token_id','token')\n",
    "        )\n",
    "        .drop('unigram')\n",
    "    )\n",
    "    \n",
    "    _sdf_list = []\n",
    "    \n",
    "    _sdf_list.append(\n",
    "                unigram_sdf\n",
    "                .withColumn(output_col,F.struct(C(\"n\"),\n",
    "                                                C(\"token\"))\n",
    "                           )\n",
    "        )\n",
    "\n",
    "#     unigram_sdf.show()  \n",
    "\n",
    "    if ngram_end == 1:\n",
    "        raise NotImplementedError('NotImplementedError')\n",
    "#         return union_dataframes()\n",
    "    \n",
    "    else:\n",
    "        for i in range(ngram_start, ngram_end + 1):\n",
    "            print(f'get {i} grams excution plan...') #2\n",
    "\n",
    "            token_span = (\n",
    "                    W.partitionBy(doc_id_col)\n",
    "                     .orderBy('token_id')\n",
    "                     .rowsBetween(0, i - 1)\n",
    "                )\n",
    "            ngram_sdf = (\n",
    "                unigram_sdf\n",
    "                # TODO add ngram position_id\n",
    "                .withColumn(\"n\", F.lit(i))\n",
    "                .withColumn(output_col, F.collect_list(C(\"token\")).over(token_span))\n",
    "                .withColumn('length_of_ngram', F.size(output_col))\n",
    "#                 .withColumn(output_col,F.concat_ws('',C(output_col)))\n",
    "                .withColumn(output_col, F.struct(C(\"n\"),C(output_col)))\n",
    "                .where(C('length_of_ngram') == i)\n",
    "                .drop('length_of_ngram')\n",
    "            )\n",
    "               \n",
    "#             ngram_sdf.show(truncate=False)\n",
    "            \n",
    "            _sdf_list.append(ngram_sdf)\n",
    "        \n",
    "    _sdf_list = [sdf.drop('token_id','token','n') for sdf in _sdf_list]\n",
    "    \n",
    "    if ngram_start > 1:\n",
    "        return union_dataframes(_sdf_list[1:])\n",
    "    else:\n",
    "        return union_dataframes(_sdf_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T07:58:59.675308Z",
     "start_time": "2022-02-28T07:58:59.414275Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------------------------------+\n",
      "|doc_id|                          document|\n",
      "+------+----------------------------------+\n",
      "|     0|                    江蘇老趙刀切麵|\n",
      "|     1|                   石研室 石頭火鍋|\n",
      "|     2|                            石二鍋|\n",
      "|     3|しゃぶしゃぶ温野菜日本涮涮鍋專門店|\n",
      "|     4|              Vivienne Westwood...|\n",
      "|     5|                          婧 shabu|\n",
      "|     6|                    mo-mo-paradise|\n",
      "|     7|                    mo mo paradise|\n",
      "+------+----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T07:58:59.878838Z",
     "start_time": "2022-02-28T07:58:59.678044Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngram_start : 2, ngram_end : 3\n",
      "get 2 grams excution plan...\n",
      "get 3 grams excution plan...\n"
     ]
    }
   ],
   "source": [
    "ngram_sdf = build_ngram(df,\n",
    "            doc_id_col='doc_id',\n",
    "            document_col='document',\n",
    "            output_col='ngram',\n",
    "            ngram_range = [2,3]\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T07:59:01.028377Z",
     "start_time": "2022-02-28T07:58:59.881326Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 133:====================================>                (274 + 2) / 400]\r",
      "\r",
      "[Stage 133:================================================>    (369 + 2) / 400]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------+---+------------+\n",
      "|doc_id|       document|  n|       ngram|\n",
      "+------+---------------+---+------------+\n",
      "|     0| 江蘇老趙刀切麵|  3|[趙, 刀, 切]|\n",
      "|     0| 江蘇老趙刀切麵|  3|[蘇, 老, 趙]|\n",
      "|     0| 江蘇老趙刀切麵|  3|[刀, 切, 麵]|\n",
      "|     0| 江蘇老趙刀切麵|  3|[江, 蘇, 老]|\n",
      "|     0| 江蘇老趙刀切麵|  3|[老, 趙, 刀]|\n",
      "|     0| 江蘇老趙刀切麵|  2|    [老, 趙]|\n",
      "|     0| 江蘇老趙刀切麵|  2|    [蘇, 老]|\n",
      "|     0| 江蘇老趙刀切麵|  2|    [趙, 刀]|\n",
      "|     0| 江蘇老趙刀切麵|  2|    [江, 蘇]|\n",
      "|     0| 江蘇老趙刀切麵|  2|    [刀, 切]|\n",
      "|     0| 江蘇老趙刀切麵|  2|    [切, 麵]|\n",
      "|     1|石研室 石頭火鍋|  3|[石, 頭, 火]|\n",
      "|     1|石研室 石頭火鍋|  3|[研, 室, 石]|\n",
      "|     1|石研室 石頭火鍋|  3|[室, 石, 頭]|\n",
      "|     1|石研室 石頭火鍋|  3|[石, 研, 室]|\n",
      "|     1|石研室 石頭火鍋|  3|[頭, 火, 鍋]|\n",
      "|     1|石研室 石頭火鍋|  2|    [研, 室]|\n",
      "|     1|石研室 石頭火鍋|  2|    [室, 石]|\n",
      "|     1|石研室 石頭火鍋|  2|    [石, 頭]|\n",
      "|     1|石研室 石頭火鍋|  2|    [頭, 火]|\n",
      "+------+---------------+---+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "(\n",
    "    ngram_sdf\n",
    "    .select(\n",
    "        'doc_id',\n",
    "        'document',\n",
    "        C('ngram.n').alias('n'),\n",
    "        C('ngram.ngram').alias('ngram'),\n",
    "    )\n",
    "    .orderBy(C('doc_id'),C('n').desc())\n",
    ").show()\n",
    "# ).show(n=200,vertical=True, truncate=False)\n",
    "# ).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ngram by slice\n",
    "\n",
    "\n",
    "Do not support column =(\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T07:59:01.287017Z",
     "start_time": "2022-02-28T07:59:01.030500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------------------------------+-----------------------------+-------+--------------------+----------------------+\n",
      "|doc_id|                          document|                        token|l_token|           slice_1_2|             slice_1_3|\n",
      "+------+----------------------------------+-----------------------------+-------+--------------------+----------------------+\n",
      "|     0|                    江蘇老趙刀切麵|   [江, 蘇, 老, 趙, 刀, 切...|      7|            [江, 蘇]|          [江, 蘇, 老]|\n",
      "|     1|                   石研室 石頭火鍋|   [石, 研, 室, 石, 頭, 火...|      7|            [石, 研]|          [石, 研, 室]|\n",
      "|     2|                            石二鍋|                 [石, 二, 鍋]|      3|            [石, 二]|          [石, 二, 鍋]|\n",
      "|     3|しゃぶしゃぶ温野菜日本涮涮鍋專門店|[しゃぶしゃぶ, 温, 野, 菜,...|     12|  [しゃぶしゃぶ, 温]|[しゃぶしゃぶ, 温, 野]|\n",
      "|     4|              Vivienne Westwood...|         [Vivienne, Westwo...|      3|[Vivienne, Westwood]|  [Vivienne, Westwo...|\n",
      "|     5|                          婧 shabu|                  [婧, shabu]|      2|         [婧, shabu]|           [婧, shabu]|\n",
      "|     6|                    mo-mo-paradise|             [mo-mo-paradise]|      1|    [mo-mo-paradise]|      [mo-mo-paradise]|\n",
      "|     7|                    mo mo paradise|           [mo, mo, paradise]|      3|            [mo, mo]|    [mo, mo, paradise]|\n",
      "+------+----------------------------------+-----------------------------+-------+--------------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentence = [\n",
    "    (0, \"江蘇老趙刀切麵\"),\n",
    "    (1, \"石研室 石頭火鍋\"),\n",
    "    (2, \"石二鍋\"),\n",
    "    (3, \"しゃぶしゃぶ温野菜日本涮涮鍋專門店\"),\n",
    "    (4, \"Vivienne Westwood Cafe\"),\n",
    "    (5, \"婧 shabu\"),\n",
    "    (6, 'mo-mo-paradise'),\n",
    "    (7, 'mo mo paradise')\n",
    "]\n",
    "\n",
    "col = [\"doc_id\",\"document\"]\n",
    "\n",
    "df = spark.createDataFrame(sentence, col)\n",
    "(\n",
    "    df\n",
    "    .withColumn('token', F.udf(unigram, 'array<string>')('document',F.lit(True)))\n",
    "    .withColumn(\"l_token\", F.size(\"token\"))\n",
    "#     .withColumn('slice_0_2', F.slice(\"x\", 0, 2)) # Unexpected value for start in function slice: SQL array indices start at 1.\n",
    "    .withColumn('slice_1_2', F.slice(\"token\", 1, 2))\n",
    "    .withColumn('slice_1_3', F.slice(\"token\", 1, 3))\n",
    "    # py4j.Py4JException: Method slice([class org.apache.spark.sql.Column, class java.lang.Integer, class org.apache.spark.sql.Column]) does not exist\n",
    "#     .withColumn('slice_1_N', F.slice(\"token\", 1, C(\"l_token\"))\n",
    "#                )\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSH API\n",
    "\n",
    "* Min-Hash for Jaccard (token 沒有權重的概念在裡面)\n",
    "* Bucketed Random Projection for L2 (Threshold 很難切，因為你不知道要取多少)\n",
    "* signed random projection for cosine ( spark 沒有官方實作 )\n",
    "\n",
    "* Currently, the hyper-parameter of Pyspark(Scala) LSH \n",
    "    * $r$ and $b$\n",
    "    * $r$ (rows per band - used for AND amplifier), fixed to 1 \n",
    "    * $b$ (bands - used for OR amplifer)\n",
    "    * if tuning is not right, you'll get fp and fn for your pairs\n",
    "* threshold $b, r$ for tuning LSH according to the similarity threshold\n",
    "    * https://towardsdatascience.com/understanding-locality-sensitive-hashing-49f6d1f6134\n",
    "* Somebody point out the $r$ is always equal to `1`\n",
    "    * https://stackoverflow.com/questions/65259348/is-the-number-of-rows-always-1-in-each-band-in-the-spark-implementation-of-minha\n",
    "* JaccardDistance in LSH API is equal to  `1 - JaccardSimilarity`\n",
    "\n",
    "* Data Skew problem in LSH\n",
    "    * https://stackoverflow.com/questions/62065607/all-executors-dead-minhash-lsh-pyspark-approxsimilarityjoin-self-join-on-emr-clu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T07:59:01.307318Z",
     "start_time": "2022-02-28T07:59:01.289649Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.feature import MinHashLSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T07:59:04.880946Z",
     "start_time": "2022-02-28T07:59:01.309062Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- doc_id: long (nullable = true)\n",
      " |-- document: string (nullable = true)\n",
      " |-- token: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- token_set: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n",
      "+------+----------------------+--------------------------+--------------------------+\n",
      "|doc_id|document              |token                     |token_set                 |\n",
      "+------+----------------------+--------------------------+--------------------------+\n",
      "|0     |火鍋                  |[火, 鍋]                  |[火, 鍋]                  |\n",
      "|1     |小火鍋                |[小, 火, 鍋]              |[小, 火, 鍋]              |\n",
      "|2     |Westwood Cafe         |[Westwood, Cafe]          |[Westwood, Cafe]          |\n",
      "|4     |Vivienne Westwood Cafe|[Vivienne, Westwood, Cafe]|[Vivienne, Westwood, Cafe]|\n",
      "|5     |火 Cafe               |[火, Cafe]                |[火, Cafe]                |\n",
      "|6     |海底撈                |[海, 底, 撈]              |[海, 底, 撈]              |\n",
      "|7     |海抵撈                |[海, 抵, 撈]              |[海, 抵, 撈]              |\n",
      "+------+----------------------+--------------------------+--------------------------+\n",
      "\n",
      "root\n",
      " |-- doc_id: long (nullable = true)\n",
      " |-- document: string (nullable = true)\n",
      " |-- token: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- token_set: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- token_set_vec: vector (nullable = true)\n",
      "\n",
      "-RECORD 0-----------------------------------\n",
      " doc_id        | 0                          \n",
      " document      | 火鍋                       \n",
      " token         | [火, 鍋]                   \n",
      " token_set     | [火, 鍋]                   \n",
      " token_set_vec | (10,[1,2],[1.0,1.0])       \n",
      "-RECORD 1-----------------------------------\n",
      " doc_id        | 1                          \n",
      " document      | 小火鍋                     \n",
      " token         | [小, 火, 鍋]               \n",
      " token_set     | [小, 火, 鍋]               \n",
      " token_set_vec | (10,[1,2,6],[1.0,1.0,1.0]) \n",
      "-RECORD 2-----------------------------------\n",
      " doc_id        | 2                          \n",
      " document      | Westwood Cafe              \n",
      " token         | [Westwood, Cafe]           \n",
      " token_set     | [Westwood, Cafe]           \n",
      " token_set_vec | (10,[0,5],[1.0,1.0])       \n",
      "-RECORD 3-----------------------------------\n",
      " doc_id        | 4                          \n",
      " document      | Vivienne Westwood Cafe     \n",
      " token         | [Vivienne, Westwood, Cafe] \n",
      " token_set     | [Vivienne, Westwood, Cafe] \n",
      " token_set_vec | (10,[0,5,7],[1.0,1.0,1.0]) \n",
      "-RECORD 4-----------------------------------\n",
      " doc_id        | 5                          \n",
      " document      | 火 Cafe                    \n",
      " token         | [火, Cafe]                 \n",
      " token_set     | [火, Cafe]                 \n",
      " token_set_vec | (10,[0,1],[1.0,1.0])       \n",
      "-RECORD 5-----------------------------------\n",
      " doc_id        | 6                          \n",
      " document      | 海底撈                     \n",
      " token         | [海, 底, 撈]               \n",
      " token_set     | [海, 底, 撈]               \n",
      " token_set_vec | (10,[3,4,9],[1.0,1.0,1.0]) \n",
      "-RECORD 6-----------------------------------\n",
      " doc_id        | 7                          \n",
      " document      | 海抵撈                     \n",
      " token         | [海, 抵, 撈]               \n",
      " token_set     | [海, 抵, 撈]               \n",
      " token_set_vec | (10,[3,4,8],[1.0,1.0,1.0]) \n",
      "\n",
      "['Cafe', '火', '鍋', '海', '撈', 'Westwood', '小', 'Vivienne', '抵', '底']\n",
      "Approximately joining dfA and dfB on similarity greater than 0.1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0----------------------------------------\n",
      " lead_token_set     | [火, Cafe]                 \n",
      " followed_token_set | [Westwood, Cafe]           \n",
      " JaccardSimilarity  | 0.33333333333333326        \n",
      "-RECORD 1----------------------------------------\n",
      " lead_token_set     | [火, 鍋]                   \n",
      " followed_token_set | [小, 火, 鍋]               \n",
      " JaccardSimilarity  | 0.6666666666666666         \n",
      "-RECORD 2----------------------------------------\n",
      " lead_token_set     | [小, 火, 鍋]               \n",
      " followed_token_set | [火, 鍋]                   \n",
      " JaccardSimilarity  | 0.6666666666666666         \n",
      "-RECORD 3----------------------------------------\n",
      " lead_token_set     | [Westwood, Cafe]           \n",
      " followed_token_set | [Vivienne, Westwood, Cafe] \n",
      " JaccardSimilarity  | 0.6666666666666666         \n",
      "-RECORD 4----------------------------------------\n",
      " lead_token_set     | [Westwood, Cafe]           \n",
      " followed_token_set | [火, Cafe]                 \n",
      " JaccardSimilarity  | 0.33333333333333326        \n",
      "-RECORD 5----------------------------------------\n",
      " lead_token_set     | [火, Cafe]                 \n",
      " followed_token_set | [火, 鍋]                   \n",
      " JaccardSimilarity  | 0.33333333333333326        \n",
      "-RECORD 6----------------------------------------\n",
      " lead_token_set     | [Vivienne, Westwood, Cafe] \n",
      " followed_token_set | [Westwood, Cafe]           \n",
      " JaccardSimilarity  | 0.6666666666666666         \n",
      "-RECORD 7----------------------------------------\n",
      " lead_token_set     | [海, 抵, 撈]               \n",
      " followed_token_set | [海, 底, 撈]               \n",
      " JaccardSimilarity  | 0.5                        \n",
      "-RECORD 8----------------------------------------\n",
      " lead_token_set     | [火, 鍋]                   \n",
      " followed_token_set | [火, Cafe]                 \n",
      " JaccardSimilarity  | 0.33333333333333326        \n",
      "-RECORD 9----------------------------------------\n",
      " lead_token_set     | [火, Cafe]                 \n",
      " followed_token_set | [Vivienne, Westwood, Cafe] \n",
      " JaccardSimilarity  | 0.25                       \n",
      "-RECORD 10---------------------------------------\n",
      " lead_token_set     | [Vivienne, Westwood, Cafe] \n",
      " followed_token_set | [火, Cafe]                 \n",
      " JaccardSimilarity  | 0.25                       \n",
      "-RECORD 11---------------------------------------\n",
      " lead_token_set     | [海, 底, 撈]               \n",
      " followed_token_set | [海, 抵, 撈]               \n",
      " JaccardSimilarity  | 0.5                        \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# preprocessing the bool vector for shingle\n",
    "\n",
    "sentence = [\n",
    "    (0, \"火鍋\"),\n",
    "    (1, \"小火鍋\"),\n",
    "    (2, \"Westwood Cafe\"),\n",
    "#     (3, \"肉多多火鍋\"),\n",
    "    (4, \"Vivienne Westwood Cafe\"),\n",
    "    (5, \"火 Cafe\"),\n",
    "    (6, \"海底撈\"),\n",
    "    (7, \"海抵撈\")\n",
    "]\n",
    "\n",
    "col = [\"doc_id\",\"document\"]\n",
    "\n",
    "df = spark.createDataFrame(sentence, col)\n",
    "\n",
    "doc_token_sdf = (\n",
    "    df\n",
    "    .withColumn('token',F.udf(unigram,\n",
    "                             returnType=\"array<string>\")(\"document\", F.lit(True))\n",
    "               )\n",
    "    .withColumn('token_set',F.array_distinct(C(\"token\")))\n",
    ")\n",
    "\n",
    "doc_token_sdf.printSchema()\n",
    "doc_token_sdf.show(truncate=False)\n",
    "\n",
    "# CountVectorizer(inputCol=\"words\",outputCol=\"features\")\n",
    "\n",
    "vectorizer = CountVectorizer(inputCol=\"token_set\",\n",
    "                      outputCol=\"token_set_vec\")\n",
    "\n",
    "pipeline = Pipeline(stages=[vectorizer])\n",
    "\n",
    "model = pipeline.fit(doc_token_sdf)\n",
    "\n",
    "token_vec_sdf = model.transform(doc_token_sdf)\n",
    "\n",
    "token_vec_sdf.printSchema()\n",
    "\n",
    "token_vec_sdf.show(vertical=True,truncate=False)\n",
    "\n",
    "vectorizer, = model.stages\n",
    "\n",
    "print(vectorizer.vocabulary)\n",
    "\n",
    "\n",
    "\n",
    "mh = MinHashLSH(inputCol=\"token_set_vec\",\n",
    "                outputCol=\"hashes\",\n",
    "                seed = 42,\n",
    "                # 調整 numHashTables (b) 確實和 false positive / false negtive 有關係\n",
    "                # 可以調 1，就會看到 fn, fp\n",
    "#                 numHashTables=50\n",
    "                numHashTables=20\n",
    "               )\n",
    "model = mh.fit(token_vec_sdf)\n",
    "\n",
    "# Feature Transformation\n",
    "# print(\"The hashed dataset where hashed values are stored in the column 'hashes':\")\n",
    "# model.transform(dfA).show(vertical=True,\n",
    "#                           truncate=False)\n",
    "\n",
    "# Compute the locality sensitive hashes for the input rows, then perform approximate\n",
    "# similarity join.\n",
    "# We could avoid computing hashes by passing in the already-transformed dataset, e.g.\n",
    "# `model.approxSimilarityJoin(transformedA, transformedB, 0.6)`\n",
    "\n",
    "# Jaccard distance == 1 --> 毫不相關\n",
    "# Jaccard distance == 0 --> token set 全相同\n",
    "\n",
    "sim = 0.1\n",
    "print(f\"Approximately joining dfA and dfB on similarity greater than {sim}:\")\n",
    "(\n",
    "    model\n",
    "    .approxSimilarityJoin(\n",
    "        token_vec_sdf,\n",
    "        token_vec_sdf,\n",
    "#         0.9,\n",
    "        1 - sim,\n",
    "        distCol=\"JaccardDistance\"\n",
    "    )\n",
    "    .withColumn('JaccardSimilarity', 1 - C(\"JaccardDistance\"))\n",
    "    .where(C(\"JaccardDistance\") > 0)\n",
    "    .select(\n",
    "\n",
    "        C(\"datasetA.token_set\").alias('lead_token_set'),\n",
    "        C(\"datasetB.token_set\").alias('followed_token_set'),\n",
    "        \"JaccardSimilarity\"\n",
    "    )\n",
    ").show(vertical=True,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T07:59:05.135154Z",
     "start_time": "2022-02-28T07:59:04.883072Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- doc_id: long (nullable = true)\n",
      " |-- document: string (nullable = true)\n",
      " |-- token: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- token_set: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- token_set_vec: vector (nullable = true)\n",
      " |-- hashes: array (nullable = true)\n",
      " |    |-- element: vector (containsNull = true)\n",
      "\n",
      "-RECORD 0-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " doc_id        | 0                                                                                                                                                                                                                                                                                                                                  \n",
      " document      | 火鍋                                                                                                                                                                                                                                                                                                                               \n",
      " token         | [火, 鍋]                                                                                                                                                                                                                                                                                                                           \n",
      " token_set     | [火, 鍋]                                                                                                                                                                                                                                                                                                                           \n",
      " token_set_vec | (10,[1,2],[1.0,1.0])                                                                                                                                                                                                                                                                                                               \n",
      " hashes        | [[7.2853667E8], [4.28433145E8], [1.310952724E9], [6.71765187E8], [4.07735167E8], [6.74774822E8], [3.65408103E8], [6.28932243E8], [5.47492055E8], [9.03823403E8], [1.590012954E9], [3.67650499E8], [7.11467527E8], [1.447690278E9], [2.08176728E8], [5.91929958E8], [8.12255646E8], [6.84293653E8], [2.45540252E8], [7.52849966E8]] \n",
      "-RECORD 1-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " doc_id        | 1                                                                                                                                                                                                                                                                                                                                  \n",
      " document      | 小火鍋                                                                                                                                                                                                                                                                                                                             \n",
      " token         | [小, 火, 鍋]                                                                                                                                                                                                                                                                                                                       \n",
      " token_set     | [小, 火, 鍋]                                                                                                                                                                                                                                                                                                                       \n",
      " token_set_vec | (10,[1,2,6],[1.0,1.0,1.0])                                                                                                                                                                                                                                                                                                         \n",
      " hashes        | [[7.2853667E8], [1.83053912E8], [5.49653093E8], [6.71765187E8], [1.0533018E7], [2.82384582E8], [2.49035002E8], [6.28932243E8], [4.54096397E8], [9.03823403E8], [1.183577531E9], [6.9352669E7], [5.53229523E8], [1.52621142E8], [2.08176728E8], [5.91929958E8], [8.12255646E8], [6.84293653E8], [2.45540252E8], [7.52849966E8]]     \n",
      "-RECORD 2-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " doc_id        | 2                                                                                                                                                                                                                                                                                                                                  \n",
      " document      | Westwood Cafe                                                                                                                                                                                                                                                                                                                      \n",
      " token         | [Westwood, Cafe]                                                                                                                                                                                                                                                                                                                   \n",
      " token_set     | [Westwood, Cafe]                                                                                                                                                                                                                                                                                                                   \n",
      " token_set_vec | (10,[0,5],[1.0,1.0])                                                                                                                                                                                                                                                                                                               \n",
      " hashes        | [[1.339680577E9], [7.53917406E8], [6.47982753E8], [7.6743681E7], [6.19352241E8], [3.80482142E8], [1.495154468E9], [3.6767766E7], [5.94189884E8], [1.617435401E9], [8.57249667E8], [5.16799414E8], [5.92789024E8], [5.7150103E7], [1.178071469E9], [7.33387486E8], [2.7396456E8], [9.98752094E8], [1.267664534E9], [1.64068801E8]]  \n",
      "-RECORD 3-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " doc_id        | 4                                                                                                                                                                                                                                                                                                                                  \n",
      " document      | Vivienne Westwood Cafe                                                                                                                                                                                                                                                                                                             \n",
      " token         | [Vivienne, Westwood, Cafe]                                                                                                                                                                                                                                                                                                         \n",
      " token_set     | [Vivienne, Westwood, Cafe]                                                                                                                                                                                                                                                                                                         \n",
      " token_set_vec | (10,[0,5,7],[1.0,1.0,1.0])                                                                                                                                                                                                                                                                                                         \n",
      " hashes        | [[3.88393353E8], [7.53917406E8], [6.47982753E8], [7.6743681E7], [6.19352241E8], [1.84287022E8], [1.040990279E9], [3.6767766E7], [5.94189884E8], [1.157868151E9], [8.57249667E8], [5.16799414E8], [5.13670022E8], [5.7150103E7], [1.178071469E9], [1.48334257E8], [2.7396456E8], [5.29032807E8], [1.267664534E9], [1.64068801E8]]   \n",
      "-RECORD 4-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " doc_id        | 5                                                                                                                                                                                                                                                                                                                                  \n",
      " document      | 火 Cafe                                                                                                                                                                                                                                                                                                                            \n",
      " token         | [火, Cafe]                                                                                                                                                                                                                                                                                                                         \n",
      " token_set     | [火, Cafe]                                                                                                                                                                                                                                                                                                                         \n",
      " token_set_vec | (10,[0,1],[1.0,1.0])                                                                                                                                                                                                                                                                                                               \n",
      " hashes        | [[1.204180282E9], [9.99296639E8], [6.47982753E8], [7.6743681E7], [1.01655439E9], [7.72872382E8], [3.65408103E8], [3.6767766E7], [5.94189884E8], [1.260629402E9], [1.26368509E9], [5.16799414E8], [7.51027028E8], [5.7150103E7], [1.599654977E9], [1.176983187E9], [2.7396456E8], [1.15401294E9], [1.602198807E9], [1.64068801E8]]  \n",
      "-RECORD 5-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " doc_id        | 6                                                                                                                                                                                                                                                                                                                                  \n",
      " document      | 海底撈                                                                                                                                                                                                                                                                                                                             \n",
      " token         | [海, 底, 撈]                                                                                                                                                                                                                                                                                                                       \n",
      " token_set     | [海, 底, 撈]                                                                                                                                                                                                                                                                                                                       \n",
      " token_set_vec | (10,[3,4,9],[1.0,1.0,1.0])                                                                                                                                                                                                                                                                                                         \n",
      " hashes        | [[2.52893058E8], [5.08538173E8], [5.00488263E8], [4.18754962E8], [2.22150092E8], [4.78579702E8], [5.8682609E8], [3.67350931E8], [5.00794226E8], [1.90211405E8], [1.2448638E8], [2.18501584E8], [4.3455102E8], [8.0015571E8], [4.33189714E8], [6876729.0], [3.89054161E8], [5.931352E7], [5.86248346E8], [4.81118718E8]]            \n",
      "-RECORD 6-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " doc_id        | 7                                                                                                                                                                                                                                                                                                                                  \n",
      " document      | 海抵撈                                                                                                                                                                                                                                                                                                                             \n",
      " token         | [海, 抵, 撈]                                                                                                                                                                                                                                                                                                                       \n",
      " token_set     | [海, 抵, 撈]                                                                                                                                                                                                                                                                                                                       \n",
      " token_set_vec | (10,[3,4,8],[1.0,1.0,1.0])                                                                                                                                                                                                                                                                                                         \n",
      " hashes        | [[2.52893058E8], [1.079401667E9], [5.98817923E8], [4.18754962E8], [8.30969315E8], [8.6189462E7], [7.03199191E8], [3.67350931E8], [4.07398568E8], [1.90211405E8], [2.04593939E8], [2.18501584E8], [4.74110521E8], [8.0015571E8], [1.09891467E8], [6876729.0], [3.89054161E8], [2.14574366E8], [2.51714073E8], [4.81118718E8]]       \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2 stages No way!\n",
    "\n",
    "token_with_bucket_sdf = model.transform(token_vec_sdf)\n",
    "token_with_bucket_sdf.printSchema()\n",
    "token_with_bucket_sdf.show(vertical=True,truncate=False)\n",
    "\n",
    "# the hashes using AND, OR to get similar pairs! \n",
    "# https://stackoverflow.com/questions/48927221/lsh-spark-stucks-forever-at-approxsimilarityjoin-function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T07:59:06.159174Z",
     "start_time": "2022-02-28T07:59:05.138821Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " hashes        | [[3.88393353E8], [7.53917406E8], [6.47982753E8], [7.6743681E7], [6.19352241E8], [1.84287022E8], [1.040990279E9], [3.6767766E7], [5.94189884E8], [1.157868151E9], [8.57249667E8], [5.16799414E8], [5.13670022E8], [5.7150103E7], [1.178071469E9], [1.48334257E8], [2.7396456E8], [5.29032807E8], [1.267664534E9], [1.64068801E8]]   \n",
      " doc_id        | 4                                                                                                                                                                                                                                                                                                                                  \n",
      " document      | Vivienne Westwood Cafe                                                                                                                                                                                                                                                                                                             \n",
      " token         | [Vivienne, Westwood, Cafe]                                                                                                                                                                                                                                                                                                         \n",
      " token_set     | [Vivienne, Westwood, Cafe]                                                                                                                                                                                                                                                                                                         \n",
      " token_set_vec | (10,[0,5,7],[1.0,1.0,1.0])                                                                                                                                                                                                                                                                                                         \n",
      " doc_id        | 4                                                                                                                                                                                                                                                                                                                                  \n",
      " document      | Vivienne Westwood Cafe                                                                                                                                                                                                                                                                                                             \n",
      " token         | [Vivienne, Westwood, Cafe]                                                                                                                                                                                                                                                                                                         \n",
      " token_set     | [Vivienne, Westwood, Cafe]                                                                                                                                                                                                                                                                                                         \n",
      " token_set_vec | (10,[0,5,7],[1.0,1.0,1.0])                                                                                                                                                                                                                                                                                                         \n",
      "-RECORD 1-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " hashes        | [[1.339680577E9], [7.53917406E8], [6.47982753E8], [7.6743681E7], [6.19352241E8], [3.80482142E8], [1.495154468E9], [3.6767766E7], [5.94189884E8], [1.617435401E9], [8.57249667E8], [5.16799414E8], [5.92789024E8], [5.7150103E7], [1.178071469E9], [7.33387486E8], [2.7396456E8], [9.98752094E8], [1.267664534E9], [1.64068801E8]]  \n",
      " doc_id        | 2                                                                                                                                                                                                                                                                                                                                  \n",
      " document      | Westwood Cafe                                                                                                                                                                                                                                                                                                                      \n",
      " token         | [Westwood, Cafe]                                                                                                                                                                                                                                                                                                                   \n",
      " token_set     | [Westwood, Cafe]                                                                                                                                                                                                                                                                                                                   \n",
      " token_set_vec | (10,[0,5],[1.0,1.0])                                                                                                                                                                                                                                                                                                               \n",
      " doc_id        | 2                                                                                                                                                                                                                                                                                                                                  \n",
      " document      | Westwood Cafe                                                                                                                                                                                                                                                                                                                      \n",
      " token         | [Westwood, Cafe]                                                                                                                                                                                                                                                                                                                   \n",
      " token_set     | [Westwood, Cafe]                                                                                                                                                                                                                                                                                                                   \n",
      " token_set_vec | (10,[0,5],[1.0,1.0])                                                                                                                                                                                                                                                                                                               \n",
      "-RECORD 2-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " hashes        | [[7.2853667E8], [4.28433145E8], [1.310952724E9], [6.71765187E8], [4.07735167E8], [6.74774822E8], [3.65408103E8], [6.28932243E8], [5.47492055E8], [9.03823403E8], [1.590012954E9], [3.67650499E8], [7.11467527E8], [1.447690278E9], [2.08176728E8], [5.91929958E8], [8.12255646E8], [6.84293653E8], [2.45540252E8], [7.52849966E8]] \n",
      " doc_id        | 0                                                                                                                                                                                                                                                                                                                                  \n",
      " document      | 火鍋                                                                                                                                                                                                                                                                                                                               \n",
      " token         | [火, 鍋]                                                                                                                                                                                                                                                                                                                           \n",
      " token_set     | [火, 鍋]                                                                                                                                                                                                                                                                                                                           \n",
      " token_set_vec | (10,[1,2],[1.0,1.0])                                                                                                                                                                                                                                                                                                               \n",
      " doc_id        | 0                                                                                                                                                                                                                                                                                                                                  \n",
      " document      | 火鍋                                                                                                                                                                                                                                                                                                                               \n",
      " token         | [火, 鍋]                                                                                                                                                                                                                                                                                                                           \n",
      " token_set     | [火, 鍋]                                                                                                                                                                                                                                                                                                                           \n",
      " token_set_vec | (10,[1,2],[1.0,1.0])                                                                                                                                                                                                                                                                                                               \n",
      "-RECORD 3-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " hashes        | [[2.52893058E8], [5.08538173E8], [5.00488263E8], [4.18754962E8], [2.22150092E8], [4.78579702E8], [5.8682609E8], [3.67350931E8], [5.00794226E8], [1.90211405E8], [1.2448638E8], [2.18501584E8], [4.3455102E8], [8.0015571E8], [4.33189714E8], [6876729.0], [3.89054161E8], [5.931352E7], [5.86248346E8], [4.81118718E8]]            \n",
      " doc_id        | 6                                                                                                                                                                                                                                                                                                                                  \n",
      " document      | 海底撈                                                                                                                                                                                                                                                                                                                             \n",
      " token         | [海, 底, 撈]                                                                                                                                                                                                                                                                                                                       \n",
      " token_set     | [海, 底, 撈]                                                                                                                                                                                                                                                                                                                       \n",
      " token_set_vec | (10,[3,4,9],[1.0,1.0,1.0])                                                                                                                                                                                                                                                                                                         \n",
      " doc_id        | 6                                                                                                                                                                                                                                                                                                                                  \n",
      " document      | 海底撈                                                                                                                                                                                                                                                                                                                             \n",
      " token         | [海, 底, 撈]                                                                                                                                                                                                                                                                                                                       \n",
      " token_set     | [海, 底, 撈]                                                                                                                                                                                                                                                                                                                       \n",
      " token_set_vec | (10,[3,4,9],[1.0,1.0,1.0])                                                                                                                                                                                                                                                                                                         \n",
      "-RECORD 4-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " hashes        | [[1.204180282E9], [9.99296639E8], [6.47982753E8], [7.6743681E7], [1.01655439E9], [7.72872382E8], [3.65408103E8], [3.6767766E7], [5.94189884E8], [1.260629402E9], [1.26368509E9], [5.16799414E8], [7.51027028E8], [5.7150103E7], [1.599654977E9], [1.176983187E9], [2.7396456E8], [1.15401294E9], [1.602198807E9], [1.64068801E8]]  \n",
      " doc_id        | 5                                                                                                                                                                                                                                                                                                                                  \n",
      " document      | 火 Cafe                                                                                                                                                                                                                                                                                                                            \n",
      " token         | [火, Cafe]                                                                                                                                                                                                                                                                                                                         \n",
      " token_set     | [火, Cafe]                                                                                                                                                                                                                                                                                                                         \n",
      " token_set_vec | (10,[0,1],[1.0,1.0])                                                                                                                                                                                                                                                                                                               \n",
      " doc_id        | 5                                                                                                                                                                                                                                                                                                                                  \n",
      " document      | 火 Cafe                                                                                                                                                                                                                                                                                                                            \n",
      " token         | [火, Cafe]                                                                                                                                                                                                                                                                                                                         \n",
      " token_set     | [火, Cafe]                                                                                                                                                                                                                                                                                                                         \n",
      " token_set_vec | (10,[0,1],[1.0,1.0])                                                                                                                                                                                                                                                                                                               \n",
      "-RECORD 5-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " hashes        | [[7.2853667E8], [1.83053912E8], [5.49653093E8], [6.71765187E8], [1.0533018E7], [2.82384582E8], [2.49035002E8], [6.28932243E8], [4.54096397E8], [9.03823403E8], [1.183577531E9], [6.9352669E7], [5.53229523E8], [1.52621142E8], [2.08176728E8], [5.91929958E8], [8.12255646E8], [6.84293653E8], [2.45540252E8], [7.52849966E8]]     \n",
      " doc_id        | 1                                                                                                                                                                                                                                                                                                                                  \n",
      " document      | 小火鍋                                                                                                                                                                                                                                                                                                                             \n",
      " token         | [小, 火, 鍋]                                                                                                                                                                                                                                                                                                                       \n",
      " token_set     | [小, 火, 鍋]                                                                                                                                                                                                                                                                                                                       \n",
      " token_set_vec | (10,[1,2,6],[1.0,1.0,1.0])                                                                                                                                                                                                                                                                                                         \n",
      " doc_id        | 1                                                                                                                                                                                                                                                                                                                                  \n",
      " document      | 小火鍋                                                                                                                                                                                                                                                                                                                             \n",
      " token         | [小, 火, 鍋]                                                                                                                                                                                                                                                                                                                       \n",
      " token_set     | [小, 火, 鍋]                                                                                                                                                                                                                                                                                                                       \n",
      " token_set_vec | (10,[1,2,6],[1.0,1.0,1.0])                                                                                                                                                                                                                                                                                                         \n",
      "-RECORD 6-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " hashes        | [[2.52893058E8], [1.079401667E9], [5.98817923E8], [4.18754962E8], [8.30969315E8], [8.6189462E7], [7.03199191E8], [3.67350931E8], [4.07398568E8], [1.90211405E8], [2.04593939E8], [2.18501584E8], [4.74110521E8], [8.0015571E8], [1.09891467E8], [6876729.0], [3.89054161E8], [2.14574366E8], [2.51714073E8], [4.81118718E8]]       \n",
      " doc_id        | 7                                                                                                                                                                                                                                                                                                                                  \n",
      " document      | 海抵撈                                                                                                                                                                                                                                                                                                                             \n",
      " token         | [海, 抵, 撈]                                                                                                                                                                                                                                                                                                                       \n",
      " token_set     | [海, 抵, 撈]                                                                                                                                                                                                                                                                                                                       \n",
      " token_set_vec | (10,[3,4,8],[1.0,1.0,1.0])                                                                                                                                                                                                                                                                                                         \n",
      " doc_id        | 7                                                                                                                                                                                                                                                                                                                                  \n",
      " document      | 海抵撈                                                                                                                                                                                                                                                                                                                             \n",
      " token         | [海, 抵, 撈]                                                                                                                                                                                                                                                                                                                       \n",
      " token_set     | [海, 抵, 撈]                                                                                                                                                                                                                                                                                                                       \n",
      " token_set_vec | (10,[3,4,8],[1.0,1.0,1.0])                                                                                                                                                                                                                                                                                                         \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Can we join on hashes?\n",
    "\n",
    "doc_pairs_sdf = (\n",
    "    token_with_bucket_sdf\n",
    "    .join(\n",
    "        token_with_bucket_sdf,\n",
    "        on=['hashes']\n",
    "    )\n",
    ")\n",
    "\n",
    "doc_pairs_sdf.show(vertical=True,truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pixlake_spark3",
   "language": "python",
   "name": "pixlake_spark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
