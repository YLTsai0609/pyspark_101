{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-22T13:54:45.222978Z",
     "start_time": "2021-05-22T13:54:45.198800Z"
    }
   },
   "outputs": [],
   "source": [
    "# env : pixlake\n",
    "# we focuing on pyspark dataframe processing\n",
    "# documentation https://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-22T13:54:50.985893Z",
     "start_time": "2021-05-22T13:54:50.966370Z"
    }
   },
   "outputs": [],
   "source": [
    "# make you auto compeletion faster\n",
    "# https://stackoverflow.com/questions/40536560/ipython-and-jupyter-autocomplete-not-working\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-22T13:54:58.694274Z",
     "start_time": "2021-05-22T13:54:58.677400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have pyspark version :  ['spark-2.3', 'spark-3.0', 'spark-3.0.1-bin-hadoop2.7', 'spark-2.3.4-bin-hadoop2.7', 'spark-2.4.7-bin-hadoop2.7', 'spark-2.4']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "def get_workstation_spark_path(where_are_you : str) -> str:\n",
    "    if where_are_you == 'titan':\n",
    "        return '/home/data/ryanchao2012/lib'\n",
    "    elif where_are_you == 'thor':\n",
    "        return '/opt/spark/versions'\n",
    "    else:\n",
    "        raise ValueError(\"wrong work station name\")\n",
    "\n",
    "spark_path = get_workstation_spark_path('thor')\n",
    "\n",
    "print('You have pyspark version : ', os.listdir(spark_path))\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "# spark-2.3, spark-2.4\n",
    "os.environ['SPARK_HOME'] = os.path.join(spark_path,'spark-2.3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-22T13:55:08.505622Z",
     "start_time": "2021-05-22T13:55:08.486394Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/opt/spark/versions/spark-2.3'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ['SPARK_HOME']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-22T13:55:13.207461Z",
     "start_time": "2021-05-22T13:55:12.866310Z"
    }
   },
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession as Session\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark import SparkConf as Conf\n",
    "from pyspark.sql import functions as F, Window as W, types as T\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "C = F.col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-22T13:55:18.222640Z",
     "start_time": "2021-05-22T13:55:18.206408Z"
    }
   },
   "outputs": [],
   "source": [
    "conf = (Conf()\n",
    "    .set('spark.sql.sources.partitionOverwriteMode', 'dynamic')\n",
    "    .set('spark.driver.memory', '4g')\n",
    "    .set('spark.driver.maxResultSize', '1g')\n",
    "   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-22T13:55:28.493751Z",
     "start_time": "2021-05-22T13:55:26.036168Z"
    }
   },
   "outputs": [],
   "source": [
    "spark = (Session\n",
    "     .builder\n",
    "     .appName('pyspark-challenge-nlp')\n",
    "     .master('local[2]')\n",
    "     .config(conf=conf)\n",
    "     .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-22T13:55:30.699359Z",
     "start_time": "2021-05-22T13:55:30.674799Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://thor:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[2]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-challenge-nlp</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7ff3a566a990>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CounterVectorizer | Hashing TF\n",
    "\n",
    "https://towardsdatascience.com/countvectorizer-hashingtf-e66f169e2d4e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-22T14:26:50.726050Z",
     "start_time": "2021-05-22T14:26:50.566171Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "|id |words               |\n",
      "+---+--------------------+\n",
      "|0  |[PYTHON, HIVE, HIVE]|\n",
      "|1  |[JAVA, JAVA, SQL]   |\n",
      "+---+--------------------+\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (0, \"PYTHON HIVE HIVE\".split(\" \")),\n",
    "    (1, \"JAVA JAVA SQL\".split(\" \")),\n",
    "]\n",
    "\n",
    "cols = [\"id\",\"words\"]\n",
    "\n",
    "df = spark.createDataFrame(data, cols)\n",
    "df.show(truncate=False)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-22T14:22:00.209590Z",
     "start_time": "2021-05-22T14:22:00.191286Z"
    }
   },
   "outputs": [],
   "source": [
    "## CounterVectorlizerModel\n",
    "\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "# CountVectorizer??\n",
    "\n",
    "# minTF=1.0,\n",
    "# minDF=1.0,\n",
    "# vocabSize=262144,\n",
    "# binary=False,\n",
    "# inputCol=None,\n",
    "# outputCol=None,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-22T14:25:52.818524Z",
     "start_time": "2021-05-22T14:25:52.377218Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-------------------+\n",
      "|id |words               |features           |\n",
      "+---+--------------------+-------------------+\n",
      "|0  |[PYTHON, HIVE, HIVE]|(4,[0,3],[2.0,1.0])|\n",
      "|1  |[JAVA, JAVA, SQL]   |(4,[1,2],[2.0,1.0])|\n",
      "+---+--------------------+-------------------+\n",
      "\n",
      "you can check the vocabulary :  ['HIVE', 'JAVA', 'PYTHON', 'SQL']\n",
      "the order per row (follow the counts)\n",
      "0 : {'PYTHON', 'HIVE'}\n",
      "1 : {'SQL', 'JAVA'}\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(inputCol=\"words\",outputCol=\"features\")\n",
    "\n",
    "model = cv.fit(df)\n",
    "\n",
    "res = model.transform(df)\n",
    "\n",
    "res.show(truncate=False)\n",
    "\n",
    "print('you can check the vocabulary : ',sorted(model.vocabulary))\n",
    "print('the order per row (follow the counts)')\n",
    "\n",
    "for row in df.rdd.toLocalIterator():\n",
    "    print(row.id, ':', set(row.words), sep=' ')\n",
    "    \n",
    "# Term    Freq Index\n",
    "# HIVE     2     0\n",
    "# JAVA     2     1\n",
    "# PYTHON      1     2\n",
    "# SQL   1     3\n",
    "\n",
    "# (4,             [1, 2], [2.0, 1.0])\n",
    "# (vector_legnth, vector index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-22T14:26:20.409975Z",
     "start_time": "2021-05-22T14:26:20.394827Z"
    }
   },
   "outputs": [],
   "source": [
    "# sorted(model.vocabulary)\n",
    "# model.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-22T14:26:17.694923Z",
     "start_time": "2021-05-22T14:26:17.678918Z"
    }
   },
   "outputs": [],
   "source": [
    "# dir(cv)\n",
    "# dir(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-22T14:27:50.313076Z",
     "start_time": "2021-05-22T14:27:50.295161Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-22T14:29:13.080400Z",
     "start_time": "2021-05-22T14:29:12.695936Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+----------------------------------+\n",
      "|id |words               |features                          |\n",
      "+---+--------------------+----------------------------------+\n",
      "|0  |[PYTHON, HIVE, HIVE]|(262144,[129668,134160],[2.0,1.0])|\n",
      "|1  |[JAVA, JAVA, SQL]   |(262144,[53343,167238],[2.0,1.0]) |\n",
      "+---+--------------------+----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ht = HashingTF(inputCol=\"words\",outputCol=\"features\")\n",
    "\n",
    "ht_res = ht.transform(df)\n",
    "\n",
    "ht_res.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pixlake",
   "language": "python",
   "name": "pixlake"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
