{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T08:31:44.966933Z",
     "start_time": "2022-02-28T08:31:44.945392Z"
    }
   },
   "outputs": [],
   "source": [
    "# env : pixlake\n",
    "# we focuing on pyspark dataframe processing\n",
    "# documentation https://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T08:31:45.314189Z",
     "start_time": "2022-02-28T08:31:45.295590Z"
    }
   },
   "outputs": [],
   "source": [
    "# make you auto compeletion faster\n",
    "# https://stackoverflow.com/questions/40536560/ipython-and-jupyter-autocomplete-not-working\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T08:31:45.494988Z",
     "start_time": "2022-02-28T08:31:45.478902Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have pyspark version :  ['spark-2.3', 'spark-3.1.2-bin-hadoop2.7', 'spark-3.0', 'spark-3.0.1-bin-hadoop2.7', 'spark-2.3.4-bin-hadoop2.7', 'spark-2.4.7-bin-hadoop2.7', 'spark-2.4', 'spark-3.1']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "def get_workstation_spark_path(where_are_you : str) -> str:\n",
    "    if where_are_you == 'titan':\n",
    "        return '/home/data/ryanchao2012/lib'\n",
    "    elif where_are_you == 'thor':\n",
    "        return '/opt/spark/versions'\n",
    "    else:\n",
    "        raise ValueError(\"wrong work station name\")\n",
    "\n",
    "spark_path = get_workstation_spark_path('thor')\n",
    "\n",
    "print('You have pyspark version : ', os.listdir(spark_path))\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "# spark-2.3, spark-2.4\n",
    "# os.environ['SPARK_HOME'] = os.path.join(spark_path,'spark-2.3')\n",
    "\n",
    "# use spakr 3.1\n",
    "\n",
    "os.environ['SPARK_HOME'] = '/opt/spark/versions/spark-3.1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T08:31:45.638512Z",
     "start_time": "2022-02-28T08:31:45.620706Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/opt/spark/versions/spark-3.1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ['SPARK_HOME']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T08:31:46.713668Z",
     "start_time": "2022-02-28T08:31:46.406827Z"
    }
   },
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession as Session\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark import SparkConf as Conf\n",
    "from pyspark.sql import functions as F, Window as W, types as T\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "C = F.col\n",
    "\n",
    "SEED = 42\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T08:31:50.330932Z",
     "start_time": "2022-02-28T08:31:46.715379Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/02/28 16:31:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/02/28 16:31:49 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "conf = (Conf()\n",
    "    .set('spark.sql.sources.partitionOverwriteMode', 'dynamic')\n",
    "    .set('spark.driver.memory', '4g')\n",
    "    .set('spark.driver.maxResultSize', '1g')\n",
    "   )\n",
    "\n",
    "spark = (Session\n",
    "     .builder\n",
    "     .appName('utils')\n",
    "     .master('local[2]')\n",
    "     .config(conf=conf)\n",
    "     .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T08:31:50.349115Z",
     "start_time": "2022-02-28T08:31:50.333887Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://thor:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[2]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>utils</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7feb587438e0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numbers in column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T08:32:25.427977Z",
     "start_time": "2022-02-28T08:32:25.412427Z"
    }
   },
   "outputs": [],
   "source": [
    "# L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T08:32:29.306597Z",
     "start_time": "2022-02-28T08:32:25.940493Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------------------+\n",
      "|  x|  y|          distance|\n",
      "+---+---+------------------+\n",
      "|  1|  3|3.1622776601683795|\n",
      "|  2|  4|  4.47213595499958|\n",
      "|  3|  5| 5.830951894845301|\n",
      "+---+---+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# L2 distance\n",
    "\n",
    "columns = [\"x\", \"y\"]\n",
    "data = [\n",
    "    (1,3),\n",
    "    (2,4),\n",
    "    (3,5)\n",
    "]\n",
    "\n",
    "(\n",
    "    spark.createDataFrame(data, columns)\n",
    "    .withColumn('distance',F.hypot(C(\"x\"), C(\"y\")))\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T08:32:34.962689Z",
     "start_time": "2022-02-28T08:32:29.308094Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 9:======================================================> (98 + 2) / 100]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-----+------------------+\n",
      "|article_id|component|tfidf|     vector_length|\n",
      "+----------+---------+-----+------------------+\n",
      "|         1| 奇異博士|   15|17.029386365926403|\n",
      "|         1|   marvel|    7|17.029386365926403|\n",
      "|         1|     電影|    4|17.029386365926403|\n",
      "|         3|     電影|    3|20.223748416156685|\n",
      "|         3|   蝙蝠俠|   20|20.223748416156685|\n",
      "|         2|   marvel|    8|  16.3707055437449|\n",
      "|         2| 奇異博士|   10|  16.3707055437449|\n",
      "|         2|     電影|    2|  16.3707055437449|\n",
      "|         2| 驚奇隊長|   10|  16.3707055437449|\n",
      "+----------+---------+-----+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 14:================================================>     (180 + 2) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 18:=================================>                    (125 + 2) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------------------\n",
      " component       | marvel              \n",
      " a_article_id    | 1                   \n",
      " a_tfidf         | 7                   \n",
      " a_vector_length | 17.029386365926403  \n",
      " b_article_id    | 2                   \n",
      " b_tfidf         | 8                   \n",
      " b_vector_length | 16.3707055437449    \n",
      " inner_prod      | 226                 \n",
      " cosine_sim      | 0.8106661576414668  \n",
      "-RECORD 1------------------------------\n",
      " component       | 電影                \n",
      " a_article_id    | 1                   \n",
      " a_tfidf         | 4                   \n",
      " a_vector_length | 17.029386365926403  \n",
      " b_article_id    | 2                   \n",
      " b_tfidf         | 2                   \n",
      " b_vector_length | 16.3707055437449    \n",
      " inner_prod      | 226                 \n",
      " cosine_sim      | 0.8106661576414668  \n",
      "-RECORD 2------------------------------\n",
      " component       | 奇異博士            \n",
      " a_article_id    | 1                   \n",
      " a_tfidf         | 15                  \n",
      " a_vector_length | 17.029386365926403  \n",
      " b_article_id    | 2                   \n",
      " b_tfidf         | 10                  \n",
      " b_vector_length | 16.3707055437449    \n",
      " inner_prod      | 226                 \n",
      " cosine_sim      | 0.8106661576414668  \n",
      "-RECORD 3------------------------------\n",
      " component       | 奇異博士            \n",
      " a_article_id    | 2                   \n",
      " a_tfidf         | 10                  \n",
      " a_vector_length | 16.3707055437449    \n",
      " b_article_id    | 1                   \n",
      " b_tfidf         | 15                  \n",
      " b_vector_length | 17.029386365926403  \n",
      " inner_prod      | 220                 \n",
      " cosine_sim      | 0.7891440472616048  \n",
      "-RECORD 4------------------------------\n",
      " component       | 電影                \n",
      " a_article_id    | 2                   \n",
      " a_tfidf         | 2                   \n",
      " a_vector_length | 16.3707055437449    \n",
      " b_article_id    | 1                   \n",
      " b_tfidf         | 4                   \n",
      " b_vector_length | 17.029386365926403  \n",
      " inner_prod      | 220                 \n",
      " cosine_sim      | 0.7891440472616048  \n",
      "-RECORD 5------------------------------\n",
      " component       | marvel              \n",
      " a_article_id    | 2                   \n",
      " a_tfidf         | 8                   \n",
      " a_vector_length | 16.3707055437449    \n",
      " b_article_id    | 1                   \n",
      " b_tfidf         | 7                   \n",
      " b_vector_length | 17.029386365926403  \n",
      " inner_prod      | 220                 \n",
      " cosine_sim      | 0.7891440472616048  \n",
      "-RECORD 6------------------------------\n",
      " component       | 電影                \n",
      " a_article_id    | 2                   \n",
      " a_tfidf         | 2                   \n",
      " a_vector_length | 16.3707055437449    \n",
      " b_article_id    | 3                   \n",
      " b_tfidf         | 3                   \n",
      " b_vector_length | 20.223748416156685  \n",
      " inner_prod      | 220                 \n",
      " cosine_sim      | 0.6644979260349893  \n",
      "-RECORD 7------------------------------\n",
      " component       | 電影                \n",
      " a_article_id    | 1                   \n",
      " a_tfidf         | 4                   \n",
      " a_vector_length | 17.029386365926403  \n",
      " b_article_id    | 3                   \n",
      " b_tfidf         | 3                   \n",
      " b_vector_length | 20.223748416156685  \n",
      " inner_prod      | 226                 \n",
      " cosine_sim      | 0.6562174671054624  \n",
      "-RECORD 8------------------------------\n",
      " component       | 電影                \n",
      " a_article_id    | 3                   \n",
      " a_tfidf         | 3                   \n",
      " a_vector_length | 20.223748416156685  \n",
      " b_article_id    | 2                   \n",
      " b_tfidf         | 2                   \n",
      " b_vector_length | 16.3707055437449    \n",
      " inner_prod      | 18                  \n",
      " cosine_sim      | 0.05436801213013548 \n",
      "-RECORD 9------------------------------\n",
      " component       | 電影                \n",
      " a_article_id    | 3                   \n",
      " a_tfidf         | 3                   \n",
      " a_vector_length | 20.223748416156685  \n",
      " b_article_id    | 1                   \n",
      " b_tfidf         | 4                   \n",
      " b_vector_length | 17.029386365926403  \n",
      " inner_prod      | 18                  \n",
      " cosine_sim      | 0.0522651079995501  \n",
      "\n",
      "root\n",
      " |-- component: string (nullable = true)\n",
      " |-- a_article_id: long (nullable = true)\n",
      " |-- a_tfidf: long (nullable = true)\n",
      " |-- a_vector_length: double (nullable = true)\n",
      " |-- b_article_id: long (nullable = true)\n",
      " |-- b_tfidf: long (nullable = true)\n",
      " |-- b_vector_length: double (nullable = true)\n",
      " |-- inner_prod: long (nullable = true)\n",
      " |-- cosine_sim: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 20:============================================>         (165 + 3) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# cosine similarity\n",
    "\n",
    "columns = ['article_id',\n",
    "           \"keyword\", #a.k.a. vector component\n",
    "           \"tfidf\" #a.k.a vector value\n",
    "          ]\n",
    "data = [\n",
    "    (1,'奇異博士',15),\n",
    "    (1,'marvel',7),\n",
    "    (1,'電影',4),\n",
    "    (2,'marvel',8),\n",
    "    (2,'奇異博士',10),\n",
    "    (2,'電影',2),\n",
    "    (2,'驚奇隊長',10),\n",
    "    (3,'電影',3),\n",
    "    (3,'蝙蝠俠',20),\n",
    "    \n",
    "]\n",
    "\n",
    "article = W.partitionBy('article_id')\n",
    "\n",
    "vec_sdf = (\n",
    "    spark.createDataFrame(data, columns)\n",
    "    .withColumnRenamed('keyword','component')\n",
    "    .withColumn('vector_length',\n",
    "                F.sum(F.pow(C(\"tfidf\"),2))\n",
    "                 .over(article))\n",
    "    .withColumn('vector_length',F.sqrt('vector_length'))\n",
    ")\n",
    "\n",
    "vec_sdf.show()\n",
    "\n",
    "safe_cosine_sim : C = (\n",
    "    C(\"inner_prod\") / (C(\"a_vector_length\") * C(\"b_vector_length\") + 1e-10)\n",
    ")\n",
    "\n",
    "pairs_sdf = (\n",
    "    vec_sdf\n",
    "    .select(\n",
    "        'component',\n",
    "            C('article_id').alias('a_article_id'),\n",
    "            C('tfidf').alias('a_tfidf'),\n",
    "            C('vector_length').alias('a_vector_length')\n",
    "            )\n",
    "    .join(\n",
    "        (\n",
    "            vec_sdf\n",
    "            .select(\n",
    "                'component',\n",
    "                    C('article_id').alias('b_article_id'),\n",
    "                    C('tfidf').alias('b_tfidf'),\n",
    "                    C('vector_length').alias('b_vector_length')\n",
    "                )\n",
    "        ),\n",
    "        on=['component'],\n",
    "    )\n",
    "    .where(C(\"a_article_id\") != C(\"b_article_id\"))\n",
    "    .withColumn( 'inner_prod',\n",
    "                F.sum(C(\"a_tfidf\") * C(\"b_tfidf\"))\n",
    "                 .over(W.partitionBy('a_article_id'))\n",
    "               )\n",
    "#     .withColumn('inner_prod',)\n",
    "    .withColumn('cosine_sim',safe_cosine_sim)\n",
    ")\n",
    "\n",
    "\n",
    "print(pairs_sdf.count())\n",
    "pairs_sdf.orderBy(-C(\"cosine_sim\")).show(vertical=True,truncate=False)\n",
    "pairs_sdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T09:21:53.558230Z",
     "start_time": "2022-02-28T09:21:53.540561Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors, DenseVector, SparseVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T08:32:35.395498Z",
     "start_time": "2022-02-28T08:32:35.093003Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------------------------\n",
      " id       | 0                         \n",
      " features | (6,[0,1,2],[1.0,1.0,1.0]) \n",
      "-RECORD 1-----------------------------\n",
      " id       | 1                         \n",
      " features | (6,[3,4,5],[1.0,1.0,1.0]) \n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1 create sparse vector from dict and list\n",
    "data = [\n",
    "    (0, Vectors.sparse(6, [0, 1, 2],[1.0, 1.0, 1.0])),\n",
    "    (1, Vectors.sparse(6, {3:1.0, 4:1.0, 5:1.0}))\n",
    "]\n",
    "\n",
    "cols = ['id','features']\n",
    "\n",
    "df = spark.createDataFrame(data, cols)\n",
    "df.show(n=5, vertical=True, truncate=False)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T08:32:35.410576Z",
     "start_time": "2022-02-28T08:32:35.396968Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.0,6.0]\n",
      "[1.0,0.0]\n",
      "[0.5,1.0]\n",
      "[3.0,8.0]\n",
      "[3.0,2.0]\n",
      "[1.0,0.0]\n",
      "[-1.0,-2.0]\n"
     ]
    }
   ],
   "source": [
    "# 2 create dense vector from python list\n",
    "\n",
    "v = Vectors.dense([1, 2])\n",
    "u = Vectors.dense([3, 4])\n",
    "\n",
    "print(\n",
    "    v + u,\n",
    "    2 - v,\n",
    "    v / 2,\n",
    "    v * u,\n",
    "    u / v,\n",
    "    u % 2,\n",
    "    -v,\n",
    "    sep='\\n'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T08:32:35.526225Z",
     "start_time": "2022-02-28T08:32:35.411835Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.0,4.0] [1.0,2.0] [1. 2.] [3. 4.] <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# 3 convert vector into np.array and vice versa\n",
    "# https://spark.apache.org/docs/2.3.4/api/python/pyspark.ml.html#module-pyspark.ml.linalg\n",
    "\n",
    "# dense vector MLlib use Numpy array type\n",
    "# sparse vector, scipy.sparse\n",
    "v = Vectors.dense(np.array([1, 2]))\n",
    "u = Vectors.dense(np.array([3, 4]))\n",
    "\n",
    "np_v = v.toArray()\n",
    "np_u = u.toArray()\n",
    "print(\n",
    "    u,\n",
    "    v,\n",
    "    np_v,\n",
    "    np_u,\n",
    "    type(np_u)\n",
    "     )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T09:22:21.888183Z",
     "start_time": "2022-02-28T09:22:21.870884Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0,2.0]\n",
      "(5,[0,2],[1.0,3.0])\n",
      "[1.0,2.0] vector method and attrs\n",
      "\n",
      "<class 'str'> ['__add__', '__class__', '__contains__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getnewargs__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__mod__', '__mul__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__rmod__', '__rmul__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', 'capitalize', 'casefold', 'center', 'count', 'encode', 'endswith', 'expandtabs', 'find', 'format', 'format_map', 'index', 'isalnum', 'isalpha', 'isascii', 'isdecimal', 'isdigit', 'isidentifier', 'islower', 'isnumeric', 'isprintable', 'isspace', 'istitle', 'isupper', 'join', 'ljust', 'lower', 'lstrip', 'maketrans', 'partition', 'replace', 'rfind', 'rindex', 'rjust', 'rpartition', 'rsplit', 'rstrip', 'split', 'splitlines', 'startswith', 'strip', 'swapcase', 'title', 'translate', 'upper', 'zfill']\n",
      "\n",
      "(5,[0,2],[1.0,3.0]) vector method and attrs\n",
      "\n",
      "<class 'str'> ['__add__', '__class__', '__contains__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getnewargs__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__mod__', '__mul__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__rmod__', '__rmul__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', 'capitalize', 'casefold', 'center', 'count', 'encode', 'endswith', 'expandtabs', 'find', 'format', 'format_map', 'index', 'isalnum', 'isalpha', 'isascii', 'isdecimal', 'isdigit', 'isidentifier', 'islower', 'isnumeric', 'isprintable', 'isspace', 'istitle', 'isupper', 'join', 'ljust', 'lower', 'lstrip', 'maketrans', 'partition', 'replace', 'rfind', 'rindex', 'rjust', 'rpartition', 'rsplit', 'rstrip', 'split', 'splitlines', 'startswith', 'strip', 'swapcase', 'title', 'translate', 'upper', 'zfill']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# THIS ONLY CREATE PYTHON STRING OBJECT\n",
    "# POOR PYSPARK API\n",
    "dense_vec = DenseVector(np.array([1, 2]))\n",
    "sparse_vec = SparseVector(5, [0, 2], [1.0, 3.0])\n",
    "\n",
    "print(dense_vec)\n",
    "print(sparse_vec)\n",
    "\n",
    "for mark,obj in zip([dense_vec,sparse_vec],['dense','sparse']):\n",
    "    print(f'{mark} vector method and attrs')\n",
    "    print()\n",
    "    print(type(obj),dir(obj))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T09:54:16.016990Z",
     "start_time": "2022-02-28T09:54:15.989431Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.0\n",
      "0.0\n",
      "5.0\n",
      "7.0 None\n",
      "2\n",
      "0.0\n",
      "26.0\n",
      "[0. 3. 0. 4.]\n"
     ]
    }
   ],
   "source": [
    "# method of sparse vector\n",
    "# https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.ml.linalg.SparseVector.html\n",
    "# https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.ml.linalg.DenseVector.html\n",
    "# dot\n",
    "\n",
    "a = SparseVector(4, [1, 3], [3.0, 4.0])\n",
    "b = SparseVector(4, [2], [1.0])\n",
    "\n",
    "print(a.dot(a))\n",
    "print(a.dot(b))\n",
    "\n",
    "# norm\n",
    "\n",
    "print(a.norm(1), print(a.norm(2)))\n",
    "\n",
    "# numNonzeros\n",
    "\n",
    "print(a.numNonzeros())\n",
    "\n",
    "# squared_distance\n",
    "print(a.squared_distance(a))\n",
    "print(a.squared_distance(b))\n",
    "\n",
    "# toArray()\n",
    "print(a.toArray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T08:32:35.904764Z",
     "start_time": "2022-02-28T08:32:35.528108Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- url: string (nullable = true)\n",
      " |-- embedding: vector (nullable = true)\n",
      "\n",
      "+----------------+--------------------+\n",
      "|             url|           embedding|\n",
      "+----------------+--------------------+\n",
      "|http://url_1.jpg|[0.22535035245629...|\n",
      "|http://url_2.jpg|[0.00638243237756...|\n",
      "|http://url_3.jpg|[0.98896816084838...|\n",
      "+----------------+--------------------+\n",
      "\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# 3 Converting embedding vector between python dict / pandas dataframe / pyspark dataframe\n",
    "# pandas dataframe is a fake issue, np.array will not fit in dataframe well\n",
    "embedding_size = 256\n",
    "\n",
    "image_info = {\n",
    "    'http://url_1.jpg' : np.random.random(size=embedding_size),\n",
    "    'http://url_2.jpg' : np.random.random(size=embedding_size),\n",
    "    'http://url_3.jpg' : np.random.random(size=embedding_size)\n",
    "}\n",
    "\n",
    "############# python dict to pyspark dataframe #################\n",
    "\n",
    "data = [(url, Vectors.dense(vec)) for url, vec in image_info.items()]\n",
    "\n",
    "col = ['url','embedding']\n",
    "\n",
    "sdf = spark.createDataFrame(data, col)\n",
    "\n",
    "sdf.printSchema()\n",
    "sdf.show()\n",
    "\n",
    "######### pyspark data frame to python dict #####################\n",
    "\n",
    "# of course we need to collect the vectors ro driver\n",
    "\n",
    "convert_image_info = {\n",
    "    row.url : row.embedding.toArray()\n",
    "    for row in \n",
    "    sdf.collect()\n",
    "}\n",
    "\n",
    "for url in convert_image_info.keys():\n",
    "    print(np.isclose(convert_image_info[url], image_info[url]).all())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T08:32:35.920077Z",
     "start_time": "2022-02-28T08:32:35.907025Z"
    }
   },
   "outputs": [],
   "source": [
    "# 4 convert sparse vector from scipy / np into pyspark dataframe\n",
    "# sparse vector\n",
    "# https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html#scipy.sparse.csr_matrix\n",
    "# from scipy.sparse import csr_matrix, coo_matrix\n",
    "\n",
    "# row = np.array([0, 0, 1, 2, 2, 2])\n",
    "# col = np.array([0, 2, 2, 0, 1, 2])\n",
    "# data = np.array([1, 2, 3, 4, 5, 6])\n",
    "\n",
    "# csr = csr_matrix((data, (row, col)), shape=(3, 3))\n",
    "\n",
    "# coo = coo_matrix(matrix)\n",
    "\n",
    "# for i,j,v in zip(coo.row, coo.col, coo.data):\n",
    "#     print( \"(%d, %d), %s\" % (i,j,v))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T08:32:37.831759Z",
     "start_time": "2022-02-28T08:32:36.761964Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- url: string (nullable = true)\n",
      " |-- embedding: vector (nullable = true)\n",
      "\n",
      "+----------------+--------------------------------------------------------------+\n",
      "|url             |embedding                                                     |\n",
      "+----------------+--------------------------------------------------------------+\n",
      "|http://url_1.jpg|[0.46216659110719005,0.45938098214983547,0.038087781028762424]|\n",
      "|http://url_2.jpg|[0.6118475248691172,0.10542420267186592,0.4652360199396238]   |\n",
      "|http://url_3.jpg|[0.855009243058626,0.5857817217020683,0.07805699066680649]    |\n",
      "+----------------+--------------------------------------------------------------+\n",
      "\n",
      "+----------------+--------------------------------------------------------------+-------------------+\n",
      "|url             |embedding                                                     |by_udf             |\n",
      "+----------------+--------------------------------------------------------------+-------------------+\n",
      "|http://url_1.jpg|[0.46216659110719005,0.45938098214983547,0.038087781028762424]|0.46216659110719005|\n",
      "|http://url_2.jpg|[0.6118475248691172,0.10542420267186592,0.4652360199396238]   |0.6118475248691172 |\n",
      "|http://url_3.jpg|[0.855009243058626,0.5857817217020683,0.07805699066680649]    |0.855009243058626  |\n",
      "+----------------+--------------------------------------------------------------+-------------------+\n",
      "\n",
      "+----------------+--------------------------------------------------------------+---------------------+\n",
      "|url             |embedding                                                     |by_slicer            |\n",
      "+----------------+--------------------------------------------------------------+---------------------+\n",
      "|http://url_1.jpg|[0.46216659110719005,0.45938098214983547,0.038087781028762424]|[0.46216659110719005]|\n",
      "|http://url_2.jpg|[0.6118475248691172,0.10542420267186592,0.4652360199396238]   |[0.6118475248691172] |\n",
      "|http://url_3.jpg|[0.855009243058626,0.5857817217020683,0.07805699066680649]    |[0.855009243058626]  |\n",
      "+----------------+--------------------------------------------------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6 access the element in a column of vector\n",
    "\n",
    "embedding_size = 3\n",
    "\n",
    "image_info = {\n",
    "    'http://url_1.jpg' : np.random.random(size=embedding_size),\n",
    "    'http://url_2.jpg' : np.random.random(size=embedding_size),\n",
    "    'http://url_3.jpg' : np.random.random(size=embedding_size)\n",
    "}\n",
    "\n",
    "############# python dict to pyspark dataframe #################\n",
    "\n",
    "data = [(url, Vectors.dense(vec)) for url, vec in image_info.items()]\n",
    "\n",
    "col = ['url','embedding']\n",
    "\n",
    "sdf = spark.createDataFrame(data, col)\n",
    "\n",
    "sdf.printSchema()\n",
    "sdf.show(truncate=False)\n",
    "\n",
    "##### sol 1 pyspark vector api suck ###########\n",
    "\n",
    "def ith_(vector, i):\n",
    "    try:\n",
    "        return float(vector[i])\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "ith = F.udf(ith_, T.DoubleType())\n",
    "\n",
    "##### sol 2 VectorSlicer ##########\n",
    "\n",
    "from pyspark.ml.feature import VectorSlicer\n",
    "\n",
    "slicer = VectorSlicer(inputCol='embedding', outputCol='by_slicer', indices=[0])\n",
    "\n",
    "o_sdf = slicer.transform(sdf)\n",
    "\n",
    "(\n",
    "    sdf\n",
    "    .withColumn('by_udf',ith(\"embedding\", F.lit(0)))\n",
    ").show(truncate=False)\n",
    "\n",
    "o_sdf.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T08:32:38.881868Z",
     "start_time": "2022-02-28T08:32:37.833868Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|features           |\n",
      "+-------------------+\n",
      "|(5,[0,2],[1.0,3.0])|\n",
      "|(5,[1],[4.0])      |\n",
      "|(5,[2],[1.0])      |\n",
      "+-------------------+\n",
      "\n",
      "test the typing :  [[1. 0. 3. 0. 0.]]\n",
      "\n",
      "<class 'pyspark.rdd.PipelinedRDD'>\n",
      "<class 'pyspark.rdd.PipelinedRDD'>\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "[[1. 0. 3. 0. 0.]\n",
      " [0. 4. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# 7 \n",
    "# Spark sparse vector 2 scipy csr matrix\n",
    "# https://stackoverflow.com/questions/40557577/pyspark-sparse-vectors-to-scipy-sparse-matrix\n",
    "\n",
    "from pyspark.ml.linalg import SparseVector\n",
    "from pyspark.rdd import PipelinedRDD\n",
    "from operator import attrgetter\n",
    "\n",
    "df = spark.sparkContext.parallelize([\n",
    "    (SparseVector(5, [0, 2], [1.0, 3.0]), ),\n",
    "    (SparseVector(5, [1], [4.0]), ),\n",
    "    (SparseVector(5, [2], [1.0]), )\n",
    "]).toDF([\"features\"])\n",
    "\n",
    "df.show(truncate=False)\n",
    "\n",
    "from scipy.sparse import vstack, csr_matrix\n",
    "import numpy as np\n",
    "\n",
    "def as_matrix(vec : SparseVector) -> csr_matrix:\n",
    "    data, indices = vec.values, vec.indices\n",
    "    shape = 1, vec.size\n",
    "    return csr_matrix((data, indices, np.array([0, vec.values.size])), shape)\n",
    "\n",
    "\n",
    "test_sparse_matrix = as_matrix(SparseVector(5, [0, 2], [1.0, 3.0]))\n",
    "print(\n",
    "    'test the typing : ',\n",
    "    test_sparse_matrix.todense()\n",
    ")\n",
    "print()\n",
    "\n",
    "mat : csr_matrix = (\n",
    "    df\n",
    "    .rdd\n",
    "    .map(attrgetter(\"features\"))\n",
    "    .map(as_matrix)\n",
    "    .reduce(lambda x, y: vstack([x, y]))\n",
    ")\n",
    "\n",
    "features : PipelinedRDD = df.rdd.map(attrgetter(\"features\"))\n",
    "mats : PipelinedRDD = features.map(as_matrix)\n",
    "    \n",
    "# mat : csr_matrix = mats.reduce(lambda x, y: vstack([x, y]))\n",
    "\n",
    "print(\n",
    "    type(features),\n",
    "    type(mats),\n",
    "    type(mat),\n",
    "    mat.todense(),\n",
    "    sep='\\n'\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T08:32:38.894638Z",
     "start_time": "2022-02-28T08:32:38.884307Z"
    }
   },
   "outputs": [],
   "source": [
    "# 8 create a sturct to sparse vector --> No built in api support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T09:12:24.962955Z",
     "start_time": "2022-02-28T09:12:24.780270Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+--------+-------------+\n",
      "|a_url_id|a_feature_vec      |b_url_id|b_feature_vec|\n",
      "+--------+-------------------+--------+-------------+\n",
      "|0       |(5,[0,2],[1.0,3.0])|1       |(5,[1],[4.0])|\n",
      "|0       |(5,[0,2],[1.0,3.0])|2       |(5,[2],[1.0])|\n",
      "|1       |(5,[1],[4.0])      |2       |(5,[2],[1.0])|\n",
      "+--------+-------------------+--------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 9 compute cosine of 2 sparse Vector\n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "raw_data = [\n",
    "    (0,SparseVector(5, [0, 2], [1.0, 3.0])),\n",
    "    (1,SparseVector(5, [1], [4.0])),\n",
    "    (2,SparseVector(5, [2], [1.0]))\n",
    "]\n",
    "\n",
    "# chain for flattern\n",
    "# combinsations for build Choose 2 from N\n",
    "\n",
    "data = []\n",
    "for pair in combinations(raw_data,2):\n",
    "    data.append(\n",
    "        tuple((*pair[0],*pair[1]))\n",
    "    )\n",
    "\n",
    "df = (\n",
    "    spark.createDataFrame(\n",
    "        data,\n",
    "        ['a_url_id','a_feature_vec','b_url_id','b_feature_vec']\n",
    "    )\n",
    "    )\n",
    "\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T09:12:52.490072Z",
     "start_time": "2022-02-28T09:12:52.470073Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot recognize a pipeline stage of type <class 'pyspark.mllib.feature.Normalizer'>.\n"
     ]
    }
   ],
   "source": [
    "# approach A\n",
    "# build-in normalizer\n",
    "# SUCK\n",
    "\n",
    "try:   \n",
    "    normalizers = []\n",
    "    for col in ['a_feature_vec','b_feature_vec']:\n",
    "        l2_norm = Normalizer()\n",
    "        # due to poor spark api\n",
    "        l2_norm.__setattr__('inputCol',col)\n",
    "        l2_norm.__setattr__('outputCol',f'norm_{col}')\n",
    "        normalizers.append(l2_norm)\n",
    "\n",
    "    pipeline = Pipeline(stages=normalizers)\n",
    "\n",
    "    model = pipeline.fit(df)\n",
    "\n",
    "    model.transform(df)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T11:07:04.118100Z",
     "start_time": "2022-02-28T11:07:03.679549Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+--------+-------------+------------------+\n",
      "|a_url_id|      a_feature_vec|b_url_id|b_feature_vec|            cosine|\n",
      "+--------+-------------------+--------+-------------+------------------+\n",
      "|       0|(5,[0,2],[1.0,3.0])|       1|(5,[1],[4.0])|               0.0|\n",
      "|       0|(5,[0,2],[1.0,3.0])|       2|(5,[2],[1.0])|0.9486832980205138|\n",
      "|       1|      (5,[1],[4.0])|       2|(5,[2],[1.0])|               0.0|\n",
      "+--------+-------------------+--------+-------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# another approach\n",
    "from typing import List,Tuple\n",
    "def safe_cosine(\n",
    "    a : SparseVector,\n",
    "    b : SparseVector,\n",
    "    base=1e-10,\n",
    ") -> float:\n",
    "#     print(a.dot(b))\n",
    "#     print(a.norm(2))\n",
    "    return float(\n",
    "        a.dot(b) / (a.norm(2) * b.norm(2) + base)\n",
    "#         a.dot(b)\n",
    "    )\n",
    "(\n",
    "    df\n",
    "    .withColumn('cosine',\n",
    "                F.udf(safe_cosine,T.DoubleType())\n",
    "                ('a_feature_vec','b_feature_vec')\n",
    "    )\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-26T13:39:35.743337Z",
     "start_time": "2022-02-26T13:39:34.285054Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+\n",
      "|link|category|\n",
      "+----+--------+\n",
      "|  a1|    美味食記|\n",
      "|  a2|    美味食記|\n",
      "|  a3|    美味食記|\n",
      "|  a4|    美味食記|\n",
      "|  a5|    國內旅遊|\n",
      "|  a7|    不設分類|\n",
      "|  a8|    不設分類|\n",
      "+----+--------+\n",
      "\n",
      "+----+--------+\n",
      "|link|category|\n",
      "+----+--------+\n",
      "|  a6|    國內旅遊|\n",
      "|  a9|    不設分類|\n",
      "+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  Stratified sampling in Spark\n",
    "\n",
    "data = [\n",
    "    ('a1','美味食記'),\n",
    "    ('a2','美味食記'),\n",
    "    ('a3','美味食記'),\n",
    "    ('a4','美味食記'),\n",
    "    ('a5','國內旅遊'),\n",
    "    ('a6','國內旅遊'),\n",
    "    ('a7','不設分類'),\n",
    "    ('a8','不設分類'),\n",
    "    ('a9','不設分類'),\n",
    "]\n",
    "\n",
    "sdf = (\n",
    "    spark.createDataFrame(data,['link','category'])\n",
    ")\n",
    "\n",
    "\n",
    "# Taking 60% of each category into training set\n",
    "# It's a approx solution\n",
    "\n",
    "trn = sdf.sampleBy(\"category\",fractions={\n",
    "    '美味食記':0.6,\n",
    "    '國內旅遊':0.6,\n",
    "    '不設分類':0.6,\n",
    "},seed=2)\n",
    "\n",
    "tst = sdf.subtract(trn)\n",
    "\n",
    "trn.show()\n",
    "tst.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pixlake_spark3",
   "language": "python",
   "name": "pixlake_spark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "239.774px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
