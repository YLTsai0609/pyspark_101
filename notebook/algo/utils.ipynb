{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-27T13:47:06.636013Z",
     "start_time": "2021-06-27T13:47:06.616580Z"
    }
   },
   "outputs": [],
   "source": [
    "# env : pixlake\n",
    "# we focuing on pyspark dataframe processing\n",
    "# documentation https://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-27T13:47:06.798420Z",
     "start_time": "2021-06-27T13:47:06.638130Z"
    }
   },
   "outputs": [],
   "source": [
    "# make you auto compeletion faster\n",
    "# https://stackoverflow.com/questions/40536560/ipython-and-jupyter-autocomplete-not-working\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-27T13:47:06.893673Z",
     "start_time": "2021-06-27T13:47:06.800672Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have pyspark version :  ['spark-2.3', 'spark-3.0', 'spark-3.0.1-bin-hadoop2.7', 'spark-2.3.4-bin-hadoop2.7', 'spark-2.4.7-bin-hadoop2.7', 'spark-2.4']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "def get_workstation_spark_path(where_are_you : str) -> str:\n",
    "    if where_are_you == 'titan':\n",
    "        return '/home/data/ryanchao2012/lib'\n",
    "    elif where_are_you == 'thor':\n",
    "        return '/opt/spark/versions'\n",
    "    else:\n",
    "        raise ValueError(\"wrong work station name\")\n",
    "\n",
    "spark_path = get_workstation_spark_path('thor')\n",
    "\n",
    "print('You have pyspark version : ', os.listdir(spark_path))\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "# spark-2.3, spark-2.4\n",
    "os.environ['SPARK_HOME'] = os.path.join(spark_path,'spark-2.3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-27T13:47:07.001903Z",
     "start_time": "2021-06-27T13:47:06.896200Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/opt/spark/versions/spark-2.3'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ['SPARK_HOME']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-27T13:57:16.803556Z",
     "start_time": "2021-06-27T13:57:16.786993Z"
    }
   },
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession as Session\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark import SparkConf as Conf\n",
    "from pyspark.sql import functions as F, Window as W, types as T\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "C = F.col\n",
    "\n",
    "SEED = 42\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-27T13:47:07.431684Z",
     "start_time": "2021-06-27T13:47:07.423788Z"
    }
   },
   "outputs": [],
   "source": [
    "conf = (Conf()\n",
    "    .set('spark.sql.sources.partitionOverwriteMode', 'dynamic')\n",
    "    .set('spark.driver.memory', '4g')\n",
    "    .set('spark.driver.maxResultSize', '1g')\n",
    "   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-27T13:47:09.971701Z",
     "start_time": "2021-06-27T13:47:07.432912Z"
    }
   },
   "outputs": [],
   "source": [
    "spark = (Session\n",
    "     .builder\n",
    "     .appName('ml-utils')\n",
    "     .master('local[2]')\n",
    "     .config(conf=conf)\n",
    "     .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-27T13:47:09.990785Z",
     "start_time": "2021-06-27T13:47:09.975647Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://thor:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[2]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>ml-utils</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fdf49962290>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-27T13:47:10.285818Z",
     "start_time": "2021-06-27T13:47:09.992400Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-27T13:47:13.015200Z",
     "start_time": "2021-06-27T13:47:10.287530Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------------------------\n",
      " id       | 0                         \n",
      " features | (6,[0,1,2],[1.0,1.0,1.0]) \n",
      "-RECORD 1-----------------------------\n",
      " id       | 1                         \n",
      " features | (6,[3,4,5],[1.0,1.0,1.0]) \n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1 create sparse vector from dict and list\n",
    "data = [\n",
    "    (0, Vectors.sparse(6, [0, 1, 2],[1.0, 1.0, 1.0])),\n",
    "    (1, Vectors.sparse(6, {3:1.0, 4:1.0, 5:1.0}))\n",
    "]\n",
    "\n",
    "cols = ['id','features']\n",
    "\n",
    "df = spark.createDataFrame(data, cols)\n",
    "df.show(n=5, vertical=True, truncate=False)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-27T13:48:40.053420Z",
     "start_time": "2021-06-27T13:48:40.033745Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.0,6.0]\n",
      "[1.0,0.0]\n",
      "[0.5,1.0]\n",
      "[3.0,8.0]\n",
      "[3.0,2.0]\n",
      "[1.0,0.0]\n",
      "[-1.0,-2.0]\n"
     ]
    }
   ],
   "source": [
    "# 2 create dense vector from python list\n",
    "\n",
    "v = Vectors.dense([1, 2])\n",
    "u = Vectors.dense([3, 4])\n",
    "\n",
    "print(\n",
    "    v + u,\n",
    "    2 - v,\n",
    "    v / 2,\n",
    "    v * u,\n",
    "    u / v,\n",
    "    u % 2,\n",
    "    -v,\n",
    "    sep='\\n'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-27T14:21:31.818554Z",
     "start_time": "2021-06-27T14:21:31.796833Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.0,4.0] [1.0,2.0] [1. 2.] [3. 4.] <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# 3 convert vector into np.array and vice versa\n",
    "# https://spark.apache.org/docs/2.3.4/api/python/pyspark.ml.html#module-pyspark.ml.linalg\n",
    "\n",
    "# dense vector MLlib use Numpy array type\n",
    "# sparse vector, scipy.sparse\n",
    "v = Vectors.dense(np.array([1, 2]))\n",
    "u = Vectors.dense(np.array([3, 4]))\n",
    "\n",
    "np_v = v.toArray()\n",
    "np_u = u.toArray()\n",
    "print(\n",
    "    u,\n",
    "    v,\n",
    "    np_v,\n",
    "    np_u,\n",
    "    type(np_u)\n",
    "     )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-27T14:21:32.614984Z",
     "start_time": "2021-06-27T14:21:32.336896Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- url: string (nullable = true)\n",
      " |-- embedding: vector (nullable = true)\n",
      "\n",
      "+----------------+--------------------+\n",
      "|             url|           embedding|\n",
      "+----------------+--------------------+\n",
      "|http://url_1.jpg|[0.38572632715821...|\n",
      "|http://url_2.jpg|[0.24131721949581...|\n",
      "|http://url_3.jpg|[0.29871468668232...|\n",
      "+----------------+--------------------+\n",
      "\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# 3 Converting embedding vector between python dict / pandas dataframe / pyspark dataframe\n",
    "# pandas dataframe is a fake issue, np.array will not fit in dataframe well\n",
    "embedding_size = 256\n",
    "\n",
    "image_info = {\n",
    "    'http://url_1.jpg' : np.random.random(size=embedding_size),\n",
    "    'http://url_2.jpg' : np.random.random(size=embedding_size),\n",
    "    'http://url_3.jpg' : np.random.random(size=embedding_size)\n",
    "}\n",
    "\n",
    "############# python dict to pyspark dataframe #################\n",
    "\n",
    "data = [(url, Vectors.dense(vec)) for url, vec in image_info.items()]\n",
    "\n",
    "col = ['url','embedding']\n",
    "\n",
    "sdf = spark.createDataFrame(data, col)\n",
    "\n",
    "sdf.printSchema()\n",
    "sdf.show()\n",
    "\n",
    "######### pyspark data frame to python dict #####################\n",
    "\n",
    "# of course we need to collect the vectors ro driver\n",
    "\n",
    "convert_image_info = {\n",
    "    row.url : row.embedding.toArray()\n",
    "    for row in \n",
    "    sdf.collect()\n",
    "}\n",
    "\n",
    "for url in convert_image_info.keys():\n",
    "    print(np.isclose(convert_image_info[url], image_info[url]).all())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 convert sparse vector from scipy / np into pyspark dataframe\n",
    "# sparse vector\n",
    "# https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html#scipy.sparse.csr_matrix\n",
    "# from scipy.sparse import csr_matrix, coo_matrix\n",
    "\n",
    "# row = np.array([0, 0, 1, 2, 2, 2])\n",
    "# col = np.array([0, 2, 2, 0, 1, 2])\n",
    "# data = np.array([1, 2, 3, 4, 5, 6])\n",
    "\n",
    "# csr = csr_matrix((data, (row, col)), shape=(3, 3))\n",
    "\n",
    "# coo = coo_matrix(matrix)\n",
    "\n",
    "# for i,j,v in zip(coo.row, coo.col, coo.data):\n",
    "#     print( \"(%d, %d), %s\" % (i,j,v))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-27T13:47:13.028684Z",
     "start_time": "2021-06-27T13:47:13.017268Z"
    }
   },
   "outputs": [],
   "source": [
    "# 5 Convert sparse vector to dense vector?\n",
    "# 30 mins\n",
    "# frequencyDenseVectors = frequencyVectors.map(lambda vector: DenseVector(vector.toArray()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-27T14:35:22.232836Z",
     "start_time": "2021-06-27T14:35:21.434482Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- url: string (nullable = true)\n",
      " |-- embedding: vector (nullable = true)\n",
      "\n",
      "+----------------+-----------------------------------------------------------+\n",
      "|url             |embedding                                                  |\n",
      "+----------------+-----------------------------------------------------------+\n",
      "|http://url_1.jpg|[0.639014501169914,0.1791115748733353,0.7634177225718759]  |\n",
      "|http://url_2.jpg|[0.9484816348336237,0.47793686383603895,0.5564647461191232]|\n",
      "|http://url_3.jpg|[0.41118254425923606,0.6130700148483098,0.4367534799715509]|\n",
      "+----------------+-----------------------------------------------------------+\n",
      "\n",
      "+----------------+-----------------------------------------------------------+-------------------+\n",
      "|url             |embedding                                                  |by_udf             |\n",
      "+----------------+-----------------------------------------------------------+-------------------+\n",
      "|http://url_1.jpg|[0.639014501169914,0.1791115748733353,0.7634177225718759]  |0.639014501169914  |\n",
      "|http://url_2.jpg|[0.9484816348336237,0.47793686383603895,0.5564647461191232]|0.9484816348336237 |\n",
      "|http://url_3.jpg|[0.41118254425923606,0.6130700148483098,0.4367534799715509]|0.41118254425923606|\n",
      "+----------------+-----------------------------------------------------------+-------------------+\n",
      "\n",
      "+----------------+-----------------------------------------------------------+---------------------+\n",
      "|url             |embedding                                                  |by_slicer            |\n",
      "+----------------+-----------------------------------------------------------+---------------------+\n",
      "|http://url_1.jpg|[0.639014501169914,0.1791115748733353,0.7634177225718759]  |[0.639014501169914]  |\n",
      "|http://url_2.jpg|[0.9484816348336237,0.47793686383603895,0.5564647461191232]|[0.9484816348336237] |\n",
      "|http://url_3.jpg|[0.41118254425923606,0.6130700148483098,0.4367534799715509]|[0.41118254425923606]|\n",
      "+----------------+-----------------------------------------------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6 access the element in a column of vector\n",
    "\n",
    "embedding_size = 3\n",
    "\n",
    "image_info = {\n",
    "    'http://url_1.jpg' : np.random.random(size=embedding_size),\n",
    "    'http://url_2.jpg' : np.random.random(size=embedding_size),\n",
    "    'http://url_3.jpg' : np.random.random(size=embedding_size)\n",
    "}\n",
    "\n",
    "############# python dict to pyspark dataframe #################\n",
    "\n",
    "data = [(url, Vectors.dense(vec)) for url, vec in image_info.items()]\n",
    "\n",
    "col = ['url','embedding']\n",
    "\n",
    "sdf = spark.createDataFrame(data, col)\n",
    "\n",
    "sdf.printSchema()\n",
    "sdf.show(truncate=False)\n",
    "\n",
    "##### sol 1 pyspark vector api suck ###########\n",
    "\n",
    "def ith_(vector, i):\n",
    "    try:\n",
    "        return float(vector[i])\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "ith = F.udf(ith_, T.DoubleType())\n",
    "\n",
    "##### sol 2 VectorSlicer ##########\n",
    "\n",
    "from pyspark.ml.feature import VectorSlicer\n",
    "\n",
    "slicer = VectorSlicer(inputCol='embedding', outputCol='by_slicer', indices=[0])\n",
    "\n",
    "o_sdf = slicer.transform(sdf)\n",
    "\n",
    "(\n",
    "    sdf\n",
    "    .withColumn('by_udf',ith(\"embedding\", F.lit(0)))\n",
    ").show(truncate=False)\n",
    "\n",
    "o_sdf.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pixlake",
   "language": "python",
   "name": "pixlake"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
