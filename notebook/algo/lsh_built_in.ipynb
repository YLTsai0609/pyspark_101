{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-09T14:19:12.032938Z",
     "start_time": "2021-12-09T14:19:12.011066Z"
    }
   },
   "outputs": [],
   "source": [
    "# env : pixlake\n",
    "# we focuing on pyspark dataframe processing\n",
    "# documentation https://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-09T14:19:12.185496Z",
     "start_time": "2021-12-09T14:19:12.034557Z"
    }
   },
   "outputs": [],
   "source": [
    "# make you auto compeletion faster\n",
    "# https://stackoverflow.com/questions/40536560/ipython-and-jupyter-autocomplete-not-working\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-09T14:19:12.286385Z",
     "start_time": "2021-12-09T14:19:12.188013Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have pyspark version :  ['spark-2.3', 'spark-3.1.2-bin-hadoop2.7', 'spark-3.0', 'spark-3.0.1-bin-hadoop2.7', 'spark-2.3.4-bin-hadoop2.7', 'spark-2.4.7-bin-hadoop2.7', 'spark-2.4', 'spark-3.1']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "def get_workstation_spark_path(where_are_you : str) -> str:\n",
    "    if where_are_you == 'titan':\n",
    "        return '/home/data/ryanchao2012/lib'\n",
    "    elif where_are_you == 'thor':\n",
    "        return '/opt/spark/versions'\n",
    "    else:\n",
    "        raise ValueError(\"wrong work station name\")\n",
    "\n",
    "spark_path = get_workstation_spark_path('thor')\n",
    "\n",
    "print('You have pyspark version : ', os.listdir(spark_path))\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "# spark-2.3, spark-2.4\n",
    "os.environ['SPARK_HOME'] = os.path.join(spark_path,'spark-2.3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-09T14:19:12.402162Z",
     "start_time": "2021-12-09T14:19:12.289082Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/opt/spark/versions/spark-2.3'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ['SPARK_HOME']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-09T14:19:12.831417Z",
     "start_time": "2021-12-09T14:19:12.404590Z"
    }
   },
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession as Session\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark import SparkConf as Conf\n",
    "from pyspark.sql import functions as F, Window as W, types as T\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "C = F.col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-09T14:19:12.840838Z",
     "start_time": "2021-12-09T14:19:12.832764Z"
    }
   },
   "outputs": [],
   "source": [
    "conf = (Conf()\n",
    "    .set('spark.sql.sources.partitionOverwriteMode', 'dynamic')\n",
    "    .set('spark.driver.memory', '4g')\n",
    "    .set('spark.driver.maxResultSize', '1g')\n",
    "   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-09T14:19:15.426263Z",
     "start_time": "2021-12-09T14:19:12.841985Z"
    }
   },
   "outputs": [],
   "source": [
    "spark = (Session\n",
    "     .builder\n",
    "     .appName('pyspark-challenge')\n",
    "     .master('local[2]')\n",
    "     .config(conf=conf)\n",
    "     .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-09T14:19:15.444876Z",
     "start_time": "2021-12-09T14:19:15.429105Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://thor:4045\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[2]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-challenge</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fd86687b390>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Introduction](https://spark.apache.org/docs/2.2.3/ml-features.html#minhash-for-jaccard-distance)\n",
    "\n",
    "[api doc](https://spark.apache.org/docs/2.3.4/api/python/pyspark.ml.html#pyspark.ml.feature.MinHashLSH)\n",
    "\n",
    "\n",
    "LSH used in clustering, approximate mearest neighbor search and outlier detection with large datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Min Hash for Jaccard Distance\n",
    " \n",
    " * Input data - shingling boolean vector\n",
    " * fit a model - build sig matrix\n",
    "     * you can find similarity pairs by send a query or apply a join\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-09T14:19:15.643809Z",
     "start_time": "2021-12-09T14:19:15.447116Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import MinHashLSH, BucketedRandomProjectionLSH\n",
    "from pyspark.ml.linalg import Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-09T14:19:15.758089Z",
     "start_time": "2021-12-09T14:19:15.645641Z"
    }
   },
   "outputs": [],
   "source": [
    "# Vectors.sparse??\n",
    "\n",
    "# Vector.sparse(sparse, [0,1,2], [1.0, 1.0, 1.0]) \n",
    "# means a 6 dimension vector [1.0, 1.0, 1.0, 0, 0, 0]\n",
    "# you can give a two list or a dict to create Vector.sparse\n",
    "\n",
    "MinHashLSH??\n",
    "\n",
    "# inputCol=None, outputCol=None, seed=None, numHashTables=1\n",
    "# where's the b and r ?\n",
    "#Key means the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-09T14:19:22.297003Z",
     "start_time": "2021-12-09T14:19:15.759569Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximately joining dfA and dfB on distance smaller than 0.6:\n",
      "-RECORD 0--------------\n",
      " idA             | 1   \n",
      " idB             | 5   \n",
      " JaccardDistance | 0.5 \n",
      "-RECORD 1--------------\n",
      " idA             | 0   \n",
      " idB             | 5   \n",
      " JaccardDistance | 0.5 \n",
      "-RECORD 2--------------\n",
      " idA             | 1   \n",
      " idB             | 4   \n",
      " JaccardDistance | 0.5 \n",
      "-RECORD 3--------------\n",
      " idA             | 2   \n",
      " idB             | 5   \n",
      " JaccardDistance | 0.5 \n",
      "\n",
      "Approximately searching dfA for 5 nearest neighbors of the key:\n",
      "+---+--------------------+--------------------+-------+\n",
      "| id|            features|              hashes|distCol|\n",
      "+---+--------------------+--------------------+-------+\n",
      "|  0|(6,[0,1,2],[1.0,1...|[[-1.052712271E9]...|   0.75|\n",
      "|  1|(6,[2,3,4],[1.0,1...|[[-6.60386174E8],...|   0.75|\n",
      "+---+--------------------+--------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "\n",
    "dataA = [(0, Vectors.sparse(6, [0, 1, 2], [1.0, 1.0, 1.0]),),\n",
    "         (1, Vectors.sparse(6, [2, 3, 4], [1.0, 1.0, 1.0]),),\n",
    "         (2, Vectors.sparse(6, [0, 2, 4], [1.0, 1.0, 1.0]),)]\n",
    "dfA = spark.createDataFrame(dataA, [\"id\", \"features\"])\n",
    "\n",
    "dataB = [(3, Vectors.sparse(6, [1, 3, 5], [1.0, 1.0, 1.0]),),\n",
    "         (4, Vectors.sparse(6, [2, 3, 5], [1.0, 1.0, 1.0]),),\n",
    "         (5, Vectors.sparse(6, [1, 2, 4], [1.0, 1.0, 1.0]),)]\n",
    "dfB = spark.createDataFrame(dataB, [\"id\", \"features\"])\n",
    "\n",
    "key = Vectors.sparse(6, [1, 3], [1.0, 1.0])\n",
    "\n",
    "# key = Vectors.sparse(6, [0, 1, 2], [1.0, 1.0, 1.0])\n",
    "\n",
    "mh = MinHashLSH(inputCol=\"features\",\n",
    "                outputCol=\"hashes\",\n",
    "                seed = 42,\n",
    "                numHashTables=50)\n",
    "model = mh.fit(dfA)\n",
    "\n",
    "# Feature Transformation\n",
    "# print(\"The hashed dataset where hashed values are stored in the column 'hashes':\")\n",
    "# model.transform(dfA).show(vertical=True,\n",
    "#                           truncate=False)\n",
    "\n",
    "# Compute the locality sensitive hashes for the input rows, then perform approximate\n",
    "# similarity join.\n",
    "# We could avoid computing hashes by passing in the already-transformed dataset, e.g.\n",
    "# `model.approxSimilarityJoin(transformedA, transformedB, 0.6)`\n",
    "print(\"Approximately joining dfA and dfB on distance smaller than 0.6:\")\n",
    "model.approxSimilarityJoin(dfA, dfB, 0.6, distCol=\"JaccardDistance\")\\\n",
    "    .select(C(\"datasetA.id\").alias(\"idA\"),\n",
    "            C(\"datasetB.id\").alias(\"idB\"),\n",
    "            C(\"JaccardDistance\")).show(vertical=True,truncate=False)\n",
    "\n",
    "# Compute the locality sensitive hashes for the input rows, then perform approximate nearest\n",
    "# neighbor search.\n",
    "# We could avoid computing hashes by passing in the already-transformed dataset, e.g.\n",
    "# `model.approxNearestNeighbors(transformedA, key, 2)`\n",
    "# It may return less than 2 rows when not enough approximate near-neighbor candidates are\n",
    "# found.\n",
    "print(\"Approximately searching dfA for 5 nearest neighbors of the key:\")\n",
    "model.approxNearestNeighbors(dfA, key, 5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-05T13:14:30.935832Z",
     "start_time": "2021-06-05T13:14:30.910069Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-09T14:19:22.312131Z",
     "start_time": "2021-12-09T14:19:22.298623Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__class__', '__del__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__metaclass__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_call_java', '_clear', '_copyValues', '_copy_params', '_create_from_java_class', '_create_model', '_create_params_from_java', '_defaultParamMap', '_dummy', '_empty_java_param_map', '_fit', '_fit_java', '_from_java', '_input_kwargs', '_java_obj', '_make_java_param_pair', '_new_java_array', '_new_java_obj', '_paramMap', '_params', '_randomUID', '_resetUid', '_resolveParam', '_set', '_setDefault', '_shouldOwn', '_to_java', '_transfer_param_map_from_java', '_transfer_param_map_to_java', '_transfer_params_from_java', '_transfer_params_to_java', 'copy', 'explainParam', 'explainParams', 'extractParamMap', 'fit', 'fitMultiple', 'getInputCol', 'getNumHashTables', 'getOrDefault', 'getOutputCol', 'getParam', 'getSeed', 'hasDefault', 'hasParam', 'inputCol', 'isDefined', 'isSet', 'load', 'numHashTables', 'outputCol', 'params', 'read', 'save', 'seed', 'set', 'setInputCol', 'setNumHashTables', 'setOutputCol', 'setParams', 'setSeed', 'uid', 'write']\n",
      "\n",
      "['__class__', '__del__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__metaclass__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_call_java', '_clear', '_copyValues', '_copy_params', '_create_from_java_class', '_create_params_from_java', '_defaultParamMap', '_dummy', '_empty_java_param_map', '_from_java', '_java_obj', '_make_java_param_pair', '_new_java_array', '_new_java_obj', '_paramMap', '_params', '_randomUID', '_resetUid', '_resolveParam', '_set', '_setDefault', '_shouldOwn', '_to_java', '_transfer_param_map_from_java', '_transfer_param_map_to_java', '_transfer_params_from_java', '_transfer_params_to_java', '_transform', 'approxNearestNeighbors', 'approxSimilarityJoin', 'copy', 'explainParam', 'explainParams', 'extractParamMap', 'getOrDefault', 'getParam', 'hasDefault', 'hasParam', 'inputCol', 'isDefined', 'isSet', 'load', 'numHashTables', 'outputCol', 'params', 'read', 'save', 'set', 'transform', 'uid', 'write']\n",
      "\n",
      "inputCol: input column name. (current: features)\n",
      "numHashTables: number of hash tables, where increasing number of hash tables lowers the false negative rate, and decreasing it improves the running performance. (default: 1, current: 50)\n",
      "outputCol: output column name. (default: MinHashLSH_4495a8e951f5130a0ba9__output, current: hashes)\n",
      "seed: random seed. (default: -9006160927933222303, current: 42)\n",
      "inputCol: input column name (current: features)\n",
      "numHashTables: number of hash tables, where increasing number of hash tables lowers the false negative rate, and decreasing it improves the running performance (default: 1, current: 50)\n",
      "outputCol: output column name (default: MinHashLSH_4495a8e951f5130a0ba9__output, current: hashes)\n"
     ]
    }
   ],
   "source": [
    "print(dir(mh))\n",
    "print()\n",
    "print(dir(model))\n",
    "print()\n",
    "\n",
    "print(mh.explainParams())\n",
    "print(model.explainParams())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bucketed Random Projection for L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-09T14:19:25.166148Z",
     "start_time": "2021-12-09T14:19:22.313451Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n",
      "+---+---+------------------+\n",
      "|idA|idB| EuclideanDistance|\n",
      "+---+---+------------------+\n",
      "|  1|  4|3.1622776601683795|\n",
      "|  3|  5|  2.23606797749979|\n",
      "|  3|  7|2.8284271247461903|\n",
      "|  1|  7|  4.47213595499958|\n",
      "|  2|  5| 4.123105625617661|\n",
      "|  3|  6|  2.23606797749979|\n",
      "|  3|  4|1.4142135623730951|\n",
      "|  2|  6| 3.605551275463989|\n",
      "|  0|  6|               5.0|\n",
      "|  2|  4|3.1622776601683795|\n",
      "|  0|  5|               5.0|\n",
      "|  2|  7|  4.47213595499958|\n",
      "+---+---+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(0, Vectors.dense([-1.0, -1.0 ]),),\n",
    "        (1, Vectors.dense([-1.0, 1.0 ]),),\n",
    "        (2, Vectors.dense([1.0, -1.0 ]),),\n",
    "        (3, Vectors.dense([1.0, 1.0]),)]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"id\", \"features\"])\n",
    "\n",
    "\n",
    "brp = BucketedRandomProjectionLSH(\n",
    "    inputCol=\"features\",\n",
    "    outputCol=\"hashes\",\n",
    "    seed=12345,\n",
    "    bucketLength = 1.0,\n",
    "    numHashTables = 10)\n",
    "\n",
    "\n",
    "model = brp.fit(df)\n",
    "\n",
    "# model.getBucketLength()\n",
    "\n",
    "\n",
    "# model.transform(df).show()\n",
    "\n",
    "data2 = [(4, Vectors.dense([2.0, 2.0 ]),),\n",
    "         (5, Vectors.dense([2.0, 3.0 ]),),\n",
    "         (6, Vectors.dense([3.0, 2.0 ]),),\n",
    "         (7, Vectors.dense([3.0, 3.0]),)]\n",
    "\n",
    "\n",
    "df2 = spark.createDataFrame(data2, [\"id\", \"features\"])\n",
    "model.approxNearestNeighbors(df2, Vectors.dense([1.0, 2.0]), 1).collect()\n",
    "\n",
    "df.printSchema()\n",
    "df2.printSchema()\n",
    "\n",
    "model.approxSimilarityJoin(df, df2, 100, distCol=\"EuclideanDistance\").select(\n",
    "    C(\"datasetA.id\").alias(\"idA\"),\n",
    "    C(\"datasetB.id\").alias(\"idB\"),\n",
    "    C(\"EuclideanDistance\")).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scalibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pixlake",
   "language": "python",
   "name": "pixlake"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
