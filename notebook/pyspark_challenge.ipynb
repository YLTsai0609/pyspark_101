{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T16:25:19.670002Z",
     "start_time": "2021-03-11T16:25:19.644505Z"
    }
   },
   "outputs": [],
   "source": [
    "# env : pixlake\n",
    "# we focuing on pyspark dataframe processing\n",
    "# documentation https://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T16:25:19.805505Z",
     "start_time": "2021-03-11T16:25:19.672682Z"
    }
   },
   "outputs": [],
   "source": [
    "# make you auto compeletion faster\n",
    "# https://stackoverflow.com/questions/40536560/ipython-and-jupyter-autocomplete-not-working\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T16:25:19.912556Z",
     "start_time": "2021-03-11T16:25:19.807743Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have pyspark version :  ['spark-2.3', 'spark-3.0', 'spark-3.0.1-bin-hadoop2.7', 'spark-2.3.4-bin-hadoop2.7', 'spark-2.4.7-bin-hadoop2.7', 'spark-2.4']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "print('You have pyspark version : ', os.listdir('/opt/spark/versions'))\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "# spark-2.3, spark-2.4\n",
    "os.environ['SPARK_HOME'] = '/opt/spark/versions/spark-2.3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T16:25:20.227383Z",
     "start_time": "2021-03-11T16:25:19.917207Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession as Session\n",
    "from pyspark import SparkConf as Conf\n",
    "from pyspark.sql import functions as F, Window as W, types as T\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "C = F.col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T16:25:20.244502Z",
     "start_time": "2021-03-11T16:25:20.229293Z"
    }
   },
   "outputs": [],
   "source": [
    "conf = (Conf()\n",
    "    .set('spark.sql.sources.partitionOverwriteMode', 'dynamic')\n",
    "    .set('spark.driver.memory', '8g')\n",
    "    .set('spark.driver.maxResultSize', '2g')\n",
    "   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T16:25:23.064470Z",
     "start_time": "2021-03-11T16:25:20.246589Z"
    }
   },
   "outputs": [],
   "source": [
    "spark = (Session\n",
    "     .builder\n",
    "     .appName('pyspark-challenge')\n",
    "     .master('local[2]')\n",
    "     .config(conf=conf)\n",
    "     .getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T16:25:23.078098Z",
     "start_time": "2021-03-11T16:25:23.067156Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Builder', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__enter__', '__eq__', '__exit__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_convert_from_pandas', '_createFromLocal', '_createFromRDD', '_create_from_pandas_with_arrow', '_get_numpy_record_dtype', '_inferSchema', '_inferSchemaFromList', '_instantiatedSession', '_jsc', '_jsparkSession', '_jvm', '_jwrapped', '_repr_html_', '_sc', '_wrapped', 'builder', 'catalog', 'conf', 'createDataFrame', 'newSession', 'range', 'read', 'readStream', 'sparkContext', 'sql', 'stop', 'streams', 'table', 'udf', 'version']\n",
      "\n",
      "your spark version : 2.3.4\n"
     ]
    }
   ],
   "source": [
    "# 0. know what spark session can do and its version\n",
    "print(dir(spark), f'your spark version : {spark.version}'\n",
    "      , sep='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T16:25:26.758965Z",
     "start_time": "2021-03-11T16:25:23.080161Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['zipcode1.json', 'titanic_train.csv', 'small_zipcode.csv', 'Meteorite_Landings.csv', 'zipcodes.csv', 'zipcodes.json', 'webpage_1.txt', 'multiline-zipcode.json', 'simple_text.txt', 'zipcode2.json', 'titanic_test.csv']\n",
      "root\n",
      " |-- RecordNumber: integer (nullable = true)\n",
      " |-- Zipcode: integer (nullable = true)\n",
      " |-- ZipCodeType: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- LocationType: string (nullable = true)\n",
      " |-- Lat: double (nullable = true)\n",
      " |-- Long: double (nullable = true)\n",
      " |-- Xaxis: double (nullable = true)\n",
      " |-- Yaxis: double (nullable = true)\n",
      " |-- Zaxis: double (nullable = true)\n",
      " |-- WorldRegion: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- LocationText: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- Decommisioned: boolean (nullable = true)\n",
      " |-- TaxReturnsFiled: integer (nullable = true)\n",
      " |-- EstimatedPopulation: integer (nullable = true)\n",
      " |-- TotalWages: integer (nullable = true)\n",
      " |-- Notes: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RecordNumber</th>\n",
       "      <th>Zipcode</th>\n",
       "      <th>ZipCodeType</th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>LocationType</th>\n",
       "      <th>Lat</th>\n",
       "      <th>Long</th>\n",
       "      <th>Xaxis</th>\n",
       "      <th>Yaxis</th>\n",
       "      <th>Zaxis</th>\n",
       "      <th>WorldRegion</th>\n",
       "      <th>Country</th>\n",
       "      <th>LocationText</th>\n",
       "      <th>Location</th>\n",
       "      <th>Decommisioned</th>\n",
       "      <th>TaxReturnsFiled</th>\n",
       "      <th>EstimatedPopulation</th>\n",
       "      <th>TotalWages</th>\n",
       "      <th>Notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>704</td>\n",
       "      <td>STANDARD</td>\n",
       "      <td>PARC PARQUE</td>\n",
       "      <td>PR</td>\n",
       "      <td>NOT ACCEPTABLE</td>\n",
       "      <td>17.96</td>\n",
       "      <td>-66.22</td>\n",
       "      <td>0.38</td>\n",
       "      <td>-0.87</td>\n",
       "      <td>0.30</td>\n",
       "      <td>NA</td>\n",
       "      <td>US</td>\n",
       "      <td>Parc Parque, PR</td>\n",
       "      <td>NA-US-PR-PARC PARQUE</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>704</td>\n",
       "      <td>STANDARD</td>\n",
       "      <td>PASEO COSTA DEL SUR</td>\n",
       "      <td>PR</td>\n",
       "      <td>NOT ACCEPTABLE</td>\n",
       "      <td>17.96</td>\n",
       "      <td>-66.22</td>\n",
       "      <td>0.38</td>\n",
       "      <td>-0.87</td>\n",
       "      <td>0.30</td>\n",
       "      <td>NA</td>\n",
       "      <td>US</td>\n",
       "      <td>Paseo Costa Del Sur, PR</td>\n",
       "      <td>NA-US-PR-PASEO COSTA DEL SUR</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>709</td>\n",
       "      <td>STANDARD</td>\n",
       "      <td>BDA SAN LUIS</td>\n",
       "      <td>PR</td>\n",
       "      <td>NOT ACCEPTABLE</td>\n",
       "      <td>18.14</td>\n",
       "      <td>-66.26</td>\n",
       "      <td>0.38</td>\n",
       "      <td>-0.86</td>\n",
       "      <td>0.31</td>\n",
       "      <td>NA</td>\n",
       "      <td>US</td>\n",
       "      <td>Bda San Luis, PR</td>\n",
       "      <td>NA-US-PR-BDA SAN LUIS</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>61391</td>\n",
       "      <td>76166</td>\n",
       "      <td>UNIQUE</td>\n",
       "      <td>CINGULAR WIRELESS</td>\n",
       "      <td>TX</td>\n",
       "      <td>NOT ACCEPTABLE</td>\n",
       "      <td>32.72</td>\n",
       "      <td>-97.31</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>-0.83</td>\n",
       "      <td>0.54</td>\n",
       "      <td>NA</td>\n",
       "      <td>US</td>\n",
       "      <td>Cingular Wireless, TX</td>\n",
       "      <td>NA-US-TX-CINGULAR WIRELESS</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>61392</td>\n",
       "      <td>76177</td>\n",
       "      <td>STANDARD</td>\n",
       "      <td>FORT WORTH</td>\n",
       "      <td>TX</td>\n",
       "      <td>PRIMARY</td>\n",
       "      <td>32.75</td>\n",
       "      <td>-97.33</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>-0.83</td>\n",
       "      <td>0.54</td>\n",
       "      <td>NA</td>\n",
       "      <td>US</td>\n",
       "      <td>Fort Worth, TX</td>\n",
       "      <td>NA-US-TX-FORT WORTH</td>\n",
       "      <td>False</td>\n",
       "      <td>2126.0</td>\n",
       "      <td>4053.0</td>\n",
       "      <td>122396986.0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RecordNumber  Zipcode ZipCodeType                 City State  \\\n",
       "0             1      704    STANDARD          PARC PARQUE    PR   \n",
       "1             2      704    STANDARD  PASEO COSTA DEL SUR    PR   \n",
       "2            10      709    STANDARD         BDA SAN LUIS    PR   \n",
       "3         61391    76166      UNIQUE    CINGULAR WIRELESS    TX   \n",
       "4         61392    76177    STANDARD           FORT WORTH    TX   \n",
       "\n",
       "     LocationType    Lat   Long  Xaxis  Yaxis  Zaxis WorldRegion Country  \\\n",
       "0  NOT ACCEPTABLE  17.96 -66.22   0.38  -0.87   0.30          NA      US   \n",
       "1  NOT ACCEPTABLE  17.96 -66.22   0.38  -0.87   0.30          NA      US   \n",
       "2  NOT ACCEPTABLE  18.14 -66.26   0.38  -0.86   0.31          NA      US   \n",
       "3  NOT ACCEPTABLE  32.72 -97.31  -0.10  -0.83   0.54          NA      US   \n",
       "4         PRIMARY  32.75 -97.33  -0.10  -0.83   0.54          NA      US   \n",
       "\n",
       "              LocationText                      Location  Decommisioned  \\\n",
       "0          Parc Parque, PR          NA-US-PR-PARC PARQUE          False   \n",
       "1  Paseo Costa Del Sur, PR  NA-US-PR-PASEO COSTA DEL SUR          False   \n",
       "2         Bda San Luis, PR         NA-US-PR-BDA SAN LUIS          False   \n",
       "3    Cingular Wireless, TX    NA-US-TX-CINGULAR WIRELESS          False   \n",
       "4           Fort Worth, TX           NA-US-TX-FORT WORTH          False   \n",
       "\n",
       "   TaxReturnsFiled  EstimatedPopulation   TotalWages Notes  \n",
       "0              NaN                  NaN          NaN  None  \n",
       "1              NaN                  NaN          NaN  None  \n",
       "2              NaN                  NaN          NaN  None  \n",
       "3              NaN                  NaN          NaN  None  \n",
       "4           2126.0               4053.0  122396986.0  None  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. read data from csv\n",
    "print(os.listdir('../data'))\n",
    "df_from_csv_1 = spark.read.csv('../data/zipcodes.csv',\n",
    "                               header=True,\n",
    "                              inferSchema=True)\n",
    "df_from_csv_1.printSchema()\n",
    "df_from_csv_1.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T16:25:27.060722Z",
     "start_time": "2021-03-11T16:25:26.760632Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['zipcode1.json', 'titanic_train.csv', 'small_zipcode.csv', 'Meteorite_Landings.csv', 'zipcodes.csv', 'zipcodes.json', 'webpage_1.txt', 'multiline-zipcode.json', 'simple_text.txt', 'zipcode2.json', 'titanic_test.csv']\n",
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_df', '_jreader', '_set_opts', '_spark', 'csv', 'format', 'jdbc', 'json', 'load', 'option', 'options', 'orc', 'parquet', 'schema', 'table', 'text']\n",
      "root\n",
      " |-- City: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Decommisioned: boolean (nullable = true)\n",
      " |-- EstimatedPopulation: long (nullable = true)\n",
      " |-- Lat: double (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- LocationText: string (nullable = true)\n",
      " |-- LocationType: string (nullable = true)\n",
      " |-- Long: double (nullable = true)\n",
      " |-- Notes: string (nullable = true)\n",
      " |-- RecordNumber: long (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- TaxReturnsFiled: long (nullable = true)\n",
      " |-- TotalWages: long (nullable = true)\n",
      " |-- WorldRegion: string (nullable = true)\n",
      " |-- Xaxis: double (nullable = true)\n",
      " |-- Yaxis: double (nullable = true)\n",
      " |-- Zaxis: double (nullable = true)\n",
      " |-- ZipCodeType: string (nullable = true)\n",
      " |-- Zipcode: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>Country</th>\n",
       "      <th>Decommisioned</th>\n",
       "      <th>EstimatedPopulation</th>\n",
       "      <th>Lat</th>\n",
       "      <th>Location</th>\n",
       "      <th>LocationText</th>\n",
       "      <th>LocationType</th>\n",
       "      <th>Long</th>\n",
       "      <th>Notes</th>\n",
       "      <th>RecordNumber</th>\n",
       "      <th>State</th>\n",
       "      <th>TaxReturnsFiled</th>\n",
       "      <th>TotalWages</th>\n",
       "      <th>WorldRegion</th>\n",
       "      <th>Xaxis</th>\n",
       "      <th>Yaxis</th>\n",
       "      <th>Zaxis</th>\n",
       "      <th>ZipCodeType</th>\n",
       "      <th>Zipcode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PARC PARQUE</td>\n",
       "      <td>US</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.96</td>\n",
       "      <td>NA-US-PR-PARC PARQUE</td>\n",
       "      <td>Parc Parque, PR</td>\n",
       "      <td>NOT ACCEPTABLE</td>\n",
       "      <td>-66.22</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>PR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NA</td>\n",
       "      <td>0.38</td>\n",
       "      <td>-0.87</td>\n",
       "      <td>0.30</td>\n",
       "      <td>STANDARD</td>\n",
       "      <td>704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PASEO COSTA DEL SUR</td>\n",
       "      <td>US</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.96</td>\n",
       "      <td>NA-US-PR-PASEO COSTA DEL SUR</td>\n",
       "      <td>Paseo Costa Del Sur, PR</td>\n",
       "      <td>NOT ACCEPTABLE</td>\n",
       "      <td>-66.22</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>PR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NA</td>\n",
       "      <td>0.38</td>\n",
       "      <td>-0.87</td>\n",
       "      <td>0.30</td>\n",
       "      <td>STANDARD</td>\n",
       "      <td>704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BDA SAN LUIS</td>\n",
       "      <td>US</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.14</td>\n",
       "      <td>NA-US-PR-BDA SAN LUIS</td>\n",
       "      <td>Bda San Luis, PR</td>\n",
       "      <td>NOT ACCEPTABLE</td>\n",
       "      <td>-66.26</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>PR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NA</td>\n",
       "      <td>0.38</td>\n",
       "      <td>-0.86</td>\n",
       "      <td>0.31</td>\n",
       "      <td>STANDARD</td>\n",
       "      <td>709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CINGULAR WIRELESS</td>\n",
       "      <td>US</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32.72</td>\n",
       "      <td>NA-US-TX-CINGULAR WIRELESS</td>\n",
       "      <td>Cingular Wireless, TX</td>\n",
       "      <td>NOT ACCEPTABLE</td>\n",
       "      <td>-97.31</td>\n",
       "      <td>None</td>\n",
       "      <td>61391</td>\n",
       "      <td>TX</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NA</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>-0.83</td>\n",
       "      <td>0.54</td>\n",
       "      <td>UNIQUE</td>\n",
       "      <td>76166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FORT WORTH</td>\n",
       "      <td>US</td>\n",
       "      <td>False</td>\n",
       "      <td>4053.0</td>\n",
       "      <td>32.75</td>\n",
       "      <td>NA-US-TX-FORT WORTH</td>\n",
       "      <td>Fort Worth, TX</td>\n",
       "      <td>PRIMARY</td>\n",
       "      <td>-97.33</td>\n",
       "      <td>None</td>\n",
       "      <td>61392</td>\n",
       "      <td>TX</td>\n",
       "      <td>2126.0</td>\n",
       "      <td>122396986.0</td>\n",
       "      <td>NA</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>-0.83</td>\n",
       "      <td>0.54</td>\n",
       "      <td>STANDARD</td>\n",
       "      <td>76177</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  City Country  Decommisioned  EstimatedPopulation    Lat  \\\n",
       "0          PARC PARQUE      US          False                  NaN  17.96   \n",
       "1  PASEO COSTA DEL SUR      US          False                  NaN  17.96   \n",
       "2         BDA SAN LUIS      US          False                  NaN  18.14   \n",
       "3    CINGULAR WIRELESS      US          False                  NaN  32.72   \n",
       "4           FORT WORTH      US          False               4053.0  32.75   \n",
       "\n",
       "                       Location             LocationText    LocationType  \\\n",
       "0          NA-US-PR-PARC PARQUE          Parc Parque, PR  NOT ACCEPTABLE   \n",
       "1  NA-US-PR-PASEO COSTA DEL SUR  Paseo Costa Del Sur, PR  NOT ACCEPTABLE   \n",
       "2         NA-US-PR-BDA SAN LUIS         Bda San Luis, PR  NOT ACCEPTABLE   \n",
       "3    NA-US-TX-CINGULAR WIRELESS    Cingular Wireless, TX  NOT ACCEPTABLE   \n",
       "4           NA-US-TX-FORT WORTH           Fort Worth, TX         PRIMARY   \n",
       "\n",
       "    Long Notes  RecordNumber State  TaxReturnsFiled   TotalWages WorldRegion  \\\n",
       "0 -66.22  None             1    PR              NaN          NaN          NA   \n",
       "1 -66.22  None             2    PR              NaN          NaN          NA   \n",
       "2 -66.26  None            10    PR              NaN          NaN          NA   \n",
       "3 -97.31  None         61391    TX              NaN          NaN          NA   \n",
       "4 -97.33  None         61392    TX           2126.0  122396986.0          NA   \n",
       "\n",
       "   Xaxis  Yaxis  Zaxis ZipCodeType  Zipcode  \n",
       "0   0.38  -0.87   0.30    STANDARD      704  \n",
       "1   0.38  -0.87   0.30    STANDARD      704  \n",
       "2   0.38  -0.86   0.31    STANDARD      709  \n",
       "3  -0.10  -0.83   0.54      UNIQUE    76166  \n",
       "4  -0.10  -0.83   0.54    STANDARD    76177  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2 read data from json\n",
    "print(os.listdir('../data'))\n",
    "print(dir(spark.read))\n",
    "# 沒有infer_schema\n",
    "df_from_json = spark.read.json('../data/zipcodes.json')\n",
    "df_from_json.printSchema()\n",
    "df_from_json.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T16:25:28.488430Z",
     "start_time": "2021-03-11T16:25:27.062818Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+\n",
      "|language|user_counts|\n",
      "+--------+-----------+\n",
      "|    Java|      20000|\n",
      "|  Python|     100000|\n",
      "|   Scala|       3000|\n",
      "+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3 create dataframe from rdd list\n",
    "columns = [\"language\",\"user_counts\"]\n",
    "data = [\n",
    "    (\"Java\",\"20000\"),\n",
    "    (\"Python\",\"100000\"),\n",
    "    (\"Scala\",\"3000\")\n",
    "       ]\n",
    "# 先分散到rdd\n",
    "# rdd = spark.sparkContext.parallelize(data)\n",
    "# print(dir(rdd), type(rdd), sep='\\n\\n')\n",
    "# print()\n",
    "# df_from_rdd = rdd.toDF(schema=columns)\n",
    "# df_from_rdd.show(n=5)\n",
    "\n",
    "# 直接create，讓spark dataframe進行分散\n",
    "df = spark.createDataFrame(data=data,schema=columns)\n",
    "df.show(n=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T16:25:28.628282Z",
     "start_time": "2021-03-11T16:25:28.492148Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Builder', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__enter__', '__eq__', '__exit__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_conf', '_convert_from_pandas', '_createFromLocal', '_createFromRDD', '_create_from_pandas_with_arrow', '_get_numpy_record_dtype', '_inferSchema', '_inferSchemaFromList', '_instantiatedSession', '_jsc', '_jsparkSession', '_jvm', '_jwrapped', '_repr_html_', '_sc', '_wrapped', 'builder', 'catalog', 'conf', 'createDataFrame', 'newSession', 'range', 'read', 'readStream', 'sparkContext', 'sql', 'stop', 'streams', 'table', 'udf', 'version']\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4 create 5 row fake data using spark range\n",
    "print(dir(spark))\n",
    "print(type(spark.range(start=0,end=10)))\n",
    "columns = ['row_number']\n",
    "single_column_df = spark.range(start=0,end=10)\n",
    "single_column_df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T16:25:28.640571Z",
     "start_time": "2021-03-11T16:25:28.629986Z"
    }
   },
   "outputs": [],
   "source": [
    "# 6 create empty dataframe\n",
    "\n",
    "columns = [\"language\",\"user_counts\"]\n",
    "\n",
    "# empty RDD + schema won't work\n",
    "# df_1 = spark.createDataFrame(data=spark.sparkContext.emptyRDD(),\n",
    "#                              schema=columns)\n",
    "# df_1.show(n=5)\n",
    "\n",
    "# empty list -> rdd -> df won't work\n",
    "# df2 = spark.sparkContext.parallelize([]).toDF(columns)\n",
    "\n",
    "# df3 = spark.createDataFrame([]) # won't work this version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T16:25:29.026014Z",
     "start_time": "2021-03-11T16:25:28.642099Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 2\n"
     ]
    }
   ],
   "source": [
    "# 7 get dataframe shape\n",
    "print(df.count(), len(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T16:25:29.045764Z",
     "start_time": "2021-03-11T16:25:29.028491Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AutoBatchedSerializer', 'Column', 'DataFrame', 'DataType', 'PandasUDFType', 'PickleSerializer', 'PythonEvalType', 'SparkContext', 'StringType', 'UserDefinedFunction', '__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_binary_mathfunctions', '_collect_list_doc', '_collect_set_doc', '_create_binary_mathfunction', '_create_function', '_create_udf', '_create_window_function', '_functions', '_functions_1_4', '_functions_1_6', '_functions_2_1', '_functions_deprecated', '_lit_doc', '_message', '_string_functions', '_test', '_to_java_column', '_to_seq', '_window_functions', '_wrap_deprecated_function', 'abs', 'acos', 'add_months', 'approxCountDistinct', 'approx_count_distinct', 'array', 'array_contains', 'asc', 'ascii', 'asin', 'atan', 'atan2', 'avg', 'base64', 'bin', 'bitwiseNOT', 'blacklist', 'broadcast', 'bround', 'cbrt', 'ceil', 'coalesce', 'col', 'collect_list', 'collect_set', 'column', 'concat', 'concat_ws', 'conv', 'corr', 'cos', 'cosh', 'count', 'countDistinct', 'covar_pop', 'covar_samp', 'crc32', 'create_map', 'cume_dist', 'current_date', 'current_timestamp', 'date_add', 'date_format', 'date_sub', 'date_trunc', 'datediff', 'dayofmonth', 'dayofweek', 'dayofyear', 'decode', 'degrees', 'dense_rank', 'desc', 'encode', 'exp', 'explode', 'explode_outer', 'expm1', 'expr', 'factorial', 'first', 'floor', 'format_number', 'format_string', 'from_json', 'from_unixtime', 'from_utc_timestamp', 'functools', 'get_json_object', 'greatest', 'grouping', 'grouping_id', 'hash', 'hex', 'hour', 'hypot', 'ignore_unicode_prefix', 'initcap', 'input_file_name', 'instr', 'isnan', 'isnull', 'json_tuple', 'kurtosis', 'lag', 'last', 'last_day', 'lead', 'least', 'length', 'levenshtein', 'lit', 'locate', 'log', 'log10', 'log1p', 'log2', 'lower', 'lpad', 'ltrim', 'map_keys', 'map_values', 'math', 'max', 'md5', 'mean', 'min', 'minute', 'monotonically_increasing_id', 'month', 'months_between', 'nanvl', 'next_day', 'ntile', 'pandas_udf', 'percent_rank', 'posexplode', 'posexplode_outer', 'pow', 'quarter', 'radians', 'rand', 'randn', 'rank', 'regexp_extract', 'regexp_replace', 'repeat', 'reverse', 'rint', 'round', 'row_number', 'rpad', 'rtrim', 'second', 'sha1', 'sha2', 'shiftLeft', 'shiftRight', 'shiftRightUnsigned', 'signum', 'sin', 'since', 'sinh', 'size', 'skewness', 'sort_array', 'soundex', 'spark_partition_id', 'split', 'sqrt', 'stddev', 'stddev_pop', 'stddev_samp', 'struct', 'substring', 'substring_index', 'sum', 'sumDistinct', 'sys', 'tan', 'tanh', 'toDegrees', 'toRadians', 'to_date', 'to_json', 'to_timestamp', 'to_utc_timestamp', 'translate', 'trim', 'trunc', 'udf', 'unbase64', 'unhex', 'unix_timestamp', 'upper', 'var_pop', 'var_samp', 'variance', 'warnings', 'weekofyear', 'when', 'window', 'year']\n"
     ]
    }
   ],
   "source": [
    "# 8 know what methods are supported by sql.function\n",
    "print(dir(F))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T16:25:29.350737Z",
     "start_time": "2021-03-11T16:25:29.047229Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+----------+\n",
      "|language|user_counts|new_column|\n",
      "+--------+-----------+----------+\n",
      "|    Java|      20000|       ABC|\n",
      "|  Python|     100000|       ABC|\n",
      "|   Scala|       3000|       ABC|\n",
      "+--------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 9 add const column to a existing dataframe\n",
    "\n",
    "columns = [\"language\",\"user_counts\"]\n",
    "data = [\n",
    "    (\"Java\",\"20000\"),\n",
    "    (\"Python\",\"100000\"),\n",
    "    (\"Scala\",\"3000\")\n",
    "       ]\n",
    "df = spark.createDataFrame(data=data,schema=columns)\n",
    "df = (\n",
    "    df.withColumn(\"new_column\",F.lit(\"ABC\")) \n",
    "    # F.lit means literal, retrurn a column\n",
    ")\n",
    "df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T16:25:29.898038Z",
     "start_time": "2021-03-11T16:25:29.353137Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-----+\n",
      "|language|user_counts|index|\n",
      "+--------+-----------+-----+\n",
      "|    Java|      20000|    1|\n",
      "|  Python|     100000|    2|\n",
      "|   Scala|       3000|    3|\n",
      "+--------+-----------+-----+\n",
      "\n",
      "+--------+-----------+-------+\n",
      "|language|user_counts|row_num|\n",
      "+--------+-----------+-------+\n",
      "|    Java|      20000|      1|\n",
      "|  Python|     100000|      2|\n",
      "|   Scala|       3000|      3|\n",
      "+--------+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 10 add a row_id column from a exisiting dataframe\n",
    "# https://stackoverflow.com/questions/53082891/adding-a-unique-consecutive-row-number-to-dataframe-in-pyspark\n",
    "columns = [\"language\",\"user_counts\"]\n",
    "data = [\n",
    "    (\"Java\",\"20000\"),\n",
    "    (\"Python\",\"100000\"),\n",
    "    (\"Scala\",\"3000\")\n",
    "       ]\n",
    "df_1 = spark.createDataFrame(data=data,schema=columns)\n",
    "df_1 = (\n",
    "    df_1.withColumn(\"index\", \n",
    "                  F.row_number().over(\n",
    "                      W.orderBy(F.monotonically_increasing_id() - 1)\n",
    "                  )\n",
    "                 )\n",
    "    # F.monotonically_increasing_id does not give 1 ~ N\n",
    "    # So we use window function to work around\n",
    ")\n",
    "df_1.show(n=5)\n",
    "\n",
    "# Mre clear way to do that\n",
    "df_2 = spark.createDataFrame(data=data,schema=columns)\n",
    "w = W.orderBy(F.lit('A'))\n",
    "df_2 = (\n",
    "    df_2.withColumn(\"row_num\", F.row_number().over(w))\n",
    ")\n",
    "df_2.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T16:25:30.094533Z",
     "start_time": "2021-03-11T16:25:29.900079Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+----------+------------------+\n",
      "|language|user_counts|new_column|     random_number|\n",
      "+--------+-----------+----------+------------------+\n",
      "|    Java|      20000|       ABC|0.1777257261193641|\n",
      "|  Python|     100000|       ABC|0.2258602359660491|\n",
      "|   Scala|       3000|       ABC|0.2826401010610854|\n",
      "+--------+-----------+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 10 add a random number to a exisit column\n",
    "df = (\n",
    "#     df.withColumn('random_number', F.when(F.rand() > 0.5, 1).otherwise(0))\n",
    "        df.withColumn('random_number', F.rand())\n",
    ")\n",
    "\n",
    "df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T16:25:30.293124Z",
     "start_time": "2021-03-11T16:25:30.096713Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>user_counts</th>\n",
       "      <th>new_column</th>\n",
       "      <th>random_number</th>\n",
       "      <th>binary_cut_05</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Java</td>\n",
       "      <td>20000</td>\n",
       "      <td>ABC</td>\n",
       "      <td>0.177726</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Python</td>\n",
       "      <td>100000</td>\n",
       "      <td>ABC</td>\n",
       "      <td>0.225860</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Scala</td>\n",
       "      <td>3000</td>\n",
       "      <td>ABC</td>\n",
       "      <td>0.282640</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  language user_counts new_column  random_number  binary_cut_05\n",
       "0     Java       20000        ABC       0.177726              1\n",
       "1   Python      100000        ABC       0.225860              0\n",
       "2    Scala        3000        ABC       0.282640              1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 11 add a binary 0, 1 based on condition to an exisit column\n",
    "df = (\n",
    "    df.withColumn('binary_cut_05',F.when(F.rand() > 0.5, 1).otherwise(0))\n",
    ")\n",
    "df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T16:25:30.499053Z",
     "start_time": "2021-03-11T16:25:30.294550Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-----------+\n",
      "|row_id|language|user_counts|\n",
      "+------+--------+-----------+\n",
      "|     0|    Java|      20000|\n",
      "|     1|  Python|     100000|\n",
      "|     2|   Scala|       3000|\n",
      "+------+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 12 create a dataframe contains row_index and fake data\n",
    "\n",
    "columns = [\"row_id\",\"language\",\"user_counts\"]\n",
    "data = [\n",
    "    (0, \"Java\",\"20000\"),\n",
    "    (1, \"Python\",\"100000\"),\n",
    "    (2, \"Scala\",\"3000\")\n",
    "       ]\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=columns)\n",
    "df.show(n=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T16:25:30.918321Z",
     "start_time": "2021-03-11T16:25:30.500332Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+-----+------+------+\n",
      "|firstname|middlename|lastname|   id|gender|salary|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|    James|          |   Smith|36636|     M|  3000|\n",
      "|  Michael|      Rose|        |40288|     M|  4000|\n",
      "|   Robert|          |Williams|42114|     M|  4000|\n",
      "|    Maria|      Anne|   Jones|39192|     F|  4000|\n",
      "|      Jen|      Mary|   Brown|     |     F|    -1|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "\n",
      "+--------------------+-----+------+------+\n",
      "|                name|   id|gender|salary|\n",
      "+--------------------+-----+------+------+\n",
      "|    [James, , Smith]|36636|     M|  3000|\n",
      "|   [Michael, Rose, ]|40288|     M|  4000|\n",
      "|[Robert, , Williams]|42114|     M|  4000|\n",
      "|[Maria, Anne, Jones]|39192|     F|  4000|\n",
      "|  [Jen, Mary, Brown]|     |     F|    -1|\n",
      "+--------------------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 13 construct a complex data for spark dataframe\n",
    "# using StructType\n",
    "\n",
    "# Case 1\n",
    "data = [\n",
    "    (\"James\",\"\",\"Smith\",\"36636\",\"M\", 3000),\n",
    "    (\"Michael\",\"Rose\",\"\",\"40288\",\"M\", 4000),\n",
    "    (\"Robert\",\"\",\"Williams\",\"42114\",\"M\", 4000),\n",
    "    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\", 4000),\n",
    "    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\", -1)\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"firstname\",StringType(), True), # Nullable True\n",
    "    StructField(\"middlename\",StringType(), True),\n",
    "    StructField(\"lastname\",StringType(), True),\n",
    "    StructField(\"id\",StringType(), True),\n",
    "    StructField(\"gender\",StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True)\n",
    "    ])\n",
    "df = spark.createDataFrame(data=data,schema=schema)\n",
    "df.show(n=5)\n",
    "\n",
    "# Case 2\n",
    "\n",
    "struct_data = [\n",
    "    ((\"James\",\"\",\"Smith\"),\"36636\",\"M\", 3000),\n",
    "    ((\"Michael\",\"Rose\",\"\"),\"40288\",\"M\", 4000),\n",
    "    ((\"Robert\",\"\",\"Williams\"),\"42114\",\"M\", 4000),\n",
    "    ((\"Maria\",\"Anne\",\"Jones\"),\"39192\",\"F\", 4000),\n",
    "    ((\"Jen\",\"Mary\",\"Brown\"),\"\",\"F\", -1)\n",
    "]\n",
    "\n",
    "structure_schema = StructType([\n",
    "    StructField('name',\n",
    "        StructType([\n",
    "            StructField(\"firstname\",StringType(), True), # Nullable True\n",
    "            StructField(\"middlename\",StringType(), True),\n",
    "            StructField(\"lastname\",StringType(), True),\n",
    "    ])),\n",
    "    StructField(\"id\",StringType(), True),\n",
    "    StructField(\"gender\",StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True)\n",
    "    ])\n",
    "\n",
    "df = spark.createDataFrame(data=struct_data, schema=structure_schema)\n",
    "df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T16:25:31.262341Z",
     "start_time": "2021-03-11T16:25:30.920979Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- languagesAtSchool: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- currentState: string (nullable = true)\n",
      "\n",
      "+----------------+------------------+------------+\n",
      "|name            |languagesAtSchool |currentState|\n",
      "+----------------+------------------+------------+\n",
      "|James,,Smith    |[Java, Scala, C++]|CA          |\n",
      "|Michael,Rose,   |[Spark, Java, C++]|NJ          |\n",
      "|Robert,,Williams|[CSharp, VB]      |NV          |\n",
      "+----------------+------------------+------------+\n",
      "\n",
      "+-------------+-----+\n",
      "|         name|  col|\n",
      "+-------------+-----+\n",
      "| James,,Smith| Java|\n",
      "| James,,Smith|Scala|\n",
      "| James,,Smith|  C++|\n",
      "|Michael,Rose,|Spark|\n",
      "|Michael,Rose,| Java|\n",
      "+-------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 14 construct a complex data for spark dataframe\n",
    "# using ArratyType\n",
    "# https://github.com/spark-examples/pyspark-examples/blob/master/pyspark-array-string.py\n",
    "\n",
    "columns = [\"name\",\"languagesAtSchool\",\"currentState\"]\n",
    "data = [(\"James,,Smith\",[\"Java\",\"Scala\",\"C++\"],\"CA\"), \\\n",
    "    (\"Michael,Rose,\",[\"Spark\",\"Java\",\"C++\"],\"NJ\"), \\\n",
    "    (\"Robert,,Williams\",[\"CSharp\",\"VB\"],\"NV\")]\n",
    "\n",
    "df = spark.createDataFrame(data=data,schema=columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n",
    "\n",
    "df.select(df.name, F.explode(df.languagesAtSchool)).show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T16:25:31.450659Z",
     "start_time": "2021-03-11T16:25:31.265194Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- language: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- peoperties: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      "\n",
      "+----------+--------------+--------------------+\n",
      "|      name|      language|          peoperties|\n",
      "+----------+--------------+--------------------+\n",
      "|     James| [Java, Scala]|[eye -> brown, ha...|\n",
      "|   Michael|[Spark, Java,]|[eye ->, hair -> ...|\n",
      "|    Robert|    [CSharp, ]|[eye -> , hair ->...|\n",
      "|Washington|          null|                null|\n",
      "| Jefferson|        [1, 2]|                  []|\n",
      "+----------+--------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 15 construct a complex data for spark dataframe\n",
    "# using MapType\n",
    "# https://github.com/spark-examples/pyspark-examples/blob/master/pyspark-explode-array-map.py\n",
    "data = [\n",
    "        ('James',['Java','Scala'],{'hair':'black','eye':'brown'}),\n",
    "        ('Michael',['Spark','Java',None],{'hair':'brown','eye':None}),\n",
    "        ('Robert',['CSharp',''],{'hair':'red','eye':''}),\n",
    "        ('Washington',None,None),\n",
    "        ('Jefferson',['1','2'],{})\n",
    "]\n",
    "\n",
    "columns = ['name','language','peoperties']\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=columns)\n",
    "\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T16:25:31.466925Z",
     "start_time": "2021-03-11T16:25:31.453612Z"
    }
   },
   "outputs": [],
   "source": [
    "# 16 create a nested array-type dataframe\n",
    "# https://github.com/spark-examples/pyspark-examples/blob/master/pyspark-explode-nested-array.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T16:25:31.600990Z",
     "start_time": "2021-03-11T16:25:31.468540Z"
    }
   },
   "outputs": [],
   "source": [
    "# 17 create a datetime column for spark dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Column Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T16:25:31.918147Z",
     "start_time": "2021-03-11T16:25:31.610880Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+--------------+------------------+\n",
      "|language|user_counts|user_count_100|    user_count_log|\n",
      "+--------+-----------+--------------+------------------+\n",
      "|    Java|      20000|       2000000| 4.301029995663981|\n",
      "|  Python|     100000|      10000000|               5.0|\n",
      "|   Scala|       3000|        300000|3.4771212547196626|\n",
      "+--------+-----------+--------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1 create new column based on original column\n",
    "\n",
    "columns = [\"language\", \"user_counts\"]\n",
    "data = [\n",
    "    (\"Java\",20000),\n",
    "    (\"Python\",100000),\n",
    "    (\"Scala\",3000)\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=columns)\n",
    "df = (\n",
    "    df.withColumn(\"user_count_100\", C(\"user_counts\") * 100)\\\n",
    "    .withColumn(\"user_count_log\", F.log10(C(\"user_counts\")))\n",
    ")\n",
    "\n",
    "df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T16:25:32.099907Z",
     "start_time": "2021-03-11T16:25:31.922793Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+\n",
      "|  lang|const_col|\n",
      "+------+---------+\n",
      "|  Java|      ABC|\n",
      "|Python|      ABC|\n",
      "| Scala|      ABC|\n",
      "+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2 rename, drop, add constant column to existing dataframe\n",
    "# https://stackoverflow.com/questions/34077353/how-to-change-dataframe-column-names-in-pyspark\n",
    "columns = [\"language\", \"user_counts\"]\n",
    "data = [\n",
    "    (\"Java\",20000),\n",
    "    (\"Python\",100000),\n",
    "    (\"Scala\",3000)\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df = (\n",
    "    df.withColumn('const_col',F.lit('ABC'))\\\n",
    "    .withColumnRenamed(\"language\",\"lang\")\\\n",
    "    .drop(\"user_counts\")\n",
    ")\n",
    "df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T16:25:32.121739Z",
     "start_time": "2021-03-11T16:25:32.102597Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.column.Column'>\n",
      "\n",
      "['__add__', '__and__', '__bool__', '__class__', '__contains__', '__delattr__', '__dict__', '__dir__', '__div__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__invert__', '__iter__', '__le__', '__lt__', '__mod__', '__module__', '__mul__', '__ne__', '__neg__', '__new__', '__nonzero__', '__or__', '__pow__', '__radd__', '__rand__', '__rdiv__', '__reduce__', '__reduce_ex__', '__repr__', '__rmod__', '__rmul__', '__ror__', '__rpow__', '__rsub__', '__rtruediv__', '__setattr__', '__sizeof__', '__str__', '__sub__', '__subclasshook__', '__truediv__', '__weakref__', '_asc_doc', '_bitwiseAND_doc', '_bitwiseOR_doc', '_bitwiseXOR_doc', '_contains_doc', '_desc_doc', '_endswith_doc', '_eqNullSafe_doc', '_isNotNull_doc', '_isNull_doc', '_jc', '_like_doc', '_rlike_doc', '_startswith_doc', 'alias', 'asc', 'astype', 'between', 'bitwiseAND', 'bitwiseOR', 'bitwiseXOR', 'cast', 'contains', 'desc', 'endswith', 'eqNullSafe', 'getField', 'getItem', 'isNotNull', 'isNull', 'isin', 'like', 'name', 'otherwise', 'over', 'rlike', 'startswith', 'substr', 'when']\n"
     ]
    }
   ],
   "source": [
    "# know what method and attribute can be called with column object\n",
    "\n",
    "\n",
    "print(type(C(\"language\")), dir(C(\"language\")), sep='\\n\\n')\n",
    "\n",
    "# alias - 可以換名字\n",
    "# asc, desc - 可以排序\n",
    "# astype,cast - 可以轉型\n",
    "# between - 可以傳入start_date以及end_date過濾\n",
    "# bitwiseAND, bitwiseOR, bitwiseXOR - 可以做布林運算\n",
    "# contains - 可以做字串搜尋\n",
    "# endwith, startwith, rlike, substring - 可以做字串比對\n",
    "# eqNullSafe, isNotNull, isNull - 可以檢查null值，Python須以None傳入\n",
    "# isin, like - 可以做值的比對(數值，字串值)\n",
    "# name - 可以取得欄位名稱\n",
    "# when, otherwise - 可以做條件判斷\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T16:25:32.453546Z",
     "start_time": "2021-03-11T16:25:32.123140Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-------------+\n",
      "|language|user_counts|is_many_users|\n",
      "+--------+-----------+-------------+\n",
      "|    Java|      20000|            0|\n",
      "|  Python|     100000|            1|\n",
      "|   Scala|       3000|            0|\n",
      "+--------+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3 create a new dynamic column(if else condition based on old column)\n",
    "# Case 1\n",
    "columns = [\"language\", \"user_counts\"]\n",
    "data = [\n",
    "    (\"Java\",20000),\n",
    "    (\"Python\",100000),\n",
    "    (\"Scala\",3000)\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "df = (\n",
    "    df.withColumn(\"is_many_users\",\n",
    "                  F.when(C('user_counts') > 50000, 1).otherwise(0)\n",
    "                 )\n",
    ")\n",
    "\n",
    "df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T16:25:32.912451Z",
     "start_time": "2021-03-11T16:25:32.456324Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|firstname|middlename|lastname|   id|gender|salary|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|    James|          |   Smith|36636|     M|  3000|\n",
      "|  Michael|      Rose|        |40288|     M|  4000|\n",
      "|   Robert|          |Williams|42114|     M|  4000|\n",
      "|    Maria|      Anne|   Jones|39192|     F|  4000|\n",
      "|      Jen|      Mary|   Brown|     |     F|    -1|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "\n",
      "After\n",
      "+---------+----------+--------+-----+------+------+----------------+\n",
      "|firstname|middlename|lastname|   id|gender|salary|       full_name|\n",
      "+---------+----------+--------+-----+------+------+----------------+\n",
      "|    James|      null|   Smith|36636|     M|  3000|             N/A|\n",
      "|  Michael|      Rose|    null|40288|     M|  4000|             N/A|\n",
      "|   Robert|      null|Williams|42114|     M|  4000|             N/A|\n",
      "|    Maria|      Anne|   Jones|39192|     F|  4000|Maria Anne Jones|\n",
      "|      Jen|      Mary|   Brown|     |     F|    -1|  Jen Mary Brown|\n",
      "+---------+----------+--------+-----+------+------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create a new dynamic column(if else condition based on old column) plus empty string replacement\n",
    "\n",
    "data = [\n",
    "    (\"James\",\"\",\"Smith\",\"36636\",\"M\", 3000),\n",
    "    (\"Michael\",\"Rose\",\"\",\"40288\",\"M\", 4000),\n",
    "    (\"Robert\",\"\",\"Williams\",\"42114\",\"M\", 4000),\n",
    "    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\", 4000),\n",
    "    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\", -1)\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"firstname\",StringType(), True), # Nullable True\n",
    "    StructField(\"middlename\",StringType(), True),\n",
    "    StructField(\"lastname\",StringType(), True),\n",
    "    StructField(\"id\",StringType(), True),\n",
    "    StructField(\"gender\",StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True)\n",
    "    ])\n",
    "df = spark.createDataFrame(data=data,schema=schema)\n",
    "\n",
    "# we cannot compare column values with empty string\n",
    "# so the work-around method is replace empty string to null\n",
    "# then using isNotNull()\n",
    "is_full_name_exist = (C(\"firstname\").isNotNull() & C(\"middlename\").isNotNull() & C(\"lastname\").isNotNull())\n",
    "\n",
    "\n",
    "def blank_as_null(x):\n",
    "    \"\"\"\n",
    "    helper function for converting row value from empty string to null\n",
    "    https://stackoverflow.com/questions/33287886/replace-empty-strings-with-none-null-values-in-dataframe\n",
    "    \"\"\"\n",
    "    return F.when(C(x) != \"\", C(x)).otherwise(None)\n",
    "\n",
    "print(\"Before\")\n",
    "df.show(n=5)\n",
    "df = (\n",
    "    df.withColumn(\"firstname\", blank_as_null(\"firstname\"))\\\n",
    "    .withColumn(\"middlename\", blank_as_null(\"middlename\"))\\\n",
    "    .withColumn(\"lastname\", blank_as_null(\"lastname\"))\\\n",
    "    .withColumn(\"full_name\",\n",
    "                  F.when(\n",
    "                      is_full_name_exist,\n",
    "                      F.concat(C(\"firstname\"), F.lit(' '),\n",
    "                               C(\"middlename\"), F.lit(' '),\n",
    "                               C(\"lastname\"))\n",
    "                  ).otherwise(F.lit('N/A'))\n",
    "                 )\n",
    ")\n",
    "print(\"After\")\n",
    "df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T16:25:33.092721Z",
     "start_time": "2021-03-11T16:25:32.914414Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Column<b'language'>, Column<b'user_counts'>] <class 'list'> <class 'pyspark.sql.column.Column'>\n",
      "+--------+-----------+\n",
      "|language|user_counts|\n",
      "+--------+-----------+\n",
      "|    Java|      20000|\n",
      "|  Python|     100000|\n",
      "|   Scala|       3000|\n",
      "+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4 select columns which also exist on another dataframe\n",
    "\n",
    "columns_1 = [\"language\", \"user_counts\"]\n",
    "data_1 = [\n",
    "    (\"Java\",20000),\n",
    "    (\"Python\",100000),\n",
    "    (\"Scala\",3000)\n",
    "]\n",
    "\n",
    "columns_2 = [\"language\", \"user_counts\",\"note\"]\n",
    "data_2 = [\n",
    "    (\"Java\",20000,\"nothing\"),\n",
    "    (\"Python\",100000,\"nothing\"),\n",
    "    (\"Scala\",3000,\"nothing\")\n",
    "]\n",
    "\n",
    "df_1 = spark.createDataFrame(data_1, columns_1)\n",
    "df_2 = spark.createDataFrame(data_2, columns_2)\n",
    "\n",
    "# union columns\n",
    "same_cols = [F.col(c) for c in df_2.columns if c in df_1.columns]\n",
    "print(same_cols, type(same_cols), type(same_cols[0]))\n",
    "\n",
    "df_same_col = df_1.select(*same_cols)\n",
    "df_same_col.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T16:25:33.362086Z",
     "start_time": "2021-03-11T16:25:33.095479Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-----+-----------------+-----------------+\n",
      "|language|user_counts|const|scientific_sign_1|scientific_sign_2|\n",
      "+--------+-----------+-----+-----------------+-----------------+\n",
      "|    Java|      20000|10000|           1.0E40|          1.0E-40|\n",
      "|  Python|     100000|10000|           1.0E40|          1.0E-40|\n",
      "|   Scala|       3000|10000|           1.0E40|          1.0E-40|\n",
      "+--------+-----------+-----+-----------------+-----------------+\n",
      "\n",
      "root\n",
      " |-- language: string (nullable = true)\n",
      " |-- user_counts: long (nullable = true)\n",
      " |-- const: integer (nullable = false)\n",
      " |-- scientific_sign_1: double (nullable = false)\n",
      " |-- scientific_sign_2: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create a const numerical column\n",
    "\n",
    "# 3 create a new dynamic column(if else condition based on old column)\n",
    "# Case 1\n",
    "columns = [\"language\", \"user_counts\"]\n",
    "data = [\n",
    "    (\"Java\",20000),\n",
    "    (\"Python\",100000),\n",
    "    (\"Scala\",3000)\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "df = (\n",
    "    df.withColumn(\"const\",F.lit(10000))\\\n",
    "    .withColumn(\"scientific_sign_1\", F.lit(1e40))\n",
    "    .withColumn(\"scientific_sign_2\", F.lit(1e-40))\n",
    ")\n",
    "\n",
    "df.show(n=5)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# String Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T16:25:33.379365Z",
     "start_time": "2021-03-11T16:25:33.365493Z"
    }
   },
   "outputs": [],
   "source": [
    "# 5 string concat two column values to a new column from an existing dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T16:25:33.450884Z",
     "start_time": "2021-03-11T16:25:33.380613Z"
    }
   },
   "outputs": [],
   "source": [
    "# 6 cut of left 3 char of specific column to a new column from an existing dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T16:25:33.525371Z",
     "start_time": "2021-03-11T16:25:33.453479Z"
    }
   },
   "outputs": [],
   "source": [
    "# 7 convert string type column to int/float type column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T16:25:33.600604Z",
     "start_time": "2021-03-11T16:25:33.527876Z"
    }
   },
   "outputs": [],
   "source": [
    "# 8 convert string type column to datetime type column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T16:25:33.845464Z",
     "start_time": "2021-03-11T16:25:33.603324Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+-----+------+------+\n",
      "|firstname|middlename|lastname|   id|gender|salary|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|    James|          |   Smith|36636|     M|  3000|\n",
      "|  Michael|      Rose|        |40288|     M|  4000|\n",
      "|   Robert|          |Williams|42114|     M|  4000|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. filter on equal condition\n",
    "\n",
    "data = [\n",
    "    (\"James\",\"\",\"Smith\",\"36636\",\"M\", 3000),\n",
    "    (\"Michael\",\"Rose\",\"\",\"40288\",\"M\", 4000),\n",
    "    (\"Robert\",\"\",\"Williams\",\"42114\",\"M\", 4000),\n",
    "    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\", 4000),\n",
    "    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\", -1)\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"firstname\",StringType(), True), # Nullable True\n",
    "    StructField(\"middlename\",StringType(), True),\n",
    "    StructField(\"lastname\",StringType(), True),\n",
    "    StructField(\"id\",StringType(), True),\n",
    "    StructField(\"gender\",StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True)\n",
    "    ])\n",
    "df = spark.createDataFrame(data=data,schema=schema)\n",
    "\n",
    "df = (\n",
    "    df.filter(C(\"gender\") == \"M\")\n",
    ")\n",
    "\n",
    "df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T16:25:34.034222Z",
     "start_time": "2021-03-11T16:25:33.848511Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+-----+------+------+\n",
      "|firstname|middlename|lastname|   id|gender|salary|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|  Michael|      Rose|        |40288|     M|  4000|\n",
      "|   Robert|          |Williams|42114|     M|  4000|\n",
      "|    Maria|      Anne|   Jones|39192|     F|  6000|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2 filter on >, <, >=, <=\n",
    "\n",
    "data = [\n",
    "    (\"James\",\"\",\"Smith\",\"36636\",\"M\", 3000),\n",
    "    (\"Michael\",\"Rose\",\"\",\"40288\",\"M\", 4000),\n",
    "    (\"Robert\",\"\",\"Williams\",\"42114\",\"M\", 4000),\n",
    "    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\", 6000),\n",
    "    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\", -1)\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"firstname\",StringType(), True), # Nullable True\n",
    "    StructField(\"middlename\",StringType(), True),\n",
    "    StructField(\"lastname\",StringType(), True),\n",
    "    StructField(\"id\",StringType(), True),\n",
    "    StructField(\"gender\",StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True)\n",
    "    ])\n",
    "df = spark.createDataFrame(data=data,schema=schema)\n",
    "\n",
    "df = (\n",
    "    df.filter(C(\"salary\") >= 4000)\n",
    ")\n",
    "\n",
    "df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T16:25:34.237262Z",
     "start_time": "2021-03-11T16:25:34.037128Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+-----+------+------+\n",
      "|firstname|middlename|lastname|   id|gender|salary|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|  Michael|      Rose|        |40288|     M|  4000|\n",
      "|   Robert|          |Williams|42114|     M|  4000|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3 multiple conditions require parenthese around each condition\n",
    "\n",
    "\n",
    "data = [\n",
    "    (\"James\",\"\",\"Smith\",\"36636\",\"M\", 3000),\n",
    "    (\"Michael\",\"Rose\",\"\",\"40288\",\"M\", 4000),\n",
    "    (\"Robert\",\"\",\"Williams\",\"42114\",\"M\", 4000),\n",
    "    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\", 6000),\n",
    "    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\", -1)\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"firstname\",StringType(), True), # Nullable True\n",
    "    StructField(\"middlename\",StringType(), True),\n",
    "    StructField(\"lastname\",StringType(), True),\n",
    "    StructField(\"id\",StringType(), True),\n",
    "    StructField(\"gender\",StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True)\n",
    "    ])\n",
    "df = spark.createDataFrame(data=data,schema=schema)\n",
    "\n",
    "# This is lazy computing\n",
    "rich_man_who_worth_married = (\n",
    "    (C(\"gender\") == \"M\") &\n",
    "    (C(\"salary\") >= 4000)\n",
    ")\n",
    "\n",
    "df = (\n",
    "    df.filter(rich_man_who_worth_married)\n",
    ")\n",
    "\n",
    "df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T16:25:34.445090Z",
     "start_time": "2021-03-11T16:25:34.240160Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+-----+------+------+\n",
      "|firstname|middlename|lastname|   id|gender|salary|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|    Maria|      Anne|   Jones|39192|     F|  6000|\n",
      "|      Jen|      Mary|   Brown|     |     F|    -1|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4 Compare against a list of allowed values\n",
    "\n",
    "\n",
    "data = [\n",
    "    (\"James\",\"\",\"Smith\",\"36636\",\"M\", 3000),\n",
    "    (\"Michael\",\"Rose\",\"\",\"40288\",\"M\", 4000),\n",
    "    (\"Robert\",\"\",\"Williams\",\"42114\",\"M\", 4000),\n",
    "    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\", 6000),\n",
    "    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\", -1)\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"firstname\",StringType(), True), # Nullable True\n",
    "    StructField(\"middlename\",StringType(), True),\n",
    "    StructField(\"lastname\",StringType(), True),\n",
    "    StructField(\"id\",StringType(), True),\n",
    "    StructField(\"gender\",StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True)\n",
    "    ])\n",
    "df = spark.createDataFrame(data=data,schema=schema)\n",
    "\n",
    "df = (\n",
    "    df.filter(C(\"gender\").isin([\"F\"]))\n",
    ")\n",
    "\n",
    "df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T16:25:34.820524Z",
     "start_time": "2021-03-11T16:25:34.447767Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+-----+------+------+\n",
      "|firstname|middlename|lastname|   id|gender|salary|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|    Maria|      Anne|   Jones|39192|     F|  6000|\n",
      "|   Robert|          |Williams|42114|     M|  4000|\n",
      "|  Michael|      Rose|        |40288|     M|  4000|\n",
      "|    James|          |   Smith|36636|     M|  3000|\n",
      "|      Jen|      Mary|   Brown|     |     F|    -1|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|firstname|middlename|lastname|   id|gender|salary|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|      Jen|      Mary|   Brown|     |     F|    -1|\n",
      "|    James|          |   Smith|36636|     M|  3000|\n",
      "|  Michael|      Rose|        |40288|     M|  4000|\n",
      "|   Robert|          |Williams|42114|     M|  4000|\n",
      "|    Maria|      Anne|   Jones|39192|     F|  6000|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5 Sort result\n",
    "\n",
    "data = [\n",
    "    (\"James\",\"\",\"Smith\",\"36636\",\"M\", 3000),\n",
    "    (\"Michael\",\"Rose\",\"\",\"40288\",\"M\", 4000),\n",
    "    (\"Robert\",\"\",\"Williams\",\"42114\",\"M\", 4000),\n",
    "    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\", 6000),\n",
    "    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\", -1)\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"firstname\",StringType(), True), # Nullable True\n",
    "    StructField(\"middlename\",StringType(), True),\n",
    "    StructField(\"lastname\",StringType(), True),\n",
    "    StructField(\"id\",StringType(), True),\n",
    "    StructField(\"gender\",StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True)\n",
    "    ])\n",
    "df = spark.createDataFrame(data=data,schema=schema)\n",
    "\n",
    "\n",
    "df = df.orderBy(C(\"salary\").desc())\n",
    "\n",
    "df.show(n=5)\n",
    "\n",
    "df = df.orderBy(C(\"salary\").asc())\n",
    "\n",
    "df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T16:25:36.225710Z",
     "start_time": "2021-03-11T16:25:34.822655Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before\n",
      "+----------+-------+--------------------+\n",
      "|article_id|   pred|             img_url|\n",
      "+----------+-------+--------------------+\n",
      "|     67789|0.93422|https://pic.pimg....|\n",
      "|     67789|0.94231|https://pic.pimg....|\n",
      "|     14431|0.97611|https://pic.pimg....|\n",
      "|     14431|0.99834|https://pic.pimg....|\n",
      "|     14431|0.99834|https://pic.pimg....|\n",
      "+----------+-------+--------------------+\n",
      "\n",
      "Sol 1 FAILED SOMETIMES WHEN PARTITION != 1\n",
      "+----------+-------+--------------------+\n",
      "|article_id|   pred|             img_url|\n",
      "+----------+-------+--------------------+\n",
      "|     67789|0.93422|https://pic.pimg....|\n",
      "|     67789|0.94231|https://pic.pimg....|\n",
      "|     14431|0.97611|https://pic.pimg....|\n",
      "|     14431|0.99834|https://pic.pimg....|\n",
      "+----------+-------+--------------------+\n",
      "\n",
      "Sol 2 WORKED WITH ANY PARTITION\n",
      "+----------+-------+--------------------+\n",
      "|article_id|   pred|             img_url|\n",
      "+----------+-------+--------------------+\n",
      "|     67789|0.93422|https://pic.pimg....|\n",
      "|     67789|0.94231|https://pic.pimg....|\n",
      "|     14431|0.97611|https://pic.pimg....|\n",
      "|     14431|0.99834|https://pic.pimg....|\n",
      "+----------+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6 select distinct rows based on certain column but keep first row\n",
    "# In this case, model prediction to filter the same images\n",
    "\n",
    "############## DROP DUPLICATED doesn't work in this case\n",
    "# https://stackoverflow.com/questions/38687212/spark-dataframe-drop-duplicates-and-keep-first\n",
    "data = [\n",
    "    (14431,0.99834,'https://pic.pimg.tw/happy78/1528543947-685380499_n.jpg'),\n",
    "    (14431,0.99834,'https://pic.pimg.tw/happy78/1528543947-362759723_n.jpg'),\n",
    "    (14431,0.97611,'https://pic.pimg.tw/happy78/1528543962-2265924582_n.jpg'),\n",
    "    (67789,0.93422,'https://pic.pimg.tw/happy78/1528543962-4007835890_n.jpg'),\n",
    "    (67789,0.94231,'https://pic.pimg.tw/happy78/1528543962-45890_n.jpg')\n",
    "]\n",
    "\n",
    "columns = ['article_id','pred','img_url']\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=columns)\n",
    "print('before')\n",
    "\n",
    "df.orderBy('pred').show(n=5)\n",
    "\n",
    "print('Sol 1 FAILED SOMETIMES WHEN PARTITION != 1')\n",
    "df_1 = (\n",
    "    df.drop_duplicates(subset=[\"pred\"])\n",
    ")\n",
    "\n",
    "df_1.orderBy('pred').show(n=5)\n",
    "\n",
    "\n",
    "############## Using Window Function and sort, rank, worked!\n",
    "# You can check the 5 th question of Aggregation, The solution is the same\n",
    "\n",
    "print('Sol 2 WORKED WITH ANY PARTITION')\n",
    "df_2 = (\n",
    "    df.withColumn(\"rank_by_pred\",\n",
    "                  F.row_number().over(\n",
    "                  W.partitionBy(\"pred\")\\\n",
    "                      .orderBy(F.desc(\"pred\"))\n",
    "                  )\n",
    "                 )\\\n",
    "    .filter(F.col(\"rank_by_pred\") == 1)\\\n",
    "    .drop('rank_by_pred')\n",
    ")\n",
    "df_2.orderBy('pred').show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregation Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T16:25:36.260786Z",
     "start_time": "2021-03-11T16:25:36.227327Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.group.GroupedData'>\n",
      "\n",
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_df', '_jgd', 'agg', 'apply', 'avg', 'count', 'max', 'mean', 'min', 'pivot', 'sql_ctx', 'sum']\n"
     ]
    }
   ],
   "source": [
    "# 1 knowing the groupby object method\n",
    "\n",
    "data = [\n",
    "    (\"James\",\"\",\"Smith\",\"36636\",\"RD\",\"M\", 3000),\n",
    "    (\"Michael\",\"Rose\",\"\",\"40288\",\"RD\",\"M\", 4000),\n",
    "    (\"Robert\",\"\",\"Williams\",\"42114\",\"SRE\",\"M\", 4000),\n",
    "    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"SRE\",\"F\", 4000),\n",
    "    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",\"BACKEND\", -1)\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"firstname\",StringType(), True), # Nullable True\n",
    "    StructField(\"middlename\",StringType(), True),\n",
    "    StructField(\"lastname\",StringType(), True),\n",
    "    StructField(\"id\",StringType(), True),\n",
    "    StructField(\"apartment\",StringType(), True),\n",
    "    StructField(\"gender\",StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True)\n",
    "    ])\n",
    "df = spark.createDataFrame(data=data,schema=schema)\n",
    "\n",
    "df_grp = df.groupBy(\"salary\")\n",
    "\n",
    "print(type(df_grp), dir(df_grp), sep='\\n\\n')\n",
    "\n",
    "# avg, count, max, mean, sum  - Common aggregation\n",
    "# pivot - two column x, y with value in the table\n",
    "# sql_ctx - apply sql command\n",
    "# custom function - agg, apply\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T16:25:37.038556Z",
     "start_time": "2021-03-11T16:25:36.262636Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|deparment|avg(salary)|\n",
      "+---------+-----------+\n",
      "|        F|       -1.0|\n",
      "|       RD|     3500.0|\n",
      "|      SRE|     4000.0|\n",
      "+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2 apply single aggregation fuction on groupby object\n",
    "data = [\n",
    "    (\"James\",\"\",\"Smith\",\"36636\",\"RD\",\"M\", 3000),\n",
    "    (\"Michael\",\"Rose\",\"\",\"40288\",\"RD\",\"M\", 4000),\n",
    "    (\"Robert\",\"\",\"Williams\",\"42114\",\"SRE\",\"M\", 4000),\n",
    "    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"SRE\",\"F\", 4000),\n",
    "    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",\"BACKEND\", -1)\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"firstname\",StringType(), True), # Nullable True\n",
    "    StructField(\"middlename\",StringType(), True),\n",
    "    StructField(\"lastname\",StringType(), True),\n",
    "    StructField(\"id\",StringType(), True),\n",
    "    StructField(\"deparment\",StringType(), True),\n",
    "    StructField(\"gender\",StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True)\n",
    "    ])\n",
    "df = spark.createDataFrame(data=data,schema=schema)\n",
    "\n",
    "df_grp_department = (\n",
    "    df.groupby(\"deparment\").mean(\"salary\").alias(\"mean_salary\")\n",
    ")\n",
    "\n",
    "df_grp_department.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T16:25:37.684922Z",
     "start_time": "2021-03-11T16:25:37.041481Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+----------+----------+----------+----------+\n",
      "|deparment|sum_salary|avg_salary|max_salary|min_salary|group_size|\n",
      "+---------+----------+----------+----------+----------+----------+\n",
      "|        F|        -1|      -1.0|        -1|        -1|         1|\n",
      "|       RD|      7000|    3500.0|      4000|      3000|         2|\n",
      "|      SRE|      8000|    4000.0|      4000|      4000|         2|\n",
      "+---------+----------+----------+----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3 apply multiple aggregation fuction on groupby object\n",
    "data = [\n",
    "    (\"James\",\"\",\"Smith\",\"36636\",\"RD\",\"M\", 3000),\n",
    "    (\"Michael\",\"Rose\",\"\",\"40288\",\"RD\",\"M\", 4000),\n",
    "    (\"Robert\",\"\",\"Williams\",\"42114\",\"SRE\",\"M\", 4000),\n",
    "    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"SRE\",\"F\", 4000),\n",
    "    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",\"BACKEND\", -1)\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"firstname\",StringType(), True), # Nullable True\n",
    "    StructField(\"middlename\",StringType(), True),\n",
    "    StructField(\"lastname\",StringType(), True),\n",
    "    StructField(\"id\",StringType(), True),\n",
    "    StructField(\"deparment\",StringType(), True),\n",
    "    StructField(\"gender\",StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True)\n",
    "    ])\n",
    "df = spark.createDataFrame(data=data,schema=schema)\n",
    "\n",
    "df_grp_department = df.groupby(\"deparment\").agg(\n",
    "    F.sum(\"salary\").alias(\"sum_salary\"),\n",
    "    F.avg(\"salary\").alias(\"avg_salary\"),\n",
    "    F.max(\"salary\").alias(\"max_salary\"),\n",
    "    F.min(\"salary\").alias(\"min_salary\"),\n",
    "    F.count(\"salary\").alias(\"group_size\")\n",
    ")\n",
    "\n",
    "df_grp_department.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T16:25:38.103502Z",
     "start_time": "2021-03-11T16:25:37.686405Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>deparment</th>\n",
       "      <th>sum_salary</th>\n",
       "      <th>avg_salary</th>\n",
       "      <th>max_salary</th>\n",
       "      <th>min_salary</th>\n",
       "      <th>count_rows</th>\n",
       "      <th>all_rows</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>F</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>[-1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RD</td>\n",
       "      <td>7000</td>\n",
       "      <td>3500.0</td>\n",
       "      <td>4000</td>\n",
       "      <td>3000</td>\n",
       "      <td>2</td>\n",
       "      <td>[3000, 4000]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SRE</td>\n",
       "      <td>8000</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>4000</td>\n",
       "      <td>4000</td>\n",
       "      <td>2</td>\n",
       "      <td>[4000, 4000]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  deparment  sum_salary  avg_salary  max_salary  min_salary  count_rows  \\\n",
       "0         F          -1        -1.0          -1          -1           1   \n",
       "1        RD        7000      3500.0        4000        3000           2   \n",
       "2       SRE        8000      4000.0        4000        4000           2   \n",
       "\n",
       "       all_rows  \n",
       "0          [-1]  \n",
       "1  [3000, 4000]  \n",
       "2  [4000, 4000]  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4 collect data point for each group with the stats(min, max, sum, avg, count)\n",
    "\n",
    "\n",
    "# apply multiple aggregation fuction on groupby object\n",
    "data = [\n",
    "    (\"James\",\"\",\"Smith\",\"36636\",\"RD\",\"M\", 3000),\n",
    "    (\"Michael\",\"Rose\",\"\",\"40288\",\"RD\",\"M\", 4000),\n",
    "    (\"Robert\",\"\",\"Williams\",\"42114\",\"SRE\",\"M\", 4000),\n",
    "    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"SRE\",\"F\", 4000),\n",
    "    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",\"BACKEND\", -1)\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"firstname\",StringType(), True), # Nullable True\n",
    "    StructField(\"middlename\",StringType(), True),\n",
    "    StructField(\"lastname\",StringType(), True),\n",
    "    StructField(\"id\",StringType(), True),\n",
    "    StructField(\"deparment\",StringType(), True),\n",
    "    StructField(\"gender\",StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True)\n",
    "    ])\n",
    "df = spark.createDataFrame(data=data,schema=schema)\n",
    "\n",
    "df_grp_department = df.groupby(\"deparment\").agg(\n",
    "    F.sum(\"salary\").alias(\"sum_salary\"),\n",
    "    F.avg(\"salary\").alias(\"avg_salary\"),\n",
    "    F.max(\"salary\").alias(\"max_salary\"),\n",
    "    F.min(\"salary\").alias(\"min_salary\"),\n",
    "    F.count(\"salary\").alias(\"count_rows\"),\n",
    "    F.collect_list(\"salary\").alias(\"all_rows\")\n",
    ")\n",
    "\n",
    "df_grp_department.toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T16:25:38.749101Z",
     "start_time": "2021-03-11T16:25:38.104920Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+-----+---------+-------+------+\n",
      "|firstname|middlename|lastname|   id|deparment| gender|salary|\n",
      "+---------+----------+--------+-----+---------+-------+------+\n",
      "|      Jen|      Mary|   Brown|     |        F|BACKEND|    -1|\n",
      "|  Michael|      Rose|        |40288|       RD|      M|  8000|\n",
      "|    Maria|      Anne|   Jones|39192|      SRE|      F|  6000|\n",
      "+---------+----------+--------+-----+---------+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5 get first one row in each group\n",
    "# We use Window Function here\n",
    "# Key to think about this, we rank the data in each group, then \n",
    "# filtering\n",
    "# no nothing is groupby\n",
    "# which is different in pandas\n",
    "\n",
    "data = [\n",
    "    (\"James\",\"\",\"Smith\",\"36636\",\"RD\",\"M\", 3000),\n",
    "    (\"Michael\",\"Rose\",\"\",\"40288\",\"RD\",\"M\", 8000),\n",
    "    (\"Robert\",\"\",\"Williams\",\"42114\",\"SRE\",\"M\", 4000),\n",
    "    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"SRE\",\"F\", 6000),\n",
    "    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",\"BACKEND\", -1)\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"firstname\",StringType(), True), # Nullable True\n",
    "    StructField(\"middlename\",StringType(), True),\n",
    "    StructField(\"lastname\",StringType(), True),\n",
    "    StructField(\"id\",StringType(), True),\n",
    "    StructField(\"deparment\",StringType(), True),\n",
    "    StructField(\"gender\",StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True)\n",
    "    ])\n",
    "\n",
    "\n",
    "df = spark.createDataFrame(data=data,schema=schema)\n",
    "\n",
    "df = (\n",
    "    df.withColumn(\"rank_salary_by_deparment\",\n",
    "                  F.row_number().over(\n",
    "                  W.partitionBy(\"deparment\")\\\n",
    "                      .orderBy(F.desc(\"salary\"))\n",
    "                  )\n",
    "                 )\\\n",
    "    .filter(F.col(\"rank_salary_by_deparment\") == 1)\\\n",
    "    .drop('rank_salary_by_deparment')\n",
    ")\n",
    "\n",
    "df.show(n=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T16:25:39.405474Z",
     "start_time": "2021-03-11T16:25:38.751143Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+----------+----------+----------+----------+\n",
      "|deparment|sum_salary|avg_salary|max_salary|min_salary|count_rows|\n",
      "+---------+----------+----------+----------+----------+----------+\n",
      "|       RD|      7000|    3500.0|      4000|      3000|         2|\n",
      "|      SRE|      8000|    4000.0|      4000|      4000|         2|\n",
      "+---------+----------+----------+----------+----------+----------+\n",
      "\n",
      "root\n",
      " |-- deparment: string (nullable = true)\n",
      " |-- sum_salary: long (nullable = true)\n",
      " |-- avg_salary: double (nullable = true)\n",
      " |-- max_salary: integer (nullable = true)\n",
      " |-- min_salary: integer (nullable = true)\n",
      " |-- count_rows: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6 groupby and filtering\n",
    "\n",
    "data = [\n",
    "    (\"James\",\"\",\"Smith\",\"36636\",\"RD\",\"M\", 3000),\n",
    "    (\"Michael\",\"Rose\",\"\",\"40288\",\"RD\",\"M\", 4000),\n",
    "    (\"Robert\",\"\",\"Williams\",\"42114\",\"SRE\",\"M\", 4000),\n",
    "    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"SRE\",\"F\", 4000),\n",
    "    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",\"BACKEND\", -1)\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"firstname\",StringType(), True), # Nullable True\n",
    "    StructField(\"middlename\",StringType(), True),\n",
    "    StructField(\"lastname\",StringType(), True),\n",
    "    StructField(\"id\",StringType(), True),\n",
    "    StructField(\"deparment\",StringType(), True),\n",
    "    StructField(\"gender\",StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True)\n",
    "    ])\n",
    "df = spark.createDataFrame(data=data,schema=schema)\n",
    "\n",
    "df_grp_department = (\n",
    "    df.groupby(\"deparment\").agg(\n",
    "        F.sum(\"salary\").alias(\"sum_salary\"),\n",
    "        F.avg(\"salary\").alias(\"avg_salary\"),\n",
    "        F.max(\"salary\").alias(\"max_salary\"),\n",
    "        F.min(\"salary\").alias(\"min_salary\"),\n",
    "        F.count(\"salary\").alias(\"count_rows\"))\n",
    "    .filter(C(\"sum_salary\") > 0)\n",
    "    \n",
    ")\n",
    "\n",
    "df_grp_department.show(n=5)\n",
    "df_grp_department.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T16:25:40.194819Z",
     "start_time": "2021-03-11T16:25:39.407045Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|item|score|\n",
      "+----+-----+\n",
      "|   a|   10|\n",
      "|   a|   10|\n",
      "|   a|   20|\n",
      "+----+-----+\n",
      "\n",
      "+----+-----+----+----------+----------+\n",
      "|item|score|rank|dense_rank|row_number|\n",
      "+----+-----+----+----------+----------+\n",
      "|   a|   10|   1|         1|         1|\n",
      "|   a|   10|   1|         1|         2|\n",
      "|   a|   20|   3|         2|         3|\n",
      "+----+-----+----+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 7 rank, dense_rank, and row_number\n",
    "# https://stackoverflow.com/questions/44968912/difference-in-dense-rank-and-row-number-in-spark\n",
    "# The window functions\n",
    "\n",
    "data = [\n",
    "    (\"a\",10),\n",
    "    ('a',10),\n",
    "    ('a',20)\n",
    "]\n",
    "\n",
    "columns = ['item','score']\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show(n=5)\n",
    "\n",
    "window_spec = W.partitionBy(\"item\").orderBy(\"score\")\n",
    "df = (\n",
    "    df.withColumn(\"rank\", F.rank().over(window_spec))\\\n",
    "    .withColumn(\"dense_rank\", F.dense_rank().over(window_spec))\\\n",
    "    .withColumn(\"row_number\", F.row_number().over(window_spec))\n",
    ")\n",
    "\n",
    "df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T16:35:11.582075Z",
     "start_time": "2021-03-11T16:35:11.144362Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----+\n",
      "| id|key|value|\n",
      "+---+---+-----+\n",
      "|  1|  a|  123|\n",
      "|  1|  b|  234|\n",
      "|  1|  c|  345|\n",
      "|  2|  a|   12|\n",
      "|  2|  x|   23|\n",
      "+---+---+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- collections: array (nullable = true)\n",
      " |    |-- element: map (containsNull = true)\n",
      " |    |    |-- key: string\n",
      " |    |    |-- value: long (valueContainsNull = true)\n",
      "\n",
      "+---+------------------------------------+\n",
      "|id |collections                         |\n",
      "+---+------------------------------------+\n",
      "|1  |[[a -> 123], [b -> 234], [c -> 345]]|\n",
      "|2  |[[a -> 12], [x -> 23], [y -> 123]]  |\n",
      "+---+------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# collect dict (map) with a group\n",
    "# https://stackoverflow.com/questions/55308482/pyspark-create-dictionary-within-groupby\n",
    "\n",
    "# collect_list : return a list of objects with duplicated\n",
    "# collect_set : return a set of objects without duplicated\n",
    "# struct : create a new struct column\n",
    "# ( > 2.4.0)map_from_entries : returns a map created from the given array of entries\n",
    "# create_map\n",
    "\n",
    "######### pyspark < 2.4.0\n",
    "data = [\n",
    "    (1,'a',123),\n",
    "    (1,'b',234),\n",
    "    (1,'c',345),\n",
    "    (2,'a',12),\n",
    "    (2,'x',23),\n",
    "    (2,'y',123)\n",
    "]\n",
    "\n",
    "columns = ['id','key','value']\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show(n=5)\n",
    "\n",
    "@F.udf(returnType=T.MapType(StringType(), StringType()))\n",
    "def map_array(col):\n",
    "    return dict(column)\n",
    "\n",
    "######## pyspark < 2.4.0\n",
    "\n",
    "df_agg = df.groupBy(\"id\").agg(\n",
    "    F.collect_list(F.create_map(C(\"key\"),C(\"value\"))).alias('collections')\n",
    ")\n",
    "\n",
    "df_agg.printSchema()\n",
    "df_agg.show(n=10, truncate=False)\n",
    "df_agg.collect()\n",
    "\n",
    "######### pyspark > 2.4.0\n",
    "# df.groupBy(\"id\").agg(\n",
    "#     F.map_from_entries(\n",
    "#         F.collect_list(\n",
    "#             F.struct(\"key\",\"value\"))).alias(\"key_value\")\n",
    "# ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T16:28:51.029092Z",
     "start_time": "2021-03-11T16:28:50.878463Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+------------------------+---------------+---------+\n",
      "|store_name|food_category|food_category_popularity|        img_url|author_id|\n",
      "+----------+-------------+------------------------+---------------+---------+\n",
      "|    hotpop|         Meat|                       3|https//:123.png|   rtyg11|\n",
      "|    hotpop|         Meat|                       3|https//:456.png|   rtyg11|\n",
      "|    hotpop|         Meat|                       3|https//:789.png|   rtyg11|\n",
      "|    hotpop|    Vegetable|                       2|https//:111.png|   rtyg11|\n",
      "|    hotpop|    Vegetable|                       2|https//:222.png|   rtyg11|\n",
      "|    branch|   Fried food|                       1|https//:333.png|     bvc1|\n",
      "|    branch|      Dessert|                       1|https//:444.png|     7854|\n",
      "+----------+-------------+------------------------+---------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 9 collect dict (map) within a group\n",
    "\n",
    "data = [\n",
    "    (\"hotpop\",\"Meat\",3,\"https//:123.png\",\"rtyg11\"),\n",
    "    (\"hotpop\",\"Meat\",3,\"https//:456.png\",\"rtyg11\"),\n",
    "    (\"hotpop\",\"Meat\",3,\"https//:789.png\",\"rtyg11\"),\n",
    "    (\"hotpop\",\"Vegetable\",2,\"https//:111.png\",\"rtyg11\"),\n",
    "    (\"hotpop\",\"Vegetable\",2,\"https//:222.png\",\"rtyg11\"),\n",
    "    (\"branch\",\"Fried food\",1,\"https//:333.png\",\"bvc1\"),\n",
    "    (\"branch\",\"Dessert\",1,\"https//:444.png\",\"7854\"),\n",
    "  ]\n",
    "\n",
    "columns = [\"store_name\",\"food_category\",\"food_category_popularity\",\"img_url\",\"author_id\"]\n",
    "df = spark.createDataFrame(data = data, schema = columns)\n",
    "\n",
    "df.show(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T16:25:41.600226Z",
     "start_time": "2021-03-11T16:25:40.384785Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before\n",
      "+----------+--------------------+\n",
      "|article_id|             img_url|\n",
      "+----------+--------------------+\n",
      "|     14431|https://pic.pimg....|\n",
      "|     14431|https://pic.pimg....|\n",
      "|     14431|https://pic.pimg....|\n",
      "|     67789|https://pic.pimg....|\n",
      "|     67789|https://pic.pimg....|\n",
      "+----------+--------------------+\n",
      "\n",
      "+----------+--------------------+-----------+\n",
      "|article_id|             img_url|img_b64_str|\n",
      "+----------+--------------------+-----------+\n",
      "|     14431|https://pic.pimg....| [B@4b4b03f|\n",
      "|     14431|https://pic.pimg....|[B@583a1b56|\n",
      "|     14431|https://pic.pimg....|[B@506c53fb|\n",
      "|     67789|https://pic.pimg....|[B@66bc14ef|\n",
      "|     67789|https://pic.pimg....|       null|\n",
      "+----------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Use Pyspark to send request, get image and store as b64string \n",
    "# https://stackoverflow.com/questions/49353752/use-requests-module-and-return-response-to-pyspark-dataframe\n",
    "\n",
    "data = [\n",
    "    (14431,'https://pic.pimg.tw/happy78/1528543947-685380499_n.jpg'),\n",
    "    (14431,'https://pic.pimg.tw/happy78/1528543947-362759723_n.jpg'),\n",
    "    (14431,'https://pic.pimg.tw/happy78/1528543962-2265924582_n.jpg'),\n",
    "    (67789,'https://pic.pimg.tw/happy78/1528543962-4007835890_n.jpg'),\n",
    "    (67789,'https://pic.pimg.tw/happy78/1528543962-45890_n.jpg')\n",
    "]\n",
    "\n",
    "columns = ['article_id','img_url']\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=columns)\n",
    "print('before')\n",
    "df.show(n=5)\n",
    "\n",
    "@F.udf(returnType=StringType())\n",
    "def get_img_binary(url : str) -> str:\n",
    "    import requests\n",
    "    import base64\n",
    "    resp = requests.get(url)\n",
    "    if resp.status_code == 200:\n",
    "        return base64.encodestring(resp.content)\n",
    "    else:\n",
    "        return None\n",
    "df = (\n",
    "    df.withColumn(\"img_b64_str\", get_img_binary(C(\"img_url\")))\n",
    ")\n",
    "\n",
    "df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T16:25:41.921990Z",
     "start_time": "2021-03-11T16:25:41.603099Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before : \n",
      "+---+--------+\n",
      "| id|features|\n",
      "+---+--------+\n",
      "|  1|      64|\n",
      "|  2|      76|\n",
      "|  3|      54|\n",
      "|  4|      11|\n",
      "|  5|     100|\n",
      "+---+--------+\n",
      "\n",
      "+---+--------+--------+----------+\n",
      "|id |features|category|prob      |\n",
      "+---+--------+--------+----------+\n",
      "|1  |64      |food    |0.5524358 |\n",
      "|2  |76      |env     |0.2172371 |\n",
      "|3  |54      |env     |0.94519746|\n",
      "|4  |11      |drink   |0.54831946|\n",
      "|5  |100     |env     |0.54297906|\n",
      "+---+--------+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# udf return two column values, e.g. model prediction with label and probability\n",
    "\n",
    "data = [\n",
    "    (1,64),\n",
    "    (2,76),\n",
    "    (3,54),\n",
    "    (4,11),\n",
    "    (5,100),\n",
    "]\n",
    "columns = ['id','features']\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=columns)\n",
    "print(\"Before : \")\n",
    "df.show(n=5)\n",
    "\n",
    "############# sol #################\n",
    "# using Row object to return multiple column\n",
    "from pyspark.sql import Row\n",
    "\n",
    "model_pred = StructType([\n",
    "    StructField(\"category\", StringType(), False),\n",
    "    StructField(\"prob\", T.FloatType(), False)])\n",
    "\n",
    "@F.udf(returnType=model_pred)\n",
    "def model_pred(n):\n",
    "    import random\n",
    "    category = random.choice(['food','env','compose','drink'])\n",
    "    prob = random.random()\n",
    "    return Row('category', 'prob')(category, prob)\n",
    "\n",
    "\n",
    "newDF = df.withColumn(\"pred\", model_pred(df[\"features\"]))\n",
    "newDF = newDF.select(\"id\", \"features\", \"pred.*\")\n",
    "\n",
    "newDF.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T16:25:41.937319Z",
     "start_time": "2021-03-11T16:25:41.924149Z"
    }
   },
   "outputs": [],
   "source": [
    "# 2. Use Pyspark to load a tf.keras model\n",
    "# serieslize the model and make prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T16:25:42.119488Z",
     "start_time": "2021-03-11T16:25:41.938580Z"
    }
   },
   "outputs": [],
   "source": [
    "# 3. Pandas udf\n",
    "# documentation and concept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# broadcasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T16:25:42.791473Z",
     "start_time": "2021-03-11T16:25:42.122610Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-------+-----+\n",
      "|firstname|lastname|country|state|\n",
      "+---------+--------+-------+-----+\n",
      "|    James|   Smith|    USA|   CA|\n",
      "|  Michael|    Rose|    USA|   NY|\n",
      "|   Robert|Williams|    USA|   CA|\n",
      "|    Maria|   Jones|    USA|   FL|\n",
      "+---------+--------+-------+-----+\n",
      "\n",
      "+---------+--------+-------+----------+\n",
      "|firstname|lastname|country|     state|\n",
      "+---------+--------+-------+----------+\n",
      "|    James|   Smith|    USA|California|\n",
      "|  Michael|    Rose|    USA|  New York|\n",
      "|   Robert|Williams|    USA|California|\n",
      "|    Maria|   Jones|    USA|   Florida|\n",
      "+---------+--------+-------+----------+\n",
      "\n",
      "+---------+--------+-------+-----+---------------+\n",
      "|firstname|lastname|country|state|converted_state|\n",
      "+---------+--------+-------+-----+---------------+\n",
      "|    James|   Smith|    USA|   CA|     California|\n",
      "|  Michael|    Rose|    USA|   NY|       New York|\n",
      "|   Robert|Williams|    USA|   CA|     California|\n",
      "|    Maria|   Jones|    USA|   FL|        Florida|\n",
      "+---------+--------+-------+-----+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# broadcast the dictionary to spark \n",
    "# (which is a way that enhance multi-processing cross machine using your python code)\n",
    "# the broadcast variable should be serializable\n",
    "\n",
    "states = {\"NY\":\"New York\", \"CA\":\"California\", \"FL\":\"Florida\"}\n",
    "broadcastStates = spark.sparkContext.broadcast(states)\n",
    "\n",
    "data = [(\"James\",\"Smith\",\"USA\",\"CA\"),\n",
    "    (\"Michael\",\"Rose\",\"USA\",\"NY\"),\n",
    "    (\"Robert\",\"Williams\",\"USA\",\"CA\"),\n",
    "    (\"Maria\",\"Jones\",\"USA\",\"FL\")\n",
    "  ]\n",
    "\n",
    "columns = [\"firstname\",\"lastname\",\"country\",\"state\"]\n",
    "df = spark.createDataFrame(data = data, schema = columns)\n",
    "df.show(n=5)\n",
    "\n",
    "def state_convert(code):\n",
    "    return broadcastStates.value[code]\n",
    "\n",
    "# case 1, using rdd\n",
    "result_rdd = df.rdd.map(lambda x: (x[0],x[1],x[2],state_convert(x[3]))).toDF(columns)\n",
    "result_rdd.show(n=5)\n",
    "\n",
    "\n",
    "# case 2, using pdf\n",
    "\n",
    "@F.udf(returnType=StringType())\n",
    "def state_convert_udf(code : str) -> str:\n",
    "    return broadcastStates.value[code]\n",
    "\n",
    "result_df = (\n",
    "    df.withColumn(\"converted_state\", state_convert_udf(C(\"state\")))\n",
    ")\n",
    "\n",
    "result_df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T16:25:42.818306Z",
     "start_time": "2021-03-11T16:25:42.795317Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.broadcast.Broadcast'> ['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_jbroadcast', '_path', '_pickle_registry', '_python_broadcast', '_sc', 'destroy', 'dump', 'load', 'load_from_path', 'unpersist', 'value']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'NY': 'New York', 'CA': 'California', 'FL': 'Florida'}, dict)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Knowing broacsting object\n",
    "\n",
    "# broadcast the dictionary to spark \n",
    "# (which is a way that enhance multi-processing cross machine using your python code)\n",
    "# the broadcast variable should be serializable\n",
    "# https://spark.apache.org/docs/2.3.3/api/python/_modules/pyspark/broadcast.html\n",
    "states = {\"NY\":\"New York\", \"CA\":\"California\", \"FL\":\"Florida\"}\n",
    "broadcastStates = spark.sparkContext.broadcast(states)\n",
    "print(type(broadcastStates), dir(broadcastStates))\n",
    "\n",
    "# value to access the object\n",
    "broadcastStates.value, type(broadcastStates.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T16:25:44.496240Z",
     "start_time": "2021-03-11T16:25:42.820369Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before\n",
      "+----------+-------------+------------------------+\n",
      "|store_name|food_category|food_category_popularity|\n",
      "+----------+-------------+------------------------+\n",
      "|    hotpop|         Meat|                       3|\n",
      "|    hotpop|         Meat|                       3|\n",
      "|    hotpop|         Meat|                       3|\n",
      "|    hotpop|    Vegetable|                       2|\n",
      "|    hotpop|    Vegetable|                       2|\n",
      "|    branch|   Fried food|                       1|\n",
      "|    branch|      Dessert|                       1|\n",
      "+----------+-------------+------------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>store_name</th>\n",
       "      <th>food_category</th>\n",
       "      <th>food_category_popularity</th>\n",
       "      <th>cat_idx</th>\n",
       "      <th>store_cat_pop_rank</th>\n",
       "      <th>store_cat_pop_rank_score</th>\n",
       "      <th>mix_cat_pop_rank_score</th>\n",
       "      <th>mix_cat_pop_rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hotpop</td>\n",
       "      <td>Meat</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hotpop</td>\n",
       "      <td>Vegetable</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hotpop</td>\n",
       "      <td>Meat</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hotpop</td>\n",
       "      <td>Vegetable</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2.1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hotpop</td>\n",
       "      <td>Meat</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>branch</td>\n",
       "      <td>Dessert</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>branch</td>\n",
       "      <td>Fried food</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  store_name food_category  food_category_popularity  cat_idx  \\\n",
       "0     hotpop          Meat                         3        1   \n",
       "1     hotpop     Vegetable                         2        1   \n",
       "2     hotpop          Meat                         3        2   \n",
       "3     hotpop     Vegetable                         2        2   \n",
       "4     hotpop          Meat                         3        3   \n",
       "5     branch       Dessert                         1        1   \n",
       "6     branch    Fried food                         1        1   \n",
       "\n",
       "   store_cat_pop_rank  store_cat_pop_rank_score  mix_cat_pop_rank_score  \\\n",
       "0                   1                       0.0                     1.0   \n",
       "1                   2                       0.1                     1.1   \n",
       "2                   1                       0.0                     2.0   \n",
       "3                   2                       0.1                     2.1   \n",
       "4                   1                       0.0                     3.0   \n",
       "5                   1                       0.0                     1.0   \n",
       "6                   1                       0.0                     1.0   \n",
       "\n",
       "   mix_cat_pop_rank  \n",
       "0                 1  \n",
       "1                 2  \n",
       "2                 3  \n",
       "3                 4  \n",
       "4                 5  \n",
       "5                 1  \n",
       "6                 2  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1\n",
    "# rank the food category popularity by store_name but crossed and rotated\n",
    "\n",
    "# create a mix ranking number\n",
    "# create a popularity_rank_score in each store_name\n",
    "    # Top popular in each store_name -> score 0\n",
    "    # Second popular in each store_name -> score 0.1\n",
    "# Add row number in each store_name, food_category -> cat_rank\n",
    "# Create category_popularity_mix_rank_score = cat_rank + popularity_rank_score\n",
    "# Sort the category_popularity_mix_rank_score by store_name\n",
    "\n",
    "data = [\n",
    "    (\"hotpop\",\"Meat\",3,),\n",
    "    (\"hotpop\",\"Meat\",3),\n",
    "    (\"hotpop\",\"Meat\",3),\n",
    "    (\"hotpop\",\"Vegetable\",2),\n",
    "    (\"hotpop\",\"Vegetable\",2),\n",
    "    (\"branch\",\"Fried food\",1),\n",
    "    (\"branch\",\"Dessert\",1),\n",
    "  ]\n",
    "\n",
    "columns = [\"store_name\",\"food_category\",\"food_category_popularity\"]\n",
    "df = spark.createDataFrame(data = data, schema = columns)\n",
    "\n",
    "print('before')\n",
    "df.show(n=10)\n",
    "\n",
    "################# sol #######################\n",
    "store_cat_pop_rank_score = F.when(C(\"store_cat_pop_rank\") == 1, 0)\\\n",
    "                            .when(C(\"store_cat_pop_rank\") == 2, 0.1)\n",
    "\n",
    "window_sotre_cat_pop = W.partitionBy('store_name').orderBy(C(\"food_category_popularity\").desc())\n",
    "window_sotre_cat = W.partitionBy(['store_name','food_category']).orderBy(C(\"food_category\"))\n",
    "window_sotr_cat_mix_rank = W.partitionBy(['store_name']).orderBy(C(\"mix_cat_pop_rank_score\"))\n",
    "df = (\n",
    "    df.withColumn(\"cat_idx\", F.row_number().over(window_sotre_cat))\\\n",
    "      .withColumn(\"store_cat_pop_rank\", F.dense_rank().over(window_sotre_cat_pop))\n",
    "      .withColumn(\"store_cat_pop_rank_score\", store_cat_pop_rank_score)\\\n",
    "      .withColumn(\"mix_cat_pop_rank_score\", C(\"cat_idx\") + C(\"store_cat_pop_rank_score\"))\\\n",
    "      .withColumn(\"mix_cat_pop_rank\", F.row_number().over(window_sotr_cat_mix_rank))\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    ")\n",
    "df.toPandas()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T16:31:32.684679Z",
     "start_time": "2021-03-11T16:31:32.662203Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AutoBatchedSerializer',\n",
       " 'Column',\n",
       " 'DataFrame',\n",
       " 'DataType',\n",
       " 'PandasUDFType',\n",
       " 'PickleSerializer',\n",
       " 'PythonEvalType',\n",
       " 'SparkContext',\n",
       " 'StringType',\n",
       " 'UserDefinedFunction',\n",
       " '__all__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__spec__',\n",
       " '_binary_mathfunctions',\n",
       " '_collect_list_doc',\n",
       " '_collect_set_doc',\n",
       " '_create_binary_mathfunction',\n",
       " '_create_function',\n",
       " '_create_udf',\n",
       " '_create_window_function',\n",
       " '_functions',\n",
       " '_functions_1_4',\n",
       " '_functions_1_6',\n",
       " '_functions_2_1',\n",
       " '_functions_deprecated',\n",
       " '_lit_doc',\n",
       " '_message',\n",
       " '_string_functions',\n",
       " '_test',\n",
       " '_to_java_column',\n",
       " '_to_seq',\n",
       " '_window_functions',\n",
       " '_wrap_deprecated_function',\n",
       " 'abs',\n",
       " 'acos',\n",
       " 'add_months',\n",
       " 'approxCountDistinct',\n",
       " 'approx_count_distinct',\n",
       " 'array',\n",
       " 'array_contains',\n",
       " 'asc',\n",
       " 'ascii',\n",
       " 'asin',\n",
       " 'atan',\n",
       " 'atan2',\n",
       " 'avg',\n",
       " 'base64',\n",
       " 'bin',\n",
       " 'bitwiseNOT',\n",
       " 'blacklist',\n",
       " 'broadcast',\n",
       " 'bround',\n",
       " 'cbrt',\n",
       " 'ceil',\n",
       " 'coalesce',\n",
       " 'col',\n",
       " 'collect_list',\n",
       " 'collect_set',\n",
       " 'column',\n",
       " 'concat',\n",
       " 'concat_ws',\n",
       " 'conv',\n",
       " 'corr',\n",
       " 'cos',\n",
       " 'cosh',\n",
       " 'count',\n",
       " 'countDistinct',\n",
       " 'covar_pop',\n",
       " 'covar_samp',\n",
       " 'crc32',\n",
       " 'create_map',\n",
       " 'cume_dist',\n",
       " 'current_date',\n",
       " 'current_timestamp',\n",
       " 'date_add',\n",
       " 'date_format',\n",
       " 'date_sub',\n",
       " 'date_trunc',\n",
       " 'datediff',\n",
       " 'dayofmonth',\n",
       " 'dayofweek',\n",
       " 'dayofyear',\n",
       " 'decode',\n",
       " 'degrees',\n",
       " 'dense_rank',\n",
       " 'desc',\n",
       " 'encode',\n",
       " 'exp',\n",
       " 'explode',\n",
       " 'explode_outer',\n",
       " 'expm1',\n",
       " 'expr',\n",
       " 'factorial',\n",
       " 'first',\n",
       " 'floor',\n",
       " 'format_number',\n",
       " 'format_string',\n",
       " 'from_json',\n",
       " 'from_unixtime',\n",
       " 'from_utc_timestamp',\n",
       " 'functools',\n",
       " 'get_json_object',\n",
       " 'greatest',\n",
       " 'grouping',\n",
       " 'grouping_id',\n",
       " 'hash',\n",
       " 'hex',\n",
       " 'hour',\n",
       " 'hypot',\n",
       " 'ignore_unicode_prefix',\n",
       " 'initcap',\n",
       " 'input_file_name',\n",
       " 'instr',\n",
       " 'isnan',\n",
       " 'isnull',\n",
       " 'json_tuple',\n",
       " 'kurtosis',\n",
       " 'lag',\n",
       " 'last',\n",
       " 'last_day',\n",
       " 'lead',\n",
       " 'least',\n",
       " 'length',\n",
       " 'levenshtein',\n",
       " 'lit',\n",
       " 'locate',\n",
       " 'log',\n",
       " 'log10',\n",
       " 'log1p',\n",
       " 'log2',\n",
       " 'lower',\n",
       " 'lpad',\n",
       " 'ltrim',\n",
       " 'map_keys',\n",
       " 'map_values',\n",
       " 'math',\n",
       " 'max',\n",
       " 'md5',\n",
       " 'mean',\n",
       " 'min',\n",
       " 'minute',\n",
       " 'monotonically_increasing_id',\n",
       " 'month',\n",
       " 'months_between',\n",
       " 'nanvl',\n",
       " 'next_day',\n",
       " 'ntile',\n",
       " 'pandas_udf',\n",
       " 'percent_rank',\n",
       " 'posexplode',\n",
       " 'posexplode_outer',\n",
       " 'pow',\n",
       " 'quarter',\n",
       " 'radians',\n",
       " 'rand',\n",
       " 'randn',\n",
       " 'rank',\n",
       " 'regexp_extract',\n",
       " 'regexp_replace',\n",
       " 'repeat',\n",
       " 'reverse',\n",
       " 'rint',\n",
       " 'round',\n",
       " 'row_number',\n",
       " 'rpad',\n",
       " 'rtrim',\n",
       " 'second',\n",
       " 'sha1',\n",
       " 'sha2',\n",
       " 'shiftLeft',\n",
       " 'shiftRight',\n",
       " 'shiftRightUnsigned',\n",
       " 'signum',\n",
       " 'sin',\n",
       " 'since',\n",
       " 'sinh',\n",
       " 'size',\n",
       " 'skewness',\n",
       " 'sort_array',\n",
       " 'soundex',\n",
       " 'spark_partition_id',\n",
       " 'split',\n",
       " 'sqrt',\n",
       " 'stddev',\n",
       " 'stddev_pop',\n",
       " 'stddev_samp',\n",
       " 'struct',\n",
       " 'substring',\n",
       " 'substring_index',\n",
       " 'sum',\n",
       " 'sumDistinct',\n",
       " 'sys',\n",
       " 'tan',\n",
       " 'tanh',\n",
       " 'toDegrees',\n",
       " 'toRadians',\n",
       " 'to_date',\n",
       " 'to_json',\n",
       " 'to_timestamp',\n",
       " 'to_utc_timestamp',\n",
       " 'translate',\n",
       " 'trim',\n",
       " 'trunc',\n",
       " 'udf',\n",
       " 'unbase64',\n",
       " 'unhex',\n",
       " 'unix_timestamp',\n",
       " 'upper',\n",
       " 'var_pop',\n",
       " 'var_samp',\n",
       " 'variance',\n",
       " 'warnings',\n",
       " 'weekofyear',\n",
       " 'when',\n",
       " 'window',\n",
       " 'year']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2 Knowing the functions \n",
    "\n",
    "dir(F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pixlake",
   "language": "python",
   "name": "pixlake"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "247px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
