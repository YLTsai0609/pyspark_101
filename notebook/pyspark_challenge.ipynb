{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T16:44:51.440978Z",
     "start_time": "2021-06-25T16:44:51.437991Z"
    }
   },
   "outputs": [],
   "source": [
    "# total : 58 problem and solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T16:44:51.555156Z",
     "start_time": "2021-06-25T16:44:51.443297Z"
    }
   },
   "outputs": [],
   "source": [
    "# env : pixlake\n",
    "# we focuing on pyspark dataframe processing\n",
    "# documentation https://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T16:44:51.684080Z",
     "start_time": "2021-06-25T16:44:51.557557Z"
    }
   },
   "outputs": [],
   "source": [
    "# make you auto compeletion faster\n",
    "# https://stackoverflow.com/questions/40536560/ipython-and-jupyter-autocomplete-not-working\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T16:44:51.794945Z",
     "start_time": "2021-06-25T16:44:51.686645Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have pyspark version :  ['spark-2.3', 'spark-3.0', 'spark-3.0.1-bin-hadoop2.7', 'spark-2.3.4-bin-hadoop2.7', 'spark-2.4.7-bin-hadoop2.7', 'spark-2.4']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "def get_workstation_spark_path(where_are_you : str) -> str:\n",
    "    if where_are_you == 'titan':\n",
    "        return '/home/data/ryanchao2012/lib'\n",
    "    elif where_are_you == 'thor':\n",
    "        return '/opt/spark/versions'\n",
    "    else:\n",
    "        raise ValueError(\"wrong work station name\")\n",
    "\n",
    "spark_path = get_workstation_spark_path('thor')\n",
    "\n",
    "print('You have pyspark version : ', os.listdir(spark_path))\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "# spark-2.3, spark-2.4\n",
    "os.environ['SPARK_HOME'] = os.path.join(spark_path,'spark-2.3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T16:44:51.940029Z",
     "start_time": "2021-06-25T16:44:51.797119Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/opt/spark/versions/spark-2.3'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ['SPARK_HOME']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T16:44:52.360476Z",
     "start_time": "2021-06-25T16:44:51.941957Z"
    }
   },
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession as Session\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark import SparkConf as Conf\n",
    "from pyspark.sql import functions as F, Window as W, types as T\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "C = F.col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T16:44:52.372809Z",
     "start_time": "2021-06-25T16:44:52.361998Z"
    }
   },
   "outputs": [],
   "source": [
    "conf = (Conf()\n",
    "    .set('spark.sql.sources.partitionOverwriteMode', 'dynamic')\n",
    "    .set('spark.driver.memory', '4g')\n",
    "    .set('spark.driver.maxResultSize', '1g')\n",
    "   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T16:44:54.923194Z",
     "start_time": "2021-06-25T16:44:52.374650Z"
    }
   },
   "outputs": [],
   "source": [
    "spark = (Session\n",
    "     .builder\n",
    "     .appName('pyspark-challenge')\n",
    "     .master('local[2]')\n",
    "     .config(conf=conf)\n",
    "     .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T16:44:54.943892Z",
     "start_time": "2021-06-25T16:44:54.926036Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://thor:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[2]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-challenge</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f85b0967b90>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating DataFrame (16+)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T16:44:55.084265Z",
     "start_time": "2021-06-25T16:44:54.945331Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Builder', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__enter__', '__eq__', '__exit__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_conf', '_convert_from_pandas', '_createFromLocal', '_createFromRDD', '_create_from_pandas_with_arrow', '_get_numpy_record_dtype', '_inferSchema', '_inferSchemaFromList', '_instantiatedSession', '_jsc', '_jsparkSession', '_jvm', '_jwrapped', '_repr_html_', '_sc', '_wrapped', 'builder', 'catalog', 'conf', 'createDataFrame', 'newSession', 'range', 'read', 'readStream', 'sparkContext', 'sql', 'stop', 'streams', 'table', 'udf', 'version']\n",
      "\n",
      "your spark version : 2.3.4\n"
     ]
    }
   ],
   "source": [
    "# 0. know what spark session can do and its version\n",
    "print(dir(spark), f'your spark version : {spark.version}'\n",
    "      , sep='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T16:44:55.173078Z",
     "start_time": "2021-06-25T16:44:55.086513Z"
    }
   },
   "outputs": [],
   "source": [
    "# # 1. read data from csv\n",
    "# print(os.listdir('../data'))\n",
    "# df_from_csv_1 = spark.read.csv('../data/zipcodes.csv',\n",
    "#                                header=True,\n",
    "#                               inferSchema=True)\n",
    "# df_from_csv_1.printSchema()\n",
    "# df_from_csv_1.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T16:44:55.273099Z",
     "start_time": "2021-06-25T16:44:55.175734Z"
    }
   },
   "outputs": [],
   "source": [
    "# # 2 read data from json\n",
    "# print(os.listdir('../data'))\n",
    "# print(dir(spark.read))\n",
    "# # 沒有infer_schema\n",
    "# df_from_json = spark.read.json('../data/zipcodes.json')\n",
    "# df_from_json.printSchema()\n",
    "# df_from_json.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T16:44:57.969858Z",
     "start_time": "2021-06-25T16:44:55.275818Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+\n",
      "|language|user_counts|\n",
      "+--------+-----------+\n",
      "|    Java|      20000|\n",
      "|  Python|     100000|\n",
      "|   Scala|       3000|\n",
      "+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3 create dataframe from rdd list\n",
    "columns = [\"language\",\"user_counts\"]\n",
    "data = [\n",
    "    (\"Java\",\"20000\"),\n",
    "    (\"Python\",\"100000\"),\n",
    "    (\"Scala\",\"3000\")\n",
    "       ]\n",
    "# 先分散到rdd\n",
    "# rdd = spark.sparkContext.parallelize(data)\n",
    "# print(dir(rdd), type(rdd), sep='\\n\\n')\n",
    "# print()\n",
    "# df_from_rdd = rdd.toDF(schema=columns)\n",
    "# df_from_rdd.show(n=5)\n",
    "\n",
    "# 直接create，讓spark dataframe進行分散\n",
    "df = spark.createDataFrame(data=data,schema=columns)\n",
    "df.show(n=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T16:44:58.352089Z",
     "start_time": "2021-06-25T16:44:57.971528Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Builder', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__enter__', '__eq__', '__exit__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_conf', '_convert_from_pandas', '_createFromLocal', '_createFromRDD', '_create_from_pandas_with_arrow', '_get_numpy_record_dtype', '_inferSchema', '_inferSchemaFromList', '_instantiatedSession', '_jsc', '_jsparkSession', '_jvm', '_jwrapped', '_repr_html_', '_sc', '_wrapped', 'builder', 'catalog', 'conf', 'createDataFrame', 'newSession', 'range', 'read', 'readStream', 'sparkContext', 'sql', 'stop', 'streams', 'table', 'udf', 'version']\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4 create 5 row fake data using spark range\n",
    "print(dir(spark))\n",
    "print(type(spark.range(start=0,end=10)))\n",
    "columns = ['row_number']\n",
    "single_column_df = spark.range(start=0,end=10)\n",
    "single_column_df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T16:44:58.367141Z",
     "start_time": "2021-06-25T16:44:58.354402Z"
    }
   },
   "outputs": [],
   "source": [
    "# 6 create empty dataframe\n",
    "\n",
    "columns = [\"language\",\"user_counts\"]\n",
    "\n",
    "# empty RDD + schema won't work\n",
    "# df_1 = spark.createDataFrame(data=spark.sparkContext.emptyRDD(),\n",
    "#                              schema=columns)\n",
    "# df_1.show(n=5)\n",
    "\n",
    "# empty list -> rdd -> df won't work\n",
    "# df2 = spark.sparkContext.parallelize([]).toDF(columns)\n",
    "\n",
    "# df3 = spark.createDataFrame([]) # won't work this version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T16:44:58.827324Z",
     "start_time": "2021-06-25T16:44:58.368480Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 2\n"
     ]
    }
   ],
   "source": [
    "# 7 get dataframe shape\n",
    "print(df.count(), len(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T16:44:59.071106Z",
     "start_time": "2021-06-25T16:44:58.830033Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+----------+\n",
      "|language|user_counts|new_column|\n",
      "+--------+-----------+----------+\n",
      "|    Java|      20000|       ABC|\n",
      "|  Python|     100000|       ABC|\n",
      "|   Scala|       3000|       ABC|\n",
      "+--------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 8 add const column to a existing dataframe\n",
    "\n",
    "columns = [\"language\",\"user_counts\"]\n",
    "data = [\n",
    "    (\"Java\",\"20000\"),\n",
    "    (\"Python\",\"100000\"),\n",
    "    (\"Scala\",\"3000\")\n",
    "       ]\n",
    "df = spark.createDataFrame(data=data,schema=columns)\n",
    "df = (\n",
    "    df.withColumn(\"new_column\",F.lit(\"ABC\")) \n",
    "    # F.lit means literal, retrurn a column\n",
    ")\n",
    "df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T16:44:59.687106Z",
     "start_time": "2021-06-25T16:44:59.074138Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-----+\n",
      "|language|user_counts|index|\n",
      "+--------+-----------+-----+\n",
      "|    Java|      20000|    1|\n",
      "|  Python|     100000|    2|\n",
      "|   Scala|       3000|    3|\n",
      "+--------+-----------+-----+\n",
      "\n",
      "+--------+-----------+-------+\n",
      "|language|user_counts|row_num|\n",
      "+--------+-----------+-------+\n",
      "|    Java|      20000|      1|\n",
      "|  Python|     100000|      2|\n",
      "|   Scala|       3000|      3|\n",
      "+--------+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 9 add a row_id column from a exisiting dataframe\n",
    "# https://stackoverflow.com/questions/53082891/adding-a-unique-consecutive-row-number-to-dataframe-in-pyspark\n",
    "columns = [\"language\",\"user_counts\"]\n",
    "data = [\n",
    "    (\"Java\",\"20000\"),\n",
    "    (\"Python\",\"100000\"),\n",
    "    (\"Scala\",\"3000\")\n",
    "       ]\n",
    "df_1 = spark.createDataFrame(data=data,schema=columns)\n",
    "df_1 = (\n",
    "    df_1.withColumn(\"index\", \n",
    "                  F.row_number().over(\n",
    "                      W.orderBy(F.monotonically_increasing_id() - 1)\n",
    "                  )\n",
    "                 )\n",
    "    # F.monotonically_increasing_id does not give 1 ~ N\n",
    "    # So we use window function to work around\n",
    ")\n",
    "df_1.show(n=5)\n",
    "\n",
    "# Mre clear way to do that\n",
    "df_2 = spark.createDataFrame(data=data,schema=columns)\n",
    "w = W.orderBy(F.lit('A'))\n",
    "df_2 = (\n",
    "    df_2.withColumn(\"row_num\", F.row_number().over(w))\n",
    ")\n",
    "df_2.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T16:44:59.882771Z",
     "start_time": "2021-06-25T16:44:59.690205Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+----------+-------------------+\n",
      "|language|user_counts|new_column|      random_number|\n",
      "+--------+-----------+----------+-------------------+\n",
      "|    Java|      20000|       ABC|0.16045817708499122|\n",
      "|  Python|     100000|       ABC| 0.1429905854660407|\n",
      "|   Scala|       3000|       ABC| 0.8334139670426234|\n",
      "+--------+-----------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 10 add a random number to a exisit column\n",
    "df = (\n",
    "#     df.withColumn('random_number', F.when(F.rand() > 0.5, 1).otherwise(0))\n",
    "        df.withColumn('random_number', F.rand())\n",
    ")\n",
    "\n",
    "df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T16:45:00.202570Z",
     "start_time": "2021-06-25T16:44:59.884846Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>user_counts</th>\n",
       "      <th>new_column</th>\n",
       "      <th>random_number</th>\n",
       "      <th>binary_cut_05</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Java</td>\n",
       "      <td>20000</td>\n",
       "      <td>ABC</td>\n",
       "      <td>0.160458</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Python</td>\n",
       "      <td>100000</td>\n",
       "      <td>ABC</td>\n",
       "      <td>0.142991</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Scala</td>\n",
       "      <td>3000</td>\n",
       "      <td>ABC</td>\n",
       "      <td>0.833414</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  language user_counts new_column  random_number  binary_cut_05\n",
       "0     Java       20000        ABC       0.160458              0\n",
       "1   Python      100000        ABC       0.142991              1\n",
       "2    Scala        3000        ABC       0.833414              0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 11 add a binary 0, 1 based on condition to an exisit column\n",
    "df = (\n",
    "    df.withColumn('binary_cut_05',F.when(F.rand() > 0.5, 1).otherwise(0))\n",
    ")\n",
    "df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T16:45:00.367734Z",
     "start_time": "2021-06-25T16:45:00.204290Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-----------+\n",
      "|row_id|language|user_counts|\n",
      "+------+--------+-----------+\n",
      "|     0|    Java|      20000|\n",
      "|     1|  Python|     100000|\n",
      "|     2|   Scala|       3000|\n",
      "+------+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 12 create a dataframe contains row_index and fake data\n",
    "\n",
    "columns = [\"row_id\",\"language\",\"user_counts\"]\n",
    "data = [\n",
    "    (0, \"Java\",\"20000\"),\n",
    "    (1, \"Python\",\"100000\"),\n",
    "    (2, \"Scala\",\"3000\")\n",
    "       ]\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=columns)\n",
    "df.show(n=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T16:45:00.739039Z",
     "start_time": "2021-06-25T16:45:00.370240Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+-----+------+------+\n",
      "|firstname|middlename|lastname|   id|gender|salary|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|    James|          |   Smith|36636|     M|  3000|\n",
      "|  Michael|      Rose|        |40288|     M|  4000|\n",
      "|   Robert|          |Williams|42114|     M|  4000|\n",
      "|    Maria|      Anne|   Jones|39192|     F|  4000|\n",
      "|      Jen|      Mary|   Brown|     |     F|    -1|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "\n",
      "+--------------------+-----+------+------+\n",
      "|                name|   id|gender|salary|\n",
      "+--------------------+-----+------+------+\n",
      "|    [James, , Smith]|36636|     M|  3000|\n",
      "|   [Michael, Rose, ]|40288|     M|  4000|\n",
      "|[Robert, , Williams]|42114|     M|  4000|\n",
      "|[Maria, Anne, Jones]|39192|     F|  4000|\n",
      "|  [Jen, Mary, Brown]|     |     F|    -1|\n",
      "+--------------------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 13 construct a complex data for spark dataframe\n",
    "# using StructType\n",
    "\n",
    "# Case 1\n",
    "data = [\n",
    "    (\"James\",\"\",\"Smith\",\"36636\",\"M\", 3000),\n",
    "    (\"Michael\",\"Rose\",\"\",\"40288\",\"M\", 4000),\n",
    "    (\"Robert\",\"\",\"Williams\",\"42114\",\"M\", 4000),\n",
    "    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\", 4000),\n",
    "    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\", -1)\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"firstname\",StringType(), True), # Nullable True\n",
    "    StructField(\"middlename\",StringType(), True),\n",
    "    StructField(\"lastname\",StringType(), True),\n",
    "    StructField(\"id\",StringType(), True),\n",
    "    StructField(\"gender\",StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True)\n",
    "    ])\n",
    "df = spark.createDataFrame(data=data,schema=schema)\n",
    "df.show(n=5)\n",
    "\n",
    "# Case 2\n",
    "\n",
    "struct_data = [\n",
    "    ((\"James\",\"\",\"Smith\"),\"36636\",\"M\", 3000),\n",
    "    ((\"Michael\",\"Rose\",\"\"),\"40288\",\"M\", 4000),\n",
    "    ((\"Robert\",\"\",\"Williams\"),\"42114\",\"M\", 4000),\n",
    "    ((\"Maria\",\"Anne\",\"Jones\"),\"39192\",\"F\", 4000),\n",
    "    ((\"Jen\",\"Mary\",\"Brown\"),\"\",\"F\", -1)\n",
    "]\n",
    "\n",
    "structure_schema = StructType([\n",
    "    StructField('name',\n",
    "        StructType([\n",
    "            StructField(\"firstname\",StringType(), True), # Nullable True\n",
    "            StructField(\"middlename\",StringType(), True),\n",
    "            StructField(\"lastname\",StringType(), True),\n",
    "    ])),\n",
    "    StructField(\"id\",StringType(), True),\n",
    "    StructField(\"gender\",StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True)\n",
    "    ])\n",
    "\n",
    "df = spark.createDataFrame(data=struct_data, schema=structure_schema)\n",
    "df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T16:45:01.096876Z",
     "start_time": "2021-06-25T16:45:00.745482Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- languagesAtSchool: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- currentState: string (nullable = true)\n",
      "\n",
      "+----------------+------------------+------------+\n",
      "|name            |languagesAtSchool |currentState|\n",
      "+----------------+------------------+------------+\n",
      "|James,,Smith    |[Java, Scala, C++]|CA          |\n",
      "|Michael,Rose,   |[Spark, Java, C++]|NJ          |\n",
      "|Robert,,Williams|[CSharp, VB]      |NV          |\n",
      "+----------------+------------------+------------+\n",
      "\n",
      "+-------------+-----+\n",
      "|         name|  col|\n",
      "+-------------+-----+\n",
      "| James,,Smith| Java|\n",
      "| James,,Smith|Scala|\n",
      "| James,,Smith|  C++|\n",
      "|Michael,Rose,|Spark|\n",
      "|Michael,Rose,| Java|\n",
      "+-------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 14 construct a complex data for spark dataframe\n",
    "# using ArratyType\n",
    "# https://github.com/spark-examples/pyspark-examples/blob/master/pyspark-array-string.py\n",
    "\n",
    "columns = [\"name\",\"languagesAtSchool\",\"currentState\"]\n",
    "data = [(\"James,,Smith\",[\"Java\",\"Scala\",\"C++\"],\"CA\"), \\\n",
    "    (\"Michael,Rose,\",[\"Spark\",\"Java\",\"C++\"],\"NJ\"), \\\n",
    "    (\"Robert,,Williams\",[\"CSharp\",\"VB\"],\"NV\")]\n",
    "\n",
    "df = spark.createDataFrame(data=data,schema=columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n",
    "\n",
    "df.select(df.name, F.explode(df.languagesAtSchool)).show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T16:45:01.284875Z",
     "start_time": "2021-06-25T16:45:01.099432Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- language: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- peoperties: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      "\n",
      "+----------+--------------+--------------------+\n",
      "|      name|      language|          peoperties|\n",
      "+----------+--------------+--------------------+\n",
      "|     James| [Java, Scala]|[eye -> brown, ha...|\n",
      "|   Michael|[Spark, Java,]|[eye ->, hair -> ...|\n",
      "|    Robert|    [CSharp, ]|[eye -> , hair ->...|\n",
      "|Washington|          null|                null|\n",
      "| Jefferson|        [1, 2]|                  []|\n",
      "+----------+--------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 15 construct a complex data for spark dataframe\n",
    "# using MapType\n",
    "# https://github.com/spark-examples/pyspark-examples/blob/master/pyspark-explode-array-map.py\n",
    "data = [\n",
    "        ('James',['Java','Scala'],{'hair':'black','eye':'brown'}),\n",
    "        ('Michael',['Spark','Java',None],{'hair':'brown','eye':None}),\n",
    "        ('Robert',['CSharp',''],{'hair':'red','eye':''}),\n",
    "        ('Washington',None,None),\n",
    "        ('Jefferson',['1','2'],{})\n",
    "]\n",
    "\n",
    "columns = ['name','language','peoperties']\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=columns)\n",
    "\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T16:45:01.897520Z",
     "start_time": "2021-06-25T16:45:01.287384Z"
    }
   },
   "outputs": [],
   "source": [
    "# 16 create a nested array-type dataframe\n",
    "# https://github.com/spark-examples/pyspark-examples/blob/master/pyspark-explode-nested-array.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T16:45:01.974594Z",
     "start_time": "2021-06-25T16:45:01.900517Z"
    }
   },
   "outputs": [],
   "source": [
    "# 17 create a datetime column for spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T16:45:03.123890Z",
     "start_time": "2021-06-25T16:45:01.977740Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------+\n",
      "| languagesAtSchool|currentState|\n",
      "+------------------+------------+\n",
      "|[Java, Scala, C++]|          CA|\n",
      "|[Spark, Java, C++]|          NJ|\n",
      "|      [CSharp, VB]|          NV|\n",
      "+------------------+------------+\n",
      "\n",
      "root\n",
      " |-- languagesAtSchool: string (nullable = true)\n",
      " |-- currentState: string (nullable = true)\n",
      "\n",
      "java.lang.UnsupportedOperationException: CSV data source does not support array<string> data type.\n",
      "java.lang.UnsupportedOperationException: CSV data source does not support array<string> data type.\n",
      "+------------+--------------------+\n",
      "|currentState|   languagesAtSchool|\n",
      "+------------+--------------------+\n",
      "|          CA|['Java', 'Scala',...|\n",
      "|          NJ|['Spark', 'Java',...|\n",
      "|          NV|    ['CSharp', 'VB']|\n",
      "+------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 18 read a csv with array-of-string schema (fake issue)\n",
    "# you should read it from json\n",
    "\n",
    "\n",
    "columns = [\"languagesAtSchool\",\"currentState\"]\n",
    "data = [\n",
    "    ([\"Java\",\"Scala\",\"C++\"],\"CA\"), \n",
    "    ([\"Spark\",\"Java\",\"C++\"],\"NJ\"),\n",
    "    ([\"CSharp\",\"VB\"],\"NV\")\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data=data,schema=columns)\n",
    "\n",
    "df.show()\n",
    "\n",
    "# we write the data into csv\n",
    "SAVE = True\n",
    "if SAVE:\n",
    "    df.toPandas().to_csv('tmp.csv',index=False)\n",
    "\n",
    "# You will get sting\n",
    "(\n",
    "    spark.read.csv('tmp.csv',inferSchema=True, header=True).printSchema()\n",
    ")\n",
    "\n",
    "# Convert it by schema? - No, not supported...\n",
    "\n",
    "schema = (\n",
    "    T.StructType()\n",
    "    .add(\"languagesAtSchool\", T.ArrayType(T.StringType()), True)\n",
    "    .add(\"currentState\", T.StringType(), True)\n",
    ")\n",
    "try:\n",
    "    (\n",
    "        spark.read.csv('tmp.csv',schema=schema, header=True).show()\n",
    "    )\n",
    "except Exception as e:\n",
    "#     print(e)\n",
    "    # short answer\n",
    "    print('java.lang.UnsupportedOperationException: CSV data source does not support array<string> data type.')\n",
    "\n",
    "# How about use F.json to convert that?\n",
    "\n",
    "try:\n",
    "    (\n",
    "        spark.read.csv('tmp.csv',inferSchema=True, header=True)\n",
    "        .cache()\n",
    "        .withColumn('languagesAtSchool',F.from_json(C(\"languagesAtSchool\"),'array<string>'))\n",
    "    ).show()\n",
    "except Exception as e:\n",
    "#     print(e)\n",
    "    # ashort answer\n",
    "    print(\"java.lang.UnsupportedOperationException: CSV data source does not support array<string> data type.\")\n",
    "    \n",
    "    \n",
    "# convert it into json, make your life easiler\n",
    "\n",
    "SAVE_JSON = True\n",
    "if SAVE_JSON:\n",
    "    pd.read_csv('tmp.csv').to_json('tmp.json',orient='records',force_ascii=False)\n",
    "\n",
    "# Amazing!\n",
    "spark.read.json('tmp.json').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Column Operations (8+)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T16:45:03.317260Z",
     "start_time": "2021-06-25T16:45:03.125998Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+--------------+------------------+\n",
      "|language|user_counts|user_count_100|    user_count_log|\n",
      "+--------+-----------+--------------+------------------+\n",
      "|    Java|      20000|       2000000| 4.301029995663981|\n",
      "|  Python|     100000|      10000000|               5.0|\n",
      "|   Scala|       3000|        300000|3.4771212547196626|\n",
      "+--------+-----------+--------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1 create new column based on original column\n",
    "\n",
    "columns = [\"language\", \"user_counts\"]\n",
    "data = [\n",
    "    (\"Java\",20000),\n",
    "    (\"Python\",100000),\n",
    "    (\"Scala\",3000)\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=columns)\n",
    "df = (\n",
    "    df.withColumn(\"user_count_100\", C(\"user_counts\") * 100)\\\n",
    "    .withColumn(\"user_count_log\", F.log10(C(\"user_counts\")))\n",
    ")\n",
    "\n",
    "df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T16:45:03.500432Z",
     "start_time": "2021-06-25T16:45:03.319542Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+\n",
      "|  lang|const_col|\n",
      "+------+---------+\n",
      "|  Java|      ABC|\n",
      "|Python|      ABC|\n",
      "| Scala|      ABC|\n",
      "+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2 rename, drop, add constant column to existing dataframe\n",
    "# https://stackoverflow.com/questions/34077353/how-to-change-dataframe-column-names-in-pyspark\n",
    "columns = [\"language\", \"user_counts\"]\n",
    "data = [\n",
    "    (\"Java\",20000),\n",
    "    (\"Python\",100000),\n",
    "    (\"Scala\",3000)\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df = (\n",
    "    df.withColumn('const_col',F.lit('ABC'))\\\n",
    "    .withColumnRenamed(\"language\",\"lang\")\\\n",
    "    .drop(\"user_counts\")\n",
    ")\n",
    "df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T16:45:03.517879Z",
     "start_time": "2021-06-25T16:45:03.501988Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.column.Column'>\n",
      "\n",
      "['__add__', '__and__', '__bool__', '__class__', '__contains__', '__delattr__', '__dict__', '__dir__', '__div__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__invert__', '__iter__', '__le__', '__lt__', '__mod__', '__module__', '__mul__', '__ne__', '__neg__', '__new__', '__nonzero__', '__or__', '__pow__', '__radd__', '__rand__', '__rdiv__', '__reduce__', '__reduce_ex__', '__repr__', '__rmod__', '__rmul__', '__ror__', '__rpow__', '__rsub__', '__rtruediv__', '__setattr__', '__sizeof__', '__str__', '__sub__', '__subclasshook__', '__truediv__', '__weakref__', '_asc_doc', '_bitwiseAND_doc', '_bitwiseOR_doc', '_bitwiseXOR_doc', '_contains_doc', '_desc_doc', '_endswith_doc', '_eqNullSafe_doc', '_isNotNull_doc', '_isNull_doc', '_jc', '_like_doc', '_rlike_doc', '_startswith_doc', 'alias', 'asc', 'astype', 'between', 'bitwiseAND', 'bitwiseOR', 'bitwiseXOR', 'cast', 'contains', 'desc', 'endswith', 'eqNullSafe', 'getField', 'getItem', 'isNotNull', 'isNull', 'isin', 'like', 'name', 'otherwise', 'over', 'rlike', 'startswith', 'substr', 'when']\n"
     ]
    }
   ],
   "source": [
    "# 3 know what method and attribute can be called with column object\n",
    "\n",
    "\n",
    "print(type(C(\"language\")), dir(C(\"language\")), sep='\\n\\n')\n",
    "\n",
    "# alias - 可以換名字\n",
    "# asc, desc - 可以排序\n",
    "# astype,cast - 可以轉型\n",
    "# between - 可以傳入start_date以及end_date過濾\n",
    "# bitwiseAND, bitwiseOR, bitwiseXOR - 可以做布林運算\n",
    "# contains - 可以做字串搜尋\n",
    "# endwith, startwith, rlike, substring - 可以做字串比對\n",
    "# eqNullSafe, isNotNull, isNull - 可以檢查null值，Python須以None傳入\n",
    "# isin, like - 可以做值的比對(數值，字串值)\n",
    "# name - 可以取得欄位名稱\n",
    "# when, otherwise - 可以做條件判斷\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T16:45:03.601140Z",
     "start_time": "2021-06-25T16:45:03.519373Z"
    }
   },
   "outputs": [],
   "source": [
    "# +=*/\n",
    "\n",
    "# df = spark.createDataFrame([(6,3), (7, 3), (13,6), (5, 0)], [\"x\", \"y\"])\n",
    "\n",
    "# df = (\n",
    "#     df.withColumn(\"mod_cross_col\", C(\"x\") % C(\"y\"))\n",
    "#     df.withColumn(\"mod_contant\", C(\"x\") )\n",
    "#      )\n",
    "# df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T16:45:03.864238Z",
     "start_time": "2021-06-25T16:45:03.604235Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-------------+\n",
      "|language|user_counts|is_many_users|\n",
      "+--------+-----------+-------------+\n",
      "|    Java|      20000|            0|\n",
      "|  Python|     100000|            1|\n",
      "|   Scala|       3000|            0|\n",
      "+--------+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4 create a new dynamic column(if else condition based on old column)\n",
    "columns = [\"language\", \"user_counts\"]\n",
    "data = [\n",
    "    (\"Java\",20000),\n",
    "    (\"Python\",100000),\n",
    "    (\"Scala\",3000)\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "df = (\n",
    "    df.withColumn(\"is_many_users\",\n",
    "                  F.when(C('user_counts') > 50000, 1).otherwise(0)\n",
    "                 )\n",
    ")\n",
    "\n",
    "df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T16:45:04.297197Z",
     "start_time": "2021-06-25T16:45:03.866849Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|firstname|middlename|lastname|   id|gender|salary|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|    James|          |   Smith|36636|     M|  3000|\n",
      "|  Michael|      Rose|        |40288|     M|  4000|\n",
      "|   Robert|          |Williams|42114|     M|  4000|\n",
      "|    Maria|      Anne|   Jones|39192|     F|  4000|\n",
      "|      Jen|      Mary|   Brown|     |     F|    -1|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "\n",
      "After\n",
      "+---------+----------+--------+-----+------+------+----------------+\n",
      "|firstname|middlename|lastname|   id|gender|salary|       full_name|\n",
      "+---------+----------+--------+-----+------+------+----------------+\n",
      "|    James|      null|   Smith|36636|     M|  3000|             N/A|\n",
      "|  Michael|      Rose|    null|40288|     M|  4000|             N/A|\n",
      "|   Robert|      null|Williams|42114|     M|  4000|             N/A|\n",
      "|    Maria|      Anne|   Jones|39192|     F|  4000|Maria Anne Jones|\n",
      "|      Jen|      Mary|   Brown|     |     F|    -1|  Jen Mary Brown|\n",
      "+---------+----------+--------+-----+------+------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5 create a new dynamic column(if else condition based on old column) plus empty string replacement\n",
    "\n",
    "data = [\n",
    "    (\"James\",\"\",\"Smith\",\"36636\",\"M\", 3000),\n",
    "    (\"Michael\",\"Rose\",\"\",\"40288\",\"M\", 4000),\n",
    "    (\"Robert\",\"\",\"Williams\",\"42114\",\"M\", 4000),\n",
    "    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\", 4000),\n",
    "    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\", -1)\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"firstname\",StringType(), True), # Nullable True\n",
    "    StructField(\"middlename\",StringType(), True),\n",
    "    StructField(\"lastname\",StringType(), True),\n",
    "    StructField(\"id\",StringType(), True),\n",
    "    StructField(\"gender\",StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True)\n",
    "    ])\n",
    "df = spark.createDataFrame(data=data,schema=schema)\n",
    "\n",
    "# we cannot compare column values with empty string\n",
    "# so the work-around method is replace empty string to null\n",
    "# then using isNotNull()\n",
    "is_full_name_exist = (C(\"firstname\").isNotNull() & C(\"middlename\").isNotNull() & C(\"lastname\").isNotNull())\n",
    "\n",
    "\n",
    "def blank_as_null(x):\n",
    "    \"\"\"\n",
    "    helper function for converting row value from empty string to null\n",
    "    https://stackoverflow.com/questions/33287886/replace-empty-strings-with-none-null-values-in-dataframe\n",
    "    \"\"\"\n",
    "    return F.when(C(x) != \"\", C(x)).otherwise(None)\n",
    "\n",
    "print(\"Before\")\n",
    "df.show(n=5)\n",
    "df = (\n",
    "    df.withColumn(\"firstname\", blank_as_null(\"firstname\"))\\\n",
    "    .withColumn(\"middlename\", blank_as_null(\"middlename\"))\\\n",
    "    .withColumn(\"lastname\", blank_as_null(\"lastname\"))\\\n",
    "    .withColumn(\"full_name\",\n",
    "                  F.when(\n",
    "                      is_full_name_exist,\n",
    "                      F.concat(C(\"firstname\"), F.lit(' '),\n",
    "                               C(\"middlename\"), F.lit(' '),\n",
    "                               C(\"lastname\"))\n",
    "                  ).otherwise(F.lit('N/A'))\n",
    "                 )\n",
    ")\n",
    "print(\"After\")\n",
    "df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T16:45:04.480622Z",
     "start_time": "2021-06-25T16:45:04.299881Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Column<b'language'>, Column<b'user_counts'>] <class 'list'> <class 'pyspark.sql.column.Column'>\n",
      "+--------+-----------+\n",
      "|language|user_counts|\n",
      "+--------+-----------+\n",
      "|    Java|      20000|\n",
      "|  Python|     100000|\n",
      "|   Scala|       3000|\n",
      "+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6 select columns which also exist on another dataframe\n",
    "\n",
    "columns_1 = [\"language\", \"user_counts\"]\n",
    "data_1 = [\n",
    "    (\"Java\",20000),\n",
    "    (\"Python\",100000),\n",
    "    (\"Scala\",3000)\n",
    "]\n",
    "\n",
    "columns_2 = [\"language\", \"user_counts\",\"note\"]\n",
    "data_2 = [\n",
    "    (\"Java\",20000,\"nothing\"),\n",
    "    (\"Python\",100000,\"nothing\"),\n",
    "    (\"Scala\",3000,\"nothing\")\n",
    "]\n",
    "\n",
    "df_1 = spark.createDataFrame(data_1, columns_1)\n",
    "df_2 = spark.createDataFrame(data_2, columns_2)\n",
    "\n",
    "# union columns\n",
    "same_cols = [F.col(c) for c in df_2.columns if c in df_1.columns]\n",
    "print(same_cols, type(same_cols), type(same_cols[0]))\n",
    "\n",
    "df_same_col = df_1.select(*same_cols)\n",
    "df_same_col.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T16:45:04.671124Z",
     "start_time": "2021-06-25T16:45:04.483126Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-----+-----------------+-----------------+\n",
      "|language|user_counts|const|scientific_sign_1|scientific_sign_2|\n",
      "+--------+-----------+-----+-----------------+-----------------+\n",
      "|    Java|      20000|10000|           1.0E40|          1.0E-40|\n",
      "|  Python|     100000|10000|           1.0E40|          1.0E-40|\n",
      "|   Scala|       3000|10000|           1.0E40|          1.0E-40|\n",
      "+--------+-----------+-----+-----------------+-----------------+\n",
      "\n",
      "root\n",
      " |-- language: string (nullable = true)\n",
      " |-- user_counts: long (nullable = true)\n",
      " |-- const: integer (nullable = false)\n",
      " |-- scientific_sign_1: double (nullable = false)\n",
      " |-- scientific_sign_2: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 7 create a const numerical column\n",
    "\n",
    "\n",
    "columns = [\"language\", \"user_counts\"]\n",
    "data = [\n",
    "    (\"Java\",20000),\n",
    "    (\"Python\",100000),\n",
    "    (\"Scala\",3000)\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "df = (\n",
    "    df.withColumn(\"const\",F.lit(10000))\\\n",
    "    .withColumn(\"scientific_sign_1\", F.lit(1e40))\n",
    "    .withColumn(\"scientific_sign_2\", F.lit(1e-40))\n",
    ")\n",
    "\n",
    "df.show(n=5)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T16:45:04.881598Z",
     "start_time": "2021-06-25T16:45:04.674109Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-------------------+------------------+-------------------+\n",
      "|language|user_counts|        uniform_0_1|     uniform_0_100|         normal_0_1|\n",
      "+--------+-----------+-------------------+------------------+-------------------+\n",
      "|    Java|      20000| 0.6661236774413726| 66.61236774413726| 0.4085363219031828|\n",
      "|  Python|     100000| 0.3856203005100328| 38.56203005100328|-0.7556247885860078|\n",
      "|   Scala|       3000|0.27636619934035966|27.636619934035966|-1.4773884185536659|\n",
      "+--------+-----------+-------------------+------------------+-------------------+\n",
      "\n",
      "root\n",
      " |-- language: string (nullable = true)\n",
      " |-- user_counts: long (nullable = true)\n",
      " |-- uniform_0_1: double (nullable = false)\n",
      " |-- uniform_0_100: double (nullable = false)\n",
      " |-- normal_0_1: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 8 create a random variable column\n",
    "# https://spark.apache.org/docs/2.3.4/api/python/pyspark.sql.html#pyspark.sql.DataFrame\n",
    "# rand uniform [0, 1]\n",
    "# randn Normal distribution mu = 0, sigma = 1\n",
    "\n",
    "columns = [\"language\", \"user_counts\"]\n",
    "data = [\n",
    "    (\"Java\",20000),\n",
    "    (\"Python\",100000),\n",
    "    (\"Scala\",3000)\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "df = (\n",
    "    df.withColumn(\"uniform_0_1\",F.rand(seed=42))\\\n",
    "      .withColumn(\"uniform_0_100\",100 * F.rand(seed=42))\\\n",
    "      .withColumn(\"normal_0_1\", F.randn(seed=42))\n",
    ")\n",
    "\n",
    "df.show(n=5)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T16:45:05.120865Z",
     "start_time": "2021-06-25T16:45:04.884061Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+\n",
      "|language|user_counts|\n",
      "+--------+-----------+\n",
      "|    Java|      20000|\n",
      "|  Python|     100000|\n",
      "|   Scala|       3000|\n",
      "+--------+-----------+\n",
      "\n",
      "['Java', 'Python', 'Scala']\n"
     ]
    }
   ],
   "source": [
    "# 9 convert pyspark dataframe column to a python list\n",
    "# write a def func\n",
    "\n",
    "columns = [\"language\", \"user_counts\"]\n",
    "data = [\n",
    "    (\"Java\",20000),\n",
    "    (\"Python\",100000),\n",
    "    (\"Scala\",3000)\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()\n",
    "\n",
    "row_list = df.select(\"language\").collect()\n",
    "language_list = [row.language for row in row_list]\n",
    "print(language_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T16:45:05.461339Z",
     "start_time": "2021-06-25T16:45:05.123615Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+\n",
      "|language|user_counts|\n",
      "+--------+-----------+\n",
      "|    Java|      20000|\n",
      "|  Python|     100000|\n",
      "|   Scala|       3000|\n",
      "+--------+-----------+\n",
      "\n",
      "+--------+-----------+\n",
      "|language|user_counts|\n",
      "+--------+-----------+\n",
      "|       0|          0|\n",
      "+--------+-----------+\n",
      "\n",
      "root\n",
      " |-- language: long (nullable = false)\n",
      " |-- user_counts: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 10 check NA for all columns\n",
    "def calculate_null(sdf):\n",
    "    return sdf.select([F.count(F.when(F.isnan(c), c)).alias(c)\n",
    "                for c in sdf.columns])\n",
    "    \n",
    "\n",
    "\n",
    "columns = [\"language\", \"user_counts\"]\n",
    "data = [\n",
    "    (\"Java\",20000),\n",
    "    (\"Python\",100000),\n",
    "    (\"Scala\",3000)\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()\n",
    "\n",
    "calculate_null(df).show()\n",
    "\n",
    "calculate_null(df).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T16:45:05.664668Z",
     "start_time": "2021-06-25T16:45:05.462944Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-------------+\n",
      "|language|user_counts|       fillme|\n",
      "+--------+-----------+-------------+\n",
      "|    Java|         50|FillingString|\n",
      "|  Python|         50|FillingString|\n",
      "|   Scala|         50|FillingString|\n",
      "+--------+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 11 fillna in columns\n",
    "\n",
    "columns = [\"language\", \"user_counts\"]\n",
    "data = [\n",
    "    (\"Java\",20000),\n",
    "    (\"Python\",100000),\n",
    "    (\"Scala\",3000)\n",
    "]\n",
    "\n",
    "# df.withColumn('new_column', lit(None).cast(StringType()))\n",
    "df = (\n",
    "    spark.createDataFrame(data,columns)\n",
    "    .withColumn(\"user_counts\", F.lit(None).cast(T.IntegerType()))\n",
    "    .withColumn(\"fillme\", F.lit(None).cast(T.StringType()))\n",
    ").na.fill({\n",
    "        'user_counts' : 50,\n",
    "        'fillme' : \"FillingString\" \n",
    "    })\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## String Operation (3+)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T16:45:05.997934Z",
     "start_time": "2021-06-25T16:45:05.666750Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----------+\n",
      "| id|  poi|     branch|\n",
      "+---+-----+-----------+\n",
      "|  1|肉多多火鍋|           |\n",
      "|  2|  玖佰鍋|           |\n",
      "|  3|  石二鍋|      高雄遠百店|\n",
      "|  4|   火鍋|every_where|\n",
      "+---+-----+-----------+\n",
      "\n",
      "-RECORD 0----------------------\n",
      " id           | 1              \n",
      " poi          | 肉多多火鍋          \n",
      " branch       |                \n",
      " full_name    | 肉多多火鍋          \n",
      " full_name_v2 | 肉多多火鍋-         \n",
      "-RECORD 1----------------------\n",
      " id           | 2              \n",
      " poi          | 玖佰鍋            \n",
      " branch       |                \n",
      " full_name    | 玖佰鍋            \n",
      " full_name_v2 | 玖佰鍋-           \n",
      "-RECORD 2----------------------\n",
      " id           | 3              \n",
      " poi          | 石二鍋            \n",
      " branch       | 高雄遠百店          \n",
      " full_name    | 石二鍋高雄遠百店       \n",
      " full_name_v2 | 石二鍋-高雄遠百店      \n",
      "-RECORD 3----------------------\n",
      " id           | 4              \n",
      " poi          | 火鍋             \n",
      " branch       | every_where    \n",
      " full_name    | 火鍋every_where  \n",
      " full_name_v2 | 火鍋-every_where \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1 string concat two column values to a new column from an existing dataframe\n",
    "\n",
    "\n",
    "data = [\n",
    "    (\"1\",\"肉多多火鍋\",''),\n",
    "    (\"2\",\"玖佰鍋\",''),\n",
    "    (\"3\",\"石二鍋\",'高雄遠百店'),\n",
    "    (\"4\",\"火鍋\",'every_where')\n",
    "]\n",
    "\n",
    "col = ['id','poi','branch']\n",
    "\n",
    "df = spark.createDataFrame(data, col)\n",
    "\n",
    "df.show()\n",
    "\n",
    "(\n",
    "    df\n",
    "    .withColumn(\"full_name\",F.concat(\"poi\",\"branch\"))\n",
    "    .withColumn(\"full_name_v2\",F.concat_ws('-',\"poi\",\"branch\"))\n",
    "    \n",
    ").show(vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T16:45:06.281357Z",
     "start_time": "2021-06-25T16:45:06.000002Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id|  poi|\n",
      "+---+-----+\n",
      "|  1|肉多多火鍋|\n",
      "|  2|  玖佰鍋|\n",
      "|  3|  石二鍋|\n",
      "|  4|   火鍋|\n",
      "+---+-----+\n",
      "\n",
      "+---+-----+-------+\n",
      "| id|  poi|sub_str|\n",
      "+---+-----+-------+\n",
      "|  1|肉多多火鍋|    多火鍋|\n",
      "|  2|  玖佰鍋|    玖佰鍋|\n",
      "|  3|  石二鍋|    石二鍋|\n",
      "|  4|   火鍋|     火鍋|\n",
      "+---+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2 cut of left 3 char of specific column to a new column from an existing dataframe\n",
    "\n",
    "\n",
    "## substring\n",
    "\n",
    "data = [\n",
    "    (\"1\",\"肉多多火鍋\"),\n",
    "    (\"2\",\"玖佰鍋\"),\n",
    "    (\"3\",\"石二鍋\"),\n",
    "    (\"4\",\"火鍋\")\n",
    "]\n",
    "\n",
    "col = ['id','poi']\n",
    "\n",
    "df = spark.createDataFrame(data, col)\n",
    "\n",
    "df.show()\n",
    "\n",
    "(\n",
    "    df\n",
    "    .withColumn('sub_str',F.substring('poi',pos=-3,len=3))\n",
    "    \n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T16:45:06.844478Z",
     "start_time": "2021-06-25T16:45:06.284370Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id|  poi|\n",
      "+---+-----+\n",
      "|  1|肉多多火鍋|\n",
      "|  2|  玖佰鍋|\n",
      "|  3|  石二鍋|\n",
      "|  4|   火鍋|\n",
      "+---+-----+\n",
      "\n",
      "-RECORD 0-----------------------------------\n",
      " id                       | 1               \n",
      " poi                      | 肉多多火鍋           \n",
      " term_array               | [肉, 多, 多, 火, 鍋] \n",
      " term                     | 肉               \n",
      " position_fixed_substring | 1               \n",
      " position_two_column      | 0               \n",
      "-RECORD 1-----------------------------------\n",
      " id                       | 1               \n",
      " poi                      | 肉多多火鍋           \n",
      " term_array               | [肉, 多, 多, 火, 鍋] \n",
      " term                     | 多               \n",
      " position_fixed_substring | 1               \n",
      " position_two_column      | 1               \n",
      "-RECORD 2-----------------------------------\n",
      " id                       | 1               \n",
      " poi                      | 肉多多火鍋           \n",
      " term_array               | [肉, 多, 多, 火, 鍋] \n",
      " term                     | 多               \n",
      " position_fixed_substring | 1               \n",
      " position_two_column      | 1               \n",
      "-RECORD 3-----------------------------------\n",
      " id                       | 1               \n",
      " poi                      | 肉多多火鍋           \n",
      " term_array               | [肉, 多, 多, 火, 鍋] \n",
      " term                     | 火               \n",
      " position_fixed_substring | 1               \n",
      " position_two_column      | 3               \n",
      "-RECORD 4-----------------------------------\n",
      " id                       | 1               \n",
      " poi                      | 肉多多火鍋           \n",
      " term_array               | [肉, 多, 多, 火, 鍋] \n",
      " term                     | 鍋               \n",
      " position_fixed_substring | 1               \n",
      " position_two_column      | 4               \n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3 in string\n",
    "# to get the position in string\n",
    "\n",
    "from pixlake.nlp.utils import unigram\n",
    "\n",
    "data = [\n",
    "    (\"1\",\"肉多多火鍋\"),\n",
    "    (\"2\",\"玖佰鍋\"),\n",
    "    (\"3\",\"石二鍋\"),\n",
    "    (\"4\",\"火鍋\")\n",
    "]\n",
    "\n",
    "col = ['id','poi']\n",
    "\n",
    "df = spark.createDataFrame(data, col)\n",
    "\n",
    "df.show()\n",
    "\n",
    "(\n",
    "    df\n",
    "    .withColumn(\"term_array\",\n",
    "     F.udf(unigram,\n",
    "           returnType=\"array<string>\")(\n",
    "         \"poi\", F.lit(True)\n",
    "     ))\n",
    "    .withColumn(\"term\", F.explode_outer(C(\"term_array\")))\n",
    "    # instr provide fix substring only\n",
    "    .withColumn(\"position_fixed_substring\",\n",
    "                F.instr(C(\"poi\"), '肉'))\n",
    "    # if you wanna apply two column\n",
    "    .withColumn(\"position_two_column\",\n",
    "                F.expr(\"locate(term, poi) - 1\")\n",
    "               )\n",
    ").show(n=5, vertical=True, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T16:45:07.017232Z",
     "start_time": "2021-06-25T16:45:06.846444Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+-------------+\n",
      "| id|  poi|tmp|edit_distance|\n",
      "+---+-----+---+-------------+\n",
      "|  1|肉多多火鍋| 火鍋|            3|\n",
      "|  2|  玖佰鍋|abc|            3|\n",
      "|  3|  石二鍋|   |            3|\n",
      "|  4|   火鍋| 火鍋|            0|\n",
      "+---+-----+---+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4 compute edit distance - like between two column\n",
    "# pyspark support levenshtein distance\n",
    "# https://zh.wikipedia.org/wiki/%E8%90%8A%E6%96%87%E6%96%AF%E5%9D%A6%E8%B7%9D%E9%9B%A2\n",
    "\n",
    "data = [\n",
    "    (\"1\",\"肉多多火鍋\",\"火鍋\"),\n",
    "    (\"2\",\"玖佰鍋\",\"abc\"),\n",
    "    (\"3\",\"石二鍋\",\"\"),\n",
    "    (\"4\",\"火鍋\",\"火鍋\")\n",
    "]\n",
    "\n",
    "col = ['id','poi',\"tmp\"]\n",
    "\n",
    "df = spark.createDataFrame(data, col)\n",
    "\n",
    "(\n",
    "    df\n",
    "    .withColumn(\"edit_distance\", F.levenshtein(\"poi\",\"tmp\"))\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T16:45:07.192925Z",
     "start_time": "2021-06-25T16:45:07.020262Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+--------+\n",
      "| id|  poi|tmp|left_pad|\n",
      "+---+-----+---+--------+\n",
      "|  1|肉多多火鍋| 火鍋|   肉多多火鍋|\n",
      "|  2|  玖佰鍋|abc|   ##玖佰鍋|\n",
      "|  3|  石二鍋|   |   ##石二鍋|\n",
      "|  4|   火鍋| 火鍋|   ###火鍋|\n",
      "+---+-----+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5 padding your string column\n",
    "\n",
    "data = [\n",
    "    (\"1\",\"肉多多火鍋\",\"火鍋\"),\n",
    "    (\"2\",\"玖佰鍋\",\"abc\"),\n",
    "    (\"3\",\"石二鍋\",\"\"),\n",
    "    (\"4\",\"火鍋\",\"火鍋\")\n",
    "]\n",
    "\n",
    "col = ['id','poi',\"tmp\"]\n",
    "\n",
    "df = spark.createDataFrame(data, col)\n",
    "\n",
    "(\n",
    "    df\n",
    "    .withColumn(\"left_pad\", F.lpad(\"poi\",5,\"####\"))\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complex type operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T16:45:07.576842Z",
     "start_time": "2021-06-25T16:45:07.195690Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: struct (nullable = true)\n",
      " |    |-- firstname: string (nullable = true)\n",
      " |    |-- middlename: string (nullable = true)\n",
      " |    |-- lastname: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n",
      "+--------------------+-----+------+------+\n",
      "|name                |id   |gender|salary|\n",
      "+--------------------+-----+------+------+\n",
      "|[James, , Smith]    |36636|M     |3100  |\n",
      "|[Michael, Rose, ]   |40288|M     |4300  |\n",
      "|[Robert, , Williams]|42114|M     |1400  |\n",
      "|[Maria, Anne, Jones]|39192|F     |5500  |\n",
      "|[Jen, Mary, Brown]  |     |F     |-1    |\n",
      "+--------------------+-----+------+------+\n",
      "\n",
      "root\n",
      " |-- name: struct (nullable = true)\n",
      " |    |-- firstname: string (nullable = true)\n",
      " |    |-- middlename: string (nullable = true)\n",
      " |    |-- lastname: string (nullable = true)\n",
      " |-- info: struct (nullable = false)\n",
      " |    |-- identifer: string (nullable = true)\n",
      " |    |-- sex: string (nullable = true)\n",
      " |    |-- money: integer (nullable = true)\n",
      " |    |-- salary_grade: string (nullable = false)\n",
      "\n",
      "+--------------------+------------------------+\n",
      "|name                |info                    |\n",
      "+--------------------+------------------------+\n",
      "|[James, , Smith]    |[36636, M, 3100, Medium]|\n",
      "|[Michael, Rose, ]   |[40288, M, 4300, High]  |\n",
      "|[Robert, , Williams]|[42114, M, 1400, Low]   |\n",
      "|[Maria, Anne, Jones]|[39192, F, 5500, High]  |\n",
      "|[Jen, Mary, Brown]  |[, F, -1, Low]          |\n",
      "+--------------------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1 apply struct on column\n",
    "\n",
    "structureData = [\n",
    "    ((\"James\",\"\",\"Smith\"),\"36636\",\"M\",3100),\n",
    "    ((\"Michael\",\"Rose\",\"\"),\"40288\",\"M\",4300),\n",
    "    ((\"Robert\",\"\",\"Williams\"),\"42114\",\"M\",1400),\n",
    "    ((\"Maria\",\"Anne\",\"Jones\"),\"39192\",\"F\",5500),\n",
    "    ((\"Jen\",\"Mary\",\"Brown\"),\"\",\"F\",-1)\n",
    "  ]\n",
    "structureSchema = StructType([\n",
    "        StructField('name', StructType([\n",
    "             StructField('firstname', StringType(), True),\n",
    "             StructField('middlename', StringType(), True),\n",
    "             StructField('lastname', StringType(), True)\n",
    "             ])),\n",
    "         StructField('id', StringType(), True),\n",
    "         StructField('gender', StringType(), True),\n",
    "         StructField('salary', IntegerType(), True)\n",
    "         ])\n",
    "\n",
    "df = spark.createDataFrame(data=structureData,schema=structureSchema)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n",
    "\n",
    "df_with_other_info = (\n",
    "    df\n",
    "    .withColumn('info',\n",
    "        F.struct(C(\"id\").alias('identifer'),\n",
    "                 C(\"gender\").alias('sex'),\n",
    "                 C(\"salary\").alias('money'),\n",
    "                 F.when(C('salary').cast('integer') < 2000, \"Low\")\n",
    "                  .when(C('salary').cast('integer') < 4000, \"Medium\")\n",
    "                  .otherwise(\"High\").alias('salary_grade')\n",
    "               ))\n",
    "    .drop('id','gender','salary')\n",
    ")\n",
    "\n",
    "df_with_other_info.printSchema()\n",
    "df_with_other_info.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T16:45:07.749574Z",
     "start_time": "2021-06-25T16:45:07.579628Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- identifer: string (nullable = true)\n",
      " |-- salary_grade: string (nullable = false)\n",
      "\n",
      "+---------+------------+\n",
      "|identifer|salary_grade|\n",
      "+---------+------------+\n",
      "|    36636|      Medium|\n",
      "|    40288|        High|\n",
      "|    42114|         Low|\n",
      "|    39192|        High|\n",
      "|         |         Low|\n",
      "+---------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2 select the keys in stuct\n",
    "\n",
    "flattern = (\n",
    "    df_with_other_info\n",
    "    .select('info.identifer','info.salary_grade')\n",
    ")\n",
    "\n",
    "flattern.printSchema()\n",
    "flattern.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T16:47:55.019815Z",
     "start_time": "2021-06-25T16:47:54.840283Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+-----+------------+\n",
      "|identifer|sex|money|salary_grade|\n",
      "+---------+---+-----+------------+\n",
      "|    36636|  M| 3100|      Medium|\n",
      "|    40288|  M| 4300|        High|\n",
      "|    42114|  M| 1400|         Low|\n",
      "|    39192|  F| 5500|        High|\n",
      "|         |  F|   -1|         Low|\n",
      "+---------+---+-----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select all of the keys once in struct\n",
    "\n",
    "(\n",
    "    df_with_other_info\n",
    "    .select('info.*')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical filtering (6+)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T16:45:08.020521Z",
     "start_time": "2021-06-25T16:45:07.770253Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+-----+------+------+\n",
      "|firstname|middlename|lastname|   id|gender|salary|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|    James|          |   Smith|36636|     M|  3000|\n",
      "|  Michael|      Rose|        |40288|     M|  4000|\n",
      "|   Robert|          |Williams|42114|     M|  4000|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. filter on equal condition\n",
    "\n",
    "data = [\n",
    "    (\"James\",\"\",\"Smith\",\"36636\",\"M\", 3000),\n",
    "    (\"Michael\",\"Rose\",\"\",\"40288\",\"M\", 4000),\n",
    "    (\"Robert\",\"\",\"Williams\",\"42114\",\"M\", 4000),\n",
    "    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\", 4000),\n",
    "    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\", -1)\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"firstname\",StringType(), True), # Nullable True\n",
    "    StructField(\"middlename\",StringType(), True),\n",
    "    StructField(\"lastname\",StringType(), True),\n",
    "    StructField(\"id\",StringType(), True),\n",
    "    StructField(\"gender\",StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True)\n",
    "    ])\n",
    "df = spark.createDataFrame(data=data,schema=schema)\n",
    "\n",
    "df = (\n",
    "    df.filter(C(\"gender\") == \"M\")\n",
    ")\n",
    "\n",
    "df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T16:45:08.195959Z",
     "start_time": "2021-06-25T16:45:08.022814Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+-----+------+------+\n",
      "|firstname|middlename|lastname|   id|gender|salary|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|  Michael|      Rose|        |40288|     M|  4000|\n",
      "|   Robert|          |Williams|42114|     M|  4000|\n",
      "|    Maria|      Anne|   Jones|39192|     F|  6000|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2 filter on >, <, >=, <=\n",
    "\n",
    "data = [\n",
    "    (\"James\",\"\",\"Smith\",\"36636\",\"M\", 3000),\n",
    "    (\"Michael\",\"Rose\",\"\",\"40288\",\"M\", 4000),\n",
    "    (\"Robert\",\"\",\"Williams\",\"42114\",\"M\", 4000),\n",
    "    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\", 6000),\n",
    "    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\", -1)\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"firstname\",StringType(), True), # Nullable True\n",
    "    StructField(\"middlename\",StringType(), True),\n",
    "    StructField(\"lastname\",StringType(), True),\n",
    "    StructField(\"id\",StringType(), True),\n",
    "    StructField(\"gender\",StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True)\n",
    "    ])\n",
    "df = spark.createDataFrame(data=data,schema=schema)\n",
    "\n",
    "df = (\n",
    "    df.filter(C(\"salary\") >= 4000)\n",
    ")\n",
    "\n",
    "df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T16:45:08.367595Z",
     "start_time": "2021-06-25T16:45:08.197720Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+-----+------+------+\n",
      "|firstname|middlename|lastname|   id|gender|salary|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|  Michael|      Rose|        |40288|     M|  4000|\n",
      "|   Robert|          |Williams|42114|     M|  4000|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3 multiple conditions require parenthese around each condition\n",
    "\n",
    "\n",
    "data = [\n",
    "    (\"James\",\"\",\"Smith\",\"36636\",\"M\", 3000),\n",
    "    (\"Michael\",\"Rose\",\"\",\"40288\",\"M\", 4000),\n",
    "    (\"Robert\",\"\",\"Williams\",\"42114\",\"M\", 4000),\n",
    "    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\", 6000),\n",
    "    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\", -1)\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"firstname\",StringType(), True), # Nullable True\n",
    "    StructField(\"middlename\",StringType(), True),\n",
    "    StructField(\"lastname\",StringType(), True),\n",
    "    StructField(\"id\",StringType(), True),\n",
    "    StructField(\"gender\",StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True)\n",
    "    ])\n",
    "df = spark.createDataFrame(data=data,schema=schema)\n",
    "\n",
    "# This is lazy computing\n",
    "rich_man_who_worth_married = (\n",
    "    (C(\"gender\") == \"M\") &\n",
    "    (C(\"salary\") >= 4000)\n",
    ")\n",
    "\n",
    "df = (\n",
    "    df.filter(rich_man_who_worth_married)\n",
    ")\n",
    "\n",
    "df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T16:45:08.541552Z",
     "start_time": "2021-06-25T16:45:08.370182Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+-----+------+------+\n",
      "|firstname|middlename|lastname|   id|gender|salary|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|    Maria|      Anne|   Jones|39192|     F|  6000|\n",
      "|      Jen|      Mary|   Brown|     |     F|    -1|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4 Compare against a list of allowed values\n",
    "\n",
    "\n",
    "data = [\n",
    "    (\"James\",\"\",\"Smith\",\"36636\",\"M\", 3000),\n",
    "    (\"Michael\",\"Rose\",\"\",\"40288\",\"M\", 4000),\n",
    "    (\"Robert\",\"\",\"Williams\",\"42114\",\"M\", 4000),\n",
    "    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\", 6000),\n",
    "    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\", -1)\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"firstname\",StringType(), True), # Nullable True\n",
    "    StructField(\"middlename\",StringType(), True),\n",
    "    StructField(\"lastname\",StringType(), True),\n",
    "    StructField(\"id\",StringType(), True),\n",
    "    StructField(\"gender\",StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True)\n",
    "    ])\n",
    "df = spark.createDataFrame(data=data,schema=schema)\n",
    "\n",
    "df = (\n",
    "    df.filter(C(\"gender\").isin([\"F\"]))\n",
    ")\n",
    "\n",
    "df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T16:45:08.906624Z",
     "start_time": "2021-06-25T16:45:08.544348Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+-----+------+------+\n",
      "|firstname|middlename|lastname|   id|gender|salary|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|    Maria|      Anne|   Jones|39192|     F|  6000|\n",
      "|  Michael|      Rose|        |40288|     M|  4000|\n",
      "|   Robert|          |Williams|42114|     M|  4000|\n",
      "|    James|          |   Smith|36636|     M|  3000|\n",
      "|      Jen|      Mary|   Brown|     |     F|    -1|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|firstname|middlename|lastname|   id|gender|salary|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|      Jen|      Mary|   Brown|     |     F|    -1|\n",
      "|    James|          |   Smith|36636|     M|  3000|\n",
      "|  Michael|      Rose|        |40288|     M|  4000|\n",
      "|   Robert|          |Williams|42114|     M|  4000|\n",
      "|    Maria|      Anne|   Jones|39192|     F|  6000|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5 Sort result\n",
    "\n",
    "data = [\n",
    "    (\"James\",\"\",\"Smith\",\"36636\",\"M\", 3000),\n",
    "    (\"Michael\",\"Rose\",\"\",\"40288\",\"M\", 4000),\n",
    "    (\"Robert\",\"\",\"Williams\",\"42114\",\"M\", 4000),\n",
    "    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\", 6000),\n",
    "    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\", -1)\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"firstname\",StringType(), True), # Nullable True\n",
    "    StructField(\"middlename\",StringType(), True),\n",
    "    StructField(\"lastname\",StringType(), True),\n",
    "    StructField(\"id\",StringType(), True),\n",
    "    StructField(\"gender\",StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True)\n",
    "    ])\n",
    "df = spark.createDataFrame(data=data,schema=schema)\n",
    "\n",
    "\n",
    "df = df.orderBy(C(\"salary\").desc())\n",
    "\n",
    "df.show(n=5)\n",
    "\n",
    "df = df.orderBy(C(\"salary\").asc())\n",
    "\n",
    "df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T16:45:10.195723Z",
     "start_time": "2021-06-25T16:45:08.909445Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before\n",
      "+----------+-------+--------------------+\n",
      "|article_id|   pred|             img_url|\n",
      "+----------+-------+--------------------+\n",
      "|     67789|0.93422|https://pic.pimg....|\n",
      "|     67789|0.94231|https://pic.pimg....|\n",
      "|     14431|0.97611|https://pic.pimg....|\n",
      "|     14431|0.99834|https://pic.pimg....|\n",
      "|     14431|0.99834|https://pic.pimg....|\n",
      "+----------+-------+--------------------+\n",
      "\n",
      "Sol 1 FAILED SOMETIMES WHEN PARTITION != 1\n",
      "+----------+-------+--------------------+\n",
      "|article_id|   pred|             img_url|\n",
      "+----------+-------+--------------------+\n",
      "|     67789|0.93422|https://pic.pimg....|\n",
      "|     67789|0.94231|https://pic.pimg....|\n",
      "|     14431|0.97611|https://pic.pimg....|\n",
      "|     14431|0.99834|https://pic.pimg....|\n",
      "+----------+-------+--------------------+\n",
      "\n",
      "Sol 2 WORKED WITH ANY PARTITION\n",
      "+----------+-------+--------------------+\n",
      "|article_id|   pred|             img_url|\n",
      "+----------+-------+--------------------+\n",
      "|     67789|0.93422|https://pic.pimg....|\n",
      "|     67789|0.94231|https://pic.pimg....|\n",
      "|     14431|0.97611|https://pic.pimg....|\n",
      "|     14431|0.99834|https://pic.pimg....|\n",
      "+----------+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6 select distinct rows based on certain column but keep first row\n",
    "# In this case, model prediction to filter the same images\n",
    "\n",
    "############## DROP DUPLICATED doesn't work in this case\n",
    "# https://stackoverflow.com/questions/38687212/spark-dataframe-drop-duplicates-and-keep-first\n",
    "data = [\n",
    "    (14431,0.99834,'https://pic.pimg.tw/happy78/1528543947-685380499_n.jpg'),\n",
    "    (14431,0.99834,'https://pic.pimg.tw/happy78/1528543947-362759723_n.jpg'),\n",
    "    (14431,0.97611,'https://pic.pimg.tw/happy78/1528543962-2265924582_n.jpg'),\n",
    "    (67789,0.93422,'https://pic.pimg.tw/happy78/1528543962-4007835890_n.jpg'),\n",
    "    (67789,0.94231,'https://pic.pimg.tw/happy78/1528543962-45890_n.jpg')\n",
    "]\n",
    "\n",
    "columns = ['article_id','pred','img_url']\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=columns)\n",
    "print('before')\n",
    "\n",
    "df.orderBy('pred').show(n=5)\n",
    "\n",
    "print('Sol 1 FAILED SOMETIMES WHEN PARTITION != 1')\n",
    "df_1 = (\n",
    "    df.drop_duplicates(subset=[\"pred\"])\n",
    ")\n",
    "\n",
    "df_1.orderBy('pred').show(n=5)\n",
    "\n",
    "\n",
    "############## Using Window Function and sort, rank, worked!\n",
    "# You can check the 5 th question of Aggregation, The solution is the same\n",
    "\n",
    "print('Sol 2 WORKED WITH ANY PARTITION')\n",
    "df_2 = (\n",
    "    df.withColumn(\"rank_by_pred\",\n",
    "                  F.row_number().over(\n",
    "                  W.partitionBy(\"pred\")\\\n",
    "                      .orderBy(F.desc(\"pred\"))\n",
    "                  )\n",
    "                 )\\\n",
    "    .filter(F.col(\"rank_by_pred\") == 1)\\\n",
    "    .drop('rank_by_pred')\n",
    ")\n",
    "df_2.orderBy('pred').show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## String filtering (2+)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T16:45:10.360388Z",
     "start_time": "2021-06-25T16:45:10.197441Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " article_id | 14431                                                                                                                                                                                   \n",
      " pred       | 0.99834                                                                                                                                                                                 \n",
      " img_url    | https://pic.pimg.tw/happy78/1528543947-685380499_n.jpg                                                                                                                                  \n",
      "-RECORD 1---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " article_id | 14431                                                                                                                                                                                   \n",
      " pred       | 0.99834                                                                                                                                                                                 \n",
      " img_url    |                                                                                                                                                                                         \n",
      "-RECORD 2---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " article_id | 0                                                                                                                                                                                       \n",
      " pred       | 0.78                                                                                                                                                                                    \n",
      " img_url    | https://pic.pimg.tw/happy78/1528543947-2503982521_n.jpg                                                                                                                                 \n",
      "-RECORD 3---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " article_id | 1                                                                                                                                                                                       \n",
      " pred       | 0.45                                                                                                                                                                                    \n",
      " img_url    | https://s.yimg.com/bt/api/res/1.2/OzjFf8Ov8yUBECUAWKMcDw--/YXBwaWQ9eW5ld3NfbGVnbztxPTg1O3c9NjAw/http://media.zenfs.com/zh_hant_tw/News/stormmedia/20160601-093346_U720_M161728_0c0b.jpg \n",
      "-RECORD 4---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " article_id | 14431                                                                                                                                                                                   \n",
      " pred       | 0.99834                                                                                                                                                                                 \n",
      " img_url    | https://pic.pimg.tw/happy78/1528543947-362759723_n.jpg                                                                                                                                  \n",
      "-RECORD 5---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " article_id | 14431                                                                                                                                                                                   \n",
      " pred       | 0.97611                                                                                                                                                                                 \n",
      " img_url    | https://pic.pimg.tw/happy78/1528543962-2265924582_n.jpg                                                                                                                                 \n",
      "-RECORD 6---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " article_id | 67789                                                                                                                                                                                   \n",
      " pred       | 0.93422                                                                                                                                                                                 \n",
      " img_url    | https://pic.pimg.tw/happy78/1528543962-4007835890_n.jpg                                                                                                                                 \n",
      "-RECORD 7---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " article_id | 67789                                                                                                                                                                                   \n",
      " pred       | 0.94231                                                                                                                                                                                 \n",
      " img_url    | https://pic.pimg.tw/happy78/1528543962-45890_n.jpg                                                                                                                                      \n",
      "-RECORD 8---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " article_id | 67789                                                                                                                                                                                   \n",
      " pred       | 0.94111                                                                                                                                                                                 \n",
      " img_url    | https://pic.pimg.tw/happy78/1528543962-45890_n.png                                                                                                                                      \n",
      "-RECORD 9---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " article_id | 67789                                                                                                                                                                                   \n",
      " pred       | 0.94111                                                                                                                                                                                 \n",
      " img_url    |                                                                                                                                                                                         \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. regax filtering\n",
    "# pyspark regexp_extract api cannot get all the groups\n",
    "data = [\n",
    "    (14431,0.99834,'https://pic.pimg.tw/happy78/1528543947-685380499_n.jpg mfkvmdfasx'),\n",
    "    (14431,0.99834,'https://pic.pimg.tw/happy78/1528543947-685380499_n.jpgsd,clsd,cluah'),\n",
    "    (0,0.78,'src=\"https://pic.pimg.tw/happy78/1528543947-2503982521_n.jpg\" title=\"IMG_5216.jpg\"'),\n",
    "    (1,0.45,'https://s.yimg.com/bt/api/res/1.2/OzjFf8Ov8yUBECUAWKMcDw--/YXBwaWQ9eW5ld3NfbGVnbztxPTg1O3c9NjAw/http://media.zenfs.com/zh_hant_tw/News/stormmedia/20160601-093346_U720_M161728_0c0b.jpg'),\n",
    "    (14431,0.99834,'https://pic.pimg.tw/happy78/1528543947-362759723_n.jpg'),\n",
    "    (14431,0.97611,'https://pic.pimg.tw/happy78/1528543962-2265924582_n.jpg'),\n",
    "    (67789,0.93422,'https://pic.pimg.tw/happy78/1528543962-4007835890_n.jpg'),\n",
    "    (67789,0.94231,'https://pic.pimg.tw/happy78/1528543962-45890_n.jpg'),\n",
    "    (67789,0.94111,'https://pic.pimg.tw/happy78/1528543962-45890_n.png'),\n",
    "    (67789,0.94111,'png')\n",
    "]\n",
    "\n",
    "columns = ['article_id','pred','img_url']\n",
    "\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "\n",
    "df_filted = (\n",
    "    df.withColumn(\"img_url\", F.regexp_extract(C('img_url'),\n",
    "                                              r'(http\\S+jpg\\b)|(http\\S+png\\b)',\n",
    "                                              0))\n",
    ")\n",
    "\n",
    "df_filted.show(n=20, vertical=True, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T16:45:10.756479Z",
     "start_time": "2021-06-25T16:45:10.363061Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " article_id  | 14431                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
      " raw_content | <p>今天要帶大家去吃吃台中有名的這一鍋皇室秘藏鍋物，據說會有著「帝王般」的享受（是有沒有這麼誇張），話說這一鍋北中南都有分店，但是不知道是從哪裡起家的？我們這次吃的是「台中朝富店」喔</p>\n",
      "\n",
      "<p>地點：<span style=\"-webkit-text-stroke-width:0px; background-color:rgba(0, 0, 0, 0.03); color:rgba(0, 0, 0, 0.87); display:inline !important; float:none; font-family:roboto,noto sans tc,arial,sans-serif; font-size:13px; font-style:normal; font-variant-caps:normal; font-variant-ligatures:normal; font-weight:400; letter-spacing:normal; orphans:2; text-align:left; text-decoration-color:initial; text-decoration-style:initial; text-indent:0px; text-transform:none; white-space:normal; widows:2; word-spacing:0px\">台中市西屯區朝富路36號</span></p>\n",
      "\n",
      "<p>電話：<span style=\"-webkit-text-stroke-width:0px; background-color:rgba(0, 0, 0, 0.03); color:rgba(0, 0, 0, 0.87); display:inline !important; float:none; font-family:roboto,noto sans tc,arial,sans-serif; font-size:13px; font-style:normal; font-variant-caps:normal; font-variant-ligatures:normal; font-weight:400; letter-spacing:normal; orphans:2; text-align:left; text-decoration-color:initial; text-decoration-style:initial; text-indent:0px; text-transform:none; white-space:normal; widows:2; word-spacing:0px\">04 3609 0088</span></p>\n",
      "\n",
      "<p>營業時間：11:30~03:00</p>\n",
      "\n",
      "<p><iframe class=\"\" frameborder=\"0\" height=\"350\" marginheight=\"0\" marginwidth=\"0\" scrolling=\"no\" src=\"//maps.google.com/?ie=UTF8&amp;f=q&amp;source=s_q&amp;q=loc:24.166896,+120.63769000000002+(%E5%8F%B0%E7%81%A3%E5%8F%B0%E4%B8%AD%E5%B8%82%E8%A5%BF%E5%B1%AF%E5%8D%80%E6%9C%9D%E5%AF%8C%E8%B7%AF%E9%80%99%E4%B8%80%E9%8D%8B%E5%8F%B0%E4%B8%AD%E6%9C%9D%E5%AF%8C%E6%AE%BF)&amp;sll=24.160044,120.62857&amp;ll=24.166896,120.63769&amp;marker=24.166896,120.63769&amp;mrt=loc&amp;z=15&amp;t=m&amp;output=embed\" width=\"425\"></iframe><br>\n",
      "</small></p>\n",
      "\n",
      "</span></strong></p>\n",
      "\n",
      "<p><small>朝富店的外觀非常壯闊氣派，不禁讓人點點頭「嗯，果然是台中的餐廳，建築物都要超。大」</small></p>\n",
      "\n",
      "<p><img alt=\"IMG_5208.jpg\" src=\"https://pic.pimg.tw/happy78/1528543947-685380499_n.jpg\" title=\"IMG_5208.jpg\"></p>\n",
      "\n",
      "<p>進去唄～</p>\n",
      "\n",
      "<p><img alt=\"IMG_5217.jpg\" src=\"https://pic.pimg.tw/happy78/1528543947-362759723_n.jpg\" title=\"IMG_5217.jpg\"></p>\n",
      "\n",
      "<p>一樓有個大水池，看起來很富麗堂皇</p>\n",
      "\n",
      "<p><img alt=\"IMG_5232.jpg\" src=\"https://pic.pimg.tw/happy78/1528543962-2265924582_n.jpg\" title=\"IMG_5232.jpg\"></p>\n",
      "\n",
      "</p>\n",
      "\n",
      "<p>最讓我讚嘆的就是這個牡丹（？中國風的樓梯了！！好美喔～～</p>\n",
      "\n",
      "<p><img alt=\"IMG_5230.jpg\" src=\"https://pic.pimg.tw/happy78/1528543962-4007835890_n.jpg\" title=\"IMG_5230.jpg\"></p>\n",
      "\n",
      "<p>整棟建築物看起來用餐環境很棒</p>\n",
      "\n",
      "<p><img alt=\"IMG_5229.jpg\" src=\"https://pic.pimg.tw/happy78/1528543962-1785144547_n.jpg\" title=\"IMG_5229.jpg\"></p>\n",
      "\n",
      "<p>我看到一二樓都有這些宮廷風服飾，我沒問服務生是不是可以穿（因為我也不想穿XD而且整家店我也沒看到有人在穿XD）不過如果你想穿的話我想應該是沒人會阻止</p>\n",
      "\n",
      "<p><img alt=\"IMG_5228.jpg\" src=\"https://pic.pimg.tw/happy78/1528543960-3197815885_n.jpg\" title=\"IMG_5228.jpg\"></p>\n",
      "\n",
      "<p>醬料也有滿多選擇的</p>\n",
      "\n",
      "<p><img alt=\"IMG_5216.jpg\" src=\"https://pic.pimg.tw/happy78/1528543947-2503982521_n.jpg\" title=\"IMG_5216.jpg\"></p>\n",
      "\n",
      "</p>\n",
      "\n",
      "<p>我們點了麻辣鍋跟老火湯的鴛鴦鍋</p>\n",
      "\n",
      "<p><img alt=\"IMG_5221.jpg\" src=\"https://pic.pimg.tw/happy78/1528543950-1832601825_n.jpg\" title=\"IMG_5221.jpg\"></p>\n",
      "\n",
      "看喔）</p>\n",
      "\n",
      "<p>這是梅花豬268$</p>\n",
      "\n",
      "<p><img alt=\"IMG_5219.jpg\" src=\"https://pic.pimg.tw/happy78/1528543949-4214686345_n.jpg\" title=\"IMG_5219.jpg\"></p>\n",
      "\n",
      "<p>和牛梅花598$</p>\n",
      "\n",
      "<p><img alt=\"IMG_5222.jpg\" src=\"https://pic.pimg.tw/happy78/1528543955-3093638684_n.jpg\" title=\"IMG_5222.jpg\"></p>\n",
      "\n",
      "<p>因為跟長輩吃比較不好意思慢慢拍，所以照片沒有很多，照片裡還有玉米筍跟盛味綜合丸238$，另外右上角是店家招待的香蔥油條，我覺得滿特別的，乾吃的話會很有嚼勁，泡湯吃也很美味</p>\n",
      "\n",
      "<p>&nbsp;</p>\n",
      "\n",
      "<p><img alt=\"IMG_5224.jpg\" src=\"https://pic.pimg.tw/happy78/1528543955-714147528_n.jpg\" title=\"IMG_5224.jpg\"></p>\n",
      "\n",
      "</p>\n",
      "\n",
      "<p>要營養均衡，所以當然要吃青菜啦，這是五彩鮮蔬盤 \n",
      "-RECORD 1-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " article_id  | 55444                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
      " raw_content | </p><divstyle=\"text-align:center;\">                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \n",
      "\n",
      "-RECORD 0-----------------\n",
      " article_id  | 0          \n",
      " raw_content | ['</p>\\n'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1.2 regax replace (filtering)\n",
    "# example <a href=\"url\">link text</a> --> \"\"\n",
    "# <a href=\"https://www.w3schools.com/\">Visit W3Schools.com!</a>\n",
    "\n",
    "############### Case I #############################\n",
    "data = [\n",
    "    (14431,'<p>今天要帶大家去吃吃台中有名的這一鍋皇室秘藏鍋物，據說會有著「帝王般」的享受（是有沒有這麼誇張），話說這一鍋北中南都有分店，但是不知道是從哪裡起家的？我們這次吃的是「台中朝富店」喔</p>\\n\\n<p>地點：<span style=\"-webkit-text-stroke-width:0px; background-color:rgba(0, 0, 0, 0.03); color:rgba(0, 0, 0, 0.87); display:inline !important; float:none; font-family:roboto,noto sans tc,arial,sans-serif; font-size:13px; font-style:normal; font-variant-caps:normal; font-variant-ligatures:normal; font-weight:400; letter-spacing:normal; orphans:2; text-align:left; text-decoration-color:initial; text-decoration-style:initial; text-indent:0px; text-transform:none; white-space:normal; widows:2; word-spacing:0px\">台中市西屯區朝富路36號</span></p>\\n\\n<p>電話：<span style=\"-webkit-text-stroke-width:0px; background-color:rgba(0, 0, 0, 0.03); color:rgba(0, 0, 0, 0.87); display:inline !important; float:none; font-family:roboto,noto sans tc,arial,sans-serif; font-size:13px; font-style:normal; font-variant-caps:normal; font-variant-ligatures:normal; font-weight:400; letter-spacing:normal; orphans:2; text-align:left; text-decoration-color:initial; text-decoration-style:initial; text-indent:0px; text-transform:none; white-space:normal; widows:2; word-spacing:0px\">04 3609 0088</span></p>\\n\\n<p>營業時間：11:30~03:00</p>\\n\\n<p><iframe class=\"\" frameborder=\"0\" height=\"350\" marginheight=\"0\" marginwidth=\"0\" scrolling=\"no\" src=\"//maps.google.com/?ie=UTF8&amp;f=q&amp;source=s_q&amp;q=loc:24.166896,+120.63769000000002+(%E5%8F%B0%E7%81%A3%E5%8F%B0%E4%B8%AD%E5%B8%82%E8%A5%BF%E5%B1%AF%E5%8D%80%E6%9C%9D%E5%AF%8C%E8%B7%AF%E9%80%99%E4%B8%80%E9%8D%8B%E5%8F%B0%E4%B8%AD%E6%9C%9D%E5%AF%8C%E6%AE%BF)&amp;sll=24.160044,120.62857&amp;ll=24.166896,120.63769&amp;marker=24.166896,120.63769&amp;mrt=loc&amp;z=15&amp;t=m&amp;output=embed\" width=\"425\"></iframe><br>\\n<small><a href=\"//maps.google.com/?ie=UTF8&amp;f=q&amp;source=s_q&amp;q=loc:24.166896,+120.63769000000002+(%E5%8F%B0%E7%81%A3%E5%8F%B0%E4%B8%AD%E5%B8%82%E8%A5%BF%E5%B1%AF%E5%8D%80%E6%9C%9D%E5%AF%8C%E8%B7%AF%E9%80%99%E4%B8%80%E9%8D%8B%E5%8F%B0%E4%B8%AD%E6%9C%9D%E5%AF%8C%E6%AE%BF)&amp;sll=24.160044,120.62857&amp;ll=24.166896,120.63769&amp;marker=24.166896,120.63769&amp;mrt=loc&amp;z=15&amp;t=m&amp;source=embed\" style=\"color:#0000FF;text-align:left\">檢視較大的地圖</a></small></p>\\n\\n<p><strong><span style=\"font-size:14px\"><a href=\"https://www.instagram.com/p/BhtcYEmhi9U/\" target=\"_blank\"><img alt=\"\" height=\"20\" src=\"//s.pixfs.net/f.pixnet.net/images/emotions/032.gif\" title=\"\" width=\"20\">想看IG介紹請點我<img alt=\"\" height=\"20\" src=\"//s.pixfs.net/f.pixnet.net/images/emotions/019.gif\" title=\"\" width=\"20\"></a></span></strong></p>\\n\\n<p><small>朝富店的外觀非常壯闊氣派，不禁讓人點點頭「嗯，果然是台中的餐廳，建築物都要超。大」</small></p>\\n\\n<p><img alt=\"IMG_5208.jpg\" src=\"https://pic.pimg.tw/happy78/1528543947-685380499_n.jpg\" title=\"IMG_5208.jpg\"></p>\\n\\n<p>進去唄～</p>\\n\\n<p><img alt=\"IMG_5217.jpg\" src=\"https://pic.pimg.tw/happy78/1528543947-362759723_n.jpg\" title=\"IMG_5217.jpg\"></p>\\n\\n<p>一樓有個大水池，看起來很富麗堂皇</p>\\n\\n<p><img alt=\"IMG_5232.jpg\" src=\"https://pic.pimg.tw/happy78/1528543962-2265924582_n.jpg\" title=\"IMG_5232.jpg\"></p>\\n\\n<p><a href=\"https://vbtrax.com/track/clicks/1006/ce2bc2bd9f093698a587f264d175924223674fdf2aabebf50760b4?subid_1=&amp;subid_2=&amp;subid_3=&amp;subid_4=&amp;subid_5=\"><img alt=\"\" border=\"0\" src=\"https://vbtrax.com/track/imp/img/368/ce2bc2bd9f093698a587f264d175924223674fdf2aabebf50760b4?subid_1=&amp;subid_2=&amp;subid_3=&amp;subid_4=&amp;subid_5=\" title=\"\"></a></p>\\n\\n<p>最讓我讚嘆的就是這個牡丹（？中國風的樓梯了！！好美喔～～</p>\\n\\n<p><img alt=\"IMG_5230.jpg\" src=\"https://pic.pimg.tw/happy78/1528543962-4007835890_n.jpg\" title=\"IMG_5230.jpg\"></p>\\n\\n<p>整棟建築物看起來用餐環境很棒</p>\\n\\n<p><img alt=\"IMG_5229.jpg\" src=\"https://pic.pimg.tw/happy78/1528543962-1785144547_n.jpg\" title=\"IMG_5229.jpg\"></p>\\n\\n<p>我看到一二樓都有這些宮廷風服飾，我沒問服務生是不是可以穿（因為我也不想穿XD而且整家店我也沒看到有人在穿XD）不過如果你想穿的話我想應該是沒人會阻止</p>\\n\\n<p><img alt=\"IMG_5228.jpg\" src=\"https://pic.pimg.tw/happy78/1528543960-3197815885_n.jpg\" title=\"IMG_5228.jpg\"></p>\\n\\n<p>醬料也有滿多選擇的</p>\\n\\n<p><img alt=\"IMG_5216.jpg\" src=\"https://pic.pimg.tw/happy78/1528543947-2503982521_n.jpg\" title=\"IMG_5216.jpg\"></p>\\n\\n<p><a href=\"https://vbtrax.com/track/clicks/1096/ce2bc2b79e073698a587f264d175924223674fdf2aabebf50769b4?subid_1=&amp;subid_2=&amp;subid_3=&amp;subid_4=&amp;subid_5=\"><img alt=\"\" border=\"0\" src=\"https://vbtrax.com/track/imp/img/976/ce2bc2b79e073698a587f264d175924223674fdf2aabebf50769b4?subid_1=&amp;subid_2=&amp;subid_3=&amp;subid_4=&amp;subid_5=\" title=\"\"></a></p>\\n\\n<p>我們點了麻辣鍋跟老火湯的鴛鴦鍋</p>\\n\\n<p><img alt=\"IMG_5221.jpg\" src=\"https://pic.pimg.tw/happy78/1528543950-1832601825_n.jpg\" title=\"IMG_5221.jpg\"></p>\\n\\n<p>因為我們沒有很餓，所以沒有點很多，以下就一一介紹：（對了，這次忘記拍菜單，想看菜單的人可以去它<a href=\"http://www.toponepot.com/product_detail.php\" target=\"_blank\"><strong>官網</strong></a>看喔）</p>\\n\\n<p>這是梅花豬268$</p>\\n\\n<p><img alt=\"IMG_5219.jpg\" src=\"https://pic.pimg.tw/happy78/1528543949-4214686345_n.jpg\" title=\"IMG_5219.jpg\"></p>\\n\\n<p>和牛梅花598$</p>\\n\\n<p><img alt=\"IMG_5222.jpg\" src=\"https://pic.pimg.tw/happy78/1528543955-3093638684_n.jpg\" title=\"IMG_5222.jpg\"></p>\\n\\n<p>因為跟長輩吃比較不好意思慢慢拍，所以照片沒有很多，照片裡還有玉米筍跟盛味綜合丸238$，另外右上角是店家招待的香蔥油條，我覺得滿特別的，乾吃的話會很有嚼勁，泡湯吃也很美味</p>\\n\\n<p>&nbsp;</p>\\n\\n<p><img alt=\"IMG_5224.jpg\" src=\"https://pic.pimg.tw/happy78/1528543955-714147528_n.jpg\" title=\"IMG_5224.jpg\"></p>\\n\\n<p><a href=\"https://vbtrax.com/track/clicks/3456/ce2bc2bd9c0123daefcda67f8835ce1328684bc177fbb9b20a63b60061?subid_1=&amp;subid_2=&amp;subid_3=&amp;subid_4=&amp;subid_5=\"><img alt=\"\" border=\"0\" src=\"https://vbtrax.com/track/imp/img/35035/ce2bc2bd9c0123daefcda67f8835ce1328684bc177fbb9b20a63b60061?subid_1=&amp;subid_2=&amp;subid_3=&amp;subid_4=&amp;subid_5=\" title=\"\"></a></p>\\n\\n<p>要營養均衡，所以當然要吃青菜啦，這是五彩鮮蔬盤'),\n",
    "    (55444,'<pstyle=\"text-align:left;\"><imgsrc=\"https://pic.pimg.tw/peko721/1555405696-3356669416.jpg\"alt=\"麥食達韓式料理.麥食達菜單.麥食達.台北車站美食.北車美食.石鍋拌飯.\"/></p><pstyle=\"text-align:left;\"><strong><spanstyle=\"color:#0000ff;font-size:14pt;\">麥食達</span></strong></p><pstyle=\"text-align:left;\">地址：台北市中正區懷寧街86號</p><pstyle=\"text-align:left;\">這裡不是裝潢華麗的韓式料理餐廳，而是一家有點像家庭食堂的小店，紅蔘茶、冬粉、味噌湯都是無限量供應。</p><pstyle=\"text-align:left;\">相關文章：<atitle=\"【台北車站美食】麥食達韓式料理｜228公園旁的平價石鍋拌飯、鍋物｜附麥食達菜單\"href=\"https://peko721.pixnet.net/blog/post/46669347\"target=\"_blank\">麥食達韓式料理｜228公園旁的平價石鍋拌飯、鍋物</a></p><divstyle=\"text-align:center;\">')\n",
    "    \n",
    "]\n",
    "\n",
    "columns = ['article_id','raw_content']\n",
    "\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "\n",
    "df_filted = (\n",
    "    df.withColumn(\"raw_content\", F.regexp_replace(C('raw_content'),\n",
    "#                                               r'(<a href.*</a>)|(<atitle.*href.*</a>)',\n",
    "                                              r'(<.*href.*</a>)',\n",
    "                                              ''))\n",
    ")\n",
    "\n",
    "df_filted.show(n=20, vertical=True, truncate=False)\n",
    "############### Case II ###########################\n",
    "\n",
    "\n",
    "with open(\"../data/webpage_1.txt\", \"r\") as f:\n",
    "    text = [f.read()]\n",
    "\n",
    "schema = StructType(\n",
    "    [\n",
    "    StructField(\"article_id\", IntegerType(), True),\n",
    "    StructField(\"raw_content\", StringType(), True)\n",
    "    ]\n",
    ")\n",
    "# data format\n",
    "# [(r1_col1, r1_col2, ...),\n",
    "#  (r2_col1, r2_col2, ...),\n",
    "# ]\n",
    "df = spark.createDataFrame([(0, text)], schema=schema)\n",
    "# df.show(truncate=False)\n",
    "\n",
    "df_filted = (\n",
    "    df.withColumn(\"raw_content\", F.regexp_replace(C('raw_content'),\n",
    "#                                               r'(<a href.*</a>)|(<atitle.*href.*</a>)',\n",
    "                                              r'(<.*href.*</a>)',\n",
    "                                              ''))\n",
    ")\n",
    "\n",
    "df_filted.show(n=20, vertical=True, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T16:45:11.197882Z",
     "start_time": "2021-06-25T16:45:10.759147Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------------------------------------------------------------------\n",
      " article_id | 14431                                                             \n",
      " pred       | 0.99834                                                           \n",
      " img_url    | https://pic.pimg.tw/happy78/1528543947-685380499_n.jpg mfkvmdfasx \n",
      "\n",
      "-RECORD 0-----------------------------------------------------------------------\n",
      " article_id | 14431                                                             \n",
      " pred       | 0.99834                                                           \n",
      " img_url    | https://pic.pimg.tw/happy78/1528543947-685380499_n.jpg mfkvmdfasx \n",
      "\n",
      "-RECORD 0--------------------------------------------------------\n",
      " article_id | 67789                                              \n",
      " pred       | 0.94111                                            \n",
      " img_url    | https://pic.pimg.tw/happy78/1528543962-45890_n.png \n",
      "-RECORD 1--------------------------------------------------------\n",
      " article_id | 67789                                              \n",
      " pred       | 0.94111                                            \n",
      " img_url    | png                                                \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2. regax filtering\n",
    "# contains\n",
    "# startswith\n",
    "# endwith\n",
    "\n",
    "data = [\n",
    "    (14431,0.99834,'https://pic.pimg.tw/happy78/1528543947-685380499_n.jpg mfkvmdfasx'),\n",
    "    (14431,0.99834,'https://pic.pimg.tw/happy78/1528543947-362759723_n.jpg'),\n",
    "    (14431,0.97611,'https://pic.pimg.tw/happy78/1528543962-2265924582_n.jpg'),\n",
    "    (67789,0.93422,'https://pic.pimg.tw/happy78/1528543962-4007835890_n.jpg'),\n",
    "    (67789,0.94231,'https://pic.pimg.tw/happy78/1528543962-45890_n.jpg'),\n",
    "    (67789,0.94111,'https://pic.pimg.tw/happy78/1528543962-45890_n.png'),\n",
    "    (67789,0.94111,'png')\n",
    "]\n",
    "\n",
    "columns = ['article_id','pred','img_url']\n",
    "\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "df_con = (\n",
    "    df.filter(df.img_url.contains('mfkv'))\n",
    ")\n",
    "df_con.show(vertical=True,truncate=False)\n",
    "\n",
    "df_startwith = (\n",
    "    df.filter(df.img_url.startswith(\"png\"))\n",
    ")\n",
    "\n",
    "df_con.show(vertical=True,truncate=False)\n",
    "\n",
    "df_endwith = (\n",
    "    df.filter(df.img_url.endswith(\"png\"))\n",
    ")\n",
    "\n",
    "df_endwith.show(vertical=True,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T16:45:11.470511Z",
     "start_time": "2021-06-25T16:45:11.202756Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------------------------------------------------------------------------------------------\n",
      " article_id | 14431                                                                                     \n",
      " cleaned    | 今天要帶大家去吃吃台中有名的這一鍋皇室秘藏鍋物，據說會有著「帝王般」的享受（是有沒有這麼誇張），話說這一鍋北中南都有分店，但是不知道是從哪裡起家的？我們這次吃的是「台中朝富店」喔 \n",
      " length     | 89                                                                                        \n",
      "-RECORD 1-----------------------------------------------------------------------------------------------\n",
      " article_id | 14431                                                                                     \n",
      " cleaned    | 地點：                                                                                       \n",
      " length     | 3                                                                                         \n",
      "-RECORD 2-----------------------------------------------------------------------------------------------\n",
      " article_id | 14431                                                                                     \n",
      " cleaned    | 台中市西屯區朝富路36號                                                                              \n",
      " length     | 12                                                                                        \n",
      "-RECORD 3-----------------------------------------------------------------------------------------------\n",
      " article_id | 14431                                                                                     \n",
      " cleaned    | 電話：                                                                                       \n",
      " length     | 3                                                                                         \n",
      "-RECORD 4-----------------------------------------------------------------------------------------------\n",
      " article_id | 14431                                                                                     \n",
      " cleaned    | 04                                                                                        \n",
      " length     | 2                                                                                         \n",
      "-RECORD 5-----------------------------------------------------------------------------------------------\n",
      " article_id | 14431                                                                                     \n",
      " cleaned    | 3609                                                                                      \n",
      " length     | 4                                                                                         \n",
      "-RECORD 6-----------------------------------------------------------------------------------------------\n",
      " article_id | 14431                                                                                     \n",
      " cleaned    | 0088                                                                                      \n",
      " length     | 4                                                                                         \n",
      "-RECORD 7-----------------------------------------------------------------------------------------------\n",
      " article_id | 14431                                                                                     \n",
      " cleaned    | 營業時間：11:30~03:00                                                                          \n",
      " length     | 16                                                                                        \n",
      "-RECORD 8-----------------------------------------------------------------------------------------------\n",
      " article_id | 14431                                                                                     \n",
      " cleaned    | 檢視較大的地圖                                                                                   \n",
      " length     | 7                                                                                         \n",
      "-RECORD 9-----------------------------------------------------------------------------------------------\n",
      " article_id | 14431                                                                                     \n",
      " cleaned    | 想看IG介紹請點我                                                                                 \n",
      " length     | 9                                                                                         \n",
      "-RECORD 10----------------------------------------------------------------------------------------------\n",
      " article_id | 14431                                                                                     \n",
      " cleaned    | 朝富店的外觀非常壯闊氣派，不禁讓人點點頭「嗯，果然是台中的餐廳，建築物都要超。大」                                                 \n",
      " length     | 41                                                                                        \n",
      "-RECORD 11----------------------------------------------------------------------------------------------\n",
      " article_id | 14431                                                                                     \n",
      " cleaned    | 進去唄～                                                                                      \n",
      " length     | 4                                                                                         \n",
      "-RECORD 12----------------------------------------------------------------------------------------------\n",
      " article_id | 14431                                                                                     \n",
      " cleaned    | 一樓有個大水池，看起來很富麗堂皇                                                                          \n",
      " length     | 16                                                                                        \n",
      "-RECORD 13----------------------------------------------------------------------------------------------\n",
      " article_id | 14431                                                                                     \n",
      " cleaned    | 最讓我讚嘆的就是這個牡丹（？中國風的樓梯了！！好美喔～～                                                              \n",
      " length     | 28                                                                                        \n",
      "-RECORD 14----------------------------------------------------------------------------------------------\n",
      " article_id | 14431                                                                                     \n",
      " cleaned    | 整棟建築物看起來用餐環境很棒                                                                            \n",
      " length     | 14                                                                                        \n",
      "-RECORD 15----------------------------------------------------------------------------------------------\n",
      " article_id | 14431                                                                                     \n",
      " cleaned    | 我看到一二樓都有這些宮廷風服飾，我沒問服務生是不是可以穿（因為我也不想穿XD而且整家店我也沒看到有人在穿XD）不過如果你想穿的話我想應該是沒人會阻止                \n",
      " length     | 74                                                                                        \n",
      "-RECORD 16----------------------------------------------------------------------------------------------\n",
      " article_id | 14431                                                                                     \n",
      " cleaned    | 醬料也有滿多選擇的                                                                                 \n",
      " length     | 9                                                                                         \n",
      "-RECORD 17----------------------------------------------------------------------------------------------\n",
      " article_id | 14431                                                                                     \n",
      " cleaned    | 我們點了麻辣鍋跟老火湯的鴛鴦鍋                                                                           \n",
      " length     | 15                                                                                        \n",
      "-RECORD 18----------------------------------------------------------------------------------------------\n",
      " article_id | 14431                                                                                     \n",
      " cleaned    | 因為我們沒有很餓，所以沒有點很多，以下就一一介紹：（對了，這次忘記拍菜單，想看菜單的人可以去它                                           \n",
      " length     | 47                                                                                        \n",
      "-RECORD 19----------------------------------------------------------------------------------------------\n",
      " article_id | 14431                                                                                     \n",
      " cleaned    | 官網                                                                                        \n",
      " length     | 2                                                                                         \n",
      "-RECORD 20----------------------------------------------------------------------------------------------\n",
      " article_id | 14431                                                                                     \n",
      " cleaned    | 看喔）                                                                                       \n",
      " length     | 3                                                                                         \n",
      "-RECORD 21----------------------------------------------------------------------------------------------\n",
      " article_id | 14431                                                                                     \n",
      " cleaned    | 這是梅花豬268$                                                                                 \n",
      " length     | 9                                                                                         \n",
      "-RECORD 22----------------------------------------------------------------------------------------------\n",
      " article_id | 14431                                                                                     \n",
      " cleaned    | 和牛梅花598$                                                                                  \n",
      " length     | 8                                                                                         \n",
      "-RECORD 23----------------------------------------------------------------------------------------------\n",
      " article_id | 14431                                                                                     \n",
      " cleaned    | 因為跟長輩吃比較不好意思慢慢拍，所以照片沒有很多，照片裡還有玉米筍跟盛味綜合丸238$，另外右上角是店家招待的香蔥油條，我覺得滿特別的，乾吃的話會很有嚼勁，泡湯吃也很美味     \n",
      " length     | 85                                                                                        \n",
      "-RECORD 24----------------------------------------------------------------------------------------------\n",
      " article_id | 14431                                                                                     \n",
      " cleaned    | &nbsp;                                                                                    \n",
      " length     | 6                                                                                         \n",
      "-RECORD 25----------------------------------------------------------------------------------------------\n",
      " article_id | 14431                                                                                     \n",
      " cleaned    | 要營養均衡，所以當然要吃青菜啦，這是五彩鮮蔬盤                                                                   \n",
      " length     | 23                                                                                        \n",
      "-RECORD 26----------------------------------------------------------------------------------------------\n",
      " article_id | 55444                                                                                     \n",
      " cleaned    | 麥食達                                                                                       \n",
      " length     | 3                                                                                         \n",
      "-RECORD 27----------------------------------------------------------------------------------------------\n",
      " article_id | 55444                                                                                     \n",
      " cleaned    | 地址：台北市中正區懷寧街86號                                                                           \n",
      " length     | 15                                                                                        \n",
      "-RECORD 28----------------------------------------------------------------------------------------------\n",
      " article_id | 55444                                                                                     \n",
      " cleaned    | 這裡不是裝潢華麗的韓式料理餐廳，而是一家有點像家庭食堂的小店，紅蔘茶、冬粉、味噌湯都是無限量供應。                                         \n",
      " length     | 49                                                                                        \n",
      "-RECORD 29----------------------------------------------------------------------------------------------\n",
      " article_id | 55444                                                                                     \n",
      " cleaned    | 相關文章：                                                                                     \n",
      " length     | 5                                                                                         \n",
      "-RECORD 30----------------------------------------------------------------------------------------------\n",
      " article_id | 55444                                                                                     \n",
      " cleaned    | 麥食達韓式料理｜228公園旁的平價石鍋拌飯、鍋物                                                                  \n",
      " length     | 24                                                                                        \n",
      "-RECORD 31----------------------------------------------------------------------------------------------\n",
      " article_id | 666                                                                                       \n",
      " cleaned    | This                                                                                      \n",
      " length     | 4                                                                                         \n",
      "-RECORD 32----------------------------------------------------------------------------------------------\n",
      " article_id | 666                                                                                       \n",
      " cleaned    | is                                                                                        \n",
      " length     | 2                                                                                         \n",
      "-RECORD 33----------------------------------------------------------------------------------------------\n",
      " article_id | 666                                                                                       \n",
      " cleaned    | python                                                                                    \n",
      " length     | 6                                                                                         \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# html - related, separate each part of html tag\n",
    "\n",
    "\n",
    "data = [\n",
    "    (14431,'<p>今天要帶大家去吃吃台中有名的這一鍋皇室秘藏鍋物，據說會有著「帝王般」的享受（是有沒有這麼誇張），話說這一鍋北中南都有分店，但是不知道是從哪裡起家的？我們這次吃的是「台中朝富店」喔</p>\\n\\n<p>地點：<span style=\"-webkit-text-stroke-width:0px; background-color:rgba(0, 0, 0, 0.03); color:rgba(0, 0, 0, 0.87); display:inline !important; float:none; font-family:roboto,noto sans tc,arial,sans-serif; font-size:13px; font-style:normal; font-variant-caps:normal; font-variant-ligatures:normal; font-weight:400; letter-spacing:normal; orphans:2; text-align:left; text-decoration-color:initial; text-decoration-style:initial; text-indent:0px; text-transform:none; white-space:normal; widows:2; word-spacing:0px\">台中市西屯區朝富路36號</span></p>\\n\\n<p>電話：<span style=\"-webkit-text-stroke-width:0px; background-color:rgba(0, 0, 0, 0.03); color:rgba(0, 0, 0, 0.87); display:inline !important; float:none; font-family:roboto,noto sans tc,arial,sans-serif; font-size:13px; font-style:normal; font-variant-caps:normal; font-variant-ligatures:normal; font-weight:400; letter-spacing:normal; orphans:2; text-align:left; text-decoration-color:initial; text-decoration-style:initial; text-indent:0px; text-transform:none; white-space:normal; widows:2; word-spacing:0px\">04 3609 0088</span></p>\\n\\n<p>營業時間：11:30~03:00</p>\\n\\n<p><iframe class=\"\" frameborder=\"0\" height=\"350\" marginheight=\"0\" marginwidth=\"0\" scrolling=\"no\" src=\"//maps.google.com/?ie=UTF8&amp;f=q&amp;source=s_q&amp;q=loc:24.166896,+120.63769000000002+(%E5%8F%B0%E7%81%A3%E5%8F%B0%E4%B8%AD%E5%B8%82%E8%A5%BF%E5%B1%AF%E5%8D%80%E6%9C%9D%E5%AF%8C%E8%B7%AF%E9%80%99%E4%B8%80%E9%8D%8B%E5%8F%B0%E4%B8%AD%E6%9C%9D%E5%AF%8C%E6%AE%BF)&amp;sll=24.160044,120.62857&amp;ll=24.166896,120.63769&amp;marker=24.166896,120.63769&amp;mrt=loc&amp;z=15&amp;t=m&amp;output=embed\" width=\"425\"></iframe><br>\\n<small><a href=\"//maps.google.com/?ie=UTF8&amp;f=q&amp;source=s_q&amp;q=loc:24.166896,+120.63769000000002+(%E5%8F%B0%E7%81%A3%E5%8F%B0%E4%B8%AD%E5%B8%82%E8%A5%BF%E5%B1%AF%E5%8D%80%E6%9C%9D%E5%AF%8C%E8%B7%AF%E9%80%99%E4%B8%80%E9%8D%8B%E5%8F%B0%E4%B8%AD%E6%9C%9D%E5%AF%8C%E6%AE%BF)&amp;sll=24.160044,120.62857&amp;ll=24.166896,120.63769&amp;marker=24.166896,120.63769&amp;mrt=loc&amp;z=15&amp;t=m&amp;source=embed\" style=\"color:#0000FF;text-align:left\">檢視較大的地圖</a></small></p>\\n\\n<p><strong><span style=\"font-size:14px\"><a href=\"https://www.instagram.com/p/BhtcYEmhi9U/\" target=\"_blank\"><img alt=\"\" height=\"20\" src=\"//s.pixfs.net/f.pixnet.net/images/emotions/032.gif\" title=\"\" width=\"20\">想看IG介紹請點我<img alt=\"\" height=\"20\" src=\"//s.pixfs.net/f.pixnet.net/images/emotions/019.gif\" title=\"\" width=\"20\"></a></span></strong></p>\\n\\n<p><small>朝富店的外觀非常壯闊氣派，不禁讓人點點頭「嗯，果然是台中的餐廳，建築物都要超。大」</small></p>\\n\\n<p><img alt=\"IMG_5208.jpg\" src=\"https://pic.pimg.tw/happy78/1528543947-685380499_n.jpg\" title=\"IMG_5208.jpg\"></p>\\n\\n<p>進去唄～</p>\\n\\n<p><img alt=\"IMG_5217.jpg\" src=\"https://pic.pimg.tw/happy78/1528543947-362759723_n.jpg\" title=\"IMG_5217.jpg\"></p>\\n\\n<p>一樓有個大水池，看起來很富麗堂皇</p>\\n\\n<p><img alt=\"IMG_5232.jpg\" src=\"https://pic.pimg.tw/happy78/1528543962-2265924582_n.jpg\" title=\"IMG_5232.jpg\"></p>\\n\\n<p><a href=\"https://vbtrax.com/track/clicks/1006/ce2bc2bd9f093698a587f264d175924223674fdf2aabebf50760b4?subid_1=&amp;subid_2=&amp;subid_3=&amp;subid_4=&amp;subid_5=\"><img alt=\"\" border=\"0\" src=\"https://vbtrax.com/track/imp/img/368/ce2bc2bd9f093698a587f264d175924223674fdf2aabebf50760b4?subid_1=&amp;subid_2=&amp;subid_3=&amp;subid_4=&amp;subid_5=\" title=\"\"></a></p>\\n\\n<p>最讓我讚嘆的就是這個牡丹（？中國風的樓梯了！！好美喔～～</p>\\n\\n<p><img alt=\"IMG_5230.jpg\" src=\"https://pic.pimg.tw/happy78/1528543962-4007835890_n.jpg\" title=\"IMG_5230.jpg\"></p>\\n\\n<p>整棟建築物看起來用餐環境很棒</p>\\n\\n<p><img alt=\"IMG_5229.jpg\" src=\"https://pic.pimg.tw/happy78/1528543962-1785144547_n.jpg\" title=\"IMG_5229.jpg\"></p>\\n\\n<p>我看到一二樓都有這些宮廷風服飾，我沒問服務生是不是可以穿（因為我也不想穿XD而且整家店我也沒看到有人在穿XD）不過如果你想穿的話我想應該是沒人會阻止</p>\\n\\n<p><img alt=\"IMG_5228.jpg\" src=\"https://pic.pimg.tw/happy78/1528543960-3197815885_n.jpg\" title=\"IMG_5228.jpg\"></p>\\n\\n<p>醬料也有滿多選擇的</p>\\n\\n<p><img alt=\"IMG_5216.jpg\" src=\"https://pic.pimg.tw/happy78/1528543947-2503982521_n.jpg\" title=\"IMG_5216.jpg\"></p>\\n\\n<p><a href=\"https://vbtrax.com/track/clicks/1096/ce2bc2b79e073698a587f264d175924223674fdf2aabebf50769b4?subid_1=&amp;subid_2=&amp;subid_3=&amp;subid_4=&amp;subid_5=\"><img alt=\"\" border=\"0\" src=\"https://vbtrax.com/track/imp/img/976/ce2bc2b79e073698a587f264d175924223674fdf2aabebf50769b4?subid_1=&amp;subid_2=&amp;subid_3=&amp;subid_4=&amp;subid_5=\" title=\"\"></a></p>\\n\\n<p>我們點了麻辣鍋跟老火湯的鴛鴦鍋</p>\\n\\n<p><img alt=\"IMG_5221.jpg\" src=\"https://pic.pimg.tw/happy78/1528543950-1832601825_n.jpg\" title=\"IMG_5221.jpg\"></p>\\n\\n<p>因為我們沒有很餓，所以沒有點很多，以下就一一介紹：（對了，這次忘記拍菜單，想看菜單的人可以去它<a href=\"http://www.toponepot.com/product_detail.php\" target=\"_blank\"><strong>官網</strong></a>看喔）</p>\\n\\n<p>這是梅花豬268$</p>\\n\\n<p><img alt=\"IMG_5219.jpg\" src=\"https://pic.pimg.tw/happy78/1528543949-4214686345_n.jpg\" title=\"IMG_5219.jpg\"></p>\\n\\n<p>和牛梅花598$</p>\\n\\n<p><img alt=\"IMG_5222.jpg\" src=\"https://pic.pimg.tw/happy78/1528543955-3093638684_n.jpg\" title=\"IMG_5222.jpg\"></p>\\n\\n<p>因為跟長輩吃比較不好意思慢慢拍，所以照片沒有很多，照片裡還有玉米筍跟盛味綜合丸238$，另外右上角是店家招待的香蔥油條，我覺得滿特別的，乾吃的話會很有嚼勁，泡湯吃也很美味</p>\\n\\n<p>&nbsp;</p>\\n\\n<p><img alt=\"IMG_5224.jpg\" src=\"https://pic.pimg.tw/happy78/1528543955-714147528_n.jpg\" title=\"IMG_5224.jpg\"></p>\\n\\n<p><a href=\"https://vbtrax.com/track/clicks/3456/ce2bc2bd9c0123daefcda67f8835ce1328684bc177fbb9b20a63b60061?subid_1=&amp;subid_2=&amp;subid_3=&amp;subid_4=&amp;subid_5=\"><img alt=\"\" border=\"0\" src=\"https://vbtrax.com/track/imp/img/35035/ce2bc2bd9c0123daefcda67f8835ce1328684bc177fbb9b20a63b60061?subid_1=&amp;subid_2=&amp;subid_3=&amp;subid_4=&amp;subid_5=\" title=\"\"></a></p>\\n\\n<p>要營養均衡，所以當然要吃青菜啦，這是五彩鮮蔬盤'),\n",
    "    (55444,'<pstyle=\"text-align:left;\"><imgsrc=\"https://pic.pimg.tw/peko721/1555405696-3356669416.jpg\"alt=\"麥食達韓式料理.麥食達菜單.麥食達.台北車站美食.北車美食.石鍋拌飯.\"/></p><pstyle=\"text-align:left;\"><strong><spanstyle=\"color:#0000ff;font-size:14pt;\">麥食達</span></strong></p><pstyle=\"text-align:left;\">地址：台北市中正區懷寧街86號</p><pstyle=\"text-align:left;\">這裡不是裝潢華麗的韓式料理餐廳，而是一家有點像家庭食堂的小店，紅蔘茶、冬粉、味噌湯都是無限量供應。</p><pstyle=\"text-align:left;\">相關文章：<atitle=\"【台北車站美食】麥食達韓式料理｜228公園旁的平價石鍋拌飯、鍋物｜附麥食達菜單\"href=\"https://peko721.pixnet.net/blog/post/46669347\"target=\"_blank\">麥食達韓式料理｜228公園旁的平價石鍋拌飯、鍋物</a></p><divstyle=\"text-align:center;\">'),\n",
    "    (666,'<html><head></head><body><h1>This is python</h1></body></html>')\n",
    "]\n",
    "\n",
    "columns = ['article_id','raw_content']\n",
    "\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "\n",
    "\n",
    "df_filted = (\n",
    "    df\n",
    "    .withColumn(\"cleaned\",\n",
    "                  F.explode_outer(\n",
    "                      F.split(C(\"raw_content\"),r'<[^>]*>|\\p{Z}+|\\s+'))\n",
    "                 )\n",
    "    .withColumn(\"cleaned\", F.regexp_replace(C(\"cleaned\"), r\"\\p{Z}*\", \"\"))\n",
    "    .withColumn(\"cleaned\",F.trim(C(\"cleaned\")))\n",
    "    .withColumn(\"length\", F.length(C(\"cleaned\")))\n",
    "    .where(F.length(C(\"cleaned\")) > 0)\n",
    "    .drop('raw_content')\n",
    ")\n",
    "\n",
    "df_filted.show(n=200,vertical=True, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T16:45:11.771892Z",
     "start_time": "2021-06-25T16:45:11.473794Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- languagesAtSchool: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- major_use: string (nullable = true)\n",
      " |-- raw_content: string (nullable = true)\n",
      "\n",
      "+----------------+------------------+---------+---------------------------------------------+\n",
      "|name            |languagesAtSchool |major_use|raw_content                                  |\n",
      "+----------------+------------------+---------+---------------------------------------------+\n",
      "|James,,Smith    |[Java, Scala, C++]|Java     |sdmcsmcmpkla,pxozas,pxmasmxpJavasodmcasmdcpso|\n",
      "|Michael,Rose,   |[Spark, Java, C++]|Python   |sd,mcmsdcopmsocmpsmdcpython                  |\n",
      "|Robert,,Williams|[CSharp, VB]      |         |smdkcadpmcpowmcpmspdcmpsdc                   |\n",
      "|ABC,ss,Williams |[]                |Scala    |smdoicmsocomamsodmcosdcoms                   |\n",
      "|ABC,ss,Williams |[]                |Scala    |smdoicmsocomamsodmcosdcomsSca                |\n",
      "+----------------+------------------+---------+---------------------------------------------+\n",
      "\n",
      "+----------------+------------------+---------+---------------------------------------------+-------+\n",
      "|name            |languagesAtSchool |major_use|raw_content                                  |matched|\n",
      "+----------------+------------------+---------+---------------------------------------------+-------+\n",
      "|James,,Smith    |[Java, Scala, C++]|Java     |sdmcsmcmpkla,pxozas,pxmasmxpJavasodmcasmdcpso|true   |\n",
      "|Michael,Rose,   |[Spark, Java, C++]|Python   |sd,mcmsdcopmsocmpsmdcpython                  |false  |\n",
      "|Robert,,Williams|[CSharp, VB]      |         |smdkcadpmcpowmcpmspdcmpsdc                   |true   |\n",
      "|ABC,ss,Williams |[]                |Scala    |smdoicmsocomamsodmcosdcoms                   |false  |\n",
      "|ABC,ss,Williams |[]                |Scala    |smdoicmsocomamsodmcosdcomsSca                |false  |\n",
      "+----------------+------------------+---------+---------------------------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3 reg-like\n",
    "# Search all the pattern mahor_use in raw_content\n",
    "\n",
    "\n",
    "columns = [\"name\",\"languagesAtSchool\",\"major_use\",\"raw_content\"]\n",
    "data = [\n",
    "    (\"James,,Smith\",[\"Java\",\"Scala\",\"C++\"],\"Java\",\"sdmcsmcmpkla,pxozas,pxmasmxpJavasodmcasmdcpso\"), \\\n",
    "    (\"Michael,Rose,\",[\"Spark\",\"Java\",\"C++\"],\"Python\",\"sd,mcmsdcopmsocmpsmdcpython\"), \\\n",
    "    (\"Robert,,Williams\",[\"CSharp\",\"VB\"],\"\",\"smdkcadpmcpowmcpmspdcmpsdc\"),\n",
    "    (\"ABC,ss,Williams\",[],\"Scala\",\"smdoicmsocomamsodmcosdcoms\"),\n",
    "    (\"ABC,ss,Williams\",[],\"Scala\",\"smdoicmsocomamsodmcosdcomsSca\")\n",
    "       ]\n",
    "\n",
    "\n",
    "df = spark.createDataFrame(data=data,schema=columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n",
    "\n",
    "(\n",
    "    df\n",
    "    .withColumn(\"matched\",C(\"raw_content\").contains(C(\"major_use\")))\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## filtering in complex type (2+)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T16:45:12.068526Z",
     "start_time": "2021-06-25T16:45:11.774598Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- languagesAtSchool: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- currentState: string (nullable = true)\n",
      "\n",
      "+----------------+------------------+------------+\n",
      "|name            |languagesAtSchool |currentState|\n",
      "+----------------+------------------+------------+\n",
      "|James,,Smith    |[Java, Scala, C++]|CA          |\n",
      "|Michael,Rose,   |[Spark, Java, C++]|NJ          |\n",
      "|Robert,,Williams|[CSharp, VB]      |NV          |\n",
      "+----------------+------------------+------------+\n",
      "\n",
      "+----------------+------------------+------------+------------------------+\n",
      "|name            |languagesAtSchool |currentState|filter_languagesAtSchool|\n",
      "+----------------+------------------+------------+------------------------+\n",
      "|James,,Smith    |[Java, Scala, C++]|CA          |true                    |\n",
      "|Michael,Rose,   |[Spark, Java, C++]|NJ          |true                    |\n",
      "|Robert,,Williams|[CSharp, VB]      |NV          |false                   |\n",
      "+----------------+------------------+------------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. filter in array type\n",
    "columns = [\"name\",\"languagesAtSchool\",\"currentState\"]\n",
    "data = [(\"James,,Smith\",[\"Java\",\"Scala\",\"C++\"],\"CA\"), \\\n",
    "    (\"Michael,Rose,\",[\"Spark\",\"Java\",\"C++\"],\"NJ\"), \\\n",
    "    (\"Robert,,Williams\",[\"CSharp\",\"VB\"],\"NV\")]\n",
    "\n",
    "df = spark.createDataFrame(data=data,schema=columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n",
    "\n",
    "######### Case 1, single value #############\n",
    "\n",
    "df = (\n",
    "    df.withColumn(\"filter_languagesAtSchool\",F.array_contains(C(\"languagesAtSchool\"), \"Java\"))\n",
    ")\n",
    "df.show(truncate=False)\n",
    "\n",
    "\n",
    "######### Case 2, multiple values ?\n",
    " \n",
    "# must_language = [\"Spark\",\"Java\"]\n",
    "# must_language_lit = [F.lit(i) for i in must_language]\n",
    "# print(must_language_lit)\n",
    "# df = (\n",
    "#     df.withColumn(\"filter_languagesAtSchool\",F.array_contains(C(\"languagesAtSchool\"), must_language_lit))\n",
    "# )\n",
    "# df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T16:45:12.370356Z",
     "start_time": "2021-06-25T16:45:12.071511Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- languagesAtSchool: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- major_use: string (nullable = true)\n",
      "\n",
      "+----------------+------------------+---------+\n",
      "|name            |languagesAtSchool |major_use|\n",
      "+----------------+------------------+---------+\n",
      "|James,,Smith    |[Java, Scala, C++]|Java     |\n",
      "|Michael,Rose,   |[Spark, Java, C++]|Python   |\n",
      "|Robert,,Williams|[CSharp, VB]      |         |\n",
      "|ABC,ss,Williams |[]                |Scala    |\n",
      "+----------------+------------------+---------+\n",
      "\n",
      "+----------------+------------------+---------+--------------------+\n",
      "|name            |languagesAtSchool |major_use|is_major_been_taught|\n",
      "+----------------+------------------+---------+--------------------+\n",
      "|James,,Smith    |[Java, Scala, C++]|Java     |1                   |\n",
      "|Michael,Rose,   |[Spark, Java, C++]|Python   |0                   |\n",
      "|Robert,,Williams|[CSharp, VB]      |         |0                   |\n",
      "|ABC,ss,Williams |[]                |Scala    |0                   |\n",
      "+----------------+------------------+---------+--------------------+\n",
      "\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- languagesAtSchool: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- major_use: string (nullable = true)\n",
      " |-- is_major_been_taught: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 2 column A array contains column B\n",
    "# https://stackoverflow.com/questions/48488463/use-is-in-between-2-spark-dataframe-columns\n",
    "\n",
    "columns = [\"name\",\"languagesAtSchool\",\"major_use\"]\n",
    "data = [(\"James,,Smith\",[\"Java\",\"Scala\",\"C++\"],\"Java\"), \\\n",
    "    (\"Michael,Rose,\",[\"Spark\",\"Java\",\"C++\"],\"Python\"), \\\n",
    "    (\"Robert,,Williams\",[\"CSharp\",\"VB\"],\"\"),\n",
    "    (\"ABC,ss,Williams\",[],\"Scala\")]\n",
    "\n",
    "\n",
    "df = spark.createDataFrame(data=data,schema=columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n",
    "\n",
    "######### Case 1, single value #############\n",
    "\n",
    "df = (\n",
    "    df\n",
    "    .withColumn(\"is_major_been_taught\",F.expr(\"array_contains(languagesAtSchool, major_use)\").cast(\"integer\"))\n",
    "#     .withColumn(\"is_major_been_taught\",F.expr(\"array_contains(languagesAtSchool, major_use)\"))\n",
    ")\n",
    "df.show(truncate=False)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JOIN (3+)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T16:45:15.365075Z",
     "start_time": "2021-06-25T16:45:12.373520Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|col1|co12|\n",
      "+----+----+\n",
      "|   A|   1|\n",
      "|   B|   2|\n",
      "|   C|   3|\n",
      "+----+----+\n",
      "\n",
      "+----+----+\n",
      "|col1|co12|\n",
      "+----+----+\n",
      "|   A|  20|\n",
      "|   Y|  30|\n",
      "|   Z|  50|\n",
      "+----+----+\n",
      "\n",
      "left join\n",
      "+----+----+----+\n",
      "|col1|co12|co12|\n",
      "+----+----+----+\n",
      "|   B|   2|null|\n",
      "|   C|   3|null|\n",
      "|   A|   1|  20|\n",
      "+----+----+----+\n",
      "\n",
      "right join\n",
      "+----+----+----+\n",
      "|col1|co12|co12|\n",
      "+----+----+----+\n",
      "|   Y|null|  30|\n",
      "|   Z|null|  50|\n",
      "|   A|   1|  20|\n",
      "+----+----+----+\n",
      "\n",
      "inner join\n",
      "+----+----+----+\n",
      "|col1|co12|co12|\n",
      "+----+----+----+\n",
      "|   A|   1|  20|\n",
      "+----+----+----+\n",
      "\n",
      "outer join\n",
      "+----+----+----+\n",
      "|col1|co12|co12|\n",
      "+----+----+----+\n",
      "|   B|   2|null|\n",
      "|   Y|null|  30|\n",
      "|   C|   3|null|\n",
      "|   Z|null|  50|\n",
      "|   A|   1|  20|\n",
      "+----+----+----+\n",
      "\n",
      "cross\n",
      "+----+----+----+\n",
      "|col1|co12|col1|\n",
      "+----+----+----+\n",
      "|   A|   1|   A|\n",
      "|   A|   1|   Y|\n",
      "|   A|   1|   Z|\n",
      "|   B|   2|   A|\n",
      "|   C|   3|   A|\n",
      "|   B|   2|   Y|\n",
      "|   B|   2|   Z|\n",
      "|   C|   3|   Y|\n",
      "|   C|   3|   Z|\n",
      "+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1 left, right, inner, outer, cross\n",
    "# cross join ( cartesian product with another DataFrame )\n",
    "# cross join usually use high computational cost which we should avoid to use it \n",
    "# https://spark.apache.org/docs/2.3.0/api/python/pyspark.sql.html#pyspark.sql.GroupedData.apply\n",
    "\n",
    "\n",
    "# https://www.youtube.com/watch?v=fp53QhSfQcI\n",
    "# check the viodeo 05:55 ~ 9.08\n",
    "# The shuffle hash join works best when\n",
    "# distribute evenly with the key we are joining on\n",
    "# have an adequate number of keys for parallesim\n",
    "# The problem often happens when the table you wanna join is unevenly distribute (8 : 00)\n",
    "left = spark.createDataFrame(\n",
    "    [\n",
    "        (\"A\", 1),\n",
    "        (\"B\", 2),\n",
    "        (\"C\", 3)\n",
    "    ],\n",
    "    (\"col1\",\"co12\")\n",
    ")\n",
    "\n",
    "right = spark.createDataFrame(\n",
    "    [\n",
    "        (\"A\", 20),\n",
    "        (\"Y\", 30),\n",
    "        (\"Z\", 50)\n",
    "    ],\n",
    "    (\"col1\",\"co12\")\n",
    ")\n",
    "\n",
    "left.show()\n",
    "right.show()\n",
    "\n",
    "print('left join')\n",
    "left.join(right,on='col1',how='left').show()\n",
    "print('right join')\n",
    "left.join(right,on='col1',how='right').show()\n",
    "print('inner join')\n",
    "left.join(right,on='col1',how='inner').show()\n",
    "print('outer join')\n",
    "left.join(right,on='col1',how='outer').show()\n",
    "print('cross')\n",
    "left.crossJoin(right.select('col1')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T16:45:15.876233Z",
     "start_time": "2021-06-25T16:45:15.368014Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n",
      "|article_id|             img_url|\n",
      "+----------+--------------------+\n",
      "|     14431|https://pic.pimg....|\n",
      "|     14431|https://pic.pimg....|\n",
      "|     14431|https://pic.pimg....|\n",
      "|     67789|https://pic.pimg....|\n",
      "|     67789|https://pic.pimg....|\n",
      "+----------+--------------------+\n",
      "\n",
      "+----------+--------------------+\n",
      "|article_id|             img_url|\n",
      "+----------+--------------------+\n",
      "|     14431|https://pic.pimg....|\n",
      "+----------+--------------------+\n",
      "\n",
      "== Physical Plan ==\n",
      "*(3) Project [img_url#1462, article_id#1461L, article_id#1483L]\n",
      "+- *(3) BroadcastHashJoin [img_url#1462], [img_url#1484], LeftOuter, BuildRight\n",
      "   :- Scan ExistingRDD[article_id#1461L,img_url#1462]\n",
      "   +- BroadcastExchange HashedRelationBroadcastMode(List(input[1, string, true]))\n",
      "      +- *(2) GlobalLimit 1\n",
      "         +- Exchange SinglePartition\n",
      "            +- *(1) LocalLimit 1\n",
      "               +- Scan ExistingRDD[article_id#1483L,img_url#1484]\n",
      "+--------------------+----------+----------+\n",
      "|             img_url|article_id|article_id|\n",
      "+--------------------+----------+----------+\n",
      "|https://pic.pimg....|     14431|     14431|\n",
      "|https://pic.pimg....|     14431|      null|\n",
      "|https://pic.pimg....|     14431|      null|\n",
      "|https://pic.pimg....|     67789|      null|\n",
      "|https://pic.pimg....|     67789|      null|\n",
      "+--------------------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# broadcast join\n",
    "# https://www.youtube.com/watch?v=fp53QhSfQcI\n",
    "# if we perform broadcast join 14:50 (no shuffling)\n",
    "# if we DOESNT perform broad join 05:55(shuffling a lot)\n",
    "\n",
    "data = [\n",
    "    (14431,'https://pic.pimg.tw/happy78/1528543947-685380499_n.jpg'),\n",
    "    (14431,'https://pic.pimg.tw/happy78/1528543947-362759723_n.jpg'),\n",
    "    (14431,'https://pic.pimg.tw/happy78/1528543962-2265924582_n.jpg'),\n",
    "    (67789,'https://pic.pimg.tw/happy78/1528543962-4007835890_n.jpg'),\n",
    "    (67789,'https://pic.pimg.tw/happy78/1528543962-45890_n.jpg')\n",
    "]\n",
    "\n",
    "columns = ['article_id','img_url']\n",
    "\n",
    "df_big = spark.createDataFrame(data=data, schema=columns)\n",
    "df_big.show(n=5)\n",
    "\n",
    "df_small = df_big.limit(1)\n",
    "df_small.show(n=5)\n",
    "\n",
    "# NOTE, broadcast join support only left join\n",
    "df_joined = df_big.join(F.broadcast(df_small), on=['img_url'],how='left')\n",
    "df_joined.explain()\n",
    "df_joined.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T16:45:17.103693Z",
     "start_time": "2021-06-25T16:45:15.880475Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|             img_url|popularity|\n",
      "+--------------------+----------+\n",
      "|https://pic.pimg....|         4|\n",
      "|https://pic.pimg....|         5|\n",
      "|https://pic.pimg....|         1|\n",
      "|https://pic.pimg....|         9|\n",
      "|https://pic.pimg....|        10|\n",
      "+--------------------+----------+\n",
      "\n",
      "+----------+--------------------+\n",
      "|article_id|             img_url|\n",
      "+----------+--------------------+\n",
      "|     14431|https://pic.pimg....|\n",
      "|       888|https://pic.pimg....|\n",
      "+----------+--------------------+\n",
      "\n",
      "+--------------------+----------+----------+\n",
      "|             img_url|popularity|article_id|\n",
      "+--------------------+----------+----------+\n",
      "|https://pic.pimg....|         4|     14431|\n",
      "+--------------------+----------+----------+\n",
      "\n",
      "+--------------------+----------+----------+\n",
      "|             img_url|article_id|popularity|\n",
      "+--------------------+----------+----------+\n",
      "|https://pic.pimg....|     14431|         4|\n",
      "|https://pic.pimg....|       888|      null|\n",
      "+--------------------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# broadcast hash join but keet the right hand keys\n",
    "\n",
    "# val small_df = sc.parallelize(List((\"Alice\", 15), (\"Bob\", 20)).toDF(\"name\", \"age\")\n",
    "# val large_df = sc.parallelize((\"Bob\", 40), (\"SomeOne\", 50)).toDF(\"name\", \"age\")\n",
    "\n",
    "# val df = large_df.withColumnRenamed(\"age\", \"large_age\").join(broadcast(small_df), Array(\"name\"), \"right_outer\")\n",
    "# val df2 = df.withColumn(\"age\", when($\"large_age\".isNotNull, $\"age\" + $\"large_age\").otherwise($\"age\")).select(\"name\", \"age\")\n",
    "# df2.show\n",
    "\n",
    "\n",
    "\n",
    "big = [\n",
    "    (14431,'https://pic.pimg.tw/happy78/1528543947-685380499_n.jpg','4'),\n",
    "    (14431,'https://pic.pimg.tw/happy78/1528543947-362759723_n.jpg','5'),\n",
    "    (14431,'https://pic.pimg.tw/happy78/1528543962-2265924582_n.jpg','1'),\n",
    "    (67789,'https://pic.pimg.tw/happy78/1528543962-4007835890_n.jpg','9'),\n",
    "    (67789,'https://pic.pimg.tw/happy78/1528543962-45890_n.jpg','10')\n",
    "]\n",
    "\n",
    "small =  [\n",
    "    (14431,'https://pic.pimg.tw/happy78/1528543947-685380499_n.jpg'),\n",
    "    (888,'https://pic.pimg.tw/happy78/6666_n.jpg'),\n",
    "    ]\n",
    "\n",
    "\n",
    "columns = ['article_id','img_url']\n",
    "\n",
    "df_big = (\n",
    "    spark.createDataFrame(data=big, schema=[*columns,'popularity'])\n",
    "    .select('img_url','popularity')\n",
    ")\n",
    "df_big.show(n=5)\n",
    "\n",
    "df_small = (\n",
    "    spark.createDataFrame(data=small, schema=columns)\n",
    ")\n",
    "df_small.show(n=5)\n",
    "\n",
    "# NOTE, broadcast join support only left join\n",
    "df_joined = (\n",
    "    df_big.join(F.broadcast(df_small), on=['img_url'],how='left')\n",
    "    .where(C(\"article_id\").isNotNull())\n",
    ")\n",
    "df_joined.show()\n",
    "\n",
    "# df2 = df.withColumn(\"age\",\n",
    "#                     when($\"large_age\".isNotNull, $\"age\" + $\"large_age\")\n",
    "#                     .otherwise($\"age\")).select(\"name\", \"age\")\n",
    "\n",
    "df_small_full = (\n",
    "    df_small\n",
    "    .join(df_joined.drop('article_id'),on=['img_url'],how='left')\n",
    ")\n",
    "\n",
    "df_small_full.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T16:45:17.982930Z",
     "start_time": "2021-06-25T16:45:17.105952Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------+----------+----------+\n",
      "|img_url                                               |article_id|popularity|\n",
      "+------------------------------------------------------+----------+----------+\n",
      "|https://pic.pimg.tw/happy78/1528543947-685380499_n.jpg|14431     |4         |\n",
      "|https://pic.pimg.tw/happy78/6666_n.jpg                |888       |null      |\n",
      "+------------------------------------------------------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# broadcast join and keep the right hand \n",
    "from typing import List\n",
    "\n",
    "def broadcast_right_join(\n",
    "    large_sdf : DataFrame,\n",
    "    small_sdf : DataFrame,\n",
    "    on : List[str],\n",
    "    force_broadcast : bool = False,\n",
    "    show_plan : bool = False):\n",
    "    \n",
    "    if force_broadcast:\n",
    "        small_sdf = F.broadcast(small_sdf)\n",
    "    \n",
    "    # we perform broadcast join at first pass\n",
    "    filtered_sdf = (\n",
    "        large_sdf\n",
    "        .join(small_sdf, on=on, how='left_semi')\n",
    "    )\n",
    "    \n",
    "    # Since we filter the large_sdf by small_sdf\n",
    "    # the filtered_sdf become small\n",
    "    # now we get all the rows in small_sdf with related rows in large_sdf\n",
    "    full_info_small_sdf = (\n",
    "        small_sdf\n",
    "        .join(\n",
    "            filtered_sdf,\n",
    "            on=on, how='left')\n",
    "    )\n",
    "    if show_plan:\n",
    "        print('first pass : ', filtered_sdf.explain())\n",
    "    return full_info_small_sdf\n",
    "\n",
    "df_small_full = broadcast_right_join(df_big, df_small,['img_url'], force_broadcast=False)\n",
    "df_small_full.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T16:45:18.660798Z",
     "start_time": "2021-06-25T16:45:17.985311Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n",
      "|article_id|             img_url|\n",
      "+----------+--------------------+\n",
      "|     14431|https://pic.pimg....|\n",
      "|     14431|https://pic.pimg....|\n",
      "|     14431|https://pic.pimg....|\n",
      "|     67789|https://pic.pimg....|\n",
      "|     67789|https://pic.pimg....|\n",
      "+----------+--------------------+\n",
      "\n",
      "+----------+--------------------+\n",
      "|article_id|             img_url|\n",
      "+----------+--------------------+\n",
      "|     14431|https://pic.pimg....|\n",
      "+----------+--------------------+\n",
      "\n",
      "left_semi : inner join but return only left dataframe column\n",
      "+--------------------+----------+\n",
      "|             img_url|article_id|\n",
      "+--------------------+----------+\n",
      "|https://pic.pimg....|     14431|\n",
      "+--------------------+----------+\n",
      "\n",
      "left_anti : difference (left - right) but return only left dataframe column\n",
      "+--------------------+----------+\n",
      "|             img_url|article_id|\n",
      "+--------------------+----------+\n",
      "|https://pic.pimg....|     14431|\n",
      "|https://pic.pimg....|     14431|\n",
      "|https://pic.pimg....|     67789|\n",
      "|https://pic.pimg....|     67789|\n",
      "+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# left_anti and ledt_semi\n",
    "# https://dzone.com/articles/pyspark-join-explained-with-examples\n",
    "\n",
    "\n",
    "data = [\n",
    "    (14431,'https://pic.pimg.tw/happy78/1528543947-685380499_n.jpg'),\n",
    "    (14431,'https://pic.pimg.tw/happy78/1528543947-362759723_n.jpg'),\n",
    "    (14431,'https://pic.pimg.tw/happy78/1528543962-2265924582_n.jpg'),\n",
    "    (67789,'https://pic.pimg.tw/happy78/1528543962-4007835890_n.jpg'),\n",
    "    (67789,'https://pic.pimg.tw/happy78/1528543962-45890_n.jpg')\n",
    "]\n",
    "\n",
    "columns = ['article_id','img_url']\n",
    "\n",
    "df_big = spark.createDataFrame(data=data, schema=columns)\n",
    "df_big.show(n=5)\n",
    "\n",
    "df_small = df_big.limit(1)\n",
    "df_small.show(n=5)\n",
    "\n",
    "print('left_semi : inner join but return only left dataframe column')\n",
    "\n",
    "df_left_semi = df_big.join(df_small,on=['img_url'],how='left_semi')\n",
    "df_left_semi.show()\n",
    "\n",
    "print('left_anti : difference (left - right) but return only left dataframe column')\n",
    "\n",
    "df_left_anti = df_big.join(df_small,on=['img_url'],how='left_anti')\n",
    "df_left_anti.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregation Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T16:45:19.035204Z",
     "start_time": "2021-06-25T16:45:18.663900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+------------------------+---------------+---------+\n",
      "|store_name|food_category|food_category_popularity|        img_url|author_id|\n",
      "+----------+-------------+------------------------+---------------+---------+\n",
      "|    hotpop|         Meat|                       3|https//:123.png|   rtyg11|\n",
      "+----------+-------------+------------------------+---------------+---------+\n",
      "\n",
      "== Physical Plan ==\n",
      "*(3) Project [img_url#1623, store_name#1620, food_category#1621, food_category_popularity#1622L, author_id#1624, store_name#1649, food_category#1650, food_category_popularity#1651L, author_id#1653]\n",
      "+- *(3) BroadcastHashJoin [img_url#1623], [img_url#1652], Inner, BuildRight\n",
      "   :- *(3) Filter isnotnull(img_url#1623)\n",
      "   :  +- Scan ExistingRDD[store_name#1620,food_category#1621,food_category_popularity#1622L,img_url#1623,author_id#1624]\n",
      "   +- BroadcastExchange HashedRelationBroadcastMode(List(input[3, string, false]))\n",
      "      +- *(2) Filter isnotnull(img_url#1652)\n",
      "         +- *(2) GlobalLimit 1\n",
      "            +- Exchange SinglePartition\n",
      "               +- *(1) LocalLimit 1\n",
      "                  +- Scan ExistingRDD[store_name#1649,food_category#1650,food_category_popularity#1651L,img_url#1652,author_id#1653]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_url</th>\n",
       "      <th>store_name</th>\n",
       "      <th>food_category</th>\n",
       "      <th>food_category_popularity</th>\n",
       "      <th>author_id</th>\n",
       "      <th>store_name</th>\n",
       "      <th>food_category</th>\n",
       "      <th>food_category_popularity</th>\n",
       "      <th>author_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https//:123.png</td>\n",
       "      <td>hotpop</td>\n",
       "      <td>Meat</td>\n",
       "      <td>3</td>\n",
       "      <td>rtyg11</td>\n",
       "      <td>hotpop</td>\n",
       "      <td>Meat</td>\n",
       "      <td>3</td>\n",
       "      <td>rtyg11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           img_url store_name food_category  food_category_popularity  \\\n",
       "0  https//:123.png     hotpop          Meat                         3   \n",
       "\n",
       "  author_id store_name food_category  food_category_popularity author_id  \n",
       "0    rtyg11     hotpop          Meat                         3    rtyg11  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2 broadcast join\n",
    "# boardcast join for big dataframe and small data frame join\n",
    "# broadcast small dataframe to each worker,\n",
    "# them the excute plan make narrow dependency instead of wide dependency\n",
    "# code - https://stackoverflow.com/questions/37487318/spark-sql-broadcast-hash-join\n",
    "# documentation - search broadcast join in doc https://spark.apache.org/docs/2.3.2/api/python/pyspark.sql.html\n",
    "# concept https://www.youtube.com/watch?v=fp53QhSfQcI 14:32 ~ 14:59\n",
    "data = [\n",
    "    (\"hotpop\",\"Meat\",3,\"https//:123.png\",\"rtyg11\"),\n",
    "    (\"hotpop\",\"Meat\",3,\"https//:456.png\",\"rtyg11\"),\n",
    "    (\"hotpop\",\"Meat\",3,\"https//:789.png\",\"rtyg11\"),\n",
    "    (\"hotpop\",\"Vegetable\",2,\"https//:111.png\",\"rtyg11\"),\n",
    "    (\"hotpop\",\"Vegetable\",2,\"https//:222.png\",\"rtyg11\"),\n",
    "    (\"branch\",\"Fried food\",1,\"https//:333.png\",\"bvc1\"),\n",
    "    (\"branch\",\"Dessert\",1,\"https//:444.png\",\"7854\"),\n",
    "  ]\n",
    "\n",
    "columns = [\"store_name\",\"food_category\",\"food_category_popularity\",\"img_url\",\"author_id\"]\n",
    "df_big = spark.createDataFrame(data = data, schema = columns)\n",
    "\n",
    "df_small = df_big.limit(1)\n",
    "\n",
    "df_small.show()\n",
    "\n",
    "df_broadcast_join = (\n",
    " df_big\n",
    "    .join(F.broadcast(df_small), on=\"img_url\")\n",
    ")\n",
    "\n",
    "df_broadcast_join.explain()\n",
    "\n",
    "df_broadcast_join.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T16:45:20.838993Z",
     "start_time": "2021-06-25T16:45:19.036996Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|col1|co12|\n",
      "+----+----+\n",
      "|   A|   1|\n",
      "|   B|   2|\n",
      "|   C|   3|\n",
      "+----+----+\n",
      "\n",
      "+----+----+\n",
      "|col1|co12|\n",
      "+----+----+\n",
      "|   A|  20|\n",
      "|   Y|  30|\n",
      "|   Z|  50|\n",
      "+----+----+\n",
      "\n",
      "inner - inner join and keeps columns with two tables\n",
      "which will be annoying if there are same column name or dropping column by yourself\n",
      "+----+----+----+\n",
      "|col1|co12|co12|\n",
      "+----+----+----+\n",
      "|   A|   1|  20|\n",
      "+----+----+----+\n",
      "\n",
      "left_semi - inner join but only keeps left table columns\n",
      "+----+----+\n",
      "|col1|co12|\n",
      "+----+----+\n",
      "|   A|   1|\n",
      "+----+----+\n",
      "\n",
      "left_anti - selects all rows from left that are not present in right.\n",
      "+----+----+\n",
      "|col1|co12|\n",
      "+----+----+\n",
      "|   B|   2|\n",
      "|   C|   3|\n",
      "+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3 left semi, left anti\n",
    "# right semi, right anti\n",
    "# https://dzone.com/articles/pyspark-join-explained-with-examples\n",
    "\n",
    "left = spark.createDataFrame(\n",
    "    [\n",
    "        (\"A\", 1),\n",
    "        (\"B\", 2),\n",
    "        (\"C\", 3)\n",
    "    ],\n",
    "    (\"col1\",\"co12\")\n",
    ")\n",
    "\n",
    "right = spark.createDataFrame(\n",
    "    [\n",
    "        (\"A\", 20),\n",
    "        (\"Y\", 30),\n",
    "        (\"Z\", 50)\n",
    "    ],\n",
    "    (\"col1\",\"co12\")\n",
    ")\n",
    "\n",
    "left.show()\n",
    "right.show()\n",
    "\n",
    "print('inner - inner join and keeps columns with two tables')\n",
    "print('which will be annoying if there are same column name or dropping column by yourself')\n",
    "\n",
    "left.join(right, on='col1',how='inner').show()\n",
    "\n",
    "print('left_semi - inner join but only keeps left table columns')\n",
    "\n",
    "left.join(right, on='col1',how='left_semi').show()\n",
    "\n",
    "print('left_anti - selects all rows from left that are not present in right.')\n",
    "\n",
    "left.join(right, on='col1',how='left_anti').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregation (12+)\n",
    "\n",
    "## Simple GroupBy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T16:45:20.891456Z",
     "start_time": "2021-06-25T16:45:20.851095Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.group.GroupedData'>\n",
      "\n",
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_df', '_jgd', 'agg', 'apply', 'avg', 'count', 'max', 'mean', 'min', 'pivot', 'sql_ctx', 'sum']\n"
     ]
    }
   ],
   "source": [
    "# 1 knowing the groupby object method\n",
    "\n",
    "data = [\n",
    "    (\"James\",\"\",\"Smith\",\"36636\",\"RD\",\"M\", 3000),\n",
    "    (\"Michael\",\"Rose\",\"\",\"40288\",\"RD\",\"M\", 4000),\n",
    "    (\"Robert\",\"\",\"Williams\",\"42114\",\"SRE\",\"M\", 4000),\n",
    "    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"SRE\",\"F\", 4000),\n",
    "    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",\"BACKEND\", -1)\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"firstname\",StringType(), True), # Nullable True\n",
    "    StructField(\"middlename\",StringType(), True),\n",
    "    StructField(\"lastname\",StringType(), True),\n",
    "    StructField(\"id\",StringType(), True),\n",
    "    StructField(\"apartment\",StringType(), True),\n",
    "    StructField(\"gender\",StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True)\n",
    "    ])\n",
    "df = spark.createDataFrame(data=data,schema=schema)\n",
    "\n",
    "df_grp = df.groupBy(\"salary\")\n",
    "\n",
    "print(type(df_grp), dir(df_grp), sep='\\n\\n')\n",
    "\n",
    "# avg, count, max, mean, sum  - Common aggregation\n",
    "# pivot - two column x, y with value in the table\n",
    "# sql_ctx - apply sql command\n",
    "# custom function - agg, apply\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T16:45:21.915663Z",
     "start_time": "2021-06-25T16:45:20.894807Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|deparment|avg(salary)|\n",
      "+---------+-----------+\n",
      "|        F|       -1.0|\n",
      "|       RD|     3500.0|\n",
      "|      SRE|     4000.0|\n",
      "+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2 apply single aggregation fuction on groupby object\n",
    "data = [\n",
    "    (\"James\",\"\",\"Smith\",\"36636\",\"RD\",\"M\", 3000),\n",
    "    (\"Michael\",\"Rose\",\"\",\"40288\",\"RD\",\"M\", 4000),\n",
    "    (\"Robert\",\"\",\"Williams\",\"42114\",\"SRE\",\"M\", 4000),\n",
    "    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"SRE\",\"F\", 4000),\n",
    "    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",\"BACKEND\", -1)\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"firstname\",StringType(), True), # Nullable True\n",
    "    StructField(\"middlename\",StringType(), True),\n",
    "    StructField(\"lastname\",StringType(), True),\n",
    "    StructField(\"id\",StringType(), True),\n",
    "    StructField(\"deparment\",StringType(), True),\n",
    "    StructField(\"gender\",StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True)\n",
    "    ])\n",
    "df = spark.createDataFrame(data=data,schema=schema)\n",
    "\n",
    "df_grp_department = (\n",
    "    df.groupby(\"deparment\").mean(\"salary\").alias(\"mean_salary\")\n",
    ")\n",
    "\n",
    "df_grp_department.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T16:45:22.446509Z",
     "start_time": "2021-06-25T16:45:21.918109Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+----------+----------+----------+----------+\n",
      "|deparment|sum_salary|avg_salary|max_salary|min_salary|group_size|\n",
      "+---------+----------+----------+----------+----------+----------+\n",
      "|        F|        -1|      -1.0|        -1|        -1|         1|\n",
      "|       RD|      7000|    3500.0|      4000|      3000|         2|\n",
      "|      SRE|      8000|    4000.0|      4000|      4000|         2|\n",
      "+---------+----------+----------+----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3 apply multiple aggregation fuction on groupby object\n",
    "data = [\n",
    "    (\"James\",\"\",\"Smith\",\"36636\",\"RD\",\"M\", 3000),\n",
    "    (\"Michael\",\"Rose\",\"\",\"40288\",\"RD\",\"M\", 4000),\n",
    "    (\"Robert\",\"\",\"Williams\",\"42114\",\"SRE\",\"M\", 4000),\n",
    "    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"SRE\",\"F\", 4000),\n",
    "    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",\"BACKEND\", -1)\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"firstname\",StringType(), True), # Nullable True\n",
    "    StructField(\"middlename\",StringType(), True),\n",
    "    StructField(\"lastname\",StringType(), True),\n",
    "    StructField(\"id\",StringType(), True),\n",
    "    StructField(\"deparment\",StringType(), True),\n",
    "    StructField(\"gender\",StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True)\n",
    "    ])\n",
    "df = spark.createDataFrame(data=data,schema=schema)\n",
    "\n",
    "df_grp_department = df.groupby(\"deparment\").agg(\n",
    "    F.sum(\"salary\").alias(\"sum_salary\"),\n",
    "    F.avg(\"salary\").alias(\"avg_salary\"),\n",
    "    F.max(\"salary\").alias(\"max_salary\"),\n",
    "    F.min(\"salary\").alias(\"min_salary\"),\n",
    "    F.count(\"salary\").alias(\"group_size\")\n",
    ")\n",
    "\n",
    "df_grp_department.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T16:45:22.785593Z",
     "start_time": "2021-06-25T16:45:22.449417Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>deparment</th>\n",
       "      <th>sum_salary</th>\n",
       "      <th>avg_salary</th>\n",
       "      <th>max_salary</th>\n",
       "      <th>min_salary</th>\n",
       "      <th>count_rows</th>\n",
       "      <th>all_rows</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>F</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>[-1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RD</td>\n",
       "      <td>7000</td>\n",
       "      <td>3500.0</td>\n",
       "      <td>4000</td>\n",
       "      <td>3000</td>\n",
       "      <td>2</td>\n",
       "      <td>[3000, 4000]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SRE</td>\n",
       "      <td>8000</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>4000</td>\n",
       "      <td>4000</td>\n",
       "      <td>2</td>\n",
       "      <td>[4000, 4000]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  deparment  sum_salary  avg_salary  max_salary  min_salary  count_rows  \\\n",
       "0         F          -1        -1.0          -1          -1           1   \n",
       "1        RD        7000      3500.0        4000        3000           2   \n",
       "2       SRE        8000      4000.0        4000        4000           2   \n",
       "\n",
       "       all_rows  \n",
       "0          [-1]  \n",
       "1  [3000, 4000]  \n",
       "2  [4000, 4000]  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4 collect data point for each group with the stats(min, max, sum, avg, count)\n",
    "\n",
    "\n",
    "# apply multiple aggregation fuction on groupby object\n",
    "data = [\n",
    "    (\"James\",\"\",\"Smith\",\"36636\",\"RD\",\"M\", 3000),\n",
    "    (\"Michael\",\"Rose\",\"\",\"40288\",\"RD\",\"M\", 4000),\n",
    "    (\"Robert\",\"\",\"Williams\",\"42114\",\"SRE\",\"M\", 4000),\n",
    "    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"SRE\",\"F\", 4000),\n",
    "    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",\"BACKEND\", -1)\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"firstname\",StringType(), True), # Nullable True\n",
    "    StructField(\"middlename\",StringType(), True),\n",
    "    StructField(\"lastname\",StringType(), True),\n",
    "    StructField(\"id\",StringType(), True),\n",
    "    StructField(\"deparment\",StringType(), True),\n",
    "    StructField(\"gender\",StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True)\n",
    "    ])\n",
    "df = spark.createDataFrame(data=data,schema=schema)\n",
    "\n",
    "df_grp_department = df.groupby(\"deparment\").agg(\n",
    "    F.sum(\"salary\").alias(\"sum_salary\"),\n",
    "    F.avg(\"salary\").alias(\"avg_salary\"),\n",
    "    F.max(\"salary\").alias(\"max_salary\"),\n",
    "    F.min(\"salary\").alias(\"min_salary\"),\n",
    "    F.count(\"salary\").alias(\"count_rows\"),\n",
    "    F.collect_list(\"salary\").alias(\"all_rows\")\n",
    ")\n",
    "\n",
    "df_grp_department.toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T16:45:23.313188Z",
     "start_time": "2021-06-25T16:45:22.786937Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+-----+---------+-------+------+\n",
      "|firstname|middlename|lastname|   id|deparment| gender|salary|\n",
      "+---------+----------+--------+-----+---------+-------+------+\n",
      "|      Jen|      Mary|   Brown|     |        F|BACKEND|    -1|\n",
      "|  Michael|      Rose|        |40288|       RD|      M|  8000|\n",
      "|    Maria|      Anne|   Jones|39192|      SRE|      F|  6000|\n",
      "+---------+----------+--------+-----+---------+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5 get first one row in each group\n",
    "# We use Window Function here\n",
    "# Key to think about this, we rank the data in each group, then \n",
    "# filtering\n",
    "# no nothing is groupby\n",
    "# which is different in pandas\n",
    "\n",
    "data = [\n",
    "    (\"James\",\"\",\"Smith\",\"36636\",\"RD\",\"M\", 3000),\n",
    "    (\"Michael\",\"Rose\",\"\",\"40288\",\"RD\",\"M\", 8000),\n",
    "    (\"Robert\",\"\",\"Williams\",\"42114\",\"SRE\",\"M\", 4000),\n",
    "    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"SRE\",\"F\", 6000),\n",
    "    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",\"BACKEND\", -1)\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"firstname\",StringType(), True), # Nullable True\n",
    "    StructField(\"middlename\",StringType(), True),\n",
    "    StructField(\"lastname\",StringType(), True),\n",
    "    StructField(\"id\",StringType(), True),\n",
    "    StructField(\"deparment\",StringType(), True),\n",
    "    StructField(\"gender\",StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True)\n",
    "    ])\n",
    "\n",
    "\n",
    "df = spark.createDataFrame(data=data,schema=schema)\n",
    "\n",
    "df = (\n",
    "    df.withColumn(\"rank_salary_by_deparment\",\n",
    "                  F.row_number().over(\n",
    "                  W.partitionBy(\"deparment\")\\\n",
    "                      .orderBy(F.desc(\"salary\"))\n",
    "                  )\n",
    "                 )\\\n",
    "    .filter(F.col(\"rank_salary_by_deparment\") == 1)\\\n",
    "    .drop('rank_salary_by_deparment')\n",
    ")\n",
    "\n",
    "df.show(n=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T16:45:23.818425Z",
     "start_time": "2021-06-25T16:45:23.315855Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+----------+----------+----------+----------+\n",
      "|deparment|sum_salary|avg_salary|max_salary|min_salary|count_rows|\n",
      "+---------+----------+----------+----------+----------+----------+\n",
      "|       RD|      7000|    3500.0|      4000|      3000|         2|\n",
      "|      SRE|      8000|    4000.0|      4000|      4000|         2|\n",
      "+---------+----------+----------+----------+----------+----------+\n",
      "\n",
      "root\n",
      " |-- deparment: string (nullable = true)\n",
      " |-- sum_salary: long (nullable = true)\n",
      " |-- avg_salary: double (nullable = true)\n",
      " |-- max_salary: integer (nullable = true)\n",
      " |-- min_salary: integer (nullable = true)\n",
      " |-- count_rows: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6 groupby and filtering\n",
    "\n",
    "data = [\n",
    "    (\"James\",\"\",\"Smith\",\"36636\",\"RD\",\"M\", 3000),\n",
    "    (\"Michael\",\"Rose\",\"\",\"40288\",\"RD\",\"M\", 4000),\n",
    "    (\"Robert\",\"\",\"Williams\",\"42114\",\"SRE\",\"M\", 4000),\n",
    "    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"SRE\",\"F\", 4000),\n",
    "    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",\"BACKEND\", -1)\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"firstname\",StringType(), True), # Nullable True\n",
    "    StructField(\"middlename\",StringType(), True),\n",
    "    StructField(\"lastname\",StringType(), True),\n",
    "    StructField(\"id\",StringType(), True),\n",
    "    StructField(\"deparment\",StringType(), True),\n",
    "    StructField(\"gender\",StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True)\n",
    "    ])\n",
    "df = spark.createDataFrame(data=data,schema=schema)\n",
    "\n",
    "df_grp_department = (\n",
    "    df.groupby(\"deparment\").agg(\n",
    "        F.sum(\"salary\").alias(\"sum_salary\"),\n",
    "        F.avg(\"salary\").alias(\"avg_salary\"),\n",
    "        F.max(\"salary\").alias(\"max_salary\"),\n",
    "        F.min(\"salary\").alias(\"min_salary\"),\n",
    "        F.count(\"salary\").alias(\"count_rows\"))\n",
    "    .filter(C(\"sum_salary\") > 0)\n",
    "    \n",
    ")\n",
    "\n",
    "df_grp_department.show(n=5)\n",
    "df_grp_department.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T16:45:24.585418Z",
     "start_time": "2021-06-25T16:45:23.821148Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|item|score|\n",
      "+----+-----+\n",
      "|   a|   10|\n",
      "|   a|   10|\n",
      "|   a|   15|\n",
      "|   a|   20|\n",
      "|   a|    5|\n",
      "+----+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----+-----+----+----------+----------+\n",
      "|item|score|rank|dense_rank|row_number|\n",
      "+----+-----+----+----------+----------+\n",
      "|   a|   20|   1|         1|         1|\n",
      "|   a|   15|   2|         2|         2|\n",
      "|   a|   10|   3|         3|         3|\n",
      "|   a|   10|   3|         3|         4|\n",
      "|   a|    5|   5|         4|         5|\n",
      "|   a|    0|   6|         5|         6|\n",
      "|   a|    0|   6|         5|         7|\n",
      "|   a|    0|   6|         5|         8|\n",
      "|   a|    0|   6|         5|         9|\n",
      "|   a|    0|   6|         5|        10|\n",
      "+----+-----+----+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 7 rank, dense_rank, and row_number\n",
    "# https://stackoverflow.com/questions/44968912/difference-in-dense-rank-and-row-number-in-spark\n",
    "# The window functions\n",
    "\n",
    "data = [\n",
    "    (\"a\",10),\n",
    "    ('a',10),\n",
    "    ('a',15),\n",
    "    ('a',20),\n",
    "    ('a',5),\n",
    "    ('a',0),\n",
    "    ('a',0),\n",
    "    ('a',0),\n",
    "    ('a',0),\n",
    "    ('a',0)\n",
    "]\n",
    "\n",
    "columns = ['item','score']\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show(n=5)\n",
    "\n",
    "window_spec = W.partitionBy(\"item\").orderBy(C(\"score\").desc())\n",
    "df = (\n",
    "    df.withColumn(\"rank\", F.rank().over(window_spec))\\\n",
    "    .withColumn(\"dense_rank\", F.dense_rank().over(window_spec))\\\n",
    "    .withColumn(\"row_number\", F.row_number().over(window_spec))\n",
    ")\n",
    "\n",
    "df.show(n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T16:45:25.217289Z",
     "start_time": "2021-06-25T16:45:24.587784Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-----+--------------+\n",
      "|store_name|food_category|  url|food_cat_count|\n",
      "+----------+-------------+-----+--------------+\n",
      "|    branch|      Dessert|ulr_7|             1|\n",
      "|    hotpop|         Meat|url_1|             3|\n",
      "|    hotpop|         Meat|url_2|             3|\n",
      "|    hotpop|         Meat|url_3|             3|\n",
      "|    hotpop|    Vegetable|url_4|             2|\n",
      "|    hotpop|    Vegetable|url_5|             2|\n",
      "|    branch|   Fried food|url_6|             1|\n",
      "+----------+-------------+-----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 11 groupby and sum by a window function\n",
    "data = [\n",
    "    (\"hotpop\",\"Meat\",\"url_1\",),\n",
    "    (\"hotpop\",\"Meat\",\"url_2\"),\n",
    "    (\"hotpop\",\"Meat\",\"url_3\"),\n",
    "    (\"hotpop\",\"Vegetable\",\"url_4\"),\n",
    "    (\"hotpop\",\"Vegetable\",\"url_5\"),\n",
    "    (\"branch\",\"Fried food\",\"url_6\"),\n",
    "    (\"branch\",\"Dessert\",\"ulr_7\"),\n",
    "  ]\n",
    "\n",
    "columns = [\"store_name\",\"food_category\",\"url\"]\n",
    "window_spec = w.partitionBy(\"store_name\",\"food_category\")\n",
    "\n",
    "df = spark.createDataFrame(data = data, schema = columns)\n",
    "df = (\n",
    "    df.withColumn(\"food_cat_count\", F.count(\"food_category\").over(window_spec))\n",
    ")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T16:45:26.366898Z",
     "start_time": "2021-06-25T16:45:25.219850Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+-----+----------+-------+------+\n",
      "|firstname|middlename|lastname|   id|department| gender|salary|\n",
      "+---------+----------+--------+-----+----------+-------+------+\n",
      "|    James|          |   Smith|36636|        RD|      M|  3000|\n",
      "|  Michael|      Rose|        |40288|        RD|      M|  4000|\n",
      "|   Robert|          |Williams|42114|       SRE|      M|  4000|\n",
      "|    Maria|      Anne|   Jones|39192|       SRE|      F|  4000|\n",
      "|      Jen|      Mary|   Brown|     |         F|BACKEND|    -1|\n",
      "+---------+----------+--------+-----+----------+-------+------+\n",
      "\n",
      "+----------+-------+----+----+\n",
      "|department|BACKEND|   F|   M|\n",
      "+----------+-------+----+----+\n",
      "|         F|     -1|null|null|\n",
      "|        RD|   null|null|7000|\n",
      "|       SRE|   null|4000|4000|\n",
      "+----------+-------+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 12 pivot the datafrmae when groupby\n",
    "\n",
    "data = [\n",
    "    (\"James\",\"\",\"Smith\",\"36636\",\"RD\",\"M\", 3000),\n",
    "    (\"Michael\",\"Rose\",\"\",\"40288\",\"RD\",\"M\", 4000),\n",
    "    (\"Robert\",\"\",\"Williams\",\"42114\",\"SRE\",\"M\", 4000),\n",
    "    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"SRE\",\"F\", 4000),\n",
    "    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",\"BACKEND\", -1)\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"firstname\",StringType(), True), # Nullable True\n",
    "    StructField(\"middlename\",StringType(), True),\n",
    "    StructField(\"lastname\",StringType(), True),\n",
    "    StructField(\"id\",StringType(), True),\n",
    "    StructField(\"department\",StringType(), True),\n",
    "    StructField(\"gender\",StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True)\n",
    "    ])\n",
    "df = spark.createDataFrame(data=data,schema=schema)\n",
    "\n",
    "df.show()\n",
    "\n",
    "\n",
    "# groupBy as index\n",
    "# pivot as column\n",
    "\n",
    "(\n",
    "    df.groupBy(\"department\").pivot(\"gender\")\n",
    "      .sum(\"salary\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T16:45:29.620380Z",
     "start_time": "2021-06-25T16:45:26.370452Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----+------+----+\n",
      "| id|type|cost|  date|ship|\n",
      "+---+----+----+------+----+\n",
      "|  0|   A| 223|201603|PORT|\n",
      "|  0|   A|  22|201602|PORT|\n",
      "|  0|   A| 422|201601|DOCK|\n",
      "|  1|   B|3213|201602|DOCK|\n",
      "|  1|   B|3213|201601|PORT|\n",
      "|  2|   C|2321|201601|DOCK|\n",
      "+---+----+----+------+----+\n",
      "\n",
      "+---+----+------+------+------+\n",
      "| id|type|201601|201602|201603|\n",
      "+---+----+------+------+------+\n",
      "|  1|   B|3213.0|3213.0|  null|\n",
      "|  2|   C|2321.0|  null|  null|\n",
      "|  0|   A| 422.0|  22.0| 223.0|\n",
      "+---+----+------+------+------+\n",
      "\n",
      "+---+----+------+------+------+\n",
      "| id|type|201601|201602|201603|\n",
      "+---+----+------+------+------+\n",
      "|  1|   B|  PORT|  DOCK|  null|\n",
      "|  2|   C|  DOCK|  null|  null|\n",
      "|  0|   A|  DOCK|  PORT|  PORT|\n",
      "+---+----+------+------+------+\n",
      "\n",
      "+---+----+---------+---------+---------+\n",
      "| id|type|   201601|   201602|   201603|\n",
      "+---+----+---------+---------+---------+\n",
      "|  1|   B|[1, PORT]|[1, DOCK]|     null|\n",
      "|  2|   C|[1, DOCK]|     null|     null|\n",
      "|  0|   A|[1, DOCK]|[1, PORT]|[1, PORT]|\n",
      "+---+----+---------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pivot the non-numerical column\n",
    "\n",
    "\n",
    "rdd = spark.sparkContext.parallelize(\n",
    "    [\n",
    "        (0, \"A\", 223,\"201603\", \"PORT\"), \n",
    "        (0, \"A\", 22,\"201602\", \"PORT\"), \n",
    "        (0, \"A\", 422,\"201601\", \"DOCK\"), \n",
    "        (1,\"B\", 3213,\"201602\", \"DOCK\"), \n",
    "        (1,\"B\", 3213,\"201601\", \"PORT\"), \n",
    "        (2,\"C\", 2321,\"201601\", \"DOCK\")\n",
    "    ]\n",
    ")\n",
    "df_data = spark.createDataFrame(rdd, [\"id\",\"type\", \"cost\", \"date\", \"ship\"])\n",
    "\n",
    "df_data.show()\n",
    "\n",
    "\n",
    "# numerical\n",
    "df_data.groupby(df_data.id, df_data.type).pivot(\"date\").avg(\"cost\").show()\n",
    "\n",
    "# non-numerical\n",
    "\n",
    "(df_data\n",
    "    .groupby(df_data.id, df_data.type)\n",
    "    .pivot(\"date\")\n",
    "    .agg(F.first(\"ship\"))\n",
    "    .show())\n",
    "\n",
    "# perform another aggregation\n",
    "(df_data\n",
    "    .groupby(\"id\", \"type\", \"date\", \"ship\")\n",
    "    .count()\n",
    "    .groupby(\"id\", \"type\")\n",
    "    .pivot(\"date\")\n",
    "    .agg(F.max(F.struct(\"count\", \"ship\")))\n",
    "    .show())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cube and rolling up\n",
    "\n",
    "https://stackoverflow.com/questions/37975227/what-is-the-difference-between-cube-rollup-and-groupby-operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T16:45:30.419502Z",
     "start_time": "2021-06-25T16:45:29.623025Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  x|  y|\n",
      "+---+---+\n",
      "|foo| 1L|\n",
      "|foo| 2L|\n",
      "|bar| 2L|\n",
      "|bar| 2L|\n",
      "+---+---+\n",
      "\n",
      "<class 'pyspark.sql.group.GroupedData'>\n",
      "+----+----+-----+\n",
      "|   x|   y|count|\n",
      "+----+----+-----+\n",
      "|null|  2L|    3|\n",
      "|null|  1L|    1|\n",
      "|null|null|    4|\n",
      "| bar|  2L|    2|\n",
      "| foo|  1L|    1|\n",
      "| foo|null|    2|\n",
      "| bar|null|    2|\n",
      "| foo|  2L|    1|\n",
      "+----+----+-----+\n",
      "\n",
      "+---+---+-----+\n",
      "|  x|  y|count|\n",
      "+---+---+-----+\n",
      "|foo| 2L|    1|\n",
      "|bar| 2L|    2|\n",
      "|foo| 1L|    1|\n",
      "+---+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# cube\n",
    "# cube is an extensin to groupBy\n",
    "# It takes a list of columns and applies aggregation expressions \n",
    "# to all possible combinations of thr grouping columns\n",
    "\n",
    "data = [\n",
    "    (\"foo\",\"1L\"),\n",
    "    (\"foo\",\"2L\"),\n",
    "    (\"bar\",\"2L\"),\n",
    "    (\"bar\",\"2L\"),\n",
    "]\n",
    "\n",
    "col = [\"x\", \"y\"]\n",
    "\n",
    "df = spark.createDataFrame(data, col)\n",
    "\n",
    "df.show()\n",
    "\n",
    "print(type(df.cube(\"x\",\"y\")))\n",
    "\n",
    "df.cube(\"x\",\"y\").count().show()\n",
    "\n",
    "\n",
    "# // +----+----+-----+     \n",
    "# // |   x|   y|count|\n",
    "# // +----+----+-----+\n",
    "# // |null|   1|    1|   <- count of records where y = 1\n",
    "# // |null|   2|    3|   <- count of records where y = 2\n",
    "# // | foo|null|    2|   <- count of records where x = foo\n",
    "# // | bar|   2|    2|   <- count of records where x = bar AND y = 2\n",
    "# // | foo|   1|    1|   <- count of records where x = foo AND y = 1\n",
    "# // | foo|   2|    1|   <- count of records where x = foo AND y = 2\n",
    "# // |null|null|    4|   <- total count of records\n",
    "# // | bar|null|    2|   <- count of records where x = bar\n",
    "# // +----+----+-----+\n",
    "\n",
    "\n",
    "# compare\n",
    "df.groupBy(\"x\",\"y\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T16:45:30.744189Z",
     "start_time": "2021-06-25T16:45:30.421466Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-----+\n",
      "|   x|   y|count|\n",
      "+----+----+-----+\n",
      "|null|null|    4|\n",
      "| bar|  2L|    2|\n",
      "| foo|  1L|    1|\n",
      "| foo|null|    2|\n",
      "| bar|null|    2|\n",
      "| foo|  2L|    1|\n",
      "+----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## rolling up\n",
    "\n",
    "df.rollup(\"x\",\"y\").count().show()\n",
    "\n",
    "\n",
    "# // +----+----+-----+\n",
    "# // |   x|   y|count|\n",
    "# // +----+----+-----+\n",
    "# // | foo|null|    2|   <- count where x is fixed to foo\n",
    "# // | bar|   2|    2|   <- count where x is fixed to bar and y is fixed to  2\n",
    "# // | foo|   1|    1|   ...\n",
    "# // | foo|   2|    1|   ...\n",
    "# // |null|null|    4|   <- count where no column is fixed\n",
    "# // | bar|null|    2|   <- count where x is fixed to bar\n",
    "# // +----+----+-----+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## melt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T16:45:31.118686Z",
     "start_time": "2021-06-25T16:45:30.747654Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------------------------------------------------------------------------\n",
      " poi_name           | 丹丹漢堡                                                                   \n",
      " img_url            | https://pic.pimg.tw/ksdelicacy/1438450572-4270481322.jpg               \n",
      " food_cat_pop_score | 318                                                                    \n",
      " img_article_url    | http://ksdelicacy.pixnet.net/blog/post/55774905                        \n",
      " img_author_id      | ksdelicacy                                                             \n",
      " food_cat           | Fried food                                                             \n",
      "-RECORD 1------------------------------------------------------------------------------------\n",
      " poi_name           | 拿坡里                                                                    \n",
      " img_url            | https://rmfoodie.com/wp-content/uploads/rm/1446140572-1157042739_n.jpg \n",
      " food_cat_pop_score | 333                                                                    \n",
      " img_article_url    | http://rmlove30.pixnet.net/blog/post/61986505                          \n",
      " img_author_id      | RMlove30                                                               \n",
      " food_cat           | Bread                                                                  \n",
      "\n",
      "root\n",
      " |-- poi_name: string (nullable = true)\n",
      " |-- img_url: string (nullable = true)\n",
      " |-- food_cat_pop_score: string (nullable = true)\n",
      " |-- img_article_url: string (nullable = true)\n",
      " |-- img_author_id: string (nullable = true)\n",
      " |-- food_cat: string (nullable = true)\n",
      "\n",
      "-RECORD 0--------------------------\n",
      " poi_name   | 丹丹漢堡                 \n",
      " menu_key   | img_url              \n",
      " menu_value | https://pic.pimg.... \n",
      "-RECORD 1--------------------------\n",
      " poi_name   | 丹丹漢堡                 \n",
      " menu_key   | food_cat_pop_score   \n",
      " menu_value | 318                  \n",
      "-RECORD 2--------------------------\n",
      " poi_name   | 丹丹漢堡                 \n",
      " menu_key   | img_article_url      \n",
      " menu_value | http://ksdelicacy... \n",
      "-RECORD 3--------------------------\n",
      " poi_name   | 丹丹漢堡                 \n",
      " menu_key   | img_author_id        \n",
      " menu_value | ksdelicacy           \n",
      "-RECORD 4--------------------------\n",
      " poi_name   | 丹丹漢堡                 \n",
      " menu_key   | food_cat             \n",
      " menu_value | Fried food           \n",
      "-RECORD 5--------------------------\n",
      " poi_name   | 拿坡里                  \n",
      " menu_key   | img_url              \n",
      " menu_value | https://rmfoodie.... \n",
      "-RECORD 6--------------------------\n",
      " poi_name   | 拿坡里                  \n",
      " menu_key   | food_cat_pop_score   \n",
      " menu_value | 333                  \n",
      "-RECORD 7--------------------------\n",
      " poi_name   | 拿坡里                  \n",
      " menu_key   | img_article_url      \n",
      " menu_value | http://rmlove30.p... \n",
      "-RECORD 8--------------------------\n",
      " poi_name   | 拿坡里                  \n",
      " menu_key   | img_author_id        \n",
      " menu_value | RMlove30             \n",
      "-RECORD 9--------------------------\n",
      " poi_name   | 拿坡里                  \n",
      " menu_key   | food_cat             \n",
      " menu_value | Bread                \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 9 melt the dataframe (wide dataframe to long dataframe)\n",
    "from typing import Iterable\n",
    "import pandas as pd\n",
    "from IPython.core.display import display\n",
    "# https://stackoverflow.com/questions/41670103/how-to-melt-spark-dataframe\n",
    "\n",
    "\n",
    "def melt(\n",
    "        df: DataFrame, \n",
    "        id_vars: Iterable[str], value_vars: Iterable[str], \n",
    "        var_name: str=\"variable\", value_name: str=\"value\") -> DataFrame:\n",
    "    \"\"\"Convert :class:`DataFrame` from wide to long format.\"\"\"\n",
    "\n",
    "    value_names_dtype = dict(df.select(value_vars).dtypes)\n",
    "    unique_dtype = set(value_names_dtype.values())\n",
    "    assert len(unique_dtype) == 1, f\"value_vars should be the same dtype, your dype fo columns : {value_names_dtype}\"\n",
    "\n",
    "    # Create array<struct<variable: str, value: ...>>\n",
    "    _vars_and_vals = F.array(*(\n",
    "        F.struct(F.lit(c).alias(var_name), C(c).alias(value_name)) \n",
    "        for c in value_vars))\n",
    "\n",
    "    # Add to the DataFrame and explode\n",
    "    _tmp = df.withColumn(\"_vars_and_vals\", F.explode(_vars_and_vals))\n",
    "\n",
    "    cols = id_vars + [\n",
    "            C(\"_vars_and_vals\")[x].alias(x) for x in [var_name, value_name]]\n",
    "    return _tmp.select(*cols)\n",
    "\n",
    "# pdf = pd.DataFrame({'A': {0: 'a', 1: 'b', 2: 'c', 4 : 'a'},\n",
    "#                     'B': {0: 1, 1: 3, 2: 5, 4 : 11},\n",
    "#                     'C': {0: 2, 1: 4, 2: 6, 4 : 12}\n",
    "#                    })\n",
    "\n",
    "# pdf_result = pd.melt(pdf, id_vars=['A'], value_vars=['B', 'C']).sort_values(by=['A'])\n",
    "\n",
    "\n",
    "# display(\n",
    "#     \"Pandas\",\n",
    "#     pdf,\n",
    "#     pdf_result,\n",
    "#     \"PySpark\",\n",
    "#        )\n",
    "\n",
    "# # Case 1\n",
    "# # pdf['C'] = pdf['C'].astype(str) # then you can convert to spark df\n",
    "# sdf = spark.createDataFrame(pdf)\n",
    "# sdf.show()\n",
    "# sdf.printSchema()\n",
    "# melt(sdf, id_vars=['A'], value_vars=['B', 'C']).show()\n",
    "\n",
    "############### Case 2 ##############\n",
    "\n",
    "data = [\n",
    "    (\"丹丹漢堡\",                                                                \n",
    "     \"https://pic.pimg.tw/ksdelicacy/1438450572-4270481322.jpg\",               \n",
    "     \"318\",                                                                    \n",
    "     \"http://ksdelicacy.pixnet.net/blog/post/55774905\",\n",
    "     \"ksdelicacy\",                                                             \n",
    "     \"Fried food\"\n",
    "    ),\n",
    "    (\"拿坡里\"   ,                                                                 \n",
    "\"https://rmfoodie.com/wp-content/uploads/rm/1446140572-1157042739_n.jpg\",\n",
    "\"333\",                                                                    \n",
    "\"http://rmlove30.pixnet.net/blog/post/61986505\",                          \n",
    "\"RMlove30\",                                                               \n",
    "    \"Bread\")\n",
    "]\n",
    "\n",
    "cols = ['poi_name',\n",
    "        'img_url',\n",
    "        'food_cat_pop_score',\n",
    "        'img_article_url',\n",
    "        'img_author_id',\n",
    "        'food_cat']\n",
    "\n",
    "df = spark.createDataFrame(data, cols)\n",
    "df.show(vertical=True, truncate=False)\n",
    "df.printSchema()\n",
    "melt(\n",
    "        df,\n",
    "        id_vars=['poi_name'],\n",
    "        value_vars=[\"img_url\",\n",
    "                    'food_cat_pop_score',\n",
    "                    \"img_article_url\",\n",
    "                    \"img_author_id\",\n",
    "                    'food_cat'\n",
    "                    ],\n",
    "        var_name = 'menu_key',value_name = 'menu_value'\n",
    "        ).show(vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create complex type when aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T16:45:32.647387Z",
     "start_time": "2021-06-25T16:45:31.122226Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----+\n",
      "| id|key|value|\n",
      "+---+---+-----+\n",
      "|  1|  a|  123|\n",
      "|  1|  b|  234|\n",
      "|  1|  c|  345|\n",
      "|  2|  a|   12|\n",
      "|  2|  x|   23|\n",
      "+---+---+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- collections: array (nullable = true)\n",
      " |    |-- element: map (containsNull = true)\n",
      " |    |    |-- key: string\n",
      " |    |    |-- value: long (valueContainsNull = true)\n",
      "\n",
      "+---+------------------------------------+\n",
      "|id |collections                         |\n",
      "+---+------------------------------------+\n",
      "|1  |[[a -> 123], [b -> 234], [c -> 345]]|\n",
      "|2  |[[a -> 12], [x -> 23], [y -> 123]]  |\n",
      "+---+------------------------------------+\n",
      "\n",
      "[Row(id=1, collections=[{'a': 123}, {'b': 234}, {'c': 345}]), Row(id=2, collections=[{'a': 12}, {'x': 23}, {'y': 123}])]\n",
      "\n",
      "\n",
      "\n",
      "----------------- Case 2 -------------------\n",
      "\n",
      "\n",
      "\n",
      "+----------+-------------+------------------------+---------------+---------+\n",
      "|store_name|food_category|food_category_popularity|        img_url|author_id|\n",
      "+----------+-------------+------------------------+---------------+---------+\n",
      "|    hotpop|         Meat|                       3|https//:123.png|   rtyg11|\n",
      "|    hotpop|         Meat|                       3|https//:456.png|   rtyg11|\n",
      "|    hotpop|         Meat|                       3|https//:789.png|   rtyg11|\n",
      "|    hotpop|    Vegetable|                       2|https//:111.png|   rtyg11|\n",
      "|    hotpop|    Vegetable|                       2|https//:222.png|   rtyg11|\n",
      "|    branch|   Fried food|                       1|https//:333.png|     bvc1|\n",
      "|    branch|      Dessert|                       1|https//:444.png|     7854|\n",
      "+----------+-------------+------------------------+---------------+---------+\n",
      "\n",
      "+----------+--------------------+\n",
      "|store_name|                menu|\n",
      "+----------+--------------------+\n",
      "|    hotpop|[[food_category -...|\n",
      "|    branch|[[food_category -...|\n",
      "+----------+--------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(store_name='hotpop', menu=[{'food_category': 'Meat'}, {'food_category_popularity': '3'}, {'img_url': 'https//:123.png'}, {'author_id': 'rtyg11'}, {'food_category': 'Meat'}, {'food_category_popularity': '3'}, {'img_url': 'https//:456.png'}, {'author_id': 'rtyg11'}, {'food_category': 'Meat'}, {'food_category_popularity': '3'}, {'img_url': 'https//:789.png'}, {'author_id': 'rtyg11'}, {'food_category': 'Vegetable'}, {'food_category_popularity': '2'}, {'img_url': 'https//:111.png'}, {'author_id': 'rtyg11'}, {'food_category': 'Vegetable'}, {'food_category_popularity': '2'}, {'img_url': 'https//:222.png'}, {'author_id': 'rtyg11'}]),\n",
       " Row(store_name='branch', menu=[{'food_category': 'Fried food'}, {'food_category_popularity': '1'}, {'img_url': 'https//:333.png'}, {'author_id': 'bvc1'}, {'food_category': 'Dessert'}, {'food_category_popularity': '1'}, {'img_url': 'https//:444.png'}, {'author_id': '7854'}])]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 8 collect dict (map) with a group\n",
    "# https://stackoverflow.com/questions/55308482/pyspark-create-dictionary-within-groupby\n",
    "\n",
    "# collect_list : return a list of objects with duplicated\n",
    "# collect_set : return a set of objects without duplicated\n",
    "# struct : create a new struct column\n",
    "# ( > 2.4.0)map_from_entries : returns a map created from the given array of entries\n",
    "# create_map\n",
    "\n",
    "######### pyspark < 2.4.0\n",
    "data = [\n",
    "    (1,'a',123),\n",
    "    (1,'b',234),\n",
    "    (1,'c',345),\n",
    "    (2,'a',12),\n",
    "    (2,'x',23),\n",
    "    (2,'y',123)\n",
    "]\n",
    "\n",
    "columns = ['id','key','value']\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show(n=5)\n",
    "\n",
    "######## pyspark < 2.4.0\n",
    "\n",
    "df_agg = df.groupBy(\"id\").agg(\n",
    "    F.collect_list(F.create_map(C(\"key\"),C(\"value\"))).alias('collections')\n",
    ")\n",
    "\n",
    "df_agg.printSchema()\n",
    "df_agg.show(n=10, truncate=False)\n",
    "print(df_agg.collect())\n",
    "df_agg.toPandas().to_json('output/tmp.json',\n",
    "                          orient='records',\n",
    "                          force_ascii=False,\n",
    "                          lines=True)\n",
    "\n",
    "\n",
    "# to_json(join(SERVING_POI_FOOD_IMG_FOLDER,serving_fname),\n",
    "#                                        orient='records',\n",
    "#                                        force_ascii=False,\n",
    "#                                        lines=True)\n",
    "######### pyspark > 2.4.0\n",
    "# df.groupBy(\"id\").agg(\n",
    "#     F.map_from_entries(\n",
    "#         F.collect_list(\n",
    "#             F.struct(\"key\",\"value\"))).alias(\"key_value\")\n",
    "# ).show()\n",
    "\n",
    "############## Case 2 #######################\n",
    "for i in range(3):\n",
    "    print()\n",
    "print('----------------- Case 2 -------------------')\n",
    "for i in range(3):\n",
    "    print()\n",
    "    \n",
    "data = [\n",
    "    (\"hotpop\",\"Meat\",3,\"https//:123.png\",\"rtyg11\"),\n",
    "    (\"hotpop\",\"Meat\",3,\"https//:456.png\",\"rtyg11\"),\n",
    "    (\"hotpop\",\"Meat\",3,\"https//:789.png\",\"rtyg11\"),\n",
    "    (\"hotpop\",\"Vegetable\",2,\"https//:111.png\",\"rtyg11\"),\n",
    "    (\"hotpop\",\"Vegetable\",2,\"https//:222.png\",\"rtyg11\"),\n",
    "    (\"branch\",\"Fried food\",1,\"https//:333.png\",\"bvc1\"),\n",
    "    (\"branch\",\"Dessert\",1,\"https//:444.png\",\"7854\"),\n",
    "  ]\n",
    "\n",
    "columns = [\"store_name\",\"food_category\",\"food_category_popularity\",\"img_url\",\"author_id\"]\n",
    "df = spark.createDataFrame(data = data, schema = columns)\n",
    "\n",
    "df.show(n=10)\n",
    "\n",
    "\n",
    "# convert the melt column to string\n",
    "# because the column you wanna melt should be the same dtype\n",
    "df = (\n",
    "    df.withColumn(\"food_category_popularity\", C(\"food_category_popularity\").cast(StringType()))\n",
    ")\n",
    "df_complex = (\n",
    "    melt(df, id_vars=['store_name'],\n",
    "             value_vars=['food_category','food_category_popularity','img_url','author_id'],\n",
    "             var_name = 'menu_key',value_name = 'menu_value'\n",
    "        ).groupBy(\"store_name\").agg(\n",
    "        F.collect_list(F.create_map(C(\"menu_key\"), C(\"menu_value\"))).alias(\"menu\")\n",
    "    )\n",
    ")\n",
    "\n",
    "df_complex.show()\n",
    "df_complex.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T16:45:32.661681Z",
     "start_time": "2021-06-25T16:45:32.649037Z"
    }
   },
   "outputs": [],
   "source": [
    "# group by key, create a complexy json format like\n",
    "\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T16:45:33.202980Z",
     "start_time": "2021-06-25T16:45:32.663213Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- group: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- languagesAtSchool: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- currentState: string (nullable = true)\n",
      "\n",
      "+-----+----------------+------------------+------------+\n",
      "|group|name            |languagesAtSchool |currentState|\n",
      "+-----+----------------+------------------+------------+\n",
      "|1    |James,,Smith    |[Java, Scala, C++]|CA          |\n",
      "|1    |Michael,Rose,   |[Spark, Java, C++]|NJ          |\n",
      "|2    |Robert,,Williams|[CSharp, VB]      |NV          |\n",
      "|2    |Robert,,Williams|null              |NV          |\n",
      "+-----+----------------+------------------+------------+\n",
      "\n",
      "+-----+------------------------------------+\n",
      "|group|languagesAtSchool                   |\n",
      "+-----+------------------------------------+\n",
      "|1    |[Java, Scala, C++, Spark, Java, C++]|\n",
      "|2    |[CSharp, VB]                        |\n",
      "+-----+------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 12 groupby , concat the element in array\n",
    "\n",
    "# collect in array list, and flatern then\n",
    "\n",
    "columns = [\"group\",\"name\",\"languagesAtSchool\",\"currentState\"]\n",
    "data = [\n",
    "    ('1',\"James,,Smith\",[\"Java\",\"Scala\",\"C++\"],\"CA\"), \\\n",
    "    ('1',\"Michael,Rose,\",[\"Spark\",\"Java\",\"C++\"],\"NJ\"), \\\n",
    "    ('2',\"Robert,,Williams\",[\"CSharp\",\"VB\"],\"NV\"),\n",
    "    ('2',\"Robert,,Williams\",None,\"NV\")\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data=data,schema=columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n",
    "\n",
    "# collect list with collect list\n",
    "# we need to explode it first\n",
    "\n",
    "df_agg = (\n",
    "    df\n",
    "    .withColumn(\"flattern_tags\",F.explode(C(\"languagesAtSchool\")))\n",
    "    .drop('languagesAtSchool')\n",
    "    .groupBy(\"group\")\n",
    "    .agg(\n",
    "        F.collect_list(\"flattern_tags\").alias('languagesAtSchool')\n",
    "    )\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T16:45:33.971659Z",
     "start_time": "2021-06-25T16:45:33.205510Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- group: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- languagesAtSchool: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- currentState: string (nullable = true)\n",
      "\n",
      "+-----+----------------+------------------+------------+\n",
      "|group|name            |languagesAtSchool |currentState|\n",
      "+-----+----------------+------------------+------------+\n",
      "|1    |James,,Smith    |[Java, Scala, C++]|CA          |\n",
      "|1    |Michael,Rose,   |[Spark, Java, C++]|NJ          |\n",
      "|2    |Robert,,Williams|[CSharp, VB]      |NV          |\n",
      "|2    |Robert,,Williams|null              |NV          |\n",
      "+-----+----------------+------------------+------------+\n",
      "\n",
      "+-----+----------------------------------------+\n",
      "|group|tags                                    |\n",
      "+-----+----------------------------------------+\n",
      "|1    |[[Java, Scala, C++], [Spark, Java, C++]]|\n",
      "|2    |[[CSharp, VB]]                          |\n",
      "+-----+----------------------------------------+\n",
      "\n",
      "+-----+----------------------------------------+------------------+\n",
      "|group|tags                                    |explode_tags      |\n",
      "+-----+----------------------------------------+------------------+\n",
      "|1    |[[Java, Scala, C++], [Spark, Java, C++]]|[Java, Scala, C++]|\n",
      "|1    |[[Java, Scala, C++], [Spark, Java, C++]]|[Spark, Java, C++]|\n",
      "|2    |[[CSharp, VB]]                          |[CSharp, VB]      |\n",
      "+-----+----------------------------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 13 groupby and collect arrays into arrays\n",
    "# Yes we can\n",
    "\n",
    "columns = [\"group\",\"name\",\"languagesAtSchool\",\"currentState\"]\n",
    "data = [\n",
    "    ('1',\"James,,Smith\",[\"Java\",\"Scala\",\"C++\"],\"CA\"), \\\n",
    "    ('1',\"Michael,Rose,\",[\"Spark\",\"Java\",\"C++\"],\"NJ\"), \\\n",
    "    ('2',\"Robert,,Williams\",[\"CSharp\",\"VB\"],\"NV\"),\n",
    "    ('2',\"Robert,,Williams\",None,\"NV\")\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data=data,schema=columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n",
    "\n",
    "df_agg = (\n",
    " df.groupBy(\"group\")\n",
    "    .agg(\n",
    "        F.collect_list(C(\"languagesAtSchool\")).alias(\"tags\")\n",
    "    )\n",
    ")\n",
    "\n",
    "df_agg.show(truncate=False)\n",
    "\n",
    "\n",
    "(\n",
    "    df_agg\n",
    "    .withColumn(\"explode_tags\",F.explode(C(\"tags\")))\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T16:45:34.532224Z",
     "start_time": "2021-06-25T16:45:33.974981Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+---+\n",
      "|age|      dept| id|\n",
      "+---+----------+---+\n",
      "| 38|  medicine|  1|\n",
      "| 41|  medicine|  2|\n",
      "| 55|  medicine|  3|\n",
      "| 15|technology|  4|\n",
      "| 88|technology|  5|\n",
      "| 88|technology|  6|\n",
      "| 75|technology|  7|\n",
      "| 75|       mba|  8|\n",
      "| 75|       mba|  9|\n",
      "| 75|       mba| 10|\n",
      "+---+----------+---+\n",
      "\n",
      "This is wrong answer\n",
      "+---+----------+---+-------+------+\n",
      "|age|      dept| id|firstID|lastID|\n",
      "+---+----------+---+-------+------+\n",
      "| 75|       mba|  8|      8|    10|\n",
      "| 75|       mba|  9|      8|    10|\n",
      "| 75|       mba| 10|      8|    10|\n",
      "| 38|  medicine|  1|      1|     1|\n",
      "| 41|  medicine|  2|      1|     2|\n",
      "| 55|  medicine|  3|      1|     3|\n",
      "| 15|technology|  4|      4|     4|\n",
      "| 75|technology|  7|      4|     7|\n",
      "| 88|technology|  5|      4|     6|\n",
      "| 88|technology|  6|      4|     6|\n",
      "+---+----------+---+-------+------+\n",
      "\n",
      "Because we prodive a orderBy clause, default frame is \n",
      "RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW\n",
      "== Physical Plan ==\n",
      "Window [first(id#3335, false) windowspecdefinition(dept#3331, age#3330L ASC NULLS FIRST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS firstID#3352, last(id#3335, false) windowspecdefinition(dept#3331, age#3330L ASC NULLS FIRST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS lastID#3358], [dept#3331], [age#3330L ASC NULLS FIRST]\n",
      "+- *(3) Sort [dept#3331 ASC NULLS FIRST, age#3330L ASC NULLS FIRST], false, 0\n",
      "   +- *(3) Project [age#3330L, dept#3331, id#3335]\n",
      "      +- Window [row_number() windowspecdefinition(_w0#3336L ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS id#3335], [_w0#3336L ASC NULLS FIRST]\n",
      "         +- *(2) Sort [_w0#3336L ASC NULLS FIRST], false, 0\n",
      "            +- Exchange SinglePartition\n",
      "               +- *(1) Project [age#3330L, dept#3331, (monotonically_increasing_id() - 1) AS _w0#3336L]\n",
      "                  +- Scan ExistingRDD[age#3330L,dept#3331]\n",
      "So we need to re-define the wibndow\n",
      "== Physical Plan ==\n",
      "Window [first(id#3335, false) windowspecdefinition(dept#3331, age#3330L ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS firstID#3386, last(id#3335, false) windowspecdefinition(dept#3331, age#3330L ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS lastID#3392, row_number() windowspecdefinition(dept#3331, age#3330L ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS age_rank_in_dept#3399], [dept#3331], [age#3330L ASC NULLS FIRST]\n",
      "+- *(3) Sort [dept#3331 ASC NULLS FIRST, age#3330L ASC NULLS FIRST], false, 0\n",
      "   +- *(3) Project [age#3330L, dept#3331, id#3335]\n",
      "      +- Window [row_number() windowspecdefinition(_w0#3336L ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS id#3335], [_w0#3336L ASC NULLS FIRST]\n",
      "         +- *(2) Sort [_w0#3336L ASC NULLS FIRST], false, 0\n",
      "            +- Exchange SinglePartition\n",
      "               +- *(1) Project [age#3330L, dept#3331, (monotonically_increasing_id() - 1) AS _w0#3336L]\n",
      "                  +- Scan ExistingRDD[age#3330L,dept#3331]\n",
      "+---+----------+---+-------+------+----------------+\n",
      "|age|      dept| id|firstID|lastID|age_rank_in_dept|\n",
      "+---+----------+---+-------+------+----------------+\n",
      "| 75|       mba|  8|      8|    10|               1|\n",
      "| 75|       mba|  9|      8|    10|               2|\n",
      "| 75|       mba| 10|      8|    10|               3|\n",
      "| 38|  medicine|  1|      1|     3|               1|\n",
      "| 41|  medicine|  2|      1|     3|               2|\n",
      "| 55|  medicine|  3|      1|     3|               3|\n",
      "| 15|technology|  4|      4|     6|               1|\n",
      "| 75|technology|  7|      4|     6|               2|\n",
      "| 88|technology|  5|      4|     6|               3|\n",
      "| 88|technology|  6|      4|     6|               4|\n",
      "+---+----------+---+-------+------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 14 groupby column C1 , get first row and last row once, order by column C2\n",
    "\n",
    "\n",
    "data = [\n",
    "    (38,\"medicine\"),\n",
    "    (41,\"medicine\"),\n",
    "    (55,\"medicine\"),\n",
    "    (15,\"technology\"),\n",
    "    (88,\"technology\"),\n",
    "    (88,\"technology\"),\n",
    "    (75,\"technology\"),\n",
    "    (75,\"mba\"),\n",
    "    (75,\"mba\"),\n",
    "    (75,\"mba\")\n",
    "    ]\n",
    "\n",
    "\n",
    "columns = ['age','dept']\n",
    "\n",
    "df = (\n",
    "    spark.createDataFrame(data=data, schema=columns)\n",
    "    .withColumn(\"id\",\n",
    "                F.row_number()\n",
    "                .over(\n",
    "                    W.orderBy(F.monotonically_increasing_id() - 1)\n",
    "                ))\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "df.show()\n",
    "\n",
    "# https://stackoverflow.com/questions/52273186/pyspark-spark-window-function-first-last-issue\n",
    "print(\"This is wrong answer\")\n",
    "\n",
    "age_in_dept = W.partitionBy('dept').orderBy(\"age\")\n",
    "df_res = (\n",
    "    df\n",
    "    .withColumn(\"firstID\", F.first('id').over(age_in_dept))\n",
    "    .withColumn(\"lastID\", F.last('id').over(age_in_dept))\n",
    ")\n",
    "\n",
    "df_res.show()\n",
    "\n",
    "print('Because we prodive a orderBy clause, default frame is ')\n",
    "print(\"RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW\")\n",
    "\n",
    "df_res.explain()\n",
    "\n",
    "print(\"So we need to re-define the wibndow\")\n",
    "\n",
    "age_in_dept = (\n",
    "    W.partitionBy('dept')\n",
    "    .orderBy(\"age\")\n",
    "    .rowsBetween(W.unboundedPreceding, W.unboundedFollowing)\n",
    ")\n",
    "\n",
    "\n",
    "##### withcolumn, select gives you the same answer, you need to do your own filtering\n",
    "df_res = (\n",
    "    df\n",
    "    .withColumn(\"firstID\", F.first('id').over(age_in_dept))\n",
    "    .withColumn(\"lastID\", F.last('id').over(age_in_dept))\n",
    "    .withColumn(\"age_rank_in_dept\",\n",
    "                F.row_number()\n",
    "                .over(\n",
    "                W.partitionBy('dept')\n",
    "                 .orderBy(\"age\")\n",
    "                ))\n",
    ")\n",
    "\n",
    "df_res.explain()\n",
    "df_res.show()\n",
    "\n",
    "\n",
    "# df_res = (\n",
    "#     df.select(\n",
    "#         \"*\",\n",
    "#         F.first('id').over(age_in_dept).alias('first_id'),\n",
    "#         F.last('id').over(age_in_dept).alias('last_id'),\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# df_res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T16:45:35.320182Z",
     "start_time": "2021-06-25T16:45:34.535156Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|item|score|\n",
      "+----+-----+\n",
      "|   a|   10|\n",
      "|   a|   10|\n",
      "|   a|   20|\n",
      "|   b|    5|\n",
      "|   b|    3|\n",
      "+----+-----+\n",
      "\n",
      "-RECORD 0-------------------------------------\n",
      " item                          | b            \n",
      " score                         | 3            \n",
      " collect_score_list_to_current | [3]          \n",
      " collect_score_set_to_current  | [3]          \n",
      " collect_score_list            | [3, 5]       \n",
      " collect_score_set             | [5, 3]       \n",
      "-RECORD 1-------------------------------------\n",
      " item                          | b            \n",
      " score                         | 5            \n",
      " collect_score_list_to_current | [3, 5]       \n",
      " collect_score_set_to_current  | [5, 3]       \n",
      " collect_score_list            | [3, 5]       \n",
      " collect_score_set             | [5, 3]       \n",
      "-RECORD 2-------------------------------------\n",
      " item                          | a            \n",
      " score                         | 10           \n",
      " collect_score_list_to_current | [10, 10]     \n",
      " collect_score_set_to_current  | [10]         \n",
      " collect_score_list            | [10, 10, 20] \n",
      " collect_score_set             | [20, 10]     \n",
      "-RECORD 3-------------------------------------\n",
      " item                          | a            \n",
      " score                         | 10           \n",
      " collect_score_list_to_current | [10, 10]     \n",
      " collect_score_set_to_current  | [10]         \n",
      " collect_score_list            | [10, 10, 20] \n",
      " collect_score_set             | [20, 10]     \n",
      "-RECORD 4-------------------------------------\n",
      " item                          | a            \n",
      " score                         | 20           \n",
      " collect_score_list_to_current | [10, 10, 20] \n",
      " collect_score_set_to_current  | [20, 10]     \n",
      " collect_score_list            | [10, 10, 20] \n",
      " collect_score_set             | [20, 10]     \n",
      "\n",
      "== Physical Plan ==\n",
      "Window [collect_list(score#3431L, 0, 0) windowspecdefinition(item#3430, score#3431L ASC NULLS FIRST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS collect_score_list_to_current#3443, collect_set(score#3431L, 0, 0) windowspecdefinition(item#3430, score#3431L ASC NULLS FIRST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS collect_score_set_to_current#3448, collect_list(score#3431L, 0, 0) windowspecdefinition(item#3430, score#3431L ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS collect_score_list#3454, collect_set(score#3431L, 0, 0) windowspecdefinition(item#3430, score#3431L ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS collect_score_set#3461], [item#3430], [score#3431L ASC NULLS FIRST]\n",
      "+- *(1) Sort [item#3430 ASC NULLS FIRST, score#3431L ASC NULLS FIRST], false, 0\n",
      "   +- Exchange hashpartitioning(item#3430, 200)\n",
      "      +- Scan ExistingRDD[item#3430,score#3431L]\n"
     ]
    }
   ],
   "source": [
    "# 15. Perform collect_list, colect set in a wondow\n",
    "data = [\n",
    "    (\"a\",10),\n",
    "    ('a',10),\n",
    "    ('a',20),\n",
    "    ('b',5),\n",
    "    ('b',3)\n",
    "]\n",
    "\n",
    "columns = ['item','score']\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show(n=5)\n",
    "\n",
    "score_in_item_from_unbounded_to_curr = (\n",
    "                W.partitionBy(\"item\")\n",
    "                 .orderBy(\"score\"))\n",
    "score_in_item = (\n",
    "                W.partitionBy('item')\n",
    "                 .orderBy(\"score\")\n",
    "                 .rowsBetween(W.unboundedPreceding, W.unboundedFollowing)\n",
    ")\n",
    "df_window = (\n",
    "    df\n",
    "    .withColumn(\"collect_score_list_to_current\",\n",
    "                F.collect_list(\"score\").over(score_in_item_from_unbounded_to_curr))\n",
    "    .withColumn(\"collect_score_set_to_current\",\n",
    "                F.collect_set(\"score\").over(score_in_item_from_unbounded_to_curr))\n",
    "    .withColumn(\"collect_score_list\",\n",
    "                F.collect_list(\"score\").over(score_in_item)\n",
    "               )\n",
    "    .withColumn(\"collect_score_set\",\n",
    "                F.collect_set(\"score\").over(score_in_item)\n",
    "               )\n",
    ")\n",
    "\n",
    "df_window.show(n=5, vertical=True, truncate=False)\n",
    "\n",
    "df_window.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T16:45:35.905671Z",
     "start_time": "2021-06-25T16:45:35.322022Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|item|score|\n",
      "+----+-----+\n",
      "|   a|   10|\n",
      "|   a|    0|\n",
      "|   a|   20|\n",
      "|   b|    5|\n",
      "|   b|    3|\n",
      "+----+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "-RECORD 0----------------------\n",
      " item               | c        \n",
      " score              | 0        \n",
      " collect_score_list | []       \n",
      "-RECORD 1----------------------\n",
      " item               | b        \n",
      " score              | 5        \n",
      " collect_score_list | [5, 3]   \n",
      "-RECORD 2----------------------\n",
      " item               | b        \n",
      " score              | 3        \n",
      " collect_score_list | [5, 3]   \n",
      "-RECORD 3----------------------\n",
      " item               | a        \n",
      " score              | 10       \n",
      " collect_score_list | [10, 20] \n",
      "-RECORD 4----------------------\n",
      " item               | a        \n",
      " score              | 0        \n",
      " collect_score_list | [10, 20] \n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 15. Perform collect_list with a filter\n",
    "# collect only the score > 0\n",
    "# https://stackoverflow.com/questions/61468705/pyspark-using-collect-list-over-window-with-condition\n",
    "data = [\n",
    "    (\"a\",10),\n",
    "    ('a',0),\n",
    "    ('a',20),\n",
    "    ('b',5),\n",
    "    ('b',3),\n",
    "    ('c',0)\n",
    "]\n",
    "\n",
    "columns = ['item','score']\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show(n=5)\n",
    "\n",
    "item = (\n",
    "        W.partitionBy(\"item\")\n",
    ")\n",
    "\n",
    "\n",
    "df_window = (\n",
    "    df\n",
    "    .withColumn(\"collect_score_list\",\n",
    "                F.collect_list(\n",
    "                    F.when(C(\"score\") > 0, C(\"score\"))\n",
    "                     .otherwise(F.lit(None))\n",
    "                    ).over(item)\n",
    "               )\n",
    "\n",
    ")\n",
    "\n",
    "df_window.show(n=5, vertical=True, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# row and columns (1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T16:45:35.996059Z",
     "start_time": "2021-06-25T16:45:35.907341Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attribute and method in Row :  ['__add__', '__call__', '__class__', '__contains__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getitem__', '__getnewargs__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__module__', '__mul__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__rmul__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', 'asDict', 'count', 'index']\n",
      "{'id': 14431, 'img_url': 'https://pic.pimg.tw/happy78/1528543947-685380499_n.jpg'}\n"
     ]
    }
   ],
   "source": [
    "# 1. changing rows and python dictionary \n",
    "data = [\n",
    "    (14431,'https://pic.pimg.tw/happy78/1528543947-685380499_n.jpg'),\n",
    "    (14431,'https://pic.pimg.tw/happy78/1528543947-362759723_n.jpg'),\n",
    "    (14431,'https://pic.pimg.tw/happy78/1528543962-2265924582_n.jpg'),\n",
    "    (67789,'https://pic.pimg.tw/happy78/1528543962-4007835890_n.jpg'),\n",
    "    (67789,'https://pic.pimg.tw/happy78/1528543962-45890_n.jpg')\n",
    "]\n",
    "\n",
    "cols = ['id','img_url']\n",
    "\n",
    "df = spark.createDataFrame(data, cols)\n",
    "\n",
    "rows = df.collect()\n",
    "\n",
    "r = rows[0]\n",
    "\n",
    "print('attribute and method in Row : ', dir(r))\n",
    "\n",
    "print(r.asDict(), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# udf & pandas_udf (5+)\n",
    "\n",
    "* pandas_udf return maximum 2G\n",
    "\n",
    "* https://issues.apache.org/jira/browse/ARROW-1907\n",
    "\n",
    "* [pandas_udf in classmethod, you need to write a new wrapper! which is not easy](https://stackoverflow.com/questions/58170261/how-to-use-pandas-udf-in-class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T16:45:36.824587Z",
     "start_time": "2021-06-25T16:45:35.998950Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cols: struct (nullable = true)\n",
      " |    |-- error: string (nullable = true)\n",
      " |    |-- message: string (nullable = true)\n",
      " |-- cols_2: struct (nullable = true)\n",
      " |    |-- error: string (nullable = true)\n",
      " |    |-- message: string (nullable = true)\n",
      "\n",
      "+----------+----------+\n",
      "|cols      |cols_2    |\n",
      "+----------+----------+\n",
      "|[abc, def]|[abc, def]|\n",
      "|[abc, def]|[abc, def]|\n",
      "|[abc, def]|[abc, def]|\n",
      "|[abc, def]|[abc, def]|\n",
      "|[abc, def]|[abc, def]|\n",
      "+----------+----------+\n",
      "\n",
      "+-----+-------+\n",
      "|error|message|\n",
      "+-----+-------+\n",
      "|  abc|    def|\n",
      "|  abc|    def|\n",
      "|  abc|    def|\n",
      "|  abc|    def|\n",
      "|  abc|    def|\n",
      "+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1 return dict as stuct column by udf\n",
    "\n",
    "\n",
    "# any reference for this?\n",
    "\n",
    "from pixlake.etl.user.pixtree.export_cookie_to_pixinterest import PixInterestApiLog\n",
    "PixInterestApiLog.printSchema()\n",
    "\n",
    "data = [\n",
    "    (14431,'https://pic.pimg.tw/happy78/1528543947-685380499_n.jpg'),\n",
    "    (14431,'https://pic.pimg.tw/happy78/1528543947-362759723_n.jpg'),\n",
    "    (14431,'https://pic.pimg.tw/happy78/1528543962-2265924582_n.jpg'),\n",
    "    (67789,'https://pic.pimg.tw/happy78/1528543962-4007835890_n.jpg'),\n",
    "    (67789,'https://pic.pimg.tw/happy78/1528543962-45890_n.jpg')\n",
    "]\n",
    "\n",
    "cols = ['id','img_url']\n",
    "\n",
    "df = spark.createDataFrame(data, cols)\n",
    "\n",
    "def get_two_more(col) -> dict:\n",
    "    return {'error' : 'abc', 'message' : 'def'}\n",
    "\n",
    "str_schema = 'error string, message string'\n",
    "struct_schema = 'struct<\\n error: string,\\n message: string \\n>'\n",
    "api_log_sdf = (\n",
    "    df.select(\n",
    "        F.udf(get_two_more, returnType=str_schema)('img_url').alias('cols'),\n",
    "        F.udf(get_two_more, returnType=struct_schema)('img_url').alias('cols_2')\n",
    "        \n",
    "    )\n",
    ")\n",
    "\n",
    "api_log_sdf.printSchema()\n",
    "api_log_sdf.show(truncate=False)\n",
    "\n",
    "api_log_expand = (\n",
    "    api_log_sdf\n",
    "    .select('cols_2.*')\n",
    ")\n",
    "\n",
    "api_log_expand.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T16:45:37.652515Z",
     "start_time": "2021-06-25T16:45:36.826821Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before\n",
      "+----------+--------------------+\n",
      "|article_id|             img_url|\n",
      "+----------+--------------------+\n",
      "|     14431|https://pic.pimg....|\n",
      "|     14431|https://pic.pimg....|\n",
      "|     14431|https://pic.pimg....|\n",
      "|     67789|https://pic.pimg....|\n",
      "|     67789|https://pic.pimg....|\n",
      "+----------+--------------------+\n",
      "\n",
      "+----------+--------------------+-----------+\n",
      "|article_id|             img_url|img_b64_str|\n",
      "+----------+--------------------+-----------+\n",
      "|     14431|https://pic.pimg....|[B@1b6ab838|\n",
      "|     14431|https://pic.pimg....| [B@448eaaf|\n",
      "|     14431|https://pic.pimg....|[B@15cdca58|\n",
      "|     67789|https://pic.pimg....|[B@74900691|\n",
      "|     67789|https://pic.pimg....|       null|\n",
      "+----------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2. Use Pyspark to send request, get image and store as b64string \n",
    "# https://stackoverflow.com/questions/49353752/use-requests-module-and-return-response-to-pyspark-dataframe\n",
    "\n",
    "data = [\n",
    "    (14431,'https://pic.pimg.tw/happy78/1528543947-685380499_n.jpg'),\n",
    "    (14431,'https://pic.pimg.tw/happy78/1528543947-362759723_n.jpg'),\n",
    "    (14431,'https://pic.pimg.tw/happy78/1528543962-2265924582_n.jpg'),\n",
    "    (67789,'https://pic.pimg.tw/happy78/1528543962-4007835890_n.jpg'),\n",
    "    (67789,'https://pic.pimg.tw/happy78/1528543962-45890_n.jpg')\n",
    "]\n",
    "\n",
    "columns = ['article_id','img_url']\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=columns)\n",
    "print('before')\n",
    "df.show(n=5)\n",
    "\n",
    "@F.udf(returnType=StringType())\n",
    "def get_img_binary(url : str) -> str:\n",
    "    import requests\n",
    "    import base64\n",
    "    resp = requests.get(url)\n",
    "    if resp.status_code == 200:\n",
    "        return base64.encodestring(resp.content)\n",
    "    else:\n",
    "        return None\n",
    "df = (\n",
    "    df.withColumn(\"img_b64_str\", get_img_binary(C(\"img_url\")))\n",
    ")\n",
    "\n",
    "df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T16:45:37.937959Z",
     "start_time": "2021-06-25T16:45:37.655725Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before : \n",
      "+---+--------+\n",
      "| id|features|\n",
      "+---+--------+\n",
      "|  1|      64|\n",
      "|  2|      76|\n",
      "|  3|      54|\n",
      "|  4|      11|\n",
      "|  5|     100|\n",
      "+---+--------+\n",
      "\n",
      "[('id', 'bigint'), ('features', 'bigint'), ('pred', 'struct<category:string,prob:float>')]\n",
      "+---+--------+---------------------+\n",
      "|id |features|pred                 |\n",
      "+---+--------+---------------------+\n",
      "|1  |64      |[compose, 0.67061305]|\n",
      "|2  |76      |[compose, 0.7750077] |\n",
      "|3  |54      |[compose, 0.62288284]|\n",
      "|4  |11      |[food, 0.87696433]   |\n",
      "|5  |100     |[food, 0.24219921]   |\n",
      "+---+--------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3 udf return two column values, e.g. model prediction with label and probability\n",
    "data = [\n",
    "    (1,64),\n",
    "    (2,76),\n",
    "    (3,54),\n",
    "    (4,11),\n",
    "    (5,100),\n",
    "]\n",
    "columns = ['id','features']\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=columns)\n",
    "print(\"Before : \")\n",
    "df.show(n=5)\n",
    "\n",
    "############# sol #################\n",
    "# using Row object to return multiple column\n",
    "from pyspark.sql import Row\n",
    "\n",
    "model_pred = StructType([\n",
    "    StructField(\"category\", StringType(), False),\n",
    "    StructField(\"prob\", T.FloatType(), False)\n",
    "])\n",
    "\n",
    "@F.udf(returnType=model_pred)\n",
    "def model_pred(n):\n",
    "    import random\n",
    "    category = random.choice(['food','env','compose','drink'])\n",
    "    prob = random.random()\n",
    "    return Row('category', 'prob')(category, prob)\n",
    "\n",
    "\n",
    "\n",
    "newDF = df.withColumn(\"pred\", model_pred(df[\"features\"]))\n",
    "\n",
    "print(newDF.dtypes)\n",
    "\n",
    "# newDF = newDF.select(\"id\", \"features\", \"pred.*\")\n",
    "\n",
    "newDF.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T16:45:38.542530Z",
     "start_time": "2021-06-25T16:45:37.940825Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|multiply_func(x, x)|\n",
      "+-------------------+\n",
      "|                  1|\n",
      "|                  4|\n",
      "|                  9|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4. Pandas udf\n",
    "# documentation and concept\n",
    "# user defined function, but vectorlized by Arrow\n",
    "# 2.3.0\n",
    "# https://spark.apache.org/docs/2.3.0/sql-programming-guide.html#pandas-udfs-aka-vectorized-udfs\n",
    "# 3.0 support more!\n",
    "# https://spark.apache.org/docs/3.0.0/sql-pyspark-pandas-with-arrow.html#pandas-udfs-aka-vectorized-udfs\n",
    "\n",
    "\n",
    "# for 2.3.0\n",
    "#  Currently, there are two types of Pandas UDF: Scalar and Grouped Map.\n",
    "#  Input pd.Series, Output pd.Series\n",
    "\n",
    "# Scalar type\n",
    "@F.pandas_udf(returnType=T.LongType())\n",
    "def multiply_func(a, b):\n",
    "    return a * b\n",
    "\n",
    "\n",
    "x = pd.Series([1, 2, 3])\n",
    "\n",
    "df = spark.createDataFrame(pd.DataFrame(x), schema=[\"x\"])\n",
    "df.select(multiply_func(C(\"x\"), C(\"x\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T16:45:38.563748Z",
     "start_time": "2021-06-25T16:45:38.544428Z"
    }
   },
   "outputs": [],
   "source": [
    "# 5 from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "\n",
    "# # Use pandas_udf to define a Pandas UDF\n",
    "# @pandas_udf('double', PandasUDFType.SCALAR)\n",
    "# # Input/output are both a pandas.Series of doubles\n",
    "\n",
    "# def pandas_plus_one(x):\n",
    "#     return x + 1\n",
    "\n",
    "# df.withColumn('v2', pandas_plus_one(df.x))\n",
    "# df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T16:45:49.199679Z",
     "start_time": "2021-06-25T16:45:38.565423Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| ID|   v|\n",
      "+---+----+\n",
      "|  1| 1.0|\n",
      "|  1| 2.0|\n",
      "|  2| 3.0|\n",
      "|  2| 5.0|\n",
      "|  2|10.0|\n",
      "+---+----+\n",
      "\n",
      "+---+----+------+\n",
      "| id|   v|n_rows|\n",
      "+---+----+------+\n",
      "|  1|-0.5|     2|\n",
      "|  1| 0.5|     2|\n",
      "|  2|-3.0|     3|\n",
      "|  2|-1.0|     3|\n",
      "|  2| 4.0|     3|\n",
      "+---+----+------+\n",
      "\n",
      "+---+----+---------+--------------------+\n",
      "| ID|   v|new_col_1|           new_col_2|\n",
      "+---+----+---------+--------------------+\n",
      "|  1| 1.0|      0.0| 0.25213175650595776|\n",
      "|  1| 2.0|      1.0|  0.6225855706599085|\n",
      "|  2| 3.0|      0.0|  0.4479116603107153|\n",
      "|  2| 5.0|      1.0|  0.3027519962805727|\n",
      "|  2|10.0|      2.0|0.034399011892293996|\n",
      "+---+----+---------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6 pandasUDF return a dataframe\n",
    "# like a model prediction\n",
    "# # predict label and probability\n",
    "# A grouped map UDF defines transformation:\n",
    "# A pandas.DataFrame -> A pandas.DataFrame The returnType \n",
    "# should be a StructType describing the schema of the returned pandas.DataFrame.\n",
    "# The length of the returned pandas.DataFrame can be arbitrary and the columns must be indexed so that their position matches the corresponding field in the schema.\n",
    "# Grouped map UDFs are used with pyspark.sql.GroupedData.apply().\n",
    "# https://databricks.com/blog/2017/10/30/introducing-vectorized-udfs-for-pyspark.html\n",
    "\n",
    "# We can use BucketID for this ID\n",
    "\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "\n",
    "df = spark.createDataFrame(\n",
    "    [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)],\n",
    "    (\"ID\", \"v\"))\n",
    "\n",
    "df.show()\n",
    "########## case 1 ##############\n",
    "\n",
    "@pandas_udf(\"id long, v double, n_rows long\", PandasUDFType.GROUPED_MAP)\n",
    "def substract_mean(pdf):\n",
    "    # pdf is a pandas.DataFrame\n",
    "    n_rows = len(pdf)\n",
    "    v = pdf.v\n",
    "    return pdf.assign(\n",
    "        v=v - v.mean(),\n",
    "        n_rows=n_rows\n",
    "    )\n",
    "\n",
    "df.groupby(\"ID\").apply(substract_mean).show()\n",
    "\n",
    "######### case 2 ##############\n",
    "\n",
    "@pandas_udf(\"ID long, v double, new_col_1 double, new_col_2 double\", PandasUDFType.GROUPED_MAP)\n",
    "def get_more_col(pdf):\n",
    "    # pdf is a pandas.DataFrame\n",
    "    import random\n",
    "    n_counts = len(pdf)\n",
    "    \n",
    "    return pdf.assign(\n",
    "        new_col_1 = [i for i in range(n_counts)],\n",
    "        new_col_2 = [random.random() for i in range(n_counts)]\n",
    "    )\n",
    "# sdf groupby its bucket_id and apply\n",
    "\n",
    "df.groupby(\"ID\").apply(get_more_col).show()\n",
    "\n",
    "\n",
    "######## case 3 #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T16:45:50.344003Z",
     "start_time": "2021-06-25T16:45:49.226486Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------------+\n",
      "|article_id|             img_url|         img_b64_str|\n",
      "+----------+--------------------+--------------------+\n",
      "|     14431|https://pic.pimg....|/9j/2wCEAAoHBwgHB...|\n",
      "|     14431|https://pic.pimg....|/9j/2wCEAAoHBwgHB...|\n",
      "|     67789|https://pic.pimg....|/9j/4AAQSkZJRgABA...|\n",
      "+----------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 7 pandas udf return Null used for sending request to get images\n",
    "# return string for pandas_udf\n",
    "# https://stackoverflow.com/questions/65694026/spark-exception-error-using-pandas-udf-with-logical-statement\n",
    "from typing import Union\n",
    "import requests\n",
    "import base64\n",
    "\n",
    "\n",
    "@F.udf(returnType=StringType())\n",
    "def get_img_binary(url : str) -> str:\n",
    "    import requests\n",
    "    import base64\n",
    "    resp = requests.get(url, timeout=10)\n",
    "    if resp.status_code == 200:\n",
    "        return base64.encodestring(resp.content)\n",
    "    else:\n",
    "        return None\n",
    "# df = (\n",
    "#     df.withColumn(\"img_b64_str\", get_img_binary(C(\"img_url\")))\n",
    "# )\n",
    "\n",
    "\n",
    "# If you wanna dealing with a lot of data, use the rep\n",
    "# @F.pandas_udf('string', PandasUDFType.SCALAR)\n",
    "def download_img_to_b64_pd(img_url : pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    get image in the response and save to base64 string\n",
    "    data type flow : \n",
    "    resp.content - (bytes) \n",
    "    -> base64.encode - (bytes) \n",
    "    -> decode('utf-8') - str\n",
    "    \"\"\"\n",
    "    b64_str_list = []\n",
    "    for link in img_url:   \n",
    "        try:\n",
    "            resp = requests.get(link, timeout=10)\n",
    "            if resp.status_code == 200:\n",
    "                b64_str_list.append(base64.encodebytes(resp.content).decode('utf-8'))\n",
    "            else:\n",
    "                b64_str_list.append(None)\n",
    "        except Exception as e:\n",
    "            b64_str_list.append(None)\n",
    "    return pd.Series(b64_str_list)\n",
    "\n",
    "\n",
    "\n",
    "data = [\n",
    "    (14431,'https://pic.pimg.tw/happy78/1528543947-685380499_n.jpg'),\n",
    "    (14431,'https://pic.pimg.tw/happy78/1528543947-3623_n.jpg'), # This one will be not found\n",
    "    (14431,'https://pic.pimg.tw/happy78/1528543962-2265924582_n.jpg'),\n",
    "    (67789,'https://pic.pimg.tw/happy78/1528543962-4007835890_n.jpg'),\n",
    "    (67789,'https://pic.pimg.tw/happy78/1528543962-45890_n.jpg')\n",
    "]\n",
    "\n",
    "columns = ['article_id','img_url']\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=columns)\n",
    "# df.show(vertical=True, truncate=False)\n",
    "\n",
    "#             .withColumn(\n",
    "#                 \"bucket_id\",\n",
    "#                 F.udf(self.simple_random, returnType=\"integer\")(\"bucket_size\"),\n",
    "#             )\n",
    "\n",
    "df = (\n",
    "    df\\\n",
    "#     .withColumn(\"img_b64_str\", download_img_to_b64_pd(C(\"img_url\")))\n",
    "    .withColumn(\"img_b64_str\",F.pandas_udf(download_img_to_b64_pd, \"string\",PandasUDFType.SCALAR)(\"img_url\"))\n",
    "    .filter(C(\"img_b64_str\").isNotNull())\n",
    ")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T16:45:50.422629Z",
     "start_time": "2021-06-25T16:45:50.347051Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>img_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14431</td>\n",
       "      <td>https://pic.pimg.tw/happy78/1528543947-6853804...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14431</td>\n",
       "      <td>https://pic.pimg.tw/happy78/1528543947-3623_n.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14431</td>\n",
       "      <td>https://pic.pimg.tw/happy78/1528543962-2265924...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>67789</td>\n",
       "      <td>https://pic.pimg.tw/happy78/1528543962-4007835...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>67789</td>\n",
       "      <td>https://pic.pimg.tw/happy78/1528543962-45890_n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   article_id                                            img_url\n",
       "0       14431  https://pic.pimg.tw/happy78/1528543947-6853804...\n",
       "1       14431  https://pic.pimg.tw/happy78/1528543947-3623_n.jpg\n",
       "2       14431  https://pic.pimg.tw/happy78/1528543962-2265924...\n",
       "3       67789  https://pic.pimg.tw/happy78/1528543962-4007835...\n",
       "4       67789  https://pic.pimg.tw/happy78/1528543962-45890_n..."
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 8 pandas udf using generator\n",
    "# due to pd.Series return should under 2G\n",
    "# we're using another apporoach (GroupMap)\n",
    "\n",
    "def get_img_b64_generator(img_url : pd.Series):\n",
    "    '''\n",
    "    we can move this out of the udf\n",
    "    '''\n",
    "    for link in img_url:\n",
    "        try:\n",
    "            resp = requests.get(link, timeout=10)\n",
    "            if resp.status_code == 200:\n",
    "                yield base64.encodebytes(resp.content).decode('utf-8')\n",
    "            else:\n",
    "                yield None\n",
    "        except Exception as e:\n",
    "            yield None\n",
    "\n",
    "            \n",
    "return_type = \"article_id long, img_url string\"\n",
    "@F.pandas_udf(return_type,\n",
    "            F.PandasUDFType.GROUPED_MAP)\n",
    "def download_img_to_b64(df_with_url : pd.DataFrame) -> pd.DataFrame:\n",
    "    import requests\n",
    "    import base64\n",
    "    \n",
    "    img_url_series = df_with_img_b64.img_url\n",
    "    \n",
    "    # Using generator to avoid big series OOM/Serilization error\n",
    "    \n",
    "    return df_with_url.assign(\n",
    "        img_b64_str = img_type_series,\n",
    "        )\n",
    "\n",
    "data = [\n",
    "    (14431,'https://pic.pimg.tw/happy78/1528543947-685380499_n.jpg'),\n",
    "    (14431,'https://pic.pimg.tw/happy78/1528543947-3623_n.jpg'), # This one will be not found\n",
    "    (14431,'https://pic.pimg.tw/happy78/1528543962-2265924582_n.jpg'),\n",
    "    (67789,'https://pic.pimg.tw/happy78/1528543962-4007835890_n.jpg'),\n",
    "    (67789,'https://pic.pimg.tw/happy78/1528543962-45890_n.jpg')\n",
    "]\n",
    "\n",
    "columns = ['article_id','img_url']\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=columns)\n",
    "pdf = df.toPandas()\n",
    "pdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# broadcasting (2+)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T16:45:51.045251Z",
     "start_time": "2021-06-25T16:45:50.424345Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-------+-----+\n",
      "|firstname|lastname|country|state|\n",
      "+---------+--------+-------+-----+\n",
      "|    James|   Smith|    USA|   CA|\n",
      "|  Michael|    Rose|    USA|   NY|\n",
      "|   Robert|Williams|    USA|   CA|\n",
      "|    Maria|   Jones|    USA|   FL|\n",
      "+---------+--------+-------+-----+\n",
      "\n",
      "+---------+--------+-------+----------+\n",
      "|firstname|lastname|country|     state|\n",
      "+---------+--------+-------+----------+\n",
      "|    James|   Smith|    USA|California|\n",
      "|  Michael|    Rose|    USA|  New York|\n",
      "|   Robert|Williams|    USA|California|\n",
      "|    Maria|   Jones|    USA|   Florida|\n",
      "+---------+--------+-------+----------+\n",
      "\n",
      "+---------+--------+-------+-----+---------------+\n",
      "|firstname|lastname|country|state|converted_state|\n",
      "+---------+--------+-------+-----+---------------+\n",
      "|    James|   Smith|    USA|   CA|     California|\n",
      "|  Michael|    Rose|    USA|   NY|       New York|\n",
      "|   Robert|Williams|    USA|   CA|     California|\n",
      "|    Maria|   Jones|    USA|   FL|        Florida|\n",
      "+---------+--------+-------+-----+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1\n",
    "# broadcast the dictionary to spark \n",
    "# (which is a way that enhance multi-processing cross machine using your python code)\n",
    "# the broadcast variable should be serializable\n",
    "\n",
    "states = {\"NY\":\"New York\", \"CA\":\"California\", \"FL\":\"Florida\"}\n",
    "broadcastStates = spark.sparkContext.broadcast(states)\n",
    "\n",
    "data = [(\"James\",\"Smith\",\"USA\",\"CA\"),\n",
    "    (\"Michael\",\"Rose\",\"USA\",\"NY\"),\n",
    "    (\"Robert\",\"Williams\",\"USA\",\"CA\"),\n",
    "    (\"Maria\",\"Jones\",\"USA\",\"FL\")\n",
    "  ]\n",
    "\n",
    "columns = [\"firstname\",\"lastname\",\"country\",\"state\"]\n",
    "df = spark.createDataFrame(data = data, schema = columns)\n",
    "df.show(n=5)\n",
    "\n",
    "def state_convert(code):\n",
    "    return broadcastStates.value[code]\n",
    "\n",
    "# case 1, using rdd\n",
    "result_rdd = df.rdd.map(lambda x: (x[0],x[1],x[2],state_convert(x[3]))).toDF(columns)\n",
    "result_rdd.show(n=5)\n",
    "\n",
    "\n",
    "# case 2, using pdf\n",
    "\n",
    "@F.udf(returnType=StringType())\n",
    "def state_convert_udf(code : str) -> str:\n",
    "    return broadcastStates.value[code]\n",
    "\n",
    "result_df = (\n",
    "    df.withColumn(\"converted_state\", state_convert_udf(C(\"state\")))\n",
    ")\n",
    "\n",
    "result_df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T16:45:51.075147Z",
     "start_time": "2021-06-25T16:45:51.048456Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.broadcast.Broadcast'> ['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_jbroadcast', '_path', '_pickle_registry', '_python_broadcast', '_sc', 'destroy', 'dump', 'load', 'load_from_path', 'unpersist', 'value']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'NY': 'New York', 'CA': 'California', 'FL': 'Florida'}, dict)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2\n",
    "# Knowing broacsting object\n",
    "\n",
    "# broadcast the dictionary to spark \n",
    "# (which is a way that enhance multi-processing cross machine using your python code)\n",
    "# the broadcast variable should be serializable\n",
    "# https://spark.apache.org/docs/2.3.3/api/python/_modules/pyspark/broadcast.html\n",
    "states = {\"NY\":\"New York\", \"CA\":\"California\", \"FL\":\"Florida\"}\n",
    "broadcastStates = spark.sparkContext.broadcast(states)\n",
    "print(type(broadcastStates), dir(broadcastStates))\n",
    "\n",
    "# value to access the object\n",
    "broadcastStates.value, type(broadcastStates.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T16:45:51.194284Z",
     "start_time": "2021-06-25T16:45:51.076857Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export dataframe(3+)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T16:45:51.829332Z",
     "start_time": "2021-06-25T16:45:51.196887Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n",
      "|article_id|             img_url|\n",
      "+----------+--------------------+\n",
      "|     14431|https://pic.pimg....|\n",
      "|     14431|https://pic.pimg....|\n",
      "|     14431|https://pic.pimg....|\n",
      "|     67789|https://pic.pimg....|\n",
      "|     67789|https://pic.pimg....|\n",
      "+----------+--------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>img_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14431</td>\n",
       "      <td>https://pic.pimg.tw/happy78/1528543947-6853804...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14431</td>\n",
       "      <td>https://pic.pimg.tw/happy78/1528543947-3627597...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14431</td>\n",
       "      <td>https://pic.pimg.tw/happy78/1528543962-2265924...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>67789</td>\n",
       "      <td>https://pic.pimg.tw/happy78/1528543962-4007835...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>67789</td>\n",
       "      <td>https://pic.pimg.tw/happy78/1528543962-45890_n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   article_id                                            img_url\n",
       "0       14431  https://pic.pimg.tw/happy78/1528543947-6853804...\n",
       "1       14431  https://pic.pimg.tw/happy78/1528543947-3627597...\n",
       "2       14431  https://pic.pimg.tw/happy78/1528543962-2265924...\n",
       "3       67789  https://pic.pimg.tw/happy78/1528543962-4007835...\n",
       "4       67789  https://pic.pimg.tw/happy78/1528543962-45890_n..."
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1\n",
    "# write dataframe to jsonl format\n",
    "# https://stackoverflow.com/questions/43269244/pyspark-dataframe-write-to-single-json-file-with-specific-name\n",
    "# https://sparkbyexamples.com/pyspark/pyspark-read-json-file-into-dataframe/\n",
    "data = [\n",
    "    (14431,'https://pic.pimg.tw/happy78/1528543947-685380499_n.jpg'),\n",
    "    (14431,'https://pic.pimg.tw/happy78/1528543947-362759723_n.jpg'),\n",
    "    (14431,'https://pic.pimg.tw/happy78/1528543962-2265924582_n.jpg'),\n",
    "    (67789,'https://pic.pimg.tw/happy78/1528543962-4007835890_n.jpg'),\n",
    "    (67789,'https://pic.pimg.tw/happy78/1528543962-45890_n.jpg')\n",
    "]\n",
    "\n",
    "columns = ['article_id','img_url']\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "fname_folder = join('output','jsonl_format_folder.json')\n",
    "# This one will creat a folder contains part file for better multiple worker IO\n",
    "df.coalesce(1).write.format('json').save(fname_folder, mode='overwrite')\n",
    "df_new = spark.read.json(fname_folder)\n",
    "df_new.show(n=10)\n",
    "# However, if you wanna save it in a single file, use pandas\n",
    "fname = join('output','jsonl_format.json')\n",
    "# df.toPandas().to_json('path/file_name.json', orient='records', force_ascii=False, lines=True)\n",
    "df.toPandas().to_json(fname, orient='records',force_ascii=False,lines=True)\n",
    "df_new_pd = pd.read_json(fname,orient='records',lines=True)\n",
    "df_new_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T16:45:53.172808Z",
     "start_time": "2021-06-25T16:45:51.831041Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+--------------------+\n",
      "|article_id|    date|             img_url|\n",
      "+----------+--------+--------------------+\n",
      "|     14431|20210224|https://pic.pimg....|\n",
      "|     14431|20210224|https://pic.pimg....|\n",
      "|     14431|20210224|https://pic.pimg....|\n",
      "|     67789|20210224|https://pic.pimg....|\n",
      "|     67789|20210224|https://pic.pimg....|\n",
      "+----------+--------+--------------------+\n",
      "\n",
      "+----------+--------+--------------------+\n",
      "|article_id|    date|             img_url|\n",
      "+----------+--------+--------------------+\n",
      "|     86481|20210225|https://pic.pimg....|\n",
      "|     45213|20210225|https://pic.pimg....|\n",
      "|     24561|20210225|https://pic.pimg....|\n",
      "|     75371|20210225|https://pic.pimg....|\n",
      "|     25691|20210225|https://pic.pimg....|\n",
      "+----------+--------+--------------------+\n",
      "\n",
      "+----------+--------+--------------------+\n",
      "|article_id|    date|             img_url|\n",
      "+----------+--------+--------------------+\n",
      "|      7861|20210304|https://pic.pimg....|\n",
      "|     45213|20210304|https://pic.pimg....|\n",
      "|      1111|20210304|https://pic.pimg....|\n",
      "|     76661|20210304|https://pic.pimg....|\n",
      "|      8888|20210304|https://pic.pimg....|\n",
      "+----------+--------+--------------------+\n",
      "\n",
      "+----------+--------------------+--------+\n",
      "|article_id|             img_url|    date|\n",
      "+----------+--------------------+--------+\n",
      "|     14431|https://pic.pimg....|20210224|\n",
      "|     67789|https://pic.pimg....|20210224|\n",
      "|     67789|https://pic.pimg....|20210224|\n",
      "|     14431|https://pic.pimg....|20210224|\n",
      "|     14431|https://pic.pimg....|20210224|\n",
      "|     24561|https://pic.pimg....|20210225|\n",
      "|     75371|https://pic.pimg....|20210225|\n",
      "|     25691|https://pic.pimg....|20210225|\n",
      "|     86481|https://pic.pimg....|20210225|\n",
      "|     45213|https://pic.pimg....|20210225|\n",
      "+----------+--------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2 write parquet by date parittion\n",
    "\n",
    "\n",
    "# 1\n",
    "# write dataframe to jsonl format\n",
    "# https://sparkbyexamples.com/pyspark/pyspark-read-and-write-parquet-file/\n",
    "data_d1 = [\n",
    "    (14431,20210224,'https://pic.pimg.tw/happy78/1528543947-685380499_n.jpg'),\n",
    "    (14431,20210224,'https://pic.pimg.tw/happy78/1528543947-362759723_n.jpg'),\n",
    "    (14431,20210224,'https://pic.pimg.tw/happy78/1528543962-2265924582_n.jpg'),\n",
    "    (67789,20210224,'https://pic.pimg.tw/happy78/1528543962-4007835890_n.jpg'),\n",
    "    (67789,20210224,'https://pic.pimg.tw/happy78/1528543962-45890_n.jpg')\n",
    "]\n",
    "\n",
    "data_d2 = [\n",
    "    (86481,20210225,'https://pic.pimg.tw/happy78/1528543947-685380499_n.jpg'),\n",
    "    (45213,20210225,'https://pic.pimg.tw/happy78/1528543947-362759723_n.jpg'),\n",
    "    (24561,20210225,'https://pic.pimg.tw/happy78/1528543962-2265924582_n.jpg'),\n",
    "    (75371,20210225,'https://pic.pimg.tw/happy78/1528543962-4007835890_n.jpg'),\n",
    "    (25691,20210225,'https://pic.pimg.tw/happy78/1528543962-45890_n.jpg')\n",
    "]\n",
    "\n",
    "data_d3 = [\n",
    "    (7861,20210304,'https://pic.pimg.tw/happy78/1528543947-685380499_n.jpg'),\n",
    "    (45213,20210304,'https://pic.pimg.tw/happy78/1528543947-362759723_n.jpg'),\n",
    "    (1111,20210304,'https://pic.pimg.tw/happy78/1528543962-2265924582_n.jpg'),\n",
    "    (76661,20210304,'https://pic.pimg.tw/happy78/1528543962-4007835890_n.jpg'),\n",
    "    (8888,20210304,'https://pic.pimg.tw/happy78/1528543962-45890_n.jpg')\n",
    "]\n",
    "\n",
    "\n",
    "columns = ['article_id','date','img_url']\n",
    "\n",
    "df_d1 = spark.createDataFrame(data_d1, columns)\n",
    "df_d2 = spark.createDataFrame(data_d2, columns)\n",
    "df_d3 = spark.createDataFrame(data_d3, columns)\n",
    "\n",
    "for df in [df_d1, df_d2, df_d3]:\n",
    "    df.show(n=5)\n",
    "\n",
    "# save it\n",
    "parquet_fname = join(\"output\",\"save_by_date_partition.parquet\")\n",
    "for df in [df_d1, df_d2, df_d3]:\n",
    "    df.write.parquet(parquet_fname, mode=\"overwrite\", partitionBy=\"date\")\n",
    "\n",
    "# read by date range\n",
    "start_date = 20210224\n",
    "end_date = 20210301\n",
    "# support int and daterange, string might be problem\n",
    "\n",
    "new_df = spark.read.parquet(parquet_fname)\\\n",
    "         .where(C(\"date\").between(start_date, end_date))\n",
    "\n",
    "new_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T16:45:53.309377Z",
     "start_time": "2021-06-25T16:45:53.175344Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------+\n",
      "|article_id|             img_url|    date|\n",
      "+----------+--------------------+--------+\n",
      "|     14431|https://pic.pimg....|20210224|\n",
      "|     67789|https://pic.pimg....|20210224|\n",
      "|     67789|https://pic.pimg....|20210224|\n",
      "|     14431|https://pic.pimg....|20210224|\n",
      "|     14431|https://pic.pimg....|20210224|\n",
      "|      1111|https://pic.pimg....|20210304|\n",
      "|     76661|https://pic.pimg....|20210304|\n",
      "|      8888|https://pic.pimg....|20210304|\n",
      "|     24561|https://pic.pimg....|20210225|\n",
      "|     75371|https://pic.pimg....|20210225|\n",
      "|     25691|https://pic.pimg....|20210225|\n",
      "|      7861|https://pic.pimg....|20210304|\n",
      "|     45213|https://pic.pimg....|20210304|\n",
      "|     86481|https://pic.pimg....|20210225|\n",
      "|     45213|https://pic.pimg....|20210225|\n",
      "+----------+--------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read it\n",
    "start_date = 20210224\n",
    "end_date = 20210304\n",
    "\n",
    "# df.filter(df.year >= myYear)\n",
    "new_df = spark.read.parquet(parquet_fname)\n",
    "date_range_cond = (new_df.date >= start_date) & (new_df.date <= end_date)\n",
    "new_df.filter(date_range_cond).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T16:45:53.662820Z",
     "start_time": "2021-06-25T16:45:53.310937Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+--------------------+\n",
      "|   id|    date|             img_url|\n",
      "+-----+--------+--------------------+\n",
      "|14431|20210224|https://pic.pimg....|\n",
      "|14432|20210224|https://pic.pimg....|\n",
      "|14433|20210224|https://pic.pimg....|\n",
      "|67784|20210224|https://pic.pimg....|\n",
      "|67785|20210224|https://pic.pimg....|\n",
      "+-----+--------+--------------------+\n",
      "\n",
      "+-----+--------+--------------------+\n",
      "|   id|    date|export_data(package)|\n",
      "+-----+--------+--------------------+\n",
      "|14431|20210224|[{img_url=https:/...|\n",
      "|14432|20210224|[{img_url=https:/...|\n",
      "|14433|20210224|[{img_url=https:/...|\n",
      "|67784|20210224|[{img_url=https:/...|\n",
      "|67785|20210224|[{img_url=https:/...|\n",
      "+-----+--------+--------------------+\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- date: long (nullable = true)\n",
      " |-- export_data(package): array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3 export data via api - I\n",
    "##### data sample, all of them is string type, we gonna send it by POST\n",
    "#     [\n",
    "#         {\n",
    "#             \"poi_hash\": \"aed525e93b72\",\n",
    "#             \"some-key\": \"some-data\",\n",
    "#             ...\n",
    "#         },\n",
    "#         ...\n",
    "#         {\n",
    "#             \"poi_hash\": \"aed525e93b72\",\n",
    "#             \"some-key\": \"some-data\",\n",
    "#             ...\n",
    "#         }\n",
    "#     ]\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "raw_data = [\n",
    "    (14431,20210224,'https://pic.pimg.tw/happy78/1528543947-685380499_n.jpg'),\n",
    "    (14432,20210224,'https://pic.pimg.tw/happy78/1528543947-362759723_n.jpg'),\n",
    "    (14433,20210224,'https://pic.pimg.tw/happy78/1528543962-2265924582_n.jpg'),\n",
    "    (67784,20210224,'https://pic.pimg.tw/happy78/1528543962-4007835890_n.jpg'),\n",
    "    (67785,20210224,'https://pic.pimg.tw/happy78/1528543962-45890_n.jpg')\n",
    "]\n",
    "col = ['id','date','img_url']\n",
    "raw_sdf = spark.createDataFrame(raw_data,col)\n",
    "raw_sdf.show()\n",
    "\n",
    "pack_sdf = (\n",
    "    raw_sdf\n",
    "    # Struct with build a structure like dictionary\n",
    "    .withColumn('package',F.struct('id','img_url'))\n",
    ")\n",
    "\n",
    "\n",
    "def export_data(row, key=None, endpoint=None, timeout=10):\n",
    "    # UDF will get Row object\n",
    "    row_in_list = [row.asDict()]\n",
    "#     try:\n",
    "#         resp = requests.post(\n",
    "#             url=endpoint,\n",
    "#             data=json.dumps(row_in_list),\n",
    "#             headers={\"x-authorization\" : key},\n",
    "#             timeout=timeout\n",
    "#         )\n",
    "        \n",
    "#         data = resp.json()\n",
    "#         log = ...\n",
    "#     except Exception as err:\n",
    "#         err_resp = str(resp.content, encoding='utf-8')\n",
    "#         error_msg = f\"WARNING: Got {err}. Error situation: {error_resp}\"\n",
    "        \n",
    "#         log = ...\n",
    "    return row_in_list\n",
    "\n",
    "\n",
    "api_log_sdf = (\n",
    "    pack_sdf.select(\n",
    "        'id',\n",
    "        'date',\n",
    "        F.udf(export_data, returnType='array<string>')('package')\n",
    "    )\n",
    ")\n",
    "\n",
    "api_log_sdf.show()\n",
    "api_log_sdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datetime related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T16:45:53.959508Z",
     "start_time": "2021-06-25T16:45:53.665629Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+----------+\n",
      "|_year|_month| upsert_at|\n",
      "+-----+------+----------+\n",
      "| 2010|    10|1285939860|\n",
      "+-----+------+----------+\n",
      "\n",
      "+-----+------+----------+--------+\n",
      "|_year|_month| upsert_at|    date|\n",
      "+-----+------+----------+--------+\n",
      "| 2010|    10|1285939860|20101001|\n",
      "+-----+------+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from unixtime to date\n",
    "\n",
    "data = [(2010, 10, 1285939860)]\n",
    "col = [\"_year\",\"_month\",\"upsert_at\"]\n",
    "\n",
    "df = spark.createDataFrame(data, col)\n",
    "df.show()\n",
    "\n",
    "(\n",
    "    df\n",
    "    .withColumn(\"date\",\n",
    "                F.from_unixtime(C(\"upsert_at\"),\"yyyyMMdd\").cast(\"integer\")\n",
    "               )\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T16:45:54.248116Z",
     "start_time": "2021-06-25T16:45:53.962116Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+----------+\n",
      "|_year|_month| upsert_at|\n",
      "+-----+------+----------+\n",
      "| 2010|    10|1285939860|\n",
      "+-----+------+----------+\n",
      "\n",
      "+-----+------+----------+----------+---------+\n",
      "|_year|_month| upsert_at| update_at|converted|\n",
      "+-----+------+----------+----------+---------+\n",
      "| 2010|    10|1285939860|1624639554| 20210626|\n",
      "+-----+------+----------+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create a timestamp column\n",
    "\n",
    "data = [(2010, 10, 1285939860)]\n",
    "col = [\"_year\",\"_month\",\"upsert_at\"]\n",
    "\n",
    "df = spark.createDataFrame(data, col)\n",
    "df.show()\n",
    "\n",
    "(\n",
    "    df\n",
    "    .withColumn('update_at', F.unix_timestamp())\n",
    "    .withColumn('converted', F.from_unixtime(C('update_at'), \"yyyyMMdd\").cast('integer'))\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other (3+)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T16:45:55.508556Z",
     "start_time": "2021-06-25T16:45:54.251077Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before\n",
      "+----------+-------------+------------------------+\n",
      "|store_name|food_category|food_category_popularity|\n",
      "+----------+-------------+------------------------+\n",
      "|    hotpop|         Meat|                       3|\n",
      "|    hotpop|         Meat|                       3|\n",
      "|    hotpop|         Meat|                       3|\n",
      "|    hotpop|    Vegetable|                       2|\n",
      "|    hotpop|    Vegetable|                       2|\n",
      "|    branch|   Fried food|                       1|\n",
      "|    branch|      Dessert|                       1|\n",
      "+----------+-------------+------------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>store_name</th>\n",
       "      <th>food_category</th>\n",
       "      <th>food_category_popularity</th>\n",
       "      <th>food_cat_pop_score</th>\n",
       "      <th>cat_idx</th>\n",
       "      <th>store_cat_pop_rank</th>\n",
       "      <th>store_cat_pop_rank_score</th>\n",
       "      <th>mix_cat_pop_rank_score</th>\n",
       "      <th>mix_cat_pop_rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hotpop</td>\n",
       "      <td>Meat</td>\n",
       "      <td>3</td>\n",
       "      <td>308.170726</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hotpop</td>\n",
       "      <td>Vegetable</td>\n",
       "      <td>2</td>\n",
       "      <td>184.887504</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hotpop</td>\n",
       "      <td>Meat</td>\n",
       "      <td>3</td>\n",
       "      <td>317.623586</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hotpop</td>\n",
       "      <td>Vegetable</td>\n",
       "      <td>2</td>\n",
       "      <td>170.452232</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2.1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hotpop</td>\n",
       "      <td>Meat</td>\n",
       "      <td>3</td>\n",
       "      <td>259.721563</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>branch</td>\n",
       "      <td>Dessert</td>\n",
       "      <td>1</td>\n",
       "      <td>111.860253</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>branch</td>\n",
       "      <td>Fried food</td>\n",
       "      <td>1</td>\n",
       "      <td>94.468225</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  store_name food_category  food_category_popularity  food_cat_pop_score  \\\n",
       "0     hotpop          Meat                         3          308.170726   \n",
       "1     hotpop     Vegetable                         2          184.887504   \n",
       "2     hotpop          Meat                         3          317.623586   \n",
       "3     hotpop     Vegetable                         2          170.452232   \n",
       "4     hotpop          Meat                         3          259.721563   \n",
       "5     branch       Dessert                         1          111.860253   \n",
       "6     branch    Fried food                         1           94.468225   \n",
       "\n",
       "   cat_idx  store_cat_pop_rank  store_cat_pop_rank_score  \\\n",
       "0        1                   1                       0.0   \n",
       "1        1                   2                       0.1   \n",
       "2        2                   1                       0.0   \n",
       "3        2                   2                       0.1   \n",
       "4        3                   1                       0.0   \n",
       "5        1                   1                       0.0   \n",
       "6        1                   1                       0.0   \n",
       "\n",
       "   mix_cat_pop_rank_score  mix_cat_pop_rank  \n",
       "0                     1.0                 1  \n",
       "1                     1.1                 2  \n",
       "2                     2.0                 3  \n",
       "3                     2.1                 4  \n",
       "4                     3.0                 5  \n",
       "5                     1.0                 1  \n",
       "6                     1.0                 2  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1\n",
    "# rank the food category popularity by store_name but crossed and rotated\n",
    "\n",
    "\n",
    "# create a mix ranking number\n",
    "# create a popularity_rank_score in each store_name\n",
    "    # Top popular in each store_name -> score 0\n",
    "    # Second popular in each store_name -> score 0.1\n",
    "# Add row number in each store_name, food_category -> cat_rank\n",
    "# Create category_popularity_mix_rank_score = cat_rank + popularity_rank_score\n",
    "# Sort the category_popularity_mix_rank_score by store_name\n",
    "\n",
    "data = [\n",
    "    (\"hotpop\",\"Meat\",3,),\n",
    "    (\"hotpop\",\"Meat\",3),\n",
    "    (\"hotpop\",\"Meat\",3),\n",
    "    (\"hotpop\",\"Vegetable\",2),\n",
    "    (\"hotpop\",\"Vegetable\",2),\n",
    "    (\"branch\",\"Fried food\",1),\n",
    "    (\"branch\",\"Dessert\",1),\n",
    "  ]\n",
    "\n",
    "columns = [\"store_name\",\"food_category\",\"food_category_popularity\"]\n",
    "df = spark.createDataFrame(data = data, schema = columns)\n",
    "\n",
    "print('before')\n",
    "df.show(n=10)\n",
    "\n",
    "################# sol #######################\n",
    "store_cat_pop_rank_score = F.when(C(\"store_cat_pop_rank\") == 1, 0)\\\n",
    "                            .when(C(\"store_cat_pop_rank\") == 2, 0.1)\n",
    "\n",
    "\n",
    "window_sotre_cat_pop = W.partitionBy('store_name').orderBy(C(\"food_category_popularity\").desc())\n",
    "window_sotre_cat = W.partitionBy(['store_name','food_category']).orderBy(C(\"food_category\"))\n",
    "window_sotr_cat_mix_rank = W.partitionBy(['store_name']).orderBy(C(\"mix_cat_pop_rank_score\"))\n",
    "df = (\n",
    "    df.withColumn(\"food_cat_pop_score\", 100 * C(\"food_category_popularity\") + 20 * F.randn(seed=42))\\\n",
    "      .withColumn(\"cat_idx\", F.row_number().over(window_sotre_cat))\\\n",
    "      .withColumn(\"store_cat_pop_rank\", F.dense_rank().over(window_sotre_cat_pop))\n",
    "      .withColumn(\"store_cat_pop_rank_score\", store_cat_pop_rank_score)\\\n",
    "      .withColumn(\"mix_cat_pop_rank_score\", C(\"cat_idx\") + C(\"store_cat_pop_rank_score\"))\\\n",
    "      .withColumn(\"mix_cat_pop_rank\", F.row_number().over(window_sotr_cat_mix_rank))\\\n",
    ")\n",
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T16:45:58.305108Z",
     "start_time": "2021-06-25T16:45:55.511020Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before\n",
      "+----------+-------------+------------------------+\n",
      "|store_name|food_category|food_category_popularity|\n",
      "+----------+-------------+------------------------+\n",
      "|    hotpop|         Meat|                       3|\n",
      "|    hotpop|         Meat|                       3|\n",
      "|    hotpop|         Meat|                       3|\n",
      "|    hotpop|    Vegetable|                       2|\n",
      "|    hotpop|    Vegetable|                       2|\n",
      "|    branch|   Fried food|                       1|\n",
      "|    branch|      Dessert|                       1|\n",
      "+----------+-------------+------------------------+\n",
      "\n",
      "sol - 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>store_name</th>\n",
       "      <th>food_category</th>\n",
       "      <th>food_category_popularity</th>\n",
       "      <th>food_cat_pop_score</th>\n",
       "      <th>cat_idx</th>\n",
       "      <th>store_cat_pop_rank</th>\n",
       "      <th>store_cat_pop_rank_score</th>\n",
       "      <th>mix_cat_pop_rank_score</th>\n",
       "      <th>mix_cat_pop_rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hotpop</td>\n",
       "      <td>Meat</td>\n",
       "      <td>3</td>\n",
       "      <td>318.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hotpop</td>\n",
       "      <td>Vegetable</td>\n",
       "      <td>2</td>\n",
       "      <td>185.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hotpop</td>\n",
       "      <td>Meat</td>\n",
       "      <td>3</td>\n",
       "      <td>308.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hotpop</td>\n",
       "      <td>Vegetable</td>\n",
       "      <td>2</td>\n",
       "      <td>170.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2.1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hotpop</td>\n",
       "      <td>Meat</td>\n",
       "      <td>3</td>\n",
       "      <td>260.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>branch</td>\n",
       "      <td>Dessert</td>\n",
       "      <td>1</td>\n",
       "      <td>112.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>branch</td>\n",
       "      <td>Fried food</td>\n",
       "      <td>1</td>\n",
       "      <td>94.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  store_name food_category  food_category_popularity  food_cat_pop_score  \\\n",
       "0     hotpop          Meat                         3               318.0   \n",
       "1     hotpop     Vegetable                         2               185.0   \n",
       "2     hotpop          Meat                         3               308.0   \n",
       "3     hotpop     Vegetable                         2               170.0   \n",
       "4     hotpop          Meat                         3               260.0   \n",
       "5     branch       Dessert                         1               112.0   \n",
       "6     branch    Fried food                         1                94.0   \n",
       "\n",
       "   cat_idx  store_cat_pop_rank  store_cat_pop_rank_score  \\\n",
       "0        1                   1                       0.0   \n",
       "1        1                   2                       0.1   \n",
       "2        2                   1                       0.0   \n",
       "3        2                   2                       0.1   \n",
       "4        3                   1                       0.0   \n",
       "5        1                   1                       0.0   \n",
       "6        1                   1                       0.0   \n",
       "\n",
       "   mix_cat_pop_rank_score  mix_cat_pop_rank  \n",
       "0                     1.0                 1  \n",
       "1                     1.1                 2  \n",
       "2                     2.0                 3  \n",
       "3                     2.1                 4  \n",
       "4                     3.0                 5  \n",
       "5                     1.0                 1  \n",
       "6                     1.0                 2  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "so1 - 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>store_name</th>\n",
       "      <th>food_category</th>\n",
       "      <th>food_category_popularity</th>\n",
       "      <th>food_cat_pop_score</th>\n",
       "      <th>cat_idx</th>\n",
       "      <th>store_cat_pop_rank</th>\n",
       "      <th>store_cat_pop_rank_score</th>\n",
       "      <th>mix_cat_pop_rank_score</th>\n",
       "      <th>mix_cat_pop_rank</th>\n",
       "      <th>category_id_in_poi</th>\n",
       "      <th>category_popularity_rank_in_poi</th>\n",
       "      <th>staggered_rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hotpop</td>\n",
       "      <td>Meat</td>\n",
       "      <td>3</td>\n",
       "      <td>292.716730</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hotpop</td>\n",
       "      <td>Vegetable</td>\n",
       "      <td>2</td>\n",
       "      <td>206.533627</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hotpop</td>\n",
       "      <td>Meat</td>\n",
       "      <td>3</td>\n",
       "      <td>315.162915</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hotpop</td>\n",
       "      <td>Vegetable</td>\n",
       "      <td>2</td>\n",
       "      <td>192.155336</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2.1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hotpop</td>\n",
       "      <td>Meat</td>\n",
       "      <td>3</td>\n",
       "      <td>254.477937</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>branch</td>\n",
       "      <td>Dessert</td>\n",
       "      <td>1</td>\n",
       "      <td>83.078215</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>branch</td>\n",
       "      <td>Fried food</td>\n",
       "      <td>1</td>\n",
       "      <td>127.067581</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  store_name food_category  food_category_popularity  food_cat_pop_score  \\\n",
       "0     hotpop          Meat                         3          292.716730   \n",
       "1     hotpop     Vegetable                         2          206.533627   \n",
       "2     hotpop          Meat                         3          315.162915   \n",
       "3     hotpop     Vegetable                         2          192.155336   \n",
       "4     hotpop          Meat                         3          254.477937   \n",
       "5     branch       Dessert                         1           83.078215   \n",
       "6     branch    Fried food                         1          127.067581   \n",
       "\n",
       "   cat_idx  store_cat_pop_rank  store_cat_pop_rank_score  \\\n",
       "0        1                   1                       0.0   \n",
       "1        1                   2                       0.1   \n",
       "2        2                   1                       0.0   \n",
       "3        2                   2                       0.1   \n",
       "4        3                   1                       0.0   \n",
       "5        1                   1                       0.0   \n",
       "6        1                   1                       0.0   \n",
       "\n",
       "   mix_cat_pop_rank_score  mix_cat_pop_rank  category_id_in_poi  \\\n",
       "0                     1.0                 1                   1   \n",
       "1                     1.1                 2                   1   \n",
       "2                     2.0                 3                   2   \n",
       "3                     2.1                 4                   2   \n",
       "4                     3.0                 5                   3   \n",
       "5                     1.0                 1                   1   \n",
       "6                     1.0                 2                   1   \n",
       "\n",
       "   category_popularity_rank_in_poi  staggered_rank  \n",
       "0                                1               1  \n",
       "1                                2               2  \n",
       "2                                1               3  \n",
       "3                                2               4  \n",
       "4                                1               5  \n",
       "5                                1               1  \n",
       "6                                1               2  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 2\n",
    "# create a food category popularity score\n",
    "# rank the food category popularity score but crossed and rotated\n",
    "\n",
    "\n",
    "\n",
    "# create a mix ranking number\n",
    "# create a popularity_rank_score in each store_name\n",
    "    # Top popular in each store_name -> score 0\n",
    "    # Second popular in each store_name -> score 0.1\n",
    "# Add row number in each store_name, food_category -> cat_rank\n",
    "# Create category_popularity_mix_rank_score = cat_rank + popularity_rank_score\n",
    "# Sort the category_popularity_mix_rank_score by store_name\n",
    "\n",
    "data = [\n",
    "    (\"hotpop\",\"Meat\",3,),\n",
    "    (\"hotpop\",\"Meat\",3),\n",
    "    (\"hotpop\",\"Meat\",3),\n",
    "    (\"hotpop\",\"Vegetable\",2),\n",
    "    (\"hotpop\",\"Vegetable\",2),\n",
    "    (\"branch\",\"Fried food\",1),\n",
    "    (\"branch\",\"Dessert\",1),\n",
    "  ]\n",
    "\n",
    "columns = [\"store_name\",\"food_category\",\"food_category_popularity\"]\n",
    "df = spark.createDataFrame(data = data, schema = columns)\n",
    "\n",
    "print('before')\n",
    "df.show(n=10)\n",
    "\n",
    "################# sol 1 #######################\n",
    "store_cat_pop_rank_score = F.when(C(\"store_cat_pop_rank\") == 1, 0)\\\n",
    "                            .when(C(\"store_cat_pop_rank\") == 2, 0.1)\n",
    "\n",
    "\n",
    "window_sotre_cat_pop = W.partitionBy('store_name').orderBy(C(\"food_category_popularity\").desc())\n",
    "window_sotre_cat = W.partitionBy(['store_name','food_category']).orderBy(C(\"food_cat_pop_score\").desc())\n",
    "window_sotr_cat_mix_rank = W.partitionBy(['store_name']).orderBy(C(\"mix_cat_pop_rank_score\"))\n",
    "df = (\n",
    "    df.withColumn(\"food_cat_pop_score\",\n",
    "                  F.round(100 * C(\"food_category_popularity\") + 20 * F.randn(seed=42))\n",
    "                 )\\\n",
    "      .withColumn(\"cat_idx\", F.row_number().over(window_sotre_cat))\\\n",
    "      .withColumn(\"store_cat_pop_rank\", F.dense_rank().over(window_sotre_cat_pop))\n",
    "      .withColumn(\"store_cat_pop_rank_score\", store_cat_pop_rank_score)\\\n",
    "      .withColumn(\"mix_cat_pop_rank_score\", C(\"cat_idx\") + C(\"store_cat_pop_rank_score\"))\\\n",
    "      .withColumn(\"mix_cat_pop_rank\", F.row_number().over(window_sotr_cat_mix_rank))\\\n",
    ")\n",
    "\n",
    "print('sol - 1')\n",
    "display(df.toPandas())\n",
    "\n",
    "############### sol 2 ########################\n",
    "\n",
    "# Use another way to sort it\n",
    "# sort by cat_idx and store_cat_pop_rank\n",
    "\n",
    "\n",
    "\n",
    "category_id_in_poi = W.partitionBy(['store_name','food_category']).orderBy(C(\"food_category\"))\n",
    "category_popularity_rank_in_poi = W.partitionBy('store_name').orderBy(C(\"food_category_popularity\").desc())\n",
    "window_sotr_cat_mix_rank = W.partitionBy(['store_name']).orderBy(\n",
    "    C(\"category_id_in_poi\"),\n",
    "    C(\"category_popularity_rank_in_poi\")\n",
    ")\n",
    "df_sol_2 = (\n",
    "    df.withColumn(\"food_cat_pop_score\", 100 * C(\"food_category_popularity\") + 20 * F.randn(seed=42))\\\n",
    "      .withColumn(\"category_id_in_poi\", F.row_number().over(category_id_in_poi))\\\n",
    "      .withColumn(\"category_popularity_rank_in_poi\", F.dense_rank().over(category_popularity_rank_in_poi))\n",
    "      .withColumn(\"staggered_rank\", F.row_number().over(window_sotr_cat_mix_rank))\\\n",
    ")\n",
    "\n",
    "print('so1 - 2')\n",
    "\n",
    "display(df_sol_2.toPandas())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T16:45:58.325604Z",
     "start_time": "2021-06-25T16:45:58.306548Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AutoBatchedSerializer',\n",
       " 'Column',\n",
       " 'DataFrame',\n",
       " 'DataType',\n",
       " 'PandasUDFType',\n",
       " 'PickleSerializer',\n",
       " 'PythonEvalType',\n",
       " 'SparkContext',\n",
       " 'StringType',\n",
       " 'UserDefinedFunction',\n",
       " '__all__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__spec__',\n",
       " '_binary_mathfunctions',\n",
       " '_collect_list_doc',\n",
       " '_collect_set_doc',\n",
       " '_create_binary_mathfunction',\n",
       " '_create_function',\n",
       " '_create_udf',\n",
       " '_create_window_function',\n",
       " '_functions',\n",
       " '_functions_1_4',\n",
       " '_functions_1_6',\n",
       " '_functions_2_1',\n",
       " '_functions_deprecated',\n",
       " '_lit_doc',\n",
       " '_message',\n",
       " '_string_functions',\n",
       " '_test',\n",
       " '_to_java_column',\n",
       " '_to_seq',\n",
       " '_window_functions',\n",
       " '_wrap_deprecated_function',\n",
       " 'abs',\n",
       " 'acos',\n",
       " 'add_months',\n",
       " 'approxCountDistinct',\n",
       " 'approx_count_distinct',\n",
       " 'array',\n",
       " 'array_contains',\n",
       " 'asc',\n",
       " 'ascii',\n",
       " 'asin',\n",
       " 'atan',\n",
       " 'atan2',\n",
       " 'avg',\n",
       " 'base64',\n",
       " 'bin',\n",
       " 'bitwiseNOT',\n",
       " 'blacklist',\n",
       " 'broadcast',\n",
       " 'bround',\n",
       " 'cbrt',\n",
       " 'ceil',\n",
       " 'coalesce',\n",
       " 'col',\n",
       " 'collect_list',\n",
       " 'collect_set',\n",
       " 'column',\n",
       " 'concat',\n",
       " 'concat_ws',\n",
       " 'conv',\n",
       " 'corr',\n",
       " 'cos',\n",
       " 'cosh',\n",
       " 'count',\n",
       " 'countDistinct',\n",
       " 'covar_pop',\n",
       " 'covar_samp',\n",
       " 'crc32',\n",
       " 'create_map',\n",
       " 'cume_dist',\n",
       " 'current_date',\n",
       " 'current_timestamp',\n",
       " 'date_add',\n",
       " 'date_format',\n",
       " 'date_sub',\n",
       " 'date_trunc',\n",
       " 'datediff',\n",
       " 'dayofmonth',\n",
       " 'dayofweek',\n",
       " 'dayofyear',\n",
       " 'decode',\n",
       " 'degrees',\n",
       " 'dense_rank',\n",
       " 'desc',\n",
       " 'encode',\n",
       " 'exp',\n",
       " 'explode',\n",
       " 'explode_outer',\n",
       " 'expm1',\n",
       " 'expr',\n",
       " 'factorial',\n",
       " 'first',\n",
       " 'floor',\n",
       " 'format_number',\n",
       " 'format_string',\n",
       " 'from_json',\n",
       " 'from_unixtime',\n",
       " 'from_utc_timestamp',\n",
       " 'functools',\n",
       " 'get_json_object',\n",
       " 'greatest',\n",
       " 'grouping',\n",
       " 'grouping_id',\n",
       " 'hash',\n",
       " 'hex',\n",
       " 'hour',\n",
       " 'hypot',\n",
       " 'ignore_unicode_prefix',\n",
       " 'initcap',\n",
       " 'input_file_name',\n",
       " 'instr',\n",
       " 'isnan',\n",
       " 'isnull',\n",
       " 'json_tuple',\n",
       " 'kurtosis',\n",
       " 'lag',\n",
       " 'last',\n",
       " 'last_day',\n",
       " 'lead',\n",
       " 'least',\n",
       " 'length',\n",
       " 'levenshtein',\n",
       " 'lit',\n",
       " 'locate',\n",
       " 'log',\n",
       " 'log10',\n",
       " 'log1p',\n",
       " 'log2',\n",
       " 'lower',\n",
       " 'lpad',\n",
       " 'ltrim',\n",
       " 'map_keys',\n",
       " 'map_values',\n",
       " 'math',\n",
       " 'max',\n",
       " 'md5',\n",
       " 'mean',\n",
       " 'min',\n",
       " 'minute',\n",
       " 'monotonically_increasing_id',\n",
       " 'month',\n",
       " 'months_between',\n",
       " 'nanvl',\n",
       " 'next_day',\n",
       " 'ntile',\n",
       " 'pandas_udf',\n",
       " 'percent_rank',\n",
       " 'posexplode',\n",
       " 'posexplode_outer',\n",
       " 'pow',\n",
       " 'quarter',\n",
       " 'radians',\n",
       " 'rand',\n",
       " 'randn',\n",
       " 'rank',\n",
       " 'regexp_extract',\n",
       " 'regexp_replace',\n",
       " 'repeat',\n",
       " 'reverse',\n",
       " 'rint',\n",
       " 'round',\n",
       " 'row_number',\n",
       " 'rpad',\n",
       " 'rtrim',\n",
       " 'second',\n",
       " 'sha1',\n",
       " 'sha2',\n",
       " 'shiftLeft',\n",
       " 'shiftRight',\n",
       " 'shiftRightUnsigned',\n",
       " 'signum',\n",
       " 'sin',\n",
       " 'since',\n",
       " 'sinh',\n",
       " 'size',\n",
       " 'skewness',\n",
       " 'sort_array',\n",
       " 'soundex',\n",
       " 'spark_partition_id',\n",
       " 'split',\n",
       " 'sqrt',\n",
       " 'stddev',\n",
       " 'stddev_pop',\n",
       " 'stddev_samp',\n",
       " 'struct',\n",
       " 'substring',\n",
       " 'substring_index',\n",
       " 'sum',\n",
       " 'sumDistinct',\n",
       " 'sys',\n",
       " 'tan',\n",
       " 'tanh',\n",
       " 'toDegrees',\n",
       " 'toRadians',\n",
       " 'to_date',\n",
       " 'to_json',\n",
       " 'to_timestamp',\n",
       " 'to_utc_timestamp',\n",
       " 'translate',\n",
       " 'trim',\n",
       " 'trunc',\n",
       " 'udf',\n",
       " 'unbase64',\n",
       " 'unhex',\n",
       " 'unix_timestamp',\n",
       " 'upper',\n",
       " 'var_pop',\n",
       " 'var_samp',\n",
       " 'variance',\n",
       " 'warnings',\n",
       " 'weekofyear',\n",
       " 'when',\n",
       " 'window',\n",
       " 'year']"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3 Knowing the functions of dataframe operation \n",
    "\n",
    "dir(F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T16:45:59.156057Z",
     "start_time": "2021-06-25T16:45:58.326922Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------+\n",
      "| id|  an_array|     a_map|\n",
      "+---+----------+----------+\n",
      "|  1|[foo, bar]|[x -> 1.0]|\n",
      "|  2|        []|        []|\n",
      "|  3|      null|      null|\n",
      "+---+----------+----------+\n",
      "\n",
      "+---+----+-----+\n",
      "| id| key|value|\n",
      "+---+----+-----+\n",
      "|  1|   x|  1.0|\n",
      "|  2|null| null|\n",
      "|  3|null| null|\n",
      "+---+----+-----+\n",
      "\n",
      "+---+---+-----+\n",
      "| id|key|value|\n",
      "+---+---+-----+\n",
      "|  1|  x|  1.0|\n",
      "+---+---+-----+\n",
      "\n",
      "+---+----+\n",
      "| id| col|\n",
      "+---+----+\n",
      "|  1| foo|\n",
      "|  1| bar|\n",
      "|  2|null|\n",
      "|  3|null|\n",
      "+---+----+\n",
      "\n",
      "+---+---+\n",
      "| id|col|\n",
      "+---+---+\n",
      "|  1|foo|\n",
      "|  1|bar|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4 explode_outer\n",
    "# https://spark.apache.org/docs/2.3.0/api/python/pyspark.sql.html#pyspark.sql.GroupedData.apply\n",
    "\n",
    "# return a new row for each element in the given array or map\n",
    "# Unlike explode, if the array/map is null or empty\n",
    "# the null is produced\n",
    "\n",
    "df = spark.createDataFrame(\n",
    "    [\n",
    "        (1, [\"foo\",\"bar\"], {\"x\" : 1.0}),\n",
    "        (2, [], {}),\n",
    "        (3, None, None)\n",
    "    ],\n",
    "    (\"id\", \"an_array\",\"a_map\")\n",
    ")\n",
    "\n",
    "df.show()\n",
    "\n",
    "df.select(\"id\", F.explode_outer(C(\"a_map\"))).show()\n",
    "\n",
    "df.select(\"id\", F.explode(C(\"a_map\"))).show() # null thing will be nothing\n",
    "\n",
    "df.select(\"id\", F.explode_outer(C(\"an_array\"))).show()\n",
    "\n",
    "df.select(\"id\", F.explode(C(\"an_array\"))).show() # null thing will be nothing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T16:45:59.716855Z",
     "start_time": "2021-06-25T16:45:59.159105Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default you will get row object\n",
      "you can convert row into dataframe\n",
      "sometimes you might inidcate schema\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "0 <class 'pyspark.sql.types.Row'>\n",
      "+---+----------+----------+\n",
      "| id|  an_array|     a_map|\n",
      "+---+----------+----------+\n",
      "|  1|[foo, bar]|[x -> 1.0]|\n",
      "+---+----------+----------+\n",
      "\n",
      "1 <class 'pyspark.sql.types.Row'>\n",
      "+---+--------+-----+\n",
      "| id|an_array|a_map|\n",
      "+---+--------+-----+\n",
      "|  2|      []|   []|\n",
      "+---+--------+-----+\n",
      "\n",
      "2 <class 'pyspark.sql.types.Row'>\n",
      "+---+--------+-----+\n",
      "| id|an_array|a_map|\n",
      "+---+--------+-----+\n",
      "|  3|    null| null|\n",
      "+---+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4 iterate your dataframe row by row\n",
    "# you will not use it in production\n",
    "# but it is useful when debugging and develop your algorithm\n",
    "\n",
    "df = spark.createDataFrame(\n",
    "    [\n",
    "        (1, [\"foo\",\"bar\"], {\"x\" : 1.0}),\n",
    "        (2, [], {}),\n",
    "        (3, None, None)\n",
    "    ],\n",
    "    (\"id\", \"an_array\",\"a_map\")\n",
    ")\n",
    "\n",
    "\n",
    "print('default you will get row object')\n",
    "print('you can convert row into dataframe')\n",
    "print('sometimes you might inidcate schema')\n",
    "print('\\n\\n\\n')\n",
    "    \n",
    "for row_idx, row in enumerate(df.rdd.toLocalIterator()):\n",
    "    print(row_idx, type(row))\n",
    "    slice_sdf = spark.createDataFrame([row],schema=df.schema)\n",
    "    slice_sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T16:46:00.029474Z",
     "start_time": "2021-06-25T16:45:59.719989Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "Scan ExistingRDD[store_name#5356,food_category#5357,food_category_popularity#5358L]\n",
      "df1 None\n",
      "== Physical Plan ==\n",
      "InMemoryTableScan [store_name#5356, food_category#5357, food_category_popularity#5358L]\n",
      "   +- InMemoryRelation [store_name#5356, food_category#5357, food_category_popularity#5358L], true, 10000, StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "         +- *(1) Filter (isnotnull(store_name#5356) && (store_name#5356 = branch))\n",
      "            +- Scan ExistingRDD[store_name#5356,food_category#5357,food_category_popularity#5358L]\n",
      "df2 None\n",
      "== Physical Plan ==\n",
      "*(1) Filter (isnotnull(food_category#5357) && (food_category#5357 = Dessert))\n",
      "+- InMemoryTableScan [store_name#5356, food_category#5357, food_category_popularity#5358L], [isnotnull(food_category#5357), (food_category#5357 = Dessert)]\n",
      "      +- InMemoryRelation [store_name#5356, food_category#5357, food_category_popularity#5358L], true, 10000, StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "            +- *(1) Filter (isnotnull(store_name#5356) && (store_name#5356 = branch))\n",
      "               +- Scan ExistingRDD[store_name#5356,food_category#5357,food_category_popularity#5358L]\n",
      "df3 None\n",
      "+----------+-------------+------------------------+\n",
      "|store_name|food_category|food_category_popularity|\n",
      "+----------+-------------+------------------------+\n",
      "|    branch|      Dessert|                       1|\n",
      "+----------+-------------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# cache and persistant\n",
    "\n",
    "# cache and persist, spark provides an optimization mechanism to astore thre indermediate compurtation \n",
    "# of a Spark DataFrame so they can be resued in subsequent actions\n",
    "\n",
    "# persist -> each node stores it's partitioned data in memory and reuse them in other actions on the datasaet\n",
    "\n",
    "# Cost efficient - Spark computations are very expensive hence reusing the computations are used to save cost\n",
    "\n",
    "# Time efficient - Reusing the repeated computations save lots of time.\n",
    "\n",
    "\n",
    "# SparkDataFrame.cache() storage level `MEMORY_AND_DISK`\n",
    "# RDD.cache() storage level `MEMORY_ONLY`\n",
    "\n",
    "\n",
    "data = [\n",
    "    (\"hotpop\",\"Meat\",3,),\n",
    "    (\"hotpop\",\"Meat\",3),\n",
    "    (\"hotpop\",\"Meat\",3),\n",
    "    (\"hotpop\",\"Vegetable\",2),\n",
    "    (\"hotpop\",\"Vegetable\",2),\n",
    "    (\"branch\",\"Fried food\",1),\n",
    "    (\"branch\",\"Dessert\",1),\n",
    "  ]\n",
    "\n",
    "columns = [\"store_name\",\"food_category\",\"food_category_popularity\"]\n",
    "\n",
    "# first df\n",
    "df = spark.createDataFrame(data = data, schema = columns)\n",
    "print('df1', df.explain())\n",
    "# second stage\n",
    "\n",
    "df2 = df.where(C(\"store_name\") == \"branch\").cache()\n",
    "\n",
    "print('df2', df2.explain())\n",
    "\n",
    "# third stage\n",
    "df3 = df2.where(\n",
    "    C(\"food_category\") == \"Dessert\"\n",
    ")\n",
    "\n",
    "print('df3', df3.explain())\n",
    "\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pixlake",
   "language": "python",
   "name": "pixlake"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "245px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
