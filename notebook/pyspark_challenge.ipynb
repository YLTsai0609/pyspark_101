{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:37:04.185792Z",
     "start_time": "2021-07-31T05:37:04.180321Z"
    }
   },
   "outputs": [],
   "source": [
    "# total : 58 problem and solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:37:04.345051Z",
     "start_time": "2021-07-31T05:37:04.192528Z"
    }
   },
   "outputs": [],
   "source": [
    "# env : pixlake\n",
    "# we focuing on pyspark dataframe processing\n",
    "# documentation https://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:37:04.444707Z",
     "start_time": "2021-07-31T05:37:04.347328Z"
    }
   },
   "outputs": [],
   "source": [
    "# make you auto compeletion faster\n",
    "# https://stackoverflow.com/questions/40536560/ipython-and-jupyter-autocomplete-not-working\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:37:04.559858Z",
     "start_time": "2021-07-31T05:37:04.446979Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have pyspark version :  ['spark-2.3', 'spark-3.1.2-bin-hadoop2.7', 'spark-3.0', 'spark-3.0.1-bin-hadoop2.7', 'spark-2.3.4-bin-hadoop2.7', 'spark-2.4.7-bin-hadoop2.7', 'spark-2.4', 'spark-3.1']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "def get_workstation_spark_path(where_are_you : str) -> str:\n",
    "    if where_are_you == 'titan':\n",
    "        return '/home/data/ryanchao2012/lib'\n",
    "    elif where_are_you == 'thor':\n",
    "        return '/opt/spark/versions'\n",
    "    else:\n",
    "        raise ValueError(\"wrong work station name\")\n",
    "\n",
    "spark_path = get_workstation_spark_path('thor')\n",
    "\n",
    "print('You have pyspark version : ', os.listdir(spark_path))\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "# spark-2.3, spark-2.4\n",
    "# os.environ['SPARK_HOME'] = os.path.join(spark_path,'spark-2.3')\n",
    "\n",
    "# use spakr 3.1\n",
    "\n",
    "os.environ['SPARK_HOME'] = '/home/joetsai/download/spark-3.1.2-bin-hadoop3.2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:37:04.666791Z",
     "start_time": "2021-07-31T05:37:04.563620Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/joetsai/download/spark-3.1.2-bin-hadoop3.2'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ['SPARK_HOME']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:37:05.065927Z",
     "start_time": "2021-07-31T05:37:04.669052Z"
    }
   },
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession as Session\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark import SparkConf as Conf\n",
    "from pyspark.sql import functions as F, Window as W, types as T\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "C = F.col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:37:05.075853Z",
     "start_time": "2021-07-31T05:37:05.067382Z"
    }
   },
   "outputs": [],
   "source": [
    "conf = (Conf()\n",
    "    .set('spark.sql.sources.partitionOverwriteMode', 'dynamic')\n",
    "    .set('spark.driver.memory', '4g')\n",
    "    .set('spark.driver.maxResultSize', '1g')\n",
    "   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:37:08.474094Z",
     "start_time": "2021-07-31T05:37:05.077033Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/07/31 13:37:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "21/07/31 13:37:07 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "21/07/31 13:37:07 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "spark = (Session\n",
    "     .builder\n",
    "     .appName('pyspark-challenge')\n",
    "     .master('local[2]')\n",
    "     .config(conf=conf)\n",
    "     .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:37:08.493488Z",
     "start_time": "2021-07-31T05:37:08.476374Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://thor:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[2]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-challenge</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f4f697002b0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating DataFrame (16+)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:37:08.610711Z",
     "start_time": "2021-07-31T05:37:08.494897Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Builder', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__enter__', '__eq__', '__exit__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_activeSession', '_conf', '_convert_from_pandas', '_createFromLocal', '_createFromRDD', '_create_dataframe', '_create_from_pandas_with_arrow', '_create_shell_session', '_get_numpy_record_dtype', '_inferSchema', '_inferSchemaFromList', '_instantiatedSession', '_jsc', '_jsparkSession', '_jvm', '_jwrapped', '_repr_html_', '_sc', '_wrapped', 'builder', 'catalog', 'conf', 'createDataFrame', 'getActiveSession', 'newSession', 'range', 'read', 'readStream', 'sparkContext', 'sql', 'stop', 'streams', 'table', 'udf', 'version']\n",
      "\n",
      "your spark version : 3.1.2\n"
     ]
    }
   ],
   "source": [
    "# 0. know what spark session can do and its version\n",
    "print(dir(spark), f'your spark version : {spark.version}'\n",
    "      , sep='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:37:08.700610Z",
     "start_time": "2021-07-31T05:37:08.619999Z"
    }
   },
   "outputs": [],
   "source": [
    "# # 1. read data from csv\n",
    "# print(os.listdir('../data'))\n",
    "# df_from_csv_1 = spark.read.csv('../data/zipcodes.csv',\n",
    "#                                header=True,\n",
    "#                               inferSchema=True)\n",
    "# df_from_csv_1.printSchema()\n",
    "# df_from_csv_1.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:37:08.819964Z",
     "start_time": "2021-07-31T05:37:08.702957Z"
    }
   },
   "outputs": [],
   "source": [
    "# # 2 read data from json\n",
    "# print(os.listdir('../data'))\n",
    "# print(dir(spark.read))\n",
    "# # 沒有infer_schema\n",
    "# df_from_json = spark.read.json('../data/zipcodes.json')\n",
    "# df_from_json.printSchema()\n",
    "# df_from_json.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:37:12.062064Z",
     "start_time": "2021-07-31T05:37:08.822532Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+\n",
      "|language|user_counts|\n",
      "+--------+-----------+\n",
      "|    Java|      20000|\n",
      "|  Python|     100000|\n",
      "|   Scala|       3000|\n",
      "+--------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 3 create dataframe from rdd list\n",
    "columns = [\"language\",\"user_counts\"]\n",
    "data = [\n",
    "    (\"Java\",\"20000\"),\n",
    "    (\"Python\",\"100000\"),\n",
    "    (\"Scala\",\"3000\")\n",
    "       ]\n",
    "# 先分散到rdd\n",
    "# rdd = spark.sparkContext.parallelize(data)\n",
    "# print(dir(rdd), type(rdd), sep='\\n\\n')\n",
    "# print()\n",
    "# df_from_rdd = rdd.toDF(schema=columns)\n",
    "# df_from_rdd.show(n=5)\n",
    "\n",
    "# 直接create，讓spark dataframe進行分散\n",
    "df = spark.createDataFrame(data=data,schema=columns)\n",
    "df.show(n=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:37:12.384423Z",
     "start_time": "2021-07-31T05:37:12.064189Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Builder', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__enter__', '__eq__', '__exit__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_activeSession', '_conf', '_convert_from_pandas', '_createFromLocal', '_createFromRDD', '_create_dataframe', '_create_from_pandas_with_arrow', '_create_shell_session', '_get_numpy_record_dtype', '_inferSchema', '_inferSchemaFromList', '_instantiatedSession', '_jsc', '_jsparkSession', '_jvm', '_jwrapped', '_repr_html_', '_sc', '_wrapped', 'builder', 'catalog', 'conf', 'createDataFrame', 'getActiveSession', 'newSession', 'range', 'read', 'readStream', 'sparkContext', 'sql', 'stop', 'streams', 'table', 'udf', 'version']\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4 create 5 row fake data using spark range\n",
    "print(dir(spark))\n",
    "print(type(spark.range(start=0,end=10)))\n",
    "columns = ['row_number']\n",
    "single_column_df = spark.range(start=0,end=10)\n",
    "single_column_df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:37:12.399659Z",
     "start_time": "2021-07-31T05:37:12.386182Z"
    }
   },
   "outputs": [],
   "source": [
    "# 6 create empty dataframe\n",
    "\n",
    "columns = [\"language\",\"user_counts\"]\n",
    "\n",
    "# empty RDD + schema won't work\n",
    "# df_1 = spark.createDataFrame(data=spark.sparkContext.emptyRDD(),\n",
    "#                              schema=columns)\n",
    "# df_1.show(n=5)\n",
    "\n",
    "# empty list -> rdd -> df won't work\n",
    "# df2 = spark.sparkContext.parallelize([]).toDF(columns)\n",
    "\n",
    "# df3 = spark.createDataFrame([]) # won't work this version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:37:12.917773Z",
     "start_time": "2021-07-31T05:37:12.401977Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 2\n"
     ]
    }
   ],
   "source": [
    "# 7 get dataframe shape\n",
    "print(df.count(), len(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:37:13.218047Z",
     "start_time": "2021-07-31T05:37:12.920326Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+----------+\n",
      "|language|user_counts|new_column|\n",
      "+--------+-----------+----------+\n",
      "|    Java|      20000|       ABC|\n",
      "|  Python|     100000|       ABC|\n",
      "|   Scala|       3000|       ABC|\n",
      "+--------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 8 add const column to a existing dataframe\n",
    "\n",
    "columns = [\"language\",\"user_counts\"]\n",
    "data = [\n",
    "    (\"Java\",\"20000\"),\n",
    "    (\"Python\",\"100000\"),\n",
    "    (\"Scala\",\"3000\")\n",
    "       ]\n",
    "df = spark.createDataFrame(data=data,schema=columns)\n",
    "df = (\n",
    "    df.withColumn(\"new_column\",F.lit(\"ABC\")) \n",
    "    # F.lit means literal, retrurn a column\n",
    ")\n",
    "df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:37:13.913304Z",
     "start_time": "2021-07-31T05:37:13.219920Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/07/31 13:37:13 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-----+\n",
      "|language|user_counts|index|\n",
      "+--------+-----------+-----+\n",
      "|    Java|      20000|    1|\n",
      "|  Python|     100000|    2|\n",
      "|   Scala|       3000|    3|\n",
      "+--------+-----------+-----+\n",
      "\n",
      "+--------+-----------+-------+\n",
      "|language|user_counts|row_num|\n",
      "+--------+-----------+-------+\n",
      "|    Java|      20000|      1|\n",
      "|  Python|     100000|      2|\n",
      "|   Scala|       3000|      3|\n",
      "+--------+-----------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/07/31 13:37:13 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "# 9 add a row_id column from a exisiting dataframe\n",
    "# https://stackoverflow.com/questions/53082891/adding-a-unique-consecutive-row-number-to-dataframe-in-pyspark\n",
    "columns = [\"language\",\"user_counts\"]\n",
    "data = [\n",
    "    (\"Java\",\"20000\"),\n",
    "    (\"Python\",\"100000\"),\n",
    "    (\"Scala\",\"3000\")\n",
    "       ]\n",
    "df_1 = spark.createDataFrame(data=data,schema=columns)\n",
    "df_1 = (\n",
    "    df_1.withColumn(\"index\", \n",
    "                  F.row_number().over(\n",
    "                      W.orderBy(F.monotonically_increasing_id() - 1)\n",
    "                  )\n",
    "                 )\n",
    "    # F.monotonically_increasing_id does not give 1 ~ N\n",
    "    # So we use window function to work around\n",
    ")\n",
    "df_1.show(n=5)\n",
    "\n",
    "# Mre clear way to do that\n",
    "df_2 = spark.createDataFrame(data=data,schema=columns)\n",
    "w = W.orderBy(F.lit('A'))\n",
    "df_2 = (\n",
    "    df_2.withColumn(\"row_num\", F.row_number().over(w))\n",
    ")\n",
    "df_2.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:37:14.123692Z",
     "start_time": "2021-07-31T05:37:13.914948Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+----------+-------------------+\n",
      "|language|user_counts|new_column|      random_number|\n",
      "+--------+-----------+----------+-------------------+\n",
      "|    Java|      20000|       ABC|0.07427422718356547|\n",
      "|  Python|     100000|       ABC|0.48780032779856297|\n",
      "|   Scala|       3000|       ABC|  0.682865446047525|\n",
      "+--------+-----------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 10 add a random number to a exisit column\n",
    "df = (\n",
    "#     df.withColumn('random_number', F.when(F.rand() > 0.5, 1).otherwise(0))\n",
    "        df.withColumn('random_number', F.rand())\n",
    ")\n",
    "\n",
    "df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:37:14.408644Z",
     "start_time": "2021-07-31T05:37:14.126109Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>user_counts</th>\n",
       "      <th>new_column</th>\n",
       "      <th>random_number</th>\n",
       "      <th>binary_cut_05</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Java</td>\n",
       "      <td>20000</td>\n",
       "      <td>ABC</td>\n",
       "      <td>0.074274</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Python</td>\n",
       "      <td>100000</td>\n",
       "      <td>ABC</td>\n",
       "      <td>0.487800</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Scala</td>\n",
       "      <td>3000</td>\n",
       "      <td>ABC</td>\n",
       "      <td>0.682865</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  language user_counts new_column  random_number  binary_cut_05\n",
       "0     Java       20000        ABC       0.074274              0\n",
       "1   Python      100000        ABC       0.487800              1\n",
       "2    Scala        3000        ABC       0.682865              1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 11 add a binary 0, 1 based on condition to an exisit column\n",
    "df = (\n",
    "    df.withColumn('binary_cut_05',F.when(F.rand() > 0.5, 1).otherwise(0))\n",
    ")\n",
    "df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:37:14.617396Z",
     "start_time": "2021-07-31T05:37:14.410555Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-----------+\n",
      "|row_id|language|user_counts|\n",
      "+------+--------+-----------+\n",
      "|     0|    Java|      20000|\n",
      "|     1|  Python|     100000|\n",
      "|     2|   Scala|       3000|\n",
      "+------+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 12 create a dataframe contains row_index and fake data\n",
    "\n",
    "columns = [\"row_id\",\"language\",\"user_counts\"]\n",
    "data = [\n",
    "    (0, \"Java\",\"20000\"),\n",
    "    (1, \"Python\",\"100000\"),\n",
    "    (2, \"Scala\",\"3000\")\n",
    "       ]\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=columns)\n",
    "df.show(n=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:37:15.157623Z",
     "start_time": "2021-07-31T05:37:14.620212Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+-----+------+------+\n",
      "|firstname|middlename|lastname|   id|gender|salary|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|    James|          |   Smith|36636|     M|  3000|\n",
      "|  Michael|      Rose|        |40288|     M|  4000|\n",
      "|   Robert|          |Williams|42114|     M|  4000|\n",
      "|    Maria|      Anne|   Jones|39192|     F|  4000|\n",
      "|      Jen|      Mary|   Brown|     |     F|    -1|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "\n",
      "+--------------------+-----+------+------+\n",
      "|                name|   id|gender|salary|\n",
      "+--------------------+-----+------+------+\n",
      "|    {James, , Smith}|36636|     M|  3000|\n",
      "|   {Michael, Rose, }|40288|     M|  4000|\n",
      "|{Robert, , Williams}|42114|     M|  4000|\n",
      "|{Maria, Anne, Jones}|39192|     F|  4000|\n",
      "|  {Jen, Mary, Brown}|     |     F|    -1|\n",
      "+--------------------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 13 construct a complex data for spark dataframe\n",
    "# using StructType\n",
    "\n",
    "# Case 1\n",
    "data = [\n",
    "    (\"James\",\"\",\"Smith\",\"36636\",\"M\", 3000),\n",
    "    (\"Michael\",\"Rose\",\"\",\"40288\",\"M\", 4000),\n",
    "    (\"Robert\",\"\",\"Williams\",\"42114\",\"M\", 4000),\n",
    "    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\", 4000),\n",
    "    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\", -1)\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"firstname\",StringType(), True), # Nullable True\n",
    "    StructField(\"middlename\",StringType(), True),\n",
    "    StructField(\"lastname\",StringType(), True),\n",
    "    StructField(\"id\",StringType(), True),\n",
    "    StructField(\"gender\",StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True)\n",
    "    ])\n",
    "df = spark.createDataFrame(data=data,schema=schema)\n",
    "df.show(n=5)\n",
    "\n",
    "# Case 2\n",
    "\n",
    "struct_data = [\n",
    "    ((\"James\",\"\",\"Smith\"),\"36636\",\"M\", 3000),\n",
    "    ((\"Michael\",\"Rose\",\"\"),\"40288\",\"M\", 4000),\n",
    "    ((\"Robert\",\"\",\"Williams\"),\"42114\",\"M\", 4000),\n",
    "    ((\"Maria\",\"Anne\",\"Jones\"),\"39192\",\"F\", 4000),\n",
    "    ((\"Jen\",\"Mary\",\"Brown\"),\"\",\"F\", -1)\n",
    "]\n",
    "\n",
    "structure_schema = StructType([\n",
    "    StructField('name',\n",
    "        StructType([\n",
    "            StructField(\"firstname\",StringType(), True), # Nullable True\n",
    "            StructField(\"middlename\",StringType(), True),\n",
    "            StructField(\"lastname\",StringType(), True),\n",
    "    ])),\n",
    "    StructField(\"id\",StringType(), True),\n",
    "    StructField(\"gender\",StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True)\n",
    "    ])\n",
    "\n",
    "df = spark.createDataFrame(data=struct_data, schema=structure_schema)\n",
    "df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:37:15.609126Z",
     "start_time": "2021-07-31T05:37:15.160327Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- languagesAtSchool: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- currentState: string (nullable = true)\n",
      "\n",
      "+----------------+------------------+------------+\n",
      "|name            |languagesAtSchool |currentState|\n",
      "+----------------+------------------+------------+\n",
      "|James,,Smith    |[Java, Scala, C++]|CA          |\n",
      "|Michael,Rose,   |[Spark, Java, C++]|NJ          |\n",
      "|Robert,,Williams|[CSharp, VB]      |NV          |\n",
      "+----------------+------------------+------------+\n",
      "\n",
      "+-------------+-----+\n",
      "|         name|  col|\n",
      "+-------------+-----+\n",
      "| James,,Smith| Java|\n",
      "| James,,Smith|Scala|\n",
      "| James,,Smith|  C++|\n",
      "|Michael,Rose,|Spark|\n",
      "|Michael,Rose,| Java|\n",
      "+-------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 14 construct a complex data for spark dataframe\n",
    "# using ArratyType\n",
    "# https://github.com/spark-examples/pyspark-examples/blob/master/pyspark-array-string.py\n",
    "\n",
    "columns = [\"name\",\"languagesAtSchool\",\"currentState\"]\n",
    "data = [(\"James,,Smith\",[\"Java\",\"Scala\",\"C++\"],\"CA\"), \\\n",
    "    (\"Michael,Rose,\",[\"Spark\",\"Java\",\"C++\"],\"NJ\"), \\\n",
    "    (\"Robert,,Williams\",[\"CSharp\",\"VB\"],\"NV\")]\n",
    "\n",
    "df = spark.createDataFrame(data=data,schema=columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n",
    "\n",
    "df.select(df.name, F.explode(df.languagesAtSchool)).show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:37:15.805832Z",
     "start_time": "2021-07-31T05:37:15.610595Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- language: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- peoperties: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      "\n",
      "+----------+-------------------+--------------------+\n",
      "|      name|           language|          peoperties|\n",
      "+----------+-------------------+--------------------+\n",
      "|     James|      [Java, Scala]|{eye -> brown, ha...|\n",
      "|   Michael|[Spark, Java, null]|{eye -> null, hai...|\n",
      "|    Robert|         [CSharp, ]|{eye -> , hair ->...|\n",
      "|Washington|               null|                null|\n",
      "| Jefferson|             [1, 2]|                  {}|\n",
      "+----------+-------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 15 construct a complex data for spark dataframe\n",
    "# using MapType\n",
    "# https://github.com/spark-examples/pyspark-examples/blob/master/pyspark-explode-array-map.py\n",
    "data = [\n",
    "        ('James',['Java','Scala'],{'hair':'black','eye':'brown'}),\n",
    "        ('Michael',['Spark','Java',None],{'hair':'brown','eye':None}),\n",
    "        ('Robert',['CSharp',''],{'hair':'red','eye':''}),\n",
    "        ('Washington',None,None),\n",
    "        ('Jefferson',['1','2'],{})\n",
    "]\n",
    "\n",
    "columns = ['name','language','peoperties']\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=columns)\n",
    "\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:37:15.843335Z",
     "start_time": "2021-07-31T05:37:15.809304Z"
    }
   },
   "outputs": [],
   "source": [
    "# 16 create a nested array-type dataframe\n",
    "# https://github.com/spark-examples/pyspark-examples/blob/master/pyspark-explode-nested-array.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:37:15.934787Z",
     "start_time": "2021-07-31T05:37:15.845322Z"
    }
   },
   "outputs": [],
   "source": [
    "# 17 create a datetime column for spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:37:17.740631Z",
     "start_time": "2021-07-31T05:37:15.937241Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------+\n",
      "| languagesAtSchool|currentState|\n",
      "+------------------+------------+\n",
      "|[Java, Scala, C++]|          CA|\n",
      "|[Spark, Java, C++]|          NJ|\n",
      "|      [CSharp, VB]|          NV|\n",
      "+------------------+------------+\n",
      "\n",
      "root\n",
      " |-- languagesAtSchool: string (nullable = true)\n",
      " |-- currentState: string (nullable = true)\n",
      "\n",
      "java.lang.UnsupportedOperationException: CSV data source does not support array<string> data type.\n",
      "+------------------+------------+\n",
      "| languagesAtSchool|currentState|\n",
      "+------------------+------------+\n",
      "|[Java, Scala, C++]|          CA|\n",
      "|[Spark, Java, C++]|          NJ|\n",
      "|      [CSharp, VB]|          NV|\n",
      "+------------------+------------+\n",
      "\n",
      "+------------+--------------------+\n",
      "|currentState|   languagesAtSchool|\n",
      "+------------+--------------------+\n",
      "|          CA|['Java', 'Scala',...|\n",
      "|          NJ|['Spark', 'Java',...|\n",
      "|          NV|    ['CSharp', 'VB']|\n",
      "+------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 18 read a csv with array-of-string schema (fake issue)\n",
    "# you should read it from json\n",
    "\n",
    "\n",
    "columns = [\"languagesAtSchool\",\"currentState\"]\n",
    "data = [\n",
    "    ([\"Java\",\"Scala\",\"C++\"],\"CA\"), \n",
    "    ([\"Spark\",\"Java\",\"C++\"],\"NJ\"),\n",
    "    ([\"CSharp\",\"VB\"],\"NV\")\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data=data,schema=columns)\n",
    "\n",
    "df.show()\n",
    "\n",
    "# we write the data into csv\n",
    "SAVE = True\n",
    "if SAVE:\n",
    "    df.toPandas().to_csv('tmp.csv',index=False)\n",
    "\n",
    "# You will get sting\n",
    "(\n",
    "    spark.read.csv('tmp.csv',inferSchema=True, header=True).printSchema()\n",
    ")\n",
    "\n",
    "# Convert it by schema? - No, not supported...\n",
    "\n",
    "schema = (\n",
    "    T.StructType()\n",
    "    .add(\"languagesAtSchool\", T.ArrayType(T.StringType()), True)\n",
    "    .add(\"currentState\", T.StringType(), True)\n",
    ")\n",
    "try:\n",
    "    (\n",
    "        spark.read.csv('tmp.csv',schema=schema, header=True).show()\n",
    "    )\n",
    "except Exception as e:\n",
    "#     print(e)\n",
    "    # short answer\n",
    "    print('java.lang.UnsupportedOperationException: CSV data source does not support array<string> data type.')\n",
    "\n",
    "# How about use F.json to convert that?\n",
    "\n",
    "try:\n",
    "    (\n",
    "        spark.read.csv('tmp.csv',inferSchema=True, header=True)\n",
    "        .cache()\n",
    "        .withColumn('languagesAtSchool',F.from_json(C(\"languagesAtSchool\"),'array<string>'))\n",
    "    ).show()\n",
    "except Exception as e:\n",
    "#     print(e)\n",
    "    # ashort answer\n",
    "    print(\"java.lang.UnsupportedOperationException: CSV data source does not support array<string> data type.\")\n",
    "    \n",
    "    \n",
    "# convert it into json, make your life easiler\n",
    "\n",
    "SAVE_JSON = True\n",
    "if SAVE_JSON:\n",
    "    pd.read_csv('tmp.csv').to_json('tmp.json',orient='records',force_ascii=False)\n",
    "\n",
    "# Amazing!\n",
    "spark.read.json('tmp.json').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Column Operations (8+)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:37:17.960983Z",
     "start_time": "2021-07-31T05:37:17.742413Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+--------------+------------------+\n",
      "|language|user_counts|user_count_100|    user_count_log|\n",
      "+--------+-----------+--------------+------------------+\n",
      "|    Java|      20000|       2000000| 4.301029995663981|\n",
      "|  Python|     100000|      10000000|               5.0|\n",
      "|   Scala|       3000|        300000|3.4771212547196626|\n",
      "+--------+-----------+--------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1 create new column based on original column\n",
    "\n",
    "columns = [\"language\", \"user_counts\"]\n",
    "data = [\n",
    "    (\"Java\",20000),\n",
    "    (\"Python\",100000),\n",
    "    (\"Scala\",3000)\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=columns)\n",
    "df = (\n",
    "    df.withColumn(\"user_count_100\", C(\"user_counts\") * 100)\\\n",
    "    .withColumn(\"user_count_log\", F.log10(C(\"user_counts\")))\n",
    ")\n",
    "\n",
    "df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:37:18.168760Z",
     "start_time": "2021-07-31T05:37:17.970537Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+\n",
      "|  lang|const_col|\n",
      "+------+---------+\n",
      "|  Java|      ABC|\n",
      "|Python|      ABC|\n",
      "| Scala|      ABC|\n",
      "+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2 rename, drop, add constant column to existing dataframe\n",
    "# https://stackoverflow.com/questions/34077353/how-to-change-dataframe-column-names-in-pyspark\n",
    "columns = [\"language\", \"user_counts\"]\n",
    "data = [\n",
    "    (\"Java\",20000),\n",
    "    (\"Python\",100000),\n",
    "    (\"Scala\",3000)\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df = (\n",
    "    df.withColumn('const_col',F.lit('ABC'))\\\n",
    "    .withColumnRenamed(\"language\",\"lang\")\\\n",
    "    .drop(\"user_counts\")\n",
    ")\n",
    "df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:37:18.189515Z",
     "start_time": "2021-07-31T05:37:18.171227Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.column.Column'>\n",
      "\n",
      "['__add__', '__and__', '__bool__', '__class__', '__contains__', '__delattr__', '__dict__', '__dir__', '__div__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__invert__', '__iter__', '__le__', '__lt__', '__mod__', '__module__', '__mul__', '__ne__', '__neg__', '__new__', '__nonzero__', '__or__', '__pow__', '__radd__', '__rand__', '__rdiv__', '__reduce__', '__reduce_ex__', '__repr__', '__rmod__', '__rmul__', '__ror__', '__rpow__', '__rsub__', '__rtruediv__', '__setattr__', '__sizeof__', '__str__', '__sub__', '__subclasshook__', '__truediv__', '__weakref__', '_asc_doc', '_asc_nulls_first_doc', '_asc_nulls_last_doc', '_bitwiseAND_doc', '_bitwiseOR_doc', '_bitwiseXOR_doc', '_contains_doc', '_desc_doc', '_desc_nulls_first_doc', '_desc_nulls_last_doc', '_endswith_doc', '_eqNullSafe_doc', '_isNotNull_doc', '_isNull_doc', '_jc', '_like_doc', '_rlike_doc', '_startswith_doc', 'alias', 'asc', 'asc_nulls_first', 'asc_nulls_last', 'astype', 'between', 'bitwiseAND', 'bitwiseOR', 'bitwiseXOR', 'cast', 'contains', 'desc', 'desc_nulls_first', 'desc_nulls_last', 'dropFields', 'endswith', 'eqNullSafe', 'getField', 'getItem', 'isNotNull', 'isNull', 'isin', 'like', 'name', 'otherwise', 'over', 'rlike', 'startswith', 'substr', 'when', 'withField']\n"
     ]
    }
   ],
   "source": [
    "# 3 know what method and attribute can be called with column object\n",
    "\n",
    "\n",
    "print(type(C(\"language\")), dir(C(\"language\")), sep='\\n\\n')\n",
    "\n",
    "# alias - 可以換名字\n",
    "# asc, desc - 可以排序\n",
    "# astype,cast - 可以轉型\n",
    "# between - 可以傳入start_date以及end_date過濾\n",
    "# bitwiseAND, bitwiseOR, bitwiseXOR - 可以做布林運算\n",
    "# contains - 可以做字串搜尋\n",
    "# endwith, startwith, rlike, substring - 可以做字串比對\n",
    "# eqNullSafe, isNotNull, isNull - 可以檢查null值，Python須以None傳入\n",
    "# isin, like - 可以做值的比對(數值，字串值)\n",
    "# name - 可以取得欄位名稱\n",
    "# when, otherwise - 可以做條件判斷\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:37:18.503772Z",
     "start_time": "2021-07-31T05:37:18.191116Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------------+-----------+------------------+------------------+\n",
      "|  x|  y|mod_cross_col|mod_contant|               x/y|         safed_x/y|\n",
      "+---+---+-------------+-----------+------------------+------------------+\n",
      "|  6|  3|            0|          6|               2.0|               2.0|\n",
      "|  7|  3|            1|          7|2.3333333333333335|2.3333333333333335|\n",
      "| 13|  6|            1|         13|2.1666666666666665|2.1666666666666665|\n",
      "|  5|  0|         null|          5|              null|               5.0|\n",
      "+---+---+-------------+-----------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# +=*/\n",
    "\n",
    "df = spark.createDataFrame([(6,3), (7, 3), (13,6), (5, 0)], [\"x\", \"y\"])\n",
    "\n",
    "df = (\n",
    "    df\n",
    "    .withColumn(\"mod_cross_col\", C(\"x\") % C(\"y\"))\n",
    "    .withColumn(\"mod_contant\", C(\"x\") )\n",
    "    .withColumn('x/y',C(\"x\") / C(\"y\"))\n",
    "    .withColumn(\"safed_x/y\", F.when(C(\"y\") == 0, C(\"x\") / (C(\"y\") + F.lit(1)))\n",
    "                              .otherwise(C(\"x\") / C(\"y\"))\n",
    "               )\n",
    "     )\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:37:18.715568Z",
     "start_time": "2021-07-31T05:37:18.505582Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-------------+\n",
      "|language|user_counts|is_many_users|\n",
      "+--------+-----------+-------------+\n",
      "|    Java|      20000|            0|\n",
      "|  Python|     100000|            1|\n",
      "|   Scala|       3000|            0|\n",
      "+--------+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4 create a new dynamic column(if else condition based on old column)\n",
    "columns = [\"language\", \"user_counts\"]\n",
    "data = [\n",
    "    (\"Java\",20000),\n",
    "    (\"Python\",100000),\n",
    "    (\"Scala\",3000)\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "df = (\n",
    "    df.withColumn(\"is_many_users\",\n",
    "                  F.when(C('user_counts') > 50000, 1).otherwise(0)\n",
    "                 )\n",
    ")\n",
    "\n",
    "df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:37:19.173051Z",
     "start_time": "2021-07-31T05:37:18.717736Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|firstname|middlename|lastname|   id|gender|salary|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|    James|          |   Smith|36636|     M|  3000|\n",
      "|  Michael|      Rose|        |40288|     M|  4000|\n",
      "|   Robert|          |Williams|42114|     M|  4000|\n",
      "|    Maria|      Anne|   Jones|39192|     F|  4000|\n",
      "|      Jen|      Mary|   Brown|     |     F|    -1|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "\n",
      "After\n",
      "+---------+----------+--------+-----+------+------+----------------+\n",
      "|firstname|middlename|lastname|   id|gender|salary|       full_name|\n",
      "+---------+----------+--------+-----+------+------+----------------+\n",
      "|    James|      null|   Smith|36636|     M|  3000|             N/A|\n",
      "|  Michael|      Rose|    null|40288|     M|  4000|             N/A|\n",
      "|   Robert|      null|Williams|42114|     M|  4000|             N/A|\n",
      "|    Maria|      Anne|   Jones|39192|     F|  4000|Maria Anne Jones|\n",
      "|      Jen|      Mary|   Brown|     |     F|    -1|  Jen Mary Brown|\n",
      "+---------+----------+--------+-----+------+------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5 create a new dynamic column(if else condition based on old column) plus empty string replacement\n",
    "\n",
    "data = [\n",
    "    (\"James\",\"\",\"Smith\",\"36636\",\"M\", 3000),\n",
    "    (\"Michael\",\"Rose\",\"\",\"40288\",\"M\", 4000),\n",
    "    (\"Robert\",\"\",\"Williams\",\"42114\",\"M\", 4000),\n",
    "    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\", 4000),\n",
    "    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\", -1)\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"firstname\",StringType(), True), # Nullable True\n",
    "    StructField(\"middlename\",StringType(), True),\n",
    "    StructField(\"lastname\",StringType(), True),\n",
    "    StructField(\"id\",StringType(), True),\n",
    "    StructField(\"gender\",StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True)\n",
    "    ])\n",
    "df = spark.createDataFrame(data=data,schema=schema)\n",
    "\n",
    "# we cannot compare column values with empty string\n",
    "# so the work-around method is replace empty string to null\n",
    "# then using isNotNull()\n",
    "is_full_name_exist = (C(\"firstname\").isNotNull() & C(\"middlename\").isNotNull() & C(\"lastname\").isNotNull())\n",
    "\n",
    "\n",
    "def blank_as_null(x):\n",
    "    \"\"\"\n",
    "    helper function for converting row value from empty string to null\n",
    "    https://stackoverflow.com/questions/33287886/replace-empty-strings-with-none-null-values-in-dataframe\n",
    "    \"\"\"\n",
    "    return F.when(C(x) != \"\", C(x)).otherwise(None)\n",
    "\n",
    "print(\"Before\")\n",
    "df.show(n=5)\n",
    "df = (\n",
    "    df.withColumn(\"firstname\", blank_as_null(\"firstname\"))\\\n",
    "    .withColumn(\"middlename\", blank_as_null(\"middlename\"))\\\n",
    "    .withColumn(\"lastname\", blank_as_null(\"lastname\"))\\\n",
    "    .withColumn(\"full_name\",\n",
    "                  F.when(\n",
    "                      is_full_name_exist,\n",
    "                      F.concat(C(\"firstname\"), F.lit(' '),\n",
    "                               C(\"middlename\"), F.lit(' '),\n",
    "                               C(\"lastname\"))\n",
    "                  ).otherwise(F.lit('N/A'))\n",
    "                 )\n",
    ")\n",
    "print(\"After\")\n",
    "df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:37:19.392943Z",
     "start_time": "2021-07-31T05:37:19.176169Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Column<'language'>, Column<'user_counts'>] <class 'list'> <class 'pyspark.sql.column.Column'>\n",
      "+--------+-----------+\n",
      "|language|user_counts|\n",
      "+--------+-----------+\n",
      "|    Java|      20000|\n",
      "|  Python|     100000|\n",
      "|   Scala|       3000|\n",
      "+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6 select columns which also exist on another dataframe\n",
    "\n",
    "columns_1 = [\"language\", \"user_counts\"]\n",
    "data_1 = [\n",
    "    (\"Java\",20000),\n",
    "    (\"Python\",100000),\n",
    "    (\"Scala\",3000)\n",
    "]\n",
    "\n",
    "columns_2 = [\"language\", \"user_counts\",\"note\"]\n",
    "data_2 = [\n",
    "    (\"Java\",20000,\"nothing\"),\n",
    "    (\"Python\",100000,\"nothing\"),\n",
    "    (\"Scala\",3000,\"nothing\")\n",
    "]\n",
    "\n",
    "df_1 = spark.createDataFrame(data_1, columns_1)\n",
    "df_2 = spark.createDataFrame(data_2, columns_2)\n",
    "\n",
    "# union columns\n",
    "same_cols = [F.col(c) for c in df_2.columns if c in df_1.columns]\n",
    "print(same_cols, type(same_cols), type(same_cols[0]))\n",
    "\n",
    "df_same_col = df_1.select(*same_cols)\n",
    "df_same_col.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:37:19.601770Z",
     "start_time": "2021-07-31T05:37:19.394775Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-----+-----------------+-----------------+\n",
      "|language|user_counts|const|scientific_sign_1|scientific_sign_2|\n",
      "+--------+-----------+-----+-----------------+-----------------+\n",
      "|    Java|      20000|10000|           1.0E40|          1.0E-40|\n",
      "|  Python|     100000|10000|           1.0E40|          1.0E-40|\n",
      "|   Scala|       3000|10000|           1.0E40|          1.0E-40|\n",
      "+--------+-----------+-----+-----------------+-----------------+\n",
      "\n",
      "root\n",
      " |-- language: string (nullable = true)\n",
      " |-- user_counts: long (nullable = true)\n",
      " |-- const: integer (nullable = false)\n",
      " |-- scientific_sign_1: double (nullable = false)\n",
      " |-- scientific_sign_2: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 7 create a const numerical column\n",
    "\n",
    "\n",
    "columns = [\"language\", \"user_counts\"]\n",
    "data = [\n",
    "    (\"Java\",20000),\n",
    "    (\"Python\",100000),\n",
    "    (\"Scala\",3000)\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "df = (\n",
    "    df.withColumn(\"const\",F.lit(10000))\\\n",
    "    .withColumn(\"scientific_sign_1\", F.lit(1e40))\n",
    "    .withColumn(\"scientific_sign_2\", F.lit(1e-40))\n",
    ")\n",
    "\n",
    "df.show(n=5)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:37:19.800183Z",
     "start_time": "2021-07-31T05:37:19.604177Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+------------------+------------------+------------------+\n",
      "|language|user_counts|       uniform_0_1|     uniform_0_100|        normal_0_1|\n",
      "+--------+-----------+------------------+------------------+------------------+\n",
      "|    Java|      20000| 0.619189370225301|61.918937022530095| 2.384479054241165|\n",
      "|  Python|     100000|0.8017532427858894| 80.17532427858895|1.1027054481455365|\n",
      "|   Scala|       3000|0.6565552949992319| 65.65552949992319|0.5721044623675455|\n",
      "+--------+-----------+------------------+------------------+------------------+\n",
      "\n",
      "root\n",
      " |-- language: string (nullable = true)\n",
      " |-- user_counts: long (nullable = true)\n",
      " |-- uniform_0_1: double (nullable = false)\n",
      " |-- uniform_0_100: double (nullable = false)\n",
      " |-- normal_0_1: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 8 create a random variable column\n",
    "# https://spark.apache.org/docs/2.3.4/api/python/pyspark.sql.html#pyspark.sql.DataFrame\n",
    "# rand uniform [0, 1]\n",
    "# randn Normal distribution mu = 0, sigma = 1\n",
    "\n",
    "columns = [\"language\", \"user_counts\"]\n",
    "data = [\n",
    "    (\"Java\",20000),\n",
    "    (\"Python\",100000),\n",
    "    (\"Scala\",3000)\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "df = (\n",
    "    df.withColumn(\"uniform_0_1\",F.rand(seed=42))\\\n",
    "      .withColumn(\"uniform_0_100\",100 * F.rand(seed=42))\\\n",
    "      .withColumn(\"normal_0_1\", F.randn(seed=42))\n",
    ")\n",
    "\n",
    "df.show(n=5)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:37:20.069167Z",
     "start_time": "2021-07-31T05:37:19.802576Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+\n",
      "|language|user_counts|\n",
      "+--------+-----------+\n",
      "|    Java|      20000|\n",
      "|  Python|     100000|\n",
      "|   Scala|       3000|\n",
      "+--------+-----------+\n",
      "\n",
      "['Java', 'Python', 'Scala']\n"
     ]
    }
   ],
   "source": [
    "# 9 convert pyspark dataframe column to a python list\n",
    "# write a def func\n",
    "\n",
    "columns = [\"language\", \"user_counts\"]\n",
    "data = [\n",
    "    (\"Java\",20000),\n",
    "    (\"Python\",100000),\n",
    "    (\"Scala\",3000)\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()\n",
    "\n",
    "row_list = df.select(\"language\").collect()\n",
    "language_list = [row.language for row in row_list]\n",
    "print(language_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:37:20.442722Z",
     "start_time": "2021-07-31T05:37:20.071515Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+\n",
      "|language|user_counts|\n",
      "+--------+-----------+\n",
      "|    Java|      20000|\n",
      "|  Python|     100000|\n",
      "|   Scala|       3000|\n",
      "+--------+-----------+\n",
      "\n",
      "+--------+-----------+\n",
      "|language|user_counts|\n",
      "+--------+-----------+\n",
      "|       0|          0|\n",
      "+--------+-----------+\n",
      "\n",
      "root\n",
      " |-- language: long (nullable = false)\n",
      " |-- user_counts: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 10 check NA for all columns\n",
    "def calculate_null(sdf):\n",
    "    return sdf.select([F.count(F.when(F.isnan(c), c)).alias(c)\n",
    "                for c in sdf.columns])\n",
    "    \n",
    "\n",
    "\n",
    "columns = [\"language\", \"user_counts\"]\n",
    "data = [\n",
    "    (\"Java\",20000),\n",
    "    (\"Python\",100000),\n",
    "    (\"Scala\",3000)\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()\n",
    "\n",
    "calculate_null(df).show()\n",
    "\n",
    "calculate_null(df).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:37:20.655949Z",
     "start_time": "2021-07-31T05:37:20.445355Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-------------+\n",
      "|language|user_counts|       fillme|\n",
      "+--------+-----------+-------------+\n",
      "|    Java|         50|FillingString|\n",
      "|  Python|         50|FillingString|\n",
      "|   Scala|         50|FillingString|\n",
      "+--------+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 11 fillna in columns\n",
    "\n",
    "columns = [\"language\", \"user_counts\"]\n",
    "data = [\n",
    "    (\"Java\",20000),\n",
    "    (\"Python\",100000),\n",
    "    (\"Scala\",3000)\n",
    "]\n",
    "\n",
    "# df.withColumn('new_column', lit(None).cast(StringType()))\n",
    "df = (\n",
    "    spark.createDataFrame(data,columns)\n",
    "    .withColumn(\"user_counts\", F.lit(None).cast(T.IntegerType()))\n",
    "    .withColumn(\"fillme\", F.lit(None).cast(T.StringType()))\n",
    ").na.fill({\n",
    "        'user_counts' : 50,\n",
    "        'fillme' : \"FillingString\" \n",
    "    })\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## String Operation (3+)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:37:21.008680Z",
     "start_time": "2021-07-31T05:37:20.657695Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----------+\n",
      "| id|       poi|     branch|\n",
      "+---+----------+-----------+\n",
      "|  1|肉多多火鍋|           |\n",
      "|  2|    玖佰鍋|           |\n",
      "|  3|    石二鍋| 高雄遠百店|\n",
      "|  4|      火鍋|every_where|\n",
      "+---+----------+-----------+\n",
      "\n",
      "-RECORD 0-------------------------\n",
      " id           | 1                 \n",
      " poi          | 肉多多火鍋        \n",
      " branch       |                   \n",
      " full_name    | 肉多多火鍋        \n",
      " full_name_v2 | 肉多多火鍋-       \n",
      "-RECORD 1-------------------------\n",
      " id           | 2                 \n",
      " poi          | 玖佰鍋            \n",
      " branch       |                   \n",
      " full_name    | 玖佰鍋            \n",
      " full_name_v2 | 玖佰鍋-           \n",
      "-RECORD 2-------------------------\n",
      " id           | 3                 \n",
      " poi          | 石二鍋            \n",
      " branch       | 高雄遠百店        \n",
      " full_name    | 石二鍋高雄遠百店  \n",
      " full_name_v2 | 石二鍋-高雄遠百店 \n",
      "-RECORD 3-------------------------\n",
      " id           | 4                 \n",
      " poi          | 火鍋              \n",
      " branch       | every_where       \n",
      " full_name    | 火鍋every_where   \n",
      " full_name_v2 | 火鍋-every_where  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1 string concat two column values to a new column from an existing dataframe\n",
    "\n",
    "\n",
    "data = [\n",
    "    (\"1\",\"肉多多火鍋\",''),\n",
    "    (\"2\",\"玖佰鍋\",''),\n",
    "    (\"3\",\"石二鍋\",'高雄遠百店'),\n",
    "    (\"4\",\"火鍋\",'every_where')\n",
    "]\n",
    "\n",
    "col = ['id','poi','branch']\n",
    "\n",
    "df = spark.createDataFrame(data, col)\n",
    "\n",
    "df.show()\n",
    "\n",
    "(\n",
    "    df\n",
    "    .withColumn(\"full_name\",F.concat(\"poi\",\"branch\"))\n",
    "    .withColumn(\"full_name_v2\",F.concat_ws('-',\"poi\",\"branch\"))\n",
    "    \n",
    ").show(vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:37:21.314188Z",
     "start_time": "2021-07-31T05:37:21.010544Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+\n",
      "| id|       poi|\n",
      "+---+----------+\n",
      "|  1|肉多多火鍋|\n",
      "|  2|    玖佰鍋|\n",
      "|  3|    石二鍋|\n",
      "|  4|      火鍋|\n",
      "+---+----------+\n",
      "\n",
      "+---+----------+-------+\n",
      "| id|       poi|sub_str|\n",
      "+---+----------+-------+\n",
      "|  1|肉多多火鍋| 多火鍋|\n",
      "|  2|    玖佰鍋| 玖佰鍋|\n",
      "|  3|    石二鍋| 石二鍋|\n",
      "|  4|      火鍋|   火鍋|\n",
      "+---+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2 cut of left 3 char of specific column to a new column from an existing dataframe\n",
    "\n",
    "\n",
    "## substring\n",
    "\n",
    "data = [\n",
    "    (\"1\",\"肉多多火鍋\"),\n",
    "    (\"2\",\"玖佰鍋\"),\n",
    "    (\"3\",\"石二鍋\"),\n",
    "    (\"4\",\"火鍋\")\n",
    "]\n",
    "\n",
    "col = ['id','poi']\n",
    "\n",
    "df = spark.createDataFrame(data, col)\n",
    "\n",
    "df.show()\n",
    "\n",
    "(\n",
    "    df\n",
    "    .withColumn('sub_str',F.substring('poi',pos=-3,len=3))\n",
    "    \n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:37:21.894843Z",
     "start_time": "2021-07-31T05:37:21.316742Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+\n",
      "| id|       poi|\n",
      "+---+----------+\n",
      "|  1|肉多多火鍋|\n",
      "|  2|    玖佰鍋|\n",
      "|  3|    石二鍋|\n",
      "|  4|      火鍋|\n",
      "+---+----------+\n",
      "\n",
      "-RECORD 0--------------------------\n",
      " id         | 1                    \n",
      " poi        | 肉多多火鍋           \n",
      " term_array | [肉, 多, 多, 火, 鍋] \n",
      " position   | 0                    \n",
      " term       | 肉                   \n",
      "-RECORD 1--------------------------\n",
      " id         | 1                    \n",
      " poi        | 肉多多火鍋           \n",
      " term_array | [肉, 多, 多, 火, 鍋] \n",
      " position   | 1                    \n",
      " term       | 多                   \n",
      "-RECORD 2--------------------------\n",
      " id         | 1                    \n",
      " poi        | 肉多多火鍋           \n",
      " term_array | [肉, 多, 多, 火, 鍋] \n",
      " position   | 2                    \n",
      " term       | 多                   \n",
      "-RECORD 3--------------------------\n",
      " id         | 1                    \n",
      " poi        | 肉多多火鍋           \n",
      " term_array | [肉, 多, 多, 火, 鍋] \n",
      " position   | 3                    \n",
      " term       | 火                   \n",
      "-RECORD 4--------------------------\n",
      " id         | 1                    \n",
      " poi        | 肉多多火鍋           \n",
      " term_array | [肉, 多, 多, 火, 鍋] \n",
      " position   | 4                    \n",
      " term       | 鍋                   \n",
      "-RECORD 5--------------------------\n",
      " id         | 2                    \n",
      " poi        | 玖佰鍋               \n",
      " term_array | [玖, 佰, 鍋]         \n",
      " position   | 0                    \n",
      " term       | 玖                   \n",
      "-RECORD 6--------------------------\n",
      " id         | 2                    \n",
      " poi        | 玖佰鍋               \n",
      " term_array | [玖, 佰, 鍋]         \n",
      " position   | 1                    \n",
      " term       | 佰                   \n",
      "-RECORD 7--------------------------\n",
      " id         | 2                    \n",
      " poi        | 玖佰鍋               \n",
      " term_array | [玖, 佰, 鍋]         \n",
      " position   | 2                    \n",
      " term       | 鍋                   \n",
      "-RECORD 8--------------------------\n",
      " id         | 3                    \n",
      " poi        | 石二鍋               \n",
      " term_array | [石, 二, 鍋]         \n",
      " position   | 0                    \n",
      " term       | 石                   \n",
      "-RECORD 9--------------------------\n",
      " id         | 3                    \n",
      " poi        | 石二鍋               \n",
      " term_array | [石, 二, 鍋]         \n",
      " position   | 1                    \n",
      " term       | 二                   \n",
      "-RECORD 10-------------------------\n",
      " id         | 3                    \n",
      " poi        | 石二鍋               \n",
      " term_array | [石, 二, 鍋]         \n",
      " position   | 2                    \n",
      " term       | 鍋                   \n",
      "-RECORD 11-------------------------\n",
      " id         | 4                    \n",
      " poi        | 火鍋                 \n",
      " term_array | [火, 鍋]             \n",
      " position   | 0                    \n",
      " term       | 火                   \n",
      "-RECORD 12-------------------------\n",
      " id         | 4                    \n",
      " poi        | 火鍋                 \n",
      " term_array | [火, 鍋]             \n",
      " position   | 1                    \n",
      " term       | 鍋                   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3 in string\n",
    "# to get the position in string\n",
    "\n",
    "from pixlake.nlp.utils import unigram\n",
    "\n",
    "data = [\n",
    "    (\"1\",\"肉多多火鍋\"),\n",
    "    (\"2\",\"玖佰鍋\"),\n",
    "    (\"3\",\"石二鍋\"),\n",
    "    (\"4\",\"火鍋\")\n",
    "]\n",
    "\n",
    "col = ['id','poi']\n",
    "\n",
    "df = spark.createDataFrame(data, col)\n",
    "\n",
    "df.show()\n",
    "\n",
    "(\n",
    "    df\n",
    "    .withColumn(\"term_array\",\n",
    "     F.udf(unigram,\n",
    "           returnType=\"array<string>\")(\n",
    "         \"poi\", F.lit(True)\n",
    "     ))\n",
    "    .select(\n",
    "        '*',\n",
    "        F.posexplode_outer('term_array').alias('position','term')\n",
    "    )\n",
    "#     .withColumn(\"term\", F.explode_outer(C(\"term_array\")))\n",
    "#     # instr provide fix substring only\n",
    "#     .withColumn(\"position_fixed_substring\",\n",
    "#                 F.instr(C(\"poi\"), '肉'))\n",
    "#     # if you wanna apply two column\n",
    "#     .withColumn(\"position_two_column\",\n",
    "#                 F.expr(\"locate(term, poi) - 1\")\n",
    "#                )\n",
    ").show(n=20, vertical=True, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:37:22.083163Z",
     "start_time": "2021-07-31T05:37:21.897020Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----+-------------+\n",
      "| id|       poi| tmp|edit_distance|\n",
      "+---+----------+----+-------------+\n",
      "|  1|肉多多火鍋|火鍋|            3|\n",
      "|  2|    玖佰鍋| abc|            3|\n",
      "|  3|    石二鍋|    |            3|\n",
      "|  4|      火鍋|火鍋|            0|\n",
      "+---+----------+----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4 compute edit distance - like between two column\n",
    "# pyspark support levenshtein distance\n",
    "# https://zh.wikipedia.org/wiki/%E8%90%8A%E6%96%87%E6%96%AF%E5%9D%A6%E8%B7%9D%E9%9B%A2\n",
    "\n",
    "data = [\n",
    "    (\"1\",\"肉多多火鍋\",\"火鍋\"),\n",
    "    (\"2\",\"玖佰鍋\",\"abc\"),\n",
    "    (\"3\",\"石二鍋\",\"\"),\n",
    "    (\"4\",\"火鍋\",\"火鍋\")\n",
    "]\n",
    "\n",
    "col = ['id','poi',\"tmp\"]\n",
    "\n",
    "df = spark.createDataFrame(data, col)\n",
    "\n",
    "(\n",
    "    df\n",
    "    .withColumn(\"edit_distance\", F.levenshtein(\"poi\",\"tmp\"))\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:37:22.274553Z",
     "start_time": "2021-07-31T05:37:22.085828Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----+----------+\n",
      "| id|       poi| tmp|  left_pad|\n",
      "+---+----------+----+----------+\n",
      "|  1|肉多多火鍋|火鍋|肉多多火鍋|\n",
      "|  2|    玖佰鍋| abc|  ##玖佰鍋|\n",
      "|  3|    石二鍋|    |  ##石二鍋|\n",
      "|  4|      火鍋|火鍋|   ###火鍋|\n",
      "+---+----------+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5 padding your string column\n",
    "\n",
    "data = [\n",
    "    (\"1\",\"肉多多火鍋\",\"火鍋\"),\n",
    "    (\"2\",\"玖佰鍋\",\"abc\"),\n",
    "    (\"3\",\"石二鍋\",\"\"),\n",
    "    (\"4\",\"火鍋\",\"火鍋\")\n",
    "]\n",
    "\n",
    "col = ['id','poi',\"tmp\"]\n",
    "\n",
    "df = spark.createDataFrame(data, col)\n",
    "\n",
    "(\n",
    "    df\n",
    "    .withColumn(\"left_pad\", F.lpad(\"poi\",5,\"####\"))\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:37:22.467222Z",
     "start_time": "2021-07-31T05:37:22.277337Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------------+--------------------------+\n",
      "|id |city_town                 |norm_city_town            |\n",
      "+---+--------------------------+--------------------------+\n",
      "|1  |臺北市大安區永安路145巷8弄|台北市大安區永安路145巷8弄|\n",
      "|2  |臺北市信義區              |台北市信義區              |\n",
      "|3  |台東縣長濱鄉              |台東縣長濱鄉              |\n",
      "|4  |臺東縣長濱鄉              |台東縣長濱鄉              |\n",
      "|5  |台南市中西區              |台南市中西區              |\n",
      "|6  |臺南市中西區              |台南市中西區              |\n",
      "|7  |臺中市沙鹿區              |台中市沙鹿區              |\n",
      "|8  |臺西鄉啤酒                |臺西鄉啤酒                |\n",
      "+---+--------------------------+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6 replace your column by regax pattern\n",
    "\n",
    "data = [\n",
    "    (\"1\",\"臺北市大安區永安路145巷8弄\"),\n",
    "    (\"2\",\"臺北市信義區\"),\n",
    "    (\"3\",\"台東縣長濱鄉\"),\n",
    "    (\"4\",\"臺東縣長濱鄉\"),\n",
    "    (\"5\",\"台南市中西區\"),\n",
    "    (\"6\",\"臺南市中西區\"),\n",
    "    (\"7\",\"臺中市沙鹿區\"),\n",
    "    (\"8\",\"臺西鄉啤酒\")\n",
    "    \n",
    "]\n",
    "\n",
    "col = ['id','city_town']\n",
    "\n",
    "df = spark.createDataFrame(data, col)\n",
    "\n",
    "city_normalize_from = r'(臺)(北|中|南|東)(市|縣)'\n",
    "city_normalize_to = r'台$2$3'\n",
    "\n",
    "(\n",
    "    df\n",
    "    .withColumn('norm_city_town',\n",
    "                F.regexp_replace(\n",
    "                    C(\"city_town\"),\n",
    "                    city_normalize_from,\n",
    "                    city_normalize_to\n",
    "                                ))\n",
    ").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:37:22.556027Z",
     "start_time": "2021-07-31T05:37:22.469731Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after processing ... \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[doc_id: string, html: string, caontain_imglink: boolean, contain_hyperlink: boolean, body_id: int, body: string]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7 split your string but keep the separator\n",
    "# in this case, we wanna split html into each <a ... <img..></a>\n",
    "# but not for plain_text case\n",
    "# do it in python way\n",
    "# https://stackoverflow.com/questions/2136556/in-python-how-do-i-split-a-string-and-keep-the-separators\n",
    "\n",
    "\n",
    "imgtag = '<img.*[\\'\"]>'\n",
    "hyperlink_tag = '(<a[^>]*>\\s*((?:.|\\n)*?)</a>)'\n",
    "imgtag_splitter = '<img'\n",
    "    \n",
    "data = [\n",
    "    (\"1\",'<a href=\"http://www.flickr.com/photos/126291387@N06/51051131972\" target=\"_blank\"><img alt=\"DSC01611\" src=\"//live.staticflickr.com/65535/51051131972_100386c21c_b.jpg\" title=\"\"></a><a href=\"http://www.flickr.com/photos/126291387@N06/51051047806\" target=\"_blank\"><img alt=\"DSC01610\" src=\"//live.staticflickr.com/65535/51051047806_7b2587931f_b.jpg\" title=\"\"></a>'),\n",
    "    (\"2\",'<a href=\"http://www.flickr.com/photos/126291387@N06/51051131972\" target=\"_blank\"><img alt=\"DSC01611\" src=\"//live.staticflickr.com/65535/51051131972_100386c21c_b.jpg\" title=\"\"></a>'),    \n",
    "    (\"3\",'<a href=\"http://www.flickr.com/photos/126291387@N06/51051131972\" target=\"_blank\">IamPlaintext</a><a href=\"http://www.flickr.com/photos/126291387@N06/51051047806\" target=\"_blank\">IamSecondPlainText</a>'),\n",
    "    (\"3\",'<a href=\"http://www.flickr.com/photos/126291387@N06/51051131972\" target=\"_blank\">IamPlaintext</a><a href=\"http://www.flickr.com/photos/126291387@N06/51051047806\" target=\"_blank\"><img alt=\"DSC01611\" src=\"//live.staticflickr.com/65535/51051131972_100386c21c_b.jpg\" title=\"\"></a>'),\n",
    "]\n",
    "\n",
    "col = ['doc_id','html']\n",
    "\n",
    "df = spark.createDataFrame(data, col)\n",
    "# df.show(truncate=False, vertical=True)\n",
    "\n",
    "\n",
    "\n",
    "# pdf\n",
    "print('after processing ... \\n\\n')\n",
    "\n",
    "contain_hyperlink_and_img = (\n",
    "      (C('caontain_imglink'))\n",
    "    & (C('contain_hyperlink'))\n",
    ")\n",
    "\n",
    "(\n",
    "    df\n",
    "    .withColumn('caontain_imglink',\n",
    "                    C(\"html\").rlike(fr'{imgtag}'))\n",
    "    .withColumn('contain_hyperlink',\n",
    "                    C(\"html\").rlike(fr'{hyperlink_tag}'))\n",
    "    .withColumn('html',F.when(contain_hyperlink_and_img,\n",
    "                              F.regexp_replace(C(\"html\"), fr'{hyperlink_tag}', r'$1\\\\n'))\n",
    "                        .otherwise(C(\"html\")))\n",
    "    .select(\n",
    "        '*',\n",
    "        F.posexplode_outer(F.split('html',r'\\\\n')).alias('body_id','body')\n",
    "    )\n",
    "    .withColumn('body', F.trim('body'))\n",
    "    .where(F.length(C(\"body\")) > 0)\n",
    "# ).show(truncate=False, vertical=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complex type operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:37:22.957750Z",
     "start_time": "2021-07-31T05:37:22.557663Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: struct (nullable = true)\n",
      " |    |-- firstname: string (nullable = true)\n",
      " |    |-- middlename: string (nullable = true)\n",
      " |    |-- lastname: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n",
      "+--------------------+-----+------+------+\n",
      "|name                |id   |gender|salary|\n",
      "+--------------------+-----+------+------+\n",
      "|{James, , Smith}    |36636|M     |3100  |\n",
      "|{Michael, Rose, }   |40288|M     |4300  |\n",
      "|{Robert, , Williams}|42114|M     |1400  |\n",
      "|{Maria, Anne, Jones}|39192|F     |5500  |\n",
      "|{Jen, Mary, Brown}  |     |F     |-1    |\n",
      "+--------------------+-----+------+------+\n",
      "\n",
      "root\n",
      " |-- name: struct (nullable = true)\n",
      " |    |-- firstname: string (nullable = true)\n",
      " |    |-- middlename: string (nullable = true)\n",
      " |    |-- lastname: string (nullable = true)\n",
      " |-- info: struct (nullable = false)\n",
      " |    |-- identifer: string (nullable = true)\n",
      " |    |-- sex: string (nullable = true)\n",
      " |    |-- money: integer (nullable = true)\n",
      " |    |-- salary_grade: string (nullable = false)\n",
      "\n",
      "+--------------------+------------------------+\n",
      "|name                |info                    |\n",
      "+--------------------+------------------------+\n",
      "|{James, , Smith}    |{36636, M, 3100, Medium}|\n",
      "|{Michael, Rose, }   |{40288, M, 4300, High}  |\n",
      "|{Robert, , Williams}|{42114, M, 1400, Low}   |\n",
      "|{Maria, Anne, Jones}|{39192, F, 5500, High}  |\n",
      "|{Jen, Mary, Brown}  |{, F, -1, Low}          |\n",
      "+--------------------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1 apply struct on column\n",
    "\n",
    "structureData = [\n",
    "    ((\"James\",\"\",\"Smith\"),\"36636\",\"M\",3100),\n",
    "    ((\"Michael\",\"Rose\",\"\"),\"40288\",\"M\",4300),\n",
    "    ((\"Robert\",\"\",\"Williams\"),\"42114\",\"M\",1400),\n",
    "    ((\"Maria\",\"Anne\",\"Jones\"),\"39192\",\"F\",5500),\n",
    "    ((\"Jen\",\"Mary\",\"Brown\"),\"\",\"F\",-1)\n",
    "  ]\n",
    "structureSchema = StructType([\n",
    "        StructField('name', StructType([\n",
    "             StructField('firstname', StringType(), True),\n",
    "             StructField('middlename', StringType(), True),\n",
    "             StructField('lastname', StringType(), True)\n",
    "             ])),\n",
    "         StructField('id', StringType(), True),\n",
    "         StructField('gender', StringType(), True),\n",
    "         StructField('salary', IntegerType(), True)\n",
    "         ])\n",
    "\n",
    "df = spark.createDataFrame(data=structureData,schema=structureSchema)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n",
    "\n",
    "df_with_other_info = (\n",
    "    df\n",
    "    .withColumn('info',\n",
    "        F.struct(C(\"id\").alias('identifer'),\n",
    "                 C(\"gender\").alias('sex'),\n",
    "                 C(\"salary\").alias('money'),\n",
    "                 F.when(C('salary').cast('integer') < 2000, \"Low\")\n",
    "                  .when(C('salary').cast('integer') < 4000, \"Medium\")\n",
    "                  .otherwise(\"High\").alias('salary_grade')\n",
    "               ))\n",
    "    .drop('id','gender','salary')\n",
    ")\n",
    "\n",
    "df_with_other_info.printSchema()\n",
    "df_with_other_info.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:37:23.128712Z",
     "start_time": "2021-07-31T05:37:22.959931Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- identifer: string (nullable = true)\n",
      " |-- salary_grade: string (nullable = false)\n",
      "\n",
      "+---------+------------+\n",
      "|identifer|salary_grade|\n",
      "+---------+------------+\n",
      "|    36636|      Medium|\n",
      "|    40288|        High|\n",
      "|    42114|         Low|\n",
      "|    39192|        High|\n",
      "|         |         Low|\n",
      "+---------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2 select the keys in stuct\n",
    "\n",
    "flattern = (\n",
    "    df_with_other_info\n",
    "    .select('info.identifer','info.salary_grade')\n",
    ")\n",
    "\n",
    "flattern.printSchema()\n",
    "flattern.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:37:23.298257Z",
     "start_time": "2021-07-31T05:37:23.130588Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+-----+------------+\n",
      "|identifer|sex|money|salary_grade|\n",
      "+---------+---+-----+------------+\n",
      "|    36636|  M| 3100|      Medium|\n",
      "|    40288|  M| 4300|        High|\n",
      "|    42114|  M| 1400|         Low|\n",
      "|    39192|  F| 5500|        High|\n",
      "|         |  F|   -1|         Low|\n",
      "+---------+---+-----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select all of the keys once in struct\n",
    "\n",
    "(\n",
    "    df_with_other_info\n",
    "    .select('info.*')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:37:23.803074Z",
     "start_time": "2021-07-31T05:37:23.300806Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: long (nullable = true)\n",
      " |-- value: string (nullable = true)\n",
      "\n",
      "+---+--------+\n",
      "|key|   value|\n",
      "+---+--------+\n",
      "|  1|{\"a\": 1}|\n",
      "+---+--------+\n",
      "\n",
      "root\n",
      " |-- json: struct (nullable = true)\n",
      " |    |-- a: integer (nullable = true)\n",
      "\n",
      "+----+\n",
      "|json|\n",
      "+----+\n",
      "|{1} |\n",
      "+----+\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- json: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- json: string (nullable = true)\n",
      " |-- struct_type: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- text_1: string (nullable = true)\n",
      " |    |    |-- text_2: string (nullable = true)\n",
      "\n",
      "+--------------------------------------------+------------------+\n",
      "|json                                        |struct_type       |\n",
      "+--------------------------------------------+------------------+\n",
      "|[{\"text_1\" : \"link_1\", \"text_2\" : \"link_2\"}]|[{link_1, link_2}]|\n",
      "|[{\"text_1\" : \"link_1\"}]                     |[{link_1, null}]  |\n",
      "+--------------------------------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4 convert json column into struct\n",
    "# UDF ?\n",
    "# Case I\n",
    "data = [(1, '''{\"a\": 1}''')]\n",
    "schema = StructType([StructField(\"a\", IntegerType())])\n",
    "df = spark.createDataFrame(data, (\"key\", \"value\"))\n",
    "df.printSchema()\n",
    "df.show()\n",
    "o_sdf = df.select(F.from_json(df.value, schema).alias(\"json\"))\n",
    "o_sdf.printSchema()\n",
    "o_sdf.show(truncate=False)\n",
    "\n",
    "# Case II\n",
    "\n",
    "data = [\n",
    "    (0, '''[{\"text_1\" : \"link_1\", \"text_2\" : \"link_2\"}]'''),\n",
    "    (1, '''[{\"text_1\" : \"link_1\"}]''')\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data,schema=['id','json'])\n",
    "df.printSchema()\n",
    "\n",
    "# struct_schema = 'struct<\\n error: string,\\n message: string \\n>'\n",
    "schema = 'array < struct < text_1: string, text_2 : string > >'\n",
    "\n",
    "# schema = T.ArrayType(StructType([\n",
    "#     StructField(\"text\", StringType()),\n",
    "#     StructField(\"link\", StringType()),\n",
    "# ]))\n",
    "\n",
    "\n",
    "o_sdf = (\n",
    "    df\n",
    "    .select(\n",
    "        'json',\n",
    "        F.from_json(df.json, schema = schema).alias('struct_type')\n",
    "    )\n",
    ")\n",
    "\n",
    "o_sdf.printSchema()\n",
    "\n",
    "o_sdf.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:37:23.979293Z",
     "start_time": "2021-07-31T05:37:23.805676Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- json: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- json: string (nullable = true)\n",
      " |-- array_type: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n",
      "-RECORD 0------------------------------------------------------------------------------------------------------------------------------\n",
      " json       | [\"http://www.flickr.com/photos/126291387@N06/51051131972\"]                                                               \n",
      " array_type | [http://www.flickr.com/photos/126291387@N06/51051131972]                                                                 \n",
      "-RECORD 1------------------------------------------------------------------------------------------------------------------------------\n",
      " json       | [\"http://www.flickr.com/photos/126291387@N06/51051131972\", \"//live.staticflickr.com/65535/51051131972_100386c21c_b.jpg\"] \n",
      " array_type | [http://www.flickr.com/photos/126291387@N06/51051131972, //live.staticflickr.com/65535/51051131972_100386c21c_b.jpg]     \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4 convert json column into array\n",
    "# UDF ?\n",
    "\n",
    "# Case I\n",
    "\n",
    "data = [\n",
    "    (0, '''[\"http://www.flickr.com/photos/126291387@N06/51051131972\"]'''),\n",
    "    (1, '''[\"http://www.flickr.com/photos/126291387@N06/51051131972\", \"//live.staticflickr.com/65535/51051131972_100386c21c_b.jpg\"]''')\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data,schema=['id','json'])\n",
    "df.printSchema()\n",
    "\n",
    "# struct_schema = 'struct<\\n error: string,\\n message: string \\n>'\n",
    "schema = 'array<string>'\n",
    "\n",
    "# schema = T.ArrayType(StructType([\n",
    "#     StructField(\"text\", StringType()),\n",
    "#     StructField(\"link\", StringType()),\n",
    "# ]))\n",
    "\n",
    "\n",
    "o_sdf = (\n",
    "    df\n",
    "    .select(\n",
    "        'json',\n",
    "        F.from_json(df.json, schema = schema).alias('array_type')\n",
    "    )\n",
    ")\n",
    "\n",
    "o_sdf.printSchema()\n",
    "\n",
    "o_sdf.show(vertical=True,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical filtering (6+)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:37:24.164266Z",
     "start_time": "2021-07-31T05:37:23.982001Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+-----+------+------+\n",
      "|firstname|middlename|lastname|   id|gender|salary|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|    James|          |   Smith|36636|     M|  3000|\n",
      "|  Michael|      Rose|        |40288|     M|  4000|\n",
      "|   Robert|          |Williams|42114|     M|  4000|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. filter on equal condition\n",
    "\n",
    "data = [\n",
    "    (\"James\",\"\",\"Smith\",\"36636\",\"M\", 3000),\n",
    "    (\"Michael\",\"Rose\",\"\",\"40288\",\"M\", 4000),\n",
    "    (\"Robert\",\"\",\"Williams\",\"42114\",\"M\", 4000),\n",
    "    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\", 4000),\n",
    "    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\", -1)\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"firstname\",StringType(), True), # Nullable True\n",
    "    StructField(\"middlename\",StringType(), True),\n",
    "    StructField(\"lastname\",StringType(), True),\n",
    "    StructField(\"id\",StringType(), True),\n",
    "    StructField(\"gender\",StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True)\n",
    "    ])\n",
    "df = spark.createDataFrame(data=data,schema=schema)\n",
    "\n",
    "df = (\n",
    "    df.filter(C(\"gender\") == \"M\")\n",
    ")\n",
    "\n",
    "df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:37:24.353203Z",
     "start_time": "2021-07-31T05:37:24.166660Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+-----+------+------+\n",
      "|firstname|middlename|lastname|   id|gender|salary|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|  Michael|      Rose|        |40288|     M|  4000|\n",
      "|   Robert|          |Williams|42114|     M|  4000|\n",
      "|    Maria|      Anne|   Jones|39192|     F|  6000|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2 filter on >, <, >=, <=\n",
    "\n",
    "data = [\n",
    "    (\"James\",\"\",\"Smith\",\"36636\",\"M\", 3000),\n",
    "    (\"Michael\",\"Rose\",\"\",\"40288\",\"M\", 4000),\n",
    "    (\"Robert\",\"\",\"Williams\",\"42114\",\"M\", 4000),\n",
    "    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\", 6000),\n",
    "    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\", -1)\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"firstname\",StringType(), True), # Nullable True\n",
    "    StructField(\"middlename\",StringType(), True),\n",
    "    StructField(\"lastname\",StringType(), True),\n",
    "    StructField(\"id\",StringType(), True),\n",
    "    StructField(\"gender\",StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True)\n",
    "    ])\n",
    "df = spark.createDataFrame(data=data,schema=schema)\n",
    "\n",
    "df = (\n",
    "    df.filter(C(\"salary\") >= 4000)\n",
    ")\n",
    "\n",
    "df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:37:24.563745Z",
     "start_time": "2021-07-31T05:37:24.356312Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+-----+------+------+\n",
      "|firstname|middlename|lastname|   id|gender|salary|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|  Michael|      Rose|        |40288|     M|  4000|\n",
      "|   Robert|          |Williams|42114|     M|  4000|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3 multiple conditions require parenthese around each condition\n",
    "\n",
    "\n",
    "data = [\n",
    "    (\"James\",\"\",\"Smith\",\"36636\",\"M\", 3000),\n",
    "    (\"Michael\",\"Rose\",\"\",\"40288\",\"M\", 4000),\n",
    "    (\"Robert\",\"\",\"Williams\",\"42114\",\"M\", 4000),\n",
    "    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\", 6000),\n",
    "    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\", -1)\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"firstname\",StringType(), True), # Nullable True\n",
    "    StructField(\"middlename\",StringType(), True),\n",
    "    StructField(\"lastname\",StringType(), True),\n",
    "    StructField(\"id\",StringType(), True),\n",
    "    StructField(\"gender\",StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True)\n",
    "    ])\n",
    "df = spark.createDataFrame(data=data,schema=schema)\n",
    "\n",
    "# This is lazy computing\n",
    "rich_man_who_worth_married = (\n",
    "    (C(\"gender\") == \"M\") &\n",
    "    (C(\"salary\") >= 4000)\n",
    ")\n",
    "\n",
    "df = (\n",
    "    df.filter(rich_man_who_worth_married)\n",
    ")\n",
    "\n",
    "df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:37:24.751867Z",
     "start_time": "2021-07-31T05:37:24.566528Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+-----+------+------+\n",
      "|firstname|middlename|lastname|   id|gender|salary|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|    Maria|      Anne|   Jones|39192|     F|  6000|\n",
      "|      Jen|      Mary|   Brown|     |     F|    -1|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4 Compare against a list of allowed values\n",
    "\n",
    "\n",
    "data = [\n",
    "    (\"James\",\"\",\"Smith\",\"36636\",\"M\", 3000),\n",
    "    (\"Michael\",\"Rose\",\"\",\"40288\",\"M\", 4000),\n",
    "    (\"Robert\",\"\",\"Williams\",\"42114\",\"M\", 4000),\n",
    "    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\", 6000),\n",
    "    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\", -1)\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"firstname\",StringType(), True), # Nullable True\n",
    "    StructField(\"middlename\",StringType(), True),\n",
    "    StructField(\"lastname\",StringType(), True),\n",
    "    StructField(\"id\",StringType(), True),\n",
    "    StructField(\"gender\",StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True)\n",
    "    ])\n",
    "df = spark.createDataFrame(data=data,schema=schema)\n",
    "\n",
    "df = (\n",
    "    df.filter(C(\"gender\").isin([\"F\"]))\n",
    ")\n",
    "\n",
    "df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:37:25.059145Z",
     "start_time": "2021-07-31T05:37:24.754570Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+-----+------+------+\n",
      "|firstname|middlename|lastname|   id|gender|salary|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|    Maria|      Anne|   Jones|39192|     F|  6000|\n",
      "|   Robert|          |Williams|42114|     M|  4000|\n",
      "|  Michael|      Rose|        |40288|     M|  4000|\n",
      "|    James|          |   Smith|36636|     M|  3000|\n",
      "|      Jen|      Mary|   Brown|     |     F|    -1|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|firstname|middlename|lastname|   id|gender|salary|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|      Jen|      Mary|   Brown|     |     F|    -1|\n",
      "|    James|          |   Smith|36636|     M|  3000|\n",
      "|   Robert|          |Williams|42114|     M|  4000|\n",
      "|  Michael|      Rose|        |40288|     M|  4000|\n",
      "|    Maria|      Anne|   Jones|39192|     F|  6000|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5 Sort result\n",
    "\n",
    "data = [\n",
    "    (\"James\",\"\",\"Smith\",\"36636\",\"M\", 3000),\n",
    "    (\"Michael\",\"Rose\",\"\",\"40288\",\"M\", 4000),\n",
    "    (\"Robert\",\"\",\"Williams\",\"42114\",\"M\", 4000),\n",
    "    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\", 6000),\n",
    "    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\", -1)\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"firstname\",StringType(), True), # Nullable True\n",
    "    StructField(\"middlename\",StringType(), True),\n",
    "    StructField(\"lastname\",StringType(), True),\n",
    "    StructField(\"id\",StringType(), True),\n",
    "    StructField(\"gender\",StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True)\n",
    "    ])\n",
    "df = spark.createDataFrame(data=data,schema=schema)\n",
    "\n",
    "\n",
    "df = df.orderBy(C(\"salary\").desc())\n",
    "\n",
    "df.show(n=5)\n",
    "\n",
    "df = df.orderBy(C(\"salary\").asc())\n",
    "\n",
    "df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:37:26.755770Z",
     "start_time": "2021-07-31T05:37:25.061310Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before\n",
      "+----------+-------+--------------------+\n",
      "|article_id|   pred|             img_url|\n",
      "+----------+-------+--------------------+\n",
      "|     67789|0.93422|https://pic.pimg....|\n",
      "|     67789|0.94231|https://pic.pimg....|\n",
      "|     14431|0.97611|https://pic.pimg....|\n",
      "|     14431|0.99834|https://pic.pimg....|\n",
      "|     14431|0.99834|https://pic.pimg....|\n",
      "+----------+-------+--------------------+\n",
      "\n",
      "Sol 1 FAILED SOMETIMES WHEN PARTITION != 1\n",
      "+----------+-------+--------------------+\n",
      "|article_id|   pred|             img_url|\n",
      "+----------+-------+--------------------+\n",
      "|     67789|0.93422|https://pic.pimg....|\n",
      "|     67789|0.94231|https://pic.pimg....|\n",
      "|     14431|0.97611|https://pic.pimg....|\n",
      "|     14431|0.99834|https://pic.pimg....|\n",
      "+----------+-------+--------------------+\n",
      "\n",
      "Sol 2 WORKED WITH ANY PARTITION\n",
      "+----------+-------+--------------------+\n",
      "|article_id|   pred|             img_url|\n",
      "+----------+-------+--------------------+\n",
      "|     67789|0.93422|https://pic.pimg....|\n",
      "|     67789|0.94231|https://pic.pimg....|\n",
      "|     14431|0.97611|https://pic.pimg....|\n",
      "|     14431|0.99834|https://pic.pimg....|\n",
      "+----------+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6 select distinct rows based on certain column but keep first row\n",
    "# In this case, model prediction to filter the same images\n",
    "\n",
    "############## DROP DUPLICATED doesn't work in this case\n",
    "# https://stackoverflow.com/questions/38687212/spark-dataframe-drop-duplicates-and-keep-first\n",
    "data = [\n",
    "    (14431,0.99834,'https://pic.pimg.tw/happy78/1528543947-685380499_n.jpg'),\n",
    "    (14431,0.99834,'https://pic.pimg.tw/happy78/1528543947-362759723_n.jpg'),\n",
    "    (14431,0.97611,'https://pic.pimg.tw/happy78/1528543962-2265924582_n.jpg'),\n",
    "    (67789,0.93422,'https://pic.pimg.tw/happy78/1528543962-4007835890_n.jpg'),\n",
    "    (67789,0.94231,'https://pic.pimg.tw/happy78/1528543962-45890_n.jpg')\n",
    "]\n",
    "\n",
    "columns = ['article_id','pred','img_url']\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=columns)\n",
    "print('before')\n",
    "\n",
    "df.orderBy('pred').show(n=5)\n",
    "\n",
    "print('Sol 1 FAILED SOMETIMES WHEN PARTITION != 1')\n",
    "df_1 = (\n",
    "    df.drop_duplicates(subset=[\"pred\"])\n",
    ")\n",
    "\n",
    "df_1.orderBy('pred').show(n=5)\n",
    "\n",
    "\n",
    "############## Using Window Function and sort, rank, worked!\n",
    "# You can check the 5 th question of Aggregation, The solution is the same\n",
    "\n",
    "print('Sol 2 WORKED WITH ANY PARTITION')\n",
    "df_2 = (\n",
    "    df.withColumn(\"rank_by_pred\",\n",
    "                  F.row_number().over(\n",
    "                  W.partitionBy(\"pred\")\\\n",
    "                      .orderBy(F.desc(\"pred\"))\n",
    "                  )\n",
    "                 )\\\n",
    "    .filter(F.col(\"rank_by_pred\") == 1)\\\n",
    "    .drop('rank_by_pred')\n",
    ")\n",
    "df_2.orderBy('pred').show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## String filtering (2+)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:37:26.926229Z",
     "start_time": "2021-07-31T05:37:26.757592Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " article_id | 14431                                                                                                                                                                                   \n",
      " pred       | 0.99834                                                                                                                                                                                 \n",
      " img_url    | https://pic.pimg.tw/happy78/1528543947-685380499_n.jpg                                                                                                                                  \n",
      "-RECORD 1---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " article_id | 14431                                                                                                                                                                                   \n",
      " pred       | 0.99834                                                                                                                                                                                 \n",
      " img_url    |                                                                                                                                                                                         \n",
      "-RECORD 2---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " article_id | 0                                                                                                                                                                                       \n",
      " pred       | 0.78                                                                                                                                                                                    \n",
      " img_url    | https://pic.pimg.tw/happy78/1528543947-2503982521_n.jpg                                                                                                                                 \n",
      "-RECORD 3---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " article_id | 1                                                                                                                                                                                       \n",
      " pred       | 0.45                                                                                                                                                                                    \n",
      " img_url    | https://s.yimg.com/bt/api/res/1.2/OzjFf8Ov8yUBECUAWKMcDw--/YXBwaWQ9eW5ld3NfbGVnbztxPTg1O3c9NjAw/http://media.zenfs.com/zh_hant_tw/News/stormmedia/20160601-093346_U720_M161728_0c0b.jpg \n",
      "-RECORD 4---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " article_id | 14431                                                                                                                                                                                   \n",
      " pred       | 0.99834                                                                                                                                                                                 \n",
      " img_url    | https://pic.pimg.tw/happy78/1528543947-362759723_n.jpg                                                                                                                                  \n",
      "-RECORD 5---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " article_id | 14431                                                                                                                                                                                   \n",
      " pred       | 0.97611                                                                                                                                                                                 \n",
      " img_url    | https://pic.pimg.tw/happy78/1528543962-2265924582_n.jpg                                                                                                                                 \n",
      "-RECORD 6---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " article_id | 67789                                                                                                                                                                                   \n",
      " pred       | 0.93422                                                                                                                                                                                 \n",
      " img_url    | https://pic.pimg.tw/happy78/1528543962-4007835890_n.jpg                                                                                                                                 \n",
      "-RECORD 7---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " article_id | 67789                                                                                                                                                                                   \n",
      " pred       | 0.94231                                                                                                                                                                                 \n",
      " img_url    | https://pic.pimg.tw/happy78/1528543962-45890_n.jpg                                                                                                                                      \n",
      "-RECORD 8---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " article_id | 67789                                                                                                                                                                                   \n",
      " pred       | 0.94111                                                                                                                                                                                 \n",
      " img_url    | https://pic.pimg.tw/happy78/1528543962-45890_n.png                                                                                                                                      \n",
      "-RECORD 9---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " article_id | 67789                                                                                                                                                                                   \n",
      " pred       | 0.94111                                                                                                                                                                                 \n",
      " img_url    |                                                                                                                                                                                         \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. regax filtering\n",
    "# pyspark regexp_extract api cannot get all the groups\n",
    "data = [\n",
    "    (14431,0.99834,'https://pic.pimg.tw/happy78/1528543947-685380499_n.jpg mfkvmdfasx'),\n",
    "    (14431,0.99834,'https://pic.pimg.tw/happy78/1528543947-685380499_n.jpgsd,clsd,cluah'),\n",
    "    (0,0.78,'src=\"https://pic.pimg.tw/happy78/1528543947-2503982521_n.jpg\" title=\"IMG_5216.jpg\"'),\n",
    "    (1,0.45,'https://s.yimg.com/bt/api/res/1.2/OzjFf8Ov8yUBECUAWKMcDw--/YXBwaWQ9eW5ld3NfbGVnbztxPTg1O3c9NjAw/http://media.zenfs.com/zh_hant_tw/News/stormmedia/20160601-093346_U720_M161728_0c0b.jpg'),\n",
    "    (14431,0.99834,'https://pic.pimg.tw/happy78/1528543947-362759723_n.jpg'),\n",
    "    (14431,0.97611,'https://pic.pimg.tw/happy78/1528543962-2265924582_n.jpg'),\n",
    "    (67789,0.93422,'https://pic.pimg.tw/happy78/1528543962-4007835890_n.jpg'),\n",
    "    (67789,0.94231,'https://pic.pimg.tw/happy78/1528543962-45890_n.jpg'),\n",
    "    (67789,0.94111,'https://pic.pimg.tw/happy78/1528543962-45890_n.png'),\n",
    "    (67789,0.94111,'png')\n",
    "]\n",
    "\n",
    "columns = ['article_id','pred','img_url']\n",
    "\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "\n",
    "df_filted = (\n",
    "    df.withColumn(\"img_url\", F.regexp_extract(C('img_url'),\n",
    "                                              r'(http\\S+jpg\\b)|(http\\S+png\\b)',\n",
    "                                              0))\n",
    ")\n",
    "\n",
    "df_filted.show(n=20, vertical=True, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:37:26.984207Z",
     "start_time": "2021-07-31T05:37:26.929198Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1.2 regax replace (filtering)\n",
    "# example <a href=\"url\">link text</a> --> \"\"\n",
    "# <a href=\"https://www.w3schools.com/\">Visit W3Schools.com!</a>\n",
    "\n",
    "############### Case I #############################\n",
    "data = [\n",
    "    (14431,'<p>今天要帶大家去吃吃台中有名的這一鍋皇室秘藏鍋物，據說會有著「帝王般」的享受（是有沒有這麼誇張），話說這一鍋北中南都有分店，但是不知道是從哪裡起家的？我們這次吃的是「台中朝富店」喔</p>\\n\\n<p>地點：<span style=\"-webkit-text-stroke-width:0px; background-color:rgba(0, 0, 0, 0.03); color:rgba(0, 0, 0, 0.87); display:inline !important; float:none; font-family:roboto,noto sans tc,arial,sans-serif; font-size:13px; font-style:normal; font-variant-caps:normal; font-variant-ligatures:normal; font-weight:400; letter-spacing:normal; orphans:2; text-align:left; text-decoration-color:initial; text-decoration-style:initial; text-indent:0px; text-transform:none; white-space:normal; widows:2; word-spacing:0px\">台中市西屯區朝富路36號</span></p>\\n\\n<p>電話：<span style=\"-webkit-text-stroke-width:0px; background-color:rgba(0, 0, 0, 0.03); color:rgba(0, 0, 0, 0.87); display:inline !important; float:none; font-family:roboto,noto sans tc,arial,sans-serif; font-size:13px; font-style:normal; font-variant-caps:normal; font-variant-ligatures:normal; font-weight:400; letter-spacing:normal; orphans:2; text-align:left; text-decoration-color:initial; text-decoration-style:initial; text-indent:0px; text-transform:none; white-space:normal; widows:2; word-spacing:0px\">04 3609 0088</span></p>\\n\\n<p>營業時間：11:30~03:00</p>\\n\\n<p><iframe class=\"\" frameborder=\"0\" height=\"350\" marginheight=\"0\" marginwidth=\"0\" scrolling=\"no\" src=\"//maps.google.com/?ie=UTF8&amp;f=q&amp;source=s_q&amp;q=loc:24.166896,+120.63769000000002+(%E5%8F%B0%E7%81%A3%E5%8F%B0%E4%B8%AD%E5%B8%82%E8%A5%BF%E5%B1%AF%E5%8D%80%E6%9C%9D%E5%AF%8C%E8%B7%AF%E9%80%99%E4%B8%80%E9%8D%8B%E5%8F%B0%E4%B8%AD%E6%9C%9D%E5%AF%8C%E6%AE%BF)&amp;sll=24.160044,120.62857&amp;ll=24.166896,120.63769&amp;marker=24.166896,120.63769&amp;mrt=loc&amp;z=15&amp;t=m&amp;output=embed\" width=\"425\"></iframe><br>\\n<small><a href=\"//maps.google.com/?ie=UTF8&amp;f=q&amp;source=s_q&amp;q=loc:24.166896,+120.63769000000002+(%E5%8F%B0%E7%81%A3%E5%8F%B0%E4%B8%AD%E5%B8%82%E8%A5%BF%E5%B1%AF%E5%8D%80%E6%9C%9D%E5%AF%8C%E8%B7%AF%E9%80%99%E4%B8%80%E9%8D%8B%E5%8F%B0%E4%B8%AD%E6%9C%9D%E5%AF%8C%E6%AE%BF)&amp;sll=24.160044,120.62857&amp;ll=24.166896,120.63769&amp;marker=24.166896,120.63769&amp;mrt=loc&amp;z=15&amp;t=m&amp;source=embed\" style=\"color:#0000FF;text-align:left\">檢視較大的地圖</a></small></p>\\n\\n<p><strong><span style=\"font-size:14px\"><a href=\"https://www.instagram.com/p/BhtcYEmhi9U/\" target=\"_blank\"><img alt=\"\" height=\"20\" src=\"//s.pixfs.net/f.pixnet.net/images/emotions/032.gif\" title=\"\" width=\"20\">想看IG介紹請點我<img alt=\"\" height=\"20\" src=\"//s.pixfs.net/f.pixnet.net/images/emotions/019.gif\" title=\"\" width=\"20\"></a></span></strong></p>\\n\\n<p><small>朝富店的外觀非常壯闊氣派，不禁讓人點點頭「嗯，果然是台中的餐廳，建築物都要超。大」</small></p>\\n\\n<p><img alt=\"IMG_5208.jpg\" src=\"https://pic.pimg.tw/happy78/1528543947-685380499_n.jpg\" title=\"IMG_5208.jpg\"></p>\\n\\n<p>進去唄～</p>\\n\\n<p><img alt=\"IMG_5217.jpg\" src=\"https://pic.pimg.tw/happy78/1528543947-362759723_n.jpg\" title=\"IMG_5217.jpg\"></p>\\n\\n<p>一樓有個大水池，看起來很富麗堂皇</p>\\n\\n<p><img alt=\"IMG_5232.jpg\" src=\"https://pic.pimg.tw/happy78/1528543962-2265924582_n.jpg\" title=\"IMG_5232.jpg\"></p>\\n\\n<p><a href=\"https://vbtrax.com/track/clicks/1006/ce2bc2bd9f093698a587f264d175924223674fdf2aabebf50760b4?subid_1=&amp;subid_2=&amp;subid_3=&amp;subid_4=&amp;subid_5=\"><img alt=\"\" border=\"0\" src=\"https://vbtrax.com/track/imp/img/368/ce2bc2bd9f093698a587f264d175924223674fdf2aabebf50760b4?subid_1=&amp;subid_2=&amp;subid_3=&amp;subid_4=&amp;subid_5=\" title=\"\"></a></p>\\n\\n<p>最讓我讚嘆的就是這個牡丹（？中國風的樓梯了！！好美喔～～</p>\\n\\n<p><img alt=\"IMG_5230.jpg\" src=\"https://pic.pimg.tw/happy78/1528543962-4007835890_n.jpg\" title=\"IMG_5230.jpg\"></p>\\n\\n<p>整棟建築物看起來用餐環境很棒</p>\\n\\n<p><img alt=\"IMG_5229.jpg\" src=\"https://pic.pimg.tw/happy78/1528543962-1785144547_n.jpg\" title=\"IMG_5229.jpg\"></p>\\n\\n<p>我看到一二樓都有這些宮廷風服飾，我沒問服務生是不是可以穿（因為我也不想穿XD而且整家店我也沒看到有人在穿XD）不過如果你想穿的話我想應該是沒人會阻止</p>\\n\\n<p><img alt=\"IMG_5228.jpg\" src=\"https://pic.pimg.tw/happy78/1528543960-3197815885_n.jpg\" title=\"IMG_5228.jpg\"></p>\\n\\n<p>醬料也有滿多選擇的</p>\\n\\n<p><img alt=\"IMG_5216.jpg\" src=\"https://pic.pimg.tw/happy78/1528543947-2503982521_n.jpg\" title=\"IMG_5216.jpg\"></p>\\n\\n<p><a href=\"https://vbtrax.com/track/clicks/1096/ce2bc2b79e073698a587f264d175924223674fdf2aabebf50769b4?subid_1=&amp;subid_2=&amp;subid_3=&amp;subid_4=&amp;subid_5=\"><img alt=\"\" border=\"0\" src=\"https://vbtrax.com/track/imp/img/976/ce2bc2b79e073698a587f264d175924223674fdf2aabebf50769b4?subid_1=&amp;subid_2=&amp;subid_3=&amp;subid_4=&amp;subid_5=\" title=\"\"></a></p>\\n\\n<p>我們點了麻辣鍋跟老火湯的鴛鴦鍋</p>\\n\\n<p><img alt=\"IMG_5221.jpg\" src=\"https://pic.pimg.tw/happy78/1528543950-1832601825_n.jpg\" title=\"IMG_5221.jpg\"></p>\\n\\n<p>因為我們沒有很餓，所以沒有點很多，以下就一一介紹：（對了，這次忘記拍菜單，想看菜單的人可以去它<a href=\"http://www.toponepot.com/product_detail.php\" target=\"_blank\"><strong>官網</strong></a>看喔）</p>\\n\\n<p>這是梅花豬268$</p>\\n\\n<p><img alt=\"IMG_5219.jpg\" src=\"https://pic.pimg.tw/happy78/1528543949-4214686345_n.jpg\" title=\"IMG_5219.jpg\"></p>\\n\\n<p>和牛梅花598$</p>\\n\\n<p><img alt=\"IMG_5222.jpg\" src=\"https://pic.pimg.tw/happy78/1528543955-3093638684_n.jpg\" title=\"IMG_5222.jpg\"></p>\\n\\n<p>因為跟長輩吃比較不好意思慢慢拍，所以照片沒有很多，照片裡還有玉米筍跟盛味綜合丸238$，另外右上角是店家招待的香蔥油條，我覺得滿特別的，乾吃的話會很有嚼勁，泡湯吃也很美味</p>\\n\\n<p>&nbsp;</p>\\n\\n<p><img alt=\"IMG_5224.jpg\" src=\"https://pic.pimg.tw/happy78/1528543955-714147528_n.jpg\" title=\"IMG_5224.jpg\"></p>\\n\\n<p><a href=\"https://vbtrax.com/track/clicks/3456/ce2bc2bd9c0123daefcda67f8835ce1328684bc177fbb9b20a63b60061?subid_1=&amp;subid_2=&amp;subid_3=&amp;subid_4=&amp;subid_5=\"><img alt=\"\" border=\"0\" src=\"https://vbtrax.com/track/imp/img/35035/ce2bc2bd9c0123daefcda67f8835ce1328684bc177fbb9b20a63b60061?subid_1=&amp;subid_2=&amp;subid_3=&amp;subid_4=&amp;subid_5=\" title=\"\"></a></p>\\n\\n<p>要營養均衡，所以當然要吃青菜啦，這是五彩鮮蔬盤'),\n",
    "    (55444,'<pstyle=\"text-align:left;\"><imgsrc=\"https://pic.pimg.tw/peko721/1555405696-3356669416.jpg\"alt=\"麥食達韓式料理.麥食達菜單.麥食達.台北車站美食.北車美食.石鍋拌飯.\"/></p><pstyle=\"text-align:left;\"><strong><spanstyle=\"color:#0000ff;font-size:14pt;\">麥食達</span></strong></p><pstyle=\"text-align:left;\">地址：台北市中正區懷寧街86號</p><pstyle=\"text-align:left;\">這裡不是裝潢華麗的韓式料理餐廳，而是一家有點像家庭食堂的小店，紅蔘茶、冬粉、味噌湯都是無限量供應。</p><pstyle=\"text-align:left;\">相關文章：<atitle=\"【台北車站美食】麥食達韓式料理｜228公園旁的平價石鍋拌飯、鍋物｜附麥食達菜單\"href=\"https://peko721.pixnet.net/blog/post/46669347\"target=\"_blank\">麥食達韓式料理｜228公園旁的平價石鍋拌飯、鍋物</a></p><divstyle=\"text-align:center;\">')\n",
    "    \n",
    "]\n",
    "\n",
    "columns = ['article_id','raw_content']\n",
    "\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "\n",
    "df_filted = (\n",
    "    df.withColumn(\"raw_content\", F.regexp_replace(C('raw_content'),\n",
    "#                                               r'(<a href.*</a>)|(<atitle.*href.*</a>)',\n",
    "                                              r'(<.*href.*</a>)',\n",
    "                                              ''))\n",
    ")\n",
    "\n",
    "# df_filted.show(n=20, vertical=True, truncate=False)\n",
    "############### Case II ###########################\n",
    "\n",
    "\n",
    "with open(\"../data/webpage/imglink_1.txt\", \"r\") as f:\n",
    "    text = [f.read()]\n",
    "\n",
    "schema = StructType(\n",
    "    [\n",
    "    StructField(\"article_id\", IntegerType(), True),\n",
    "    StructField(\"raw_content\", StringType(), True)\n",
    "    ]\n",
    ")\n",
    "# data format\n",
    "# [(r1_col1, r1_col2, ...),\n",
    "#  (r2_col1, r2_col2, ...),\n",
    "# ]\n",
    "df = spark.createDataFrame([(0, text)], schema=schema)\n",
    "# df.show(truncate=False)\n",
    "\n",
    "df_filted = (\n",
    "    df.withColumn(\"raw_content\", F.regexp_replace(C('raw_content'),\n",
    "#                                               r'(<a href.*</a>)|(<atitle.*href.*</a>)',\n",
    "                                              r'(<.*href.*</a>)',\n",
    "                                              ''))\n",
    ")\n",
    "\n",
    "# df_filted.show(n=20, vertical=True, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:37:27.319621Z",
     "start_time": "2021-07-31T05:37:26.987174Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- article_id: long (nullable = true)\n",
      " |-- sentence_id: integer (nullable = true)\n",
      " |-- is_imglink: boolean (nullable = true)\n",
      " |-- imgtag: string (nullable = true)\n",
      " |-- text_links: array (nullable = false)\n",
      " |    |-- element: struct (containsNull = false)\n",
      " |    |    |-- text: string (nullable = true)\n",
      " |    |    |-- link: string (nullable = true)\n",
      " |-- plain_text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# extract imglink in sentence level\n",
    "# extract the text links in sub-sentence level\n",
    "# https://stackoverflow.com/questions/36336228/python-regex-extract-text-within-html-tags\n",
    "\n",
    "# algo\n",
    "# 1. using negtive ahead to avoid extracting links, but replace all the html tag\n",
    "# 2. split content by \\p{C}\n",
    "# 3. explode_outer the <a, make sure 1 row contains 1 text_links only\n",
    "# 4. extract link and text\n",
    "# 5. collect link and text partition by article_id, sentence_id\n",
    "# 6. remove all the tags in sentence --> plain_text\n",
    "\n",
    "\n",
    "# try html in W3C School\n",
    "# https://www.w3schools.com/html/tryit.asp?filename=tryhtml_links_w3schools\n",
    "\n",
    "################# sample content ################\n",
    "# http://chaos810126.pixnet.net/blog/post/353526220\n",
    "# signle poi with further reading section\n",
    "with open(\"../data/webpage/textwithlink_1.txt\", \"r\") as f:\n",
    "    textwithlink_1 = f.read()\n",
    "\n",
    "# http://jeremyckt2.pixnet.net/blog/post/230600845\n",
    "# article for dummies (we can crab poi profile)\n",
    "\n",
    "with open(\"../data/webpage/textwithlink_2.txt\", \"r\") as f:\n",
    "    textwithlink_2 = f.read()\n",
    "    \n",
    "import html\n",
    "\n",
    "data = [\n",
    "#     (1,'<p>今天要帶大家去吃吃台中有名的這一鍋皇室秘藏鍋物，據說會有著「帝王般」的享受（是有沒有這麼誇張），話說這一鍋北中南都有分店，但是不知道是從哪裡起家的？我們這次吃的是「台中朝富店」喔</p>\\n\\n<p>地點：<span style=\"-webkit-text-stroke-width:0px; background-color:rgba(0, 0, 0, 0.03); color:rgba(0, 0, 0, 0.87); display:inline !important; float:none; font-family:roboto,noto sans tc,arial,sans-serif; font-size:13px; font-style:normal; font-variant-caps:normal; font-variant-ligatures:normal; font-weight:400; letter-spacing:normal; orphans:2; text-align:left; text-decoration-color:initial; text-decoration-style:initial; text-indent:0px; text-transform:none; white-space:normal; widows:2; word-spacing:0px\">台中市西屯區朝富路36號</span></p>\\n\\n<p>電話：<span style=\"-webkit-text-stroke-width:0px; background-color:rgba(0, 0, 0, 0.03); color:rgba(0, 0, 0, 0.87); display:inline !important; float:none; font-family:roboto,noto sans tc,arial,sans-serif; font-size:13px; font-style:normal; font-variant-caps:normal; font-variant-ligatures:normal; font-weight:400; letter-spacing:normal; orphans:2; text-align:left; text-decoration-color:initial; text-decoration-style:initial; text-indent:0px; text-transform:none; white-space:normal; widows:2; word-spacing:0px\">04 3609 0088</span></p>\\n\\n<p>營業時間：11:30~03:00</p>\\n\\n<p><iframe class=\"\" frameborder=\"0\" height=\"350\" marginheight=\"0\" marginwidth=\"0\" scrolling=\"no\" src=\"//maps.google.com/?ie=UTF8&amp;f=q&amp;source=s_q&amp;q=loc:24.166896,+120.63769000000002+(%E5%8F%B0%E7%81%A3%E5%8F%B0%E4%B8%AD%E5%B8%82%E8%A5%BF%E5%B1%AF%E5%8D%80%E6%9C%9D%E5%AF%8C%E8%B7%AF%E9%80%99%E4%B8%80%E9%8D%8B%E5%8F%B0%E4%B8%AD%E6%9C%9D%E5%AF%8C%E6%AE%BF)&amp;sll=24.160044,120.62857&amp;ll=24.166896,120.63769&amp;marker=24.166896,120.63769&amp;mrt=loc&amp;z=15&amp;t=m&amp;output=embed\" width=\"425\"></iframe><br>\\n<small><a href=\"//maps.google.com/?ie=UTF8&amp;f=q&amp;source=s_q&amp;q=loc:24.166896,+120.63769000000002+(%E5%8F%B0%E7%81%A3%E5%8F%B0%E4%B8%AD%E5%B8%82%E8%A5%BF%E5%B1%AF%E5%8D%80%E6%9C%9D%E5%AF%8C%E8%B7%AF%E9%80%99%E4%B8%80%E9%8D%8B%E5%8F%B0%E4%B8%AD%E6%9C%9D%E5%AF%8C%E6%AE%BF)&amp;sll=24.160044,120.62857&amp;ll=24.166896,120.63769&amp;marker=24.166896,120.63769&amp;mrt=loc&amp;z=15&amp;t=m&amp;source=embed\" style=\"color:#0000FF;text-align:left\">檢視較大的地圖</a></small></p>\\n\\n<p><strong><span style=\"font-size:14px\"><a href=\"https://www.instagram.com/p/BhtcYEmhi9U/\" target=\"_blank\"><img alt=\"\" height=\"20\" src=\"//s.pixfs.net/f.pixnet.net/images/emotions/032.gif\" title=\"\" width=\"20\">想看IG介紹請點我<img alt=\"\" height=\"20\" src=\"//s.pixfs.net/f.pixnet.net/images/emotions/019.gif\" title=\"\" width=\"20\"></a></span></strong></p>\\n\\n<p><small>朝富店的外觀非常壯闊氣派，不禁讓人點點頭「嗯，果然是台中的餐廳，建築物都要超。大」</small></p>\\n\\n<p><img alt=\"IMG_5208.jpg\" src=\"https://pic.pimg.tw/happy78/1528543947-685380499_n.jpg\" title=\"IMG_5208.jpg\"></p>\\n\\n<p>進去唄～</p>\\n\\n<p><img alt=\"IMG_5217.jpg\" src=\"https://pic.pimg.tw/happy78/1528543947-362759723_n.jpg\" title=\"IMG_5217.jpg\"></p>\\n\\n<p>一樓有個大水池，看起來很富麗堂皇</p>\\n\\n<p><img alt=\"IMG_5232.jpg\" src=\"https://pic.pimg.tw/happy78/1528543962-2265924582_n.jpg\" title=\"IMG_5232.jpg\"></p>\\n\\n<p><a href=\"https://vbtrax.com/track/clicks/1006/ce2bc2bd9f093698a587f264d175924223674fdf2aabebf50760b4?subid_1=&amp;subid_2=&amp;subid_3=&amp;subid_4=&amp;subid_5=\"><img alt=\"\" border=\"0\" src=\"https://vbtrax.com/track/imp/img/368/ce2bc2bd9f093698a587f264d175924223674fdf2aabebf50760b4?subid_1=&amp;subid_2=&amp;subid_3=&amp;subid_4=&amp;subid_5=\" title=\"\"></a></p>\\n\\n<p>最讓我讚嘆的就是這個牡丹（？中國風的樓梯了！！好美喔～～</p>\\n\\n<p><img alt=\"IMG_5230.jpg\" src=\"https://pic.pimg.tw/happy78/1528543962-4007835890_n.jpg\" title=\"IMG_5230.jpg\"></p>\\n\\n<p>整棟建築物看起來用餐環境很棒</p>\\n\\n<p><img alt=\"IMG_5229.jpg\" src=\"https://pic.pimg.tw/happy78/1528543962-1785144547_n.jpg\" title=\"IMG_5229.jpg\"></p>\\n\\n<p>我看到一二樓都有這些宮廷風服飾，我沒問服務生是不是可以穿（因為我也不想穿XD而且整家店我也沒看到有人在穿XD）不過如果你想穿的話我想應該是沒人會阻止</p>\\n\\n<p><img alt=\"IMG_5228.jpg\" src=\"https://pic.pimg.tw/happy78/1528543960-3197815885_n.jpg\" title=\"IMG_5228.jpg\"></p>\\n\\n<p>醬料也有滿多選擇的</p>\\n\\n<p><img alt=\"IMG_5216.jpg\" src=\"https://pic.pimg.tw/happy78/1528543947-2503982521_n.jpg\" title=\"IMG_5216.jpg\"></p>\\n\\n<p><a href=\"https://vbtrax.com/track/clicks/1096/ce2bc2b79e073698a587f264d175924223674fdf2aabebf50769b4?subid_1=&amp;subid_2=&amp;subid_3=&amp;subid_4=&amp;subid_5=\"><img alt=\"\" border=\"0\" src=\"https://vbtrax.com/track/imp/img/976/ce2bc2b79e073698a587f264d175924223674fdf2aabebf50769b4?subid_1=&amp;subid_2=&amp;subid_3=&amp;subid_4=&amp;subid_5=\" title=\"\"></a></p>\\n\\n<p>我們點了麻辣鍋跟老火湯的鴛鴦鍋</p>\\n\\n<p><img alt=\"IMG_5221.jpg\" src=\"https://pic.pimg.tw/happy78/1528543950-1832601825_n.jpg\" title=\"IMG_5221.jpg\"></p>\\n\\n<p>因為我們沒有很餓，所以沒有點很多，以下就一一介紹：（對了，這次忘記拍菜單，想看菜單的人可以去它<a href=\"http://www.toponepot.com/product_detail.php\" target=\"_blank\"><strong>官網</strong></a>看喔）</p>\\n\\n<p>這是梅花豬268$</p>\\n\\n<p><img alt=\"IMG_5219.jpg\" src=\"https://pic.pimg.tw/happy78/1528543949-4214686345_n.jpg\" title=\"IMG_5219.jpg\"></p>\\n\\n<p>和牛梅花598$</p>\\n\\n<p><img alt=\"IMG_5222.jpg\" src=\"https://pic.pimg.tw/happy78/1528543955-3093638684_n.jpg\" title=\"IMG_5222.jpg\"></p>\\n\\n<p>因為跟長輩吃比較不好意思慢慢拍，所以照片沒有很多，照片裡還有玉米筍跟盛味綜合丸238$，另外右上角是店家招待的香蔥油條，我覺得滿特別的，乾吃的話會很有嚼勁，泡湯吃也很美味</p>\\n\\n<p>&nbsp;</p>\\n\\n<p><img alt=\"IMG_5224.jpg\" src=\"https://pic.pimg.tw/happy78/1528543955-714147528_n.jpg\" title=\"IMG_5224.jpg\"></p>\\n\\n<p><a href=\"https://vbtrax.com/track/clicks/3456/ce2bc2bd9c0123daefcda67f8835ce1328684bc177fbb9b20a63b60061?subid_1=&amp;subid_2=&amp;subid_3=&amp;subid_4=&amp;subid_5=\"><img alt=\"\" border=\"0\" src=\"https://vbtrax.com/track/imp/img/35035/ce2bc2bd9c0123daefcda67f8835ce1328684bc177fbb9b20a63b60061?subid_1=&amp;subid_2=&amp;subid_3=&amp;subid_4=&amp;subid_5=\" title=\"\"></a></p>\\n\\n<p>要營養均衡，所以當然要吃青菜啦，這是五彩鮮蔬盤'),\n",
    "#     (2,'<pstyle=\"text-align:left;\"><imgsrc=\"https://pic.pimg.tw/peko721/1555405696-3356669416.jpg\"alt=\"麥食達韓式料理.麥食達菜單.麥食達.台北車站美食.北車美食.石鍋拌飯.\"/></p><pstyle=\"text-align:left;\"><strong><spanstyle=\"color:#0000ff;font-size:14pt;\">麥食達</span></strong></p><pstyle=\"text-align:left;\">地址：台北市中正區懷寧街86號</p><pstyle=\"text-align:left;\">這裡不是裝潢華麗的韓式料理餐廳，而是一家有點像家庭食堂的小店，紅蔘茶、冬粉、味噌湯都是無限量供應。</p><pstyle=\"text-align:left;\">相關文章：<atitle=\"【台北車站美食】麥食達韓式料理｜228公園旁的平價石鍋拌飯、鍋物｜附麥食達菜單\"href=\"https://peko721.pixnet.net/blog/post/46669347\"target=\"_blank\">麥食達韓式料理｜228公園旁的平價石鍋拌飯、鍋物</a></p><divstyle=\"text-align:center;\">'),\n",
    "#     # id 3 is a breaking case\n",
    "# #     (3, '<div id=foodmenu><a href=\"randomlink\">\\nHot Dog</a></div>'),\n",
    "#     (4, '<div id=foodmenu><a href=\"randomlink\">Burger</a></div><div id=foodmenu>'),\n",
    "#     (5, '<div id=foodmenu><a href=\"randomlink\">Hot Dog</a></div>'),\n",
    "#     (6, '<div id=foodmenu><a href=\"randomlink\">Hot Dog</a></div><div id=foodmenu><a href=\"randomlink\">BeautifulSoup</a></div>'),\n",
    "    # pure link case\n",
    "#     (7, '<a href=\"https://vbtrax.com/track/clicks/1096/ce2bc2b79e073698a587f264d175924223674fdf2aabebf50769b4?subid_1=&amp;subid_2=&amp;subid_3=&amp;subid_4=&amp;subid_5=\"></a>')\n",
    "#     (8, textwithlink_1),\n",
    "    (9, textwithlink_2)\n",
    "    \n",
    "]\n",
    "\n",
    "columns = ['article_id','raw_content']\n",
    "\n",
    "\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "html_tag = '<[^>]*>'\n",
    "paragraph_tag = '</?p>'\n",
    "\n",
    "\n",
    "\n",
    "# <a> or <a ... > TODO\n",
    "textlink_tag = '<(a[^>]*>\\s*((?:.|\\n)*?)</a)>'\n",
    "\n",
    "# href='' or href=\"\" or href= or href ='', href =\"\", href =\n",
    "# group end end with not ', \"\", >, space\n",
    "# link end with space, ', \"\" zero or once, > zero or once\n",
    "# link and text\n",
    "# https://stackoverflow.com/questions/56982853/python-regex-to-split-string-at-a-elements-and-extract-link-text/56983057\n",
    "# content splitter\n",
    "link_splitter = '<a'\n",
    "link = 'href\\s?=\\s?[\\'\"]?([^\\'\" >]+)[\\'\"\\s]?>?'\n",
    "text = '>(.*?)</a>'\n",
    "\n",
    "# imglink case \n",
    "# # <img alt=\"Cover_Final.jpg\" src=\"https://pic.pimg.tw/jeremyckt2/1549015761-95927507_l.jpg?v=1549015766\" title=\"Cover_Final.jpg\">\n",
    "# src=\"https://pic.pimg.tw/jeremyckt2/1551455219-2943674205_l.png\"\n",
    "# src='https://pic.pimg.tw/jeremyckt2/1551455219-2943674205_l.jpg'\n",
    "# https://pic.pimg.tw/jeremyckt2/1549015761-95927507_l.jpg?v=1549015766\n",
    "\n",
    "imglink_tag = '<img.*[\\'\"]>'\n",
    "imglink = 'src=[\\'\"](http\\S+[jpg|png](\\?v=\\d*)?)[\\'\"]'\n",
    "\n",
    "\n",
    "\n",
    "sentence_in_article = (\n",
    "    W.partitionBy('article_id','sentence_id')\n",
    "     .orderBy('article_id','sentence_id')\n",
    ")\n",
    "\n",
    "contain_links = (\n",
    "    # check the length of links  only\n",
    "    # because there are some pure link without text\n",
    "      (F.length(C(\"link\")) > 0)\n",
    ")\n",
    "df_filted = (\n",
    "    df\n",
    "    # remove html tag exclude hyperlink\n",
    "    .withColumn(\"cleaned\", F.udf(html.unescape)(\"raw_content\"))\n",
    "    .withColumn(\"cleaned\", F.regexp_replace(C(\"cleaned\"), fr'{paragraph_tag}','\\n'))\n",
    "    .withColumn(\"cleaned\",\n",
    "                F.regexp_replace(\n",
    "                    C(\"cleaned\"),\n",
    "                    # negtive look ahead\n",
    "                    fr'(?!{imglink_tag}|{textlink_tag}|</a>){html_tag}',\n",
    "                    ''\n",
    "                )\n",
    "               )\n",
    "    .select(\n",
    "        'article_id',\n",
    "#         'cleaned',\n",
    "#         'raw_content',\n",
    "        F.posexplode_outer(\n",
    "            F.split(C(\"cleaned\"), fr\"{paragraph_tag}|[\\p{{C}}]\")\n",
    "        )\n",
    "        .alias('sentence_id','sentence')\n",
    "    )\n",
    "    ######## extract imglink at sentence level ###############\n",
    "#     NOTE : \n",
    "#     There are some imglink from href=... from flickr or other website\n",
    "#     so the is_imglink is under-estimate the actual image link\n",
    "    .withColumn('is_imglink',\n",
    "                C(\"sentence\").rlike(fr'{imglink}'))\n",
    "    .withColumn('imgtag',\n",
    "                F.when(C(\"is_imglink\"), F.regexp_extract(\n",
    "                                         C(\"sentence\"),\n",
    "                                         fr'{imglink}',1))\n",
    "                 .otherwise(None)\n",
    "    )\n",
    "    .withColumn(\"imgtag\",\n",
    "                F.regexp_replace(C(\"sentence\"),r'^https://',r'http://'))\n",
    "    .withColumn('sentence',\n",
    "                F.when(C('is_imglink'), F.regexp_replace(\n",
    "                                        C(\"sentence\"),\n",
    "                                        fr'{html_tag}',\n",
    "                                        ''))\n",
    "                 .otherwise(C(\"sentence\"))\n",
    "    )\n",
    "    ####### extract textlink at sub-sentence level ################\n",
    "    # explode the sentence into candidate\n",
    "    # then extract text and hyperlink\n",
    "    .withColumn(\"candidate\",\n",
    "              F.explode_outer(F.split('sentence', fr'{link_splitter}')))\n",
    "    .withColumn(\"link\",F.regexp_extract(C(\"candidate\"),fr'{link}', 1))\n",
    "    .withColumn(\"text\",F.regexp_extract(C(\"candidate\"),fr'{text}', 1))\n",
    "    .drop('candidate')\n",
    "    # clean the hyperlink and text\n",
    "    .withColumn(\"link\",F.trim(\"link\"))\n",
    "    .withColumn(\"text\",F.trim(\"text\"))\n",
    "    .withColumn(\"link\",F.regexp_replace(C(\"link\"),r'^https://',r'http://'))\n",
    "    # normalize hyperlink\n",
    "#     # collect the text and link at article_sentence partition\n",
    "    .withColumn(\"text_links\",\n",
    "                   F.collect_list(\n",
    "                    F.when(contain_links,\n",
    "                           F.struct(C(\"text\"),C(\"link\")))\n",
    "                     .otherwise(F.lit(None))\n",
    "                    ).over(sentence_in_article)\n",
    "                     )\n",
    "               \n",
    "    .withColumn(\"rank_sentence_in_article\",\n",
    "                F.row_number()\n",
    "                .over(sentence_in_article))\n",
    "    .where(C(\"rank_sentence_in_article\") == 1)\n",
    "#     # now clean the sentence into plain text\n",
    "#     # plain_text --> rename to sentence\n",
    "    .withColumn(\"plain_text\",F.regexp_replace(C(\"sentence\"),fr\"{html_tag}\",\" \"))\n",
    "    .withColumn(\"plain_text\",F.trim(\"plain_text\"))\n",
    "      # merge plain text with space here\n",
    "    .orderBy(\"article_id\",'sentence_id','rank_sentence_in_article')\n",
    "#     .where(C(\"sentence_id\") > 500)\n",
    "#     .where(C(\"is_imglink\") == True)\n",
    "#     .where(C(\"plain_text\").rlike(\"Cuiqu Coffee\"))\n",
    "#     .where(F.size(C(\"text_links\")) > 0)\n",
    "    ###### Collect meta information #############\n",
    "    .where(F.length(C(\"plain_text\")) > 0)\n",
    "#     .where(C(\"sentence_id\") == 153)\n",
    "#     .where(C(\"is_imglink\") == True)\n",
    "    .drop('rank_sentence_in_article','link','text','sentence')\n",
    "    \n",
    ")\n",
    "\n",
    "df_filted.printSchema()\n",
    "\n",
    "# df_filted.show(n=500, vertical=True, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:37:27.773707Z",
     "start_time": "2021-07-31T05:37:27.322125Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------------------------------------------------------------------\n",
      " article_id | 14431                                                             \n",
      " pred       | 0.99834                                                           \n",
      " img_url    | https://pic.pimg.tw/happy78/1528543947-685380499_n.jpg mfkvmdfasx \n",
      "\n",
      "-RECORD 0-----------------------------------------------------------------------\n",
      " article_id | 14431                                                             \n",
      " pred       | 0.99834                                                           \n",
      " img_url    | https://pic.pimg.tw/happy78/1528543947-685380499_n.jpg mfkvmdfasx \n",
      "\n",
      "-RECORD 0--------------------------------------------------------\n",
      " article_id | 67789                                              \n",
      " pred       | 0.94111                                            \n",
      " img_url    | https://pic.pimg.tw/happy78/1528543962-45890_n.png \n",
      "-RECORD 1--------------------------------------------------------\n",
      " article_id | 67789                                              \n",
      " pred       | 0.94111                                            \n",
      " img_url    | png                                                \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2. regax filtering\n",
    "# contains\n",
    "# startswith\n",
    "# endwith\n",
    "\n",
    "data = [\n",
    "    (14431,0.99834,'https://pic.pimg.tw/happy78/1528543947-685380499_n.jpg mfkvmdfasx'),\n",
    "    (14431,0.99834,'https://pic.pimg.tw/happy78/1528543947-362759723_n.jpg'),\n",
    "    (14431,0.97611,'https://pic.pimg.tw/happy78/1528543962-2265924582_n.jpg'),\n",
    "    (67789,0.93422,'https://pic.pimg.tw/happy78/1528543962-4007835890_n.jpg'),\n",
    "    (67789,0.94231,'https://pic.pimg.tw/happy78/1528543962-45890_n.jpg'),\n",
    "    (67789,0.94111,'https://pic.pimg.tw/happy78/1528543962-45890_n.png'),\n",
    "    (67789,0.94111,'png')\n",
    "]\n",
    "\n",
    "columns = ['article_id','pred','img_url']\n",
    "\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "df_con = (\n",
    "    df.filter(df.img_url.contains('mfkv'))\n",
    ")\n",
    "df_con.show(vertical=True,truncate=False)\n",
    "\n",
    "df_startwith = (\n",
    "    df.filter(df.img_url.startswith(\"png\"))\n",
    ")\n",
    "\n",
    "df_con.show(vertical=True,truncate=False)\n",
    "\n",
    "df_endwith = (\n",
    "    df.filter(df.img_url.endswith(\"png\"))\n",
    ")\n",
    "\n",
    "df_endwith.show(vertical=True,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:37:27.944500Z",
     "start_time": "2021-07-31T05:37:27.776549Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " article_id | 14431                                                                                                                                                                              \n",
      " cleaned    | 今天要帶大家去吃吃台中有名的這一鍋皇室秘藏鍋物，據說會有著「帝王般」的享受（是有沒有這麼誇張），話說這一鍋北中南都有分店，但是不知道是從哪裡起家的？我們這次吃的是「台中朝富店」喔 \n",
      " length     | 89                                                                                                                                                                                 \n",
      "-RECORD 1----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " article_id | 14431                                                                                                                                                                              \n",
      " cleaned    | 地點：                                                                                                                                                                             \n",
      " length     | 3                                                                                                                                                                                  \n",
      "-RECORD 2----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " article_id | 14431                                                                                                                                                                              \n",
      " cleaned    | 台中市西屯區朝富路36號                                                                                                                                                             \n",
      " length     | 12                                                                                                                                                                                 \n",
      "-RECORD 3----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " article_id | 14431                                                                                                                                                                              \n",
      " cleaned    | 電話：                                                                                                                                                                             \n",
      " length     | 3                                                                                                                                                                                  \n",
      "-RECORD 4----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " article_id | 14431                                                                                                                                                                              \n",
      " cleaned    | 04                                                                                                                                                                                 \n",
      " length     | 2                                                                                                                                                                                  \n",
      "-RECORD 5----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " article_id | 14431                                                                                                                                                                              \n",
      " cleaned    | 3609                                                                                                                                                                               \n",
      " length     | 4                                                                                                                                                                                  \n",
      "-RECORD 6----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " article_id | 14431                                                                                                                                                                              \n",
      " cleaned    | 0088                                                                                                                                                                               \n",
      " length     | 4                                                                                                                                                                                  \n",
      "-RECORD 7----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " article_id | 14431                                                                                                                                                                              \n",
      " cleaned    | 營業時間：11:30~03:00                                                                                                                                                              \n",
      " length     | 16                                                                                                                                                                                 \n",
      "-RECORD 8----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " article_id | 14431                                                                                                                                                                              \n",
      " cleaned    | 檢視較大的地圖                                                                                                                                                                     \n",
      " length     | 7                                                                                                                                                                                  \n",
      "-RECORD 9----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " article_id | 14431                                                                                                                                                                              \n",
      " cleaned    | 想看IG介紹請點我                                                                                                                                                                   \n",
      " length     | 9                                                                                                                                                                                  \n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# html - related, separate each part of html tag\n",
    "\n",
    "\n",
    "data = [\n",
    "    (14431,'<p>今天要帶大家去吃吃台中有名的這一鍋皇室秘藏鍋物，據說會有著「帝王般」的享受（是有沒有這麼誇張），話說這一鍋北中南都有分店，但是不知道是從哪裡起家的？我們這次吃的是「台中朝富店」喔</p>\\n\\n<p>地點：<span style=\"-webkit-text-stroke-width:0px; background-color:rgba(0, 0, 0, 0.03); color:rgba(0, 0, 0, 0.87); display:inline !important; float:none; font-family:roboto,noto sans tc,arial,sans-serif; font-size:13px; font-style:normal; font-variant-caps:normal; font-variant-ligatures:normal; font-weight:400; letter-spacing:normal; orphans:2; text-align:left; text-decoration-color:initial; text-decoration-style:initial; text-indent:0px; text-transform:none; white-space:normal; widows:2; word-spacing:0px\">台中市西屯區朝富路36號</span></p>\\n\\n<p>電話：<span style=\"-webkit-text-stroke-width:0px; background-color:rgba(0, 0, 0, 0.03); color:rgba(0, 0, 0, 0.87); display:inline !important; float:none; font-family:roboto,noto sans tc,arial,sans-serif; font-size:13px; font-style:normal; font-variant-caps:normal; font-variant-ligatures:normal; font-weight:400; letter-spacing:normal; orphans:2; text-align:left; text-decoration-color:initial; text-decoration-style:initial; text-indent:0px; text-transform:none; white-space:normal; widows:2; word-spacing:0px\">04 3609 0088</span></p>\\n\\n<p>營業時間：11:30~03:00</p>\\n\\n<p><iframe class=\"\" frameborder=\"0\" height=\"350\" marginheight=\"0\" marginwidth=\"0\" scrolling=\"no\" src=\"//maps.google.com/?ie=UTF8&amp;f=q&amp;source=s_q&amp;q=loc:24.166896,+120.63769000000002+(%E5%8F%B0%E7%81%A3%E5%8F%B0%E4%B8%AD%E5%B8%82%E8%A5%BF%E5%B1%AF%E5%8D%80%E6%9C%9D%E5%AF%8C%E8%B7%AF%E9%80%99%E4%B8%80%E9%8D%8B%E5%8F%B0%E4%B8%AD%E6%9C%9D%E5%AF%8C%E6%AE%BF)&amp;sll=24.160044,120.62857&amp;ll=24.166896,120.63769&amp;marker=24.166896,120.63769&amp;mrt=loc&amp;z=15&amp;t=m&amp;output=embed\" width=\"425\"></iframe><br>\\n<small><a href=\"//maps.google.com/?ie=UTF8&amp;f=q&amp;source=s_q&amp;q=loc:24.166896,+120.63769000000002+(%E5%8F%B0%E7%81%A3%E5%8F%B0%E4%B8%AD%E5%B8%82%E8%A5%BF%E5%B1%AF%E5%8D%80%E6%9C%9D%E5%AF%8C%E8%B7%AF%E9%80%99%E4%B8%80%E9%8D%8B%E5%8F%B0%E4%B8%AD%E6%9C%9D%E5%AF%8C%E6%AE%BF)&amp;sll=24.160044,120.62857&amp;ll=24.166896,120.63769&amp;marker=24.166896,120.63769&amp;mrt=loc&amp;z=15&amp;t=m&amp;source=embed\" style=\"color:#0000FF;text-align:left\">檢視較大的地圖</a></small></p>\\n\\n<p><strong><span style=\"font-size:14px\"><a href=\"https://www.instagram.com/p/BhtcYEmhi9U/\" target=\"_blank\"><img alt=\"\" height=\"20\" src=\"//s.pixfs.net/f.pixnet.net/images/emotions/032.gif\" title=\"\" width=\"20\">想看IG介紹請點我<img alt=\"\" height=\"20\" src=\"//s.pixfs.net/f.pixnet.net/images/emotions/019.gif\" title=\"\" width=\"20\"></a></span></strong></p>\\n\\n<p><small>朝富店的外觀非常壯闊氣派，不禁讓人點點頭「嗯，果然是台中的餐廳，建築物都要超。大」</small></p>\\n\\n<p><img alt=\"IMG_5208.jpg\" src=\"https://pic.pimg.tw/happy78/1528543947-685380499_n.jpg\" title=\"IMG_5208.jpg\"></p>\\n\\n<p>進去唄～</p>\\n\\n<p><img alt=\"IMG_5217.jpg\" src=\"https://pic.pimg.tw/happy78/1528543947-362759723_n.jpg\" title=\"IMG_5217.jpg\"></p>\\n\\n<p>一樓有個大水池，看起來很富麗堂皇</p>\\n\\n<p><img alt=\"IMG_5232.jpg\" src=\"https://pic.pimg.tw/happy78/1528543962-2265924582_n.jpg\" title=\"IMG_5232.jpg\"></p>\\n\\n<p><a href=\"https://vbtrax.com/track/clicks/1006/ce2bc2bd9f093698a587f264d175924223674fdf2aabebf50760b4?subid_1=&amp;subid_2=&amp;subid_3=&amp;subid_4=&amp;subid_5=\"><img alt=\"\" border=\"0\" src=\"https://vbtrax.com/track/imp/img/368/ce2bc2bd9f093698a587f264d175924223674fdf2aabebf50760b4?subid_1=&amp;subid_2=&amp;subid_3=&amp;subid_4=&amp;subid_5=\" title=\"\"></a></p>\\n\\n<p>最讓我讚嘆的就是這個牡丹（？中國風的樓梯了！！好美喔～～</p>\\n\\n<p><img alt=\"IMG_5230.jpg\" src=\"https://pic.pimg.tw/happy78/1528543962-4007835890_n.jpg\" title=\"IMG_5230.jpg\"></p>\\n\\n<p>整棟建築物看起來用餐環境很棒</p>\\n\\n<p><img alt=\"IMG_5229.jpg\" src=\"https://pic.pimg.tw/happy78/1528543962-1785144547_n.jpg\" title=\"IMG_5229.jpg\"></p>\\n\\n<p>我看到一二樓都有這些宮廷風服飾，我沒問服務生是不是可以穿（因為我也不想穿XD而且整家店我也沒看到有人在穿XD）不過如果你想穿的話我想應該是沒人會阻止</p>\\n\\n<p><img alt=\"IMG_5228.jpg\" src=\"https://pic.pimg.tw/happy78/1528543960-3197815885_n.jpg\" title=\"IMG_5228.jpg\"></p>\\n\\n<p>醬料也有滿多選擇的</p>\\n\\n<p><img alt=\"IMG_5216.jpg\" src=\"https://pic.pimg.tw/happy78/1528543947-2503982521_n.jpg\" title=\"IMG_5216.jpg\"></p>\\n\\n<p><a href=\"https://vbtrax.com/track/clicks/1096/ce2bc2b79e073698a587f264d175924223674fdf2aabebf50769b4?subid_1=&amp;subid_2=&amp;subid_3=&amp;subid_4=&amp;subid_5=\"><img alt=\"\" border=\"0\" src=\"https://vbtrax.com/track/imp/img/976/ce2bc2b79e073698a587f264d175924223674fdf2aabebf50769b4?subid_1=&amp;subid_2=&amp;subid_3=&amp;subid_4=&amp;subid_5=\" title=\"\"></a></p>\\n\\n<p>我們點了麻辣鍋跟老火湯的鴛鴦鍋</p>\\n\\n<p><img alt=\"IMG_5221.jpg\" src=\"https://pic.pimg.tw/happy78/1528543950-1832601825_n.jpg\" title=\"IMG_5221.jpg\"></p>\\n\\n<p>因為我們沒有很餓，所以沒有點很多，以下就一一介紹：（對了，這次忘記拍菜單，想看菜單的人可以去它<a href=\"http://www.toponepot.com/product_detail.php\" target=\"_blank\"><strong>官網</strong></a>看喔）</p>\\n\\n<p>這是梅花豬268$</p>\\n\\n<p><img alt=\"IMG_5219.jpg\" src=\"https://pic.pimg.tw/happy78/1528543949-4214686345_n.jpg\" title=\"IMG_5219.jpg\"></p>\\n\\n<p>和牛梅花598$</p>\\n\\n<p><img alt=\"IMG_5222.jpg\" src=\"https://pic.pimg.tw/happy78/1528543955-3093638684_n.jpg\" title=\"IMG_5222.jpg\"></p>\\n\\n<p>因為跟長輩吃比較不好意思慢慢拍，所以照片沒有很多，照片裡還有玉米筍跟盛味綜合丸238$，另外右上角是店家招待的香蔥油條，我覺得滿特別的，乾吃的話會很有嚼勁，泡湯吃也很美味</p>\\n\\n<p>&nbsp;</p>\\n\\n<p><img alt=\"IMG_5224.jpg\" src=\"https://pic.pimg.tw/happy78/1528543955-714147528_n.jpg\" title=\"IMG_5224.jpg\"></p>\\n\\n<p><a href=\"https://vbtrax.com/track/clicks/3456/ce2bc2bd9c0123daefcda67f8835ce1328684bc177fbb9b20a63b60061?subid_1=&amp;subid_2=&amp;subid_3=&amp;subid_4=&amp;subid_5=\"><img alt=\"\" border=\"0\" src=\"https://vbtrax.com/track/imp/img/35035/ce2bc2bd9c0123daefcda67f8835ce1328684bc177fbb9b20a63b60061?subid_1=&amp;subid_2=&amp;subid_3=&amp;subid_4=&amp;subid_5=\" title=\"\"></a></p>\\n\\n<p>要營養均衡，所以當然要吃青菜啦，這是五彩鮮蔬盤'),\n",
    "    (55444,'<pstyle=\"text-align:left;\"><imgsrc=\"https://pic.pimg.tw/peko721/1555405696-3356669416.jpg\"alt=\"麥食達韓式料理.麥食達菜單.麥食達.台北車站美食.北車美食.石鍋拌飯.\"/></p><pstyle=\"text-align:left;\"><strong><spanstyle=\"color:#0000ff;font-size:14pt;\">麥食達</span></strong></p><pstyle=\"text-align:left;\">地址：台北市中正區懷寧街86號</p><pstyle=\"text-align:left;\">這裡不是裝潢華麗的韓式料理餐廳，而是一家有點像家庭食堂的小店，紅蔘茶、冬粉、味噌湯都是無限量供應。</p><pstyle=\"text-align:left;\">相關文章：<atitle=\"【台北車站美食】麥食達韓式料理｜228公園旁的平價石鍋拌飯、鍋物｜附麥食達菜單\"href=\"https://peko721.pixnet.net/blog/post/46669347\"target=\"_blank\">麥食達韓式料理｜228公園旁的平價石鍋拌飯、鍋物</a></p><divstyle=\"text-align:center;\">'),\n",
    "    (666,'<html><head></head><body><h1>This is python</h1></body></html>')\n",
    "]\n",
    "\n",
    "columns = ['article_id','raw_content']\n",
    "\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "\n",
    "\n",
    "df_filted = (\n",
    "    df\n",
    "    .withColumn(\"cleaned\",\n",
    "                  F.explode_outer(\n",
    "                      F.split(C(\"raw_content\"),r'<[^>]*>|\\p{Z}+|\\s+'))\n",
    "                 )\n",
    "    .withColumn(\"cleaned\", F.regexp_replace(C(\"cleaned\"), r\"\\p{Z}*\", \"\"))\n",
    "    .withColumn(\"cleaned\",F.trim(C(\"cleaned\")))\n",
    "    .withColumn(\"length\", F.length(C(\"cleaned\")))\n",
    "    .where(F.length(C(\"cleaned\")) > 0)\n",
    "    .drop('raw_content')\n",
    ")\n",
    "\n",
    "df_filted.show(n=10,vertical=True, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:37:28.264652Z",
     "start_time": "2021-07-31T05:37:27.945925Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- languagesAtSchool: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- major_use: string (nullable = true)\n",
      " |-- raw_content: string (nullable = true)\n",
      "\n",
      "+----------------+------------------+---------+---------------------------------------------+\n",
      "|name            |languagesAtSchool |major_use|raw_content                                  |\n",
      "+----------------+------------------+---------+---------------------------------------------+\n",
      "|James,,Smith    |[Java, Scala, C++]|Java     |sdmcsmcmpkla,pxozas,pxmasmxpJavasodmcasmdcpso|\n",
      "|Michael,Rose,   |[Spark, Java, C++]|Python   |sd,mcmsdcopmsocmpsmdcpython                  |\n",
      "|Robert,,Williams|[CSharp, VB]      |         |smdkcadpmcpowmcpmspdcmpsdc                   |\n",
      "|ABC,ss,Williams |[]                |Scala    |smdoicmsocomamsodmcosdcoms                   |\n",
      "|ABC,ss,Williams |[]                |Scala    |smdoicmsocomamsodmcosdcomsSca                |\n",
      "+----------------+------------------+---------+---------------------------------------------+\n",
      "\n",
      "+----------------+------------------+---------+---------------------------------------------+-------+\n",
      "|name            |languagesAtSchool |major_use|raw_content                                  |matched|\n",
      "+----------------+------------------+---------+---------------------------------------------+-------+\n",
      "|James,,Smith    |[Java, Scala, C++]|Java     |sdmcsmcmpkla,pxozas,pxmasmxpJavasodmcasmdcpso|true   |\n",
      "|Michael,Rose,   |[Spark, Java, C++]|Python   |sd,mcmsdcopmsocmpsmdcpython                  |false  |\n",
      "|Robert,,Williams|[CSharp, VB]      |         |smdkcadpmcpowmcpmspdcmpsdc                   |true   |\n",
      "|ABC,ss,Williams |[]                |Scala    |smdoicmsocomamsodmcosdcoms                   |false  |\n",
      "|ABC,ss,Williams |[]                |Scala    |smdoicmsocomamsodmcosdcomsSca                |false  |\n",
      "+----------------+------------------+---------+---------------------------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3 reg-like\n",
    "# Search all the pattern mahor_use in raw_content\n",
    "\n",
    "\n",
    "columns = [\"name\",\"languagesAtSchool\",\"major_use\",\"raw_content\"]\n",
    "data = [\n",
    "    (\"James,,Smith\",[\"Java\",\"Scala\",\"C++\"],\"Java\",\"sdmcsmcmpkla,pxozas,pxmasmxpJavasodmcasmdcpso\"), \\\n",
    "    (\"Michael,Rose,\",[\"Spark\",\"Java\",\"C++\"],\"Python\",\"sd,mcmsdcopmsocmpsmdcpython\"), \\\n",
    "    (\"Robert,,Williams\",[\"CSharp\",\"VB\"],\"\",\"smdkcadpmcpowmcpmspdcmpsdc\"),\n",
    "    (\"ABC,ss,Williams\",[],\"Scala\",\"smdoicmsocomamsodmcosdcoms\"),\n",
    "    (\"ABC,ss,Williams\",[],\"Scala\",\"smdoicmsocomamsodmcosdcomsSca\")\n",
    "       ]\n",
    "\n",
    "\n",
    "df = spark.createDataFrame(data=data,schema=columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n",
    "\n",
    "(\n",
    "    df\n",
    "    .withColumn(\"matched\",C(\"raw_content\").contains(C(\"major_use\")))\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## filtering in complex type (2+)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:37:28.587373Z",
     "start_time": "2021-07-31T05:37:28.267460Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- languagesAtSchool: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- currentState: string (nullable = true)\n",
      "\n",
      "+----------------+------------------+------------+\n",
      "|name            |languagesAtSchool |currentState|\n",
      "+----------------+------------------+------------+\n",
      "|James,,Smith    |[Java, Scala, C++]|CA          |\n",
      "|Michael,Rose,   |[Spark, Java, C++]|NJ          |\n",
      "|Robert,,Williams|[CSharp, VB]      |NV          |\n",
      "+----------------+------------------+------------+\n",
      "\n",
      "+----------------+------------------+------------+------------------------+\n",
      "|name            |languagesAtSchool |currentState|filter_languagesAtSchool|\n",
      "+----------------+------------------+------------+------------------------+\n",
      "|James,,Smith    |[Java, Scala, C++]|CA          |true                    |\n",
      "|Michael,Rose,   |[Spark, Java, C++]|NJ          |true                    |\n",
      "|Robert,,Williams|[CSharp, VB]      |NV          |false                   |\n",
      "+----------------+------------------+------------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. filter in array type\n",
    "columns = [\"name\",\"languagesAtSchool\",\"currentState\"]\n",
    "data = [(\"James,,Smith\",[\"Java\",\"Scala\",\"C++\"],\"CA\"), \\\n",
    "    (\"Michael,Rose,\",[\"Spark\",\"Java\",\"C++\"],\"NJ\"), \\\n",
    "    (\"Robert,,Williams\",[\"CSharp\",\"VB\"],\"NV\")]\n",
    "\n",
    "df = spark.createDataFrame(data=data,schema=columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n",
    "\n",
    "######### Case 1, single value #############\n",
    "\n",
    "df = (\n",
    "    df.withColumn(\"filter_languagesAtSchool\",F.array_contains(C(\"languagesAtSchool\"), \"Java\"))\n",
    ")\n",
    "df.show(truncate=False)\n",
    "\n",
    "\n",
    "######### Case 2, multiple values ?\n",
    " \n",
    "# must_language = [\"Spark\",\"Java\"]\n",
    "# must_language_lit = [F.lit(i) for i in must_language]\n",
    "# print(must_language_lit)\n",
    "# df = (\n",
    "#     df.withColumn(\"filter_languagesAtSchool\",F.array_contains(C(\"languagesAtSchool\"), must_language_lit))\n",
    "# )\n",
    "# df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:37:28.931834Z",
     "start_time": "2021-07-31T05:37:28.589543Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- languagesAtSchool: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- major_use: string (nullable = true)\n",
      "\n",
      "+----------------+------------------+---------+\n",
      "|name            |languagesAtSchool |major_use|\n",
      "+----------------+------------------+---------+\n",
      "|James,,Smith    |[Java, Scala, C++]|Java     |\n",
      "|Michael,Rose,   |[Spark, Java, C++]|Python   |\n",
      "|Robert,,Williams|[CSharp, VB]      |         |\n",
      "|ABC,ss,Williams |[]                |Scala    |\n",
      "+----------------+------------------+---------+\n",
      "\n",
      "+----------------+------------------+---------+--------------------+\n",
      "|name            |languagesAtSchool |major_use|is_major_been_taught|\n",
      "+----------------+------------------+---------+--------------------+\n",
      "|James,,Smith    |[Java, Scala, C++]|Java     |1                   |\n",
      "|Michael,Rose,   |[Spark, Java, C++]|Python   |0                   |\n",
      "|Robert,,Williams|[CSharp, VB]      |         |0                   |\n",
      "|ABC,ss,Williams |[]                |Scala    |0                   |\n",
      "+----------------+------------------+---------+--------------------+\n",
      "\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- languagesAtSchool: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- major_use: string (nullable = true)\n",
      " |-- is_major_been_taught: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 2 column A array contains column B\n",
    "# https://stackoverflow.com/questions/48488463/use-is-in-between-2-spark-dataframe-columns\n",
    "\n",
    "columns = [\"name\",\"languagesAtSchool\",\"major_use\"]\n",
    "data = [(\"James,,Smith\",[\"Java\",\"Scala\",\"C++\"],\"Java\"), \\\n",
    "    (\"Michael,Rose,\",[\"Spark\",\"Java\",\"C++\"],\"Python\"), \\\n",
    "    (\"Robert,,Williams\",[\"CSharp\",\"VB\"],\"\"),\n",
    "    (\"ABC,ss,Williams\",[],\"Scala\")]\n",
    "\n",
    "\n",
    "df = spark.createDataFrame(data=data,schema=columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n",
    "\n",
    "######### Case 1, single value #############\n",
    "\n",
    "df = (\n",
    "    df\n",
    "    .withColumn(\"is_major_been_taught\",F.expr(\"array_contains(languagesAtSchool, major_use)\").cast(\"integer\"))\n",
    "#     .withColumn(\"is_major_been_taught\",F.expr(\"array_contains(languagesAtSchool, major_use)\"))\n",
    ")\n",
    "df.show(truncate=False)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:37:29.261163Z",
     "start_time": "2021-07-31T05:37:28.935638Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- languagesAtSchool: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- major_use: string (nullable = true)\n",
      "\n",
      "+----------------+------------------+---------+\n",
      "|name            |languagesAtSchool |major_use|\n",
      "+----------------+------------------+---------+\n",
      "|James,,Smith    |[Java, Scala, C++]|Java     |\n",
      "|Michael,Rose,   |[Spark, Java, C++]|Python   |\n",
      "|Robert,,Williams|[CSharp, VB]      |         |\n",
      "|ABC,ss,Williams |[]                |Scala    |\n",
      "|                |[]                |         |\n",
      "+----------------+------------------+---------+\n",
      "\n",
      "+----------------+------------------+---------+---------------------+\n",
      "|name            |languagesAtSchool |major_use|language_without_Java|\n",
      "+----------------+------------------+---------+---------------------+\n",
      "|James,,Smith    |[Java, Scala, C++]|Java     |[Scala, C++]         |\n",
      "|Michael,Rose,   |[Spark, Java, C++]|Python   |[Spark, C++]         |\n",
      "|Robert,,Williams|[CSharp, VB]      |         |[CSharp, VB]         |\n",
      "|ABC,ss,Williams |[]                |Scala    |[]                   |\n",
      "|                |[]                |         |[]                   |\n",
      "+----------------+------------------+---------+---------------------+\n",
      "\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- languagesAtSchool: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- major_use: string (nullable = true)\n",
      " |-- language_without_Java: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3 remove stuff in complex df\n",
    "\n",
    "columns = [\"name\",\"languagesAtSchool\",\"major_use\"]\n",
    "data = [(\"James,,Smith\",[\"Java\",\"Scala\",\"C++\"],\"Java\"), \\\n",
    "    (\"Michael,Rose,\",[\"Spark\",\"Java\",\"C++\"],\"Python\"), \\\n",
    "    (\"Robert,,Williams\",[\"CSharp\",\"VB\"],\"\"),\n",
    "    (\"ABC,ss,Williams\",[],\"Scala\"),\n",
    "    (\"\",[],\"\")\n",
    "       ]\n",
    "\n",
    "\n",
    "all_student = (\n",
    "    W.partitionBy('const')\n",
    ")\n",
    "\n",
    "df = (\n",
    "    spark.createDataFrame(data=data,schema=columns)\n",
    ")\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n",
    "\n",
    "\n",
    "df = (\n",
    "    df\n",
    "    .withColumn('language_without_Java',\n",
    "                F.array_remove(C(\"languagesAtSchool\"),\"Java\"))\n",
    ")\n",
    "df.show(truncate=False)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JOIN (3+)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:37:32.673504Z",
     "start_time": "2021-07-31T05:37:29.264686Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|col1|co12|\n",
      "+----+----+\n",
      "|   A|   1|\n",
      "|   B|   2|\n",
      "|   C|   3|\n",
      "+----+----+\n",
      "\n",
      "+----+----+\n",
      "|col1|co12|\n",
      "+----+----+\n",
      "|   A|  20|\n",
      "|   Y|  30|\n",
      "|   Z|  50|\n",
      "+----+----+\n",
      "\n",
      "left join\n",
      "+----+----+----+\n",
      "|col1|co12|co12|\n",
      "+----+----+----+\n",
      "|   B|   2|null|\n",
      "|   C|   3|null|\n",
      "|   A|   1|  20|\n",
      "+----+----+----+\n",
      "\n",
      "right join\n",
      "+----+----+----+\n",
      "|col1|co12|co12|\n",
      "+----+----+----+\n",
      "|   Y|null|  30|\n",
      "|   Z|null|  50|\n",
      "|   A|   1|  20|\n",
      "+----+----+----+\n",
      "\n",
      "inner join\n",
      "+----+----+----+\n",
      "|col1|co12|co12|\n",
      "+----+----+----+\n",
      "|   A|   1|  20|\n",
      "+----+----+----+\n",
      "\n",
      "outer join\n",
      "+----+----+----+\n",
      "|col1|co12|co12|\n",
      "+----+----+----+\n",
      "|   B|   2|null|\n",
      "|   Y|null|  30|\n",
      "|   C|   3|null|\n",
      "|   Z|null|  50|\n",
      "|   A|   1|  20|\n",
      "+----+----+----+\n",
      "\n",
      "cross\n",
      "+----+----+----+\n",
      "|col1|co12|col1|\n",
      "+----+----+----+\n",
      "|   A|   1|   A|\n",
      "|   A|   1|   Y|\n",
      "|   A|   1|   Z|\n",
      "|   B|   2|   A|\n",
      "|   C|   3|   A|\n",
      "|   B|   2|   Y|\n",
      "|   B|   2|   Z|\n",
      "|   C|   3|   Y|\n",
      "|   C|   3|   Z|\n",
      "+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1 left, right, inner, outer, cross\n",
    "# cross join ( cartesian product with another DataFrame )\n",
    "# cross join usually use high computational cost which we should avoid to use it \n",
    "# https://spark.apache.org/docs/2.3.0/api/python/pyspark.sql.html#pyspark.sql.GroupedData.apply\n",
    "\n",
    "\n",
    "# https://www.youtube.com/watch?v=fp53QhSfQcI\n",
    "# check the viodeo 05:55 ~ 9.08\n",
    "# The shuffle hash join works best when\n",
    "# distribute evenly with the key we are joining on\n",
    "# have an adequate number of keys for parallesim\n",
    "# The problem often happens when the table you wanna join is unevenly distribute (8 : 00)\n",
    "left = spark.createDataFrame(\n",
    "    [\n",
    "        (\"A\", 1),\n",
    "        (\"B\", 2),\n",
    "        (\"C\", 3)\n",
    "    ],\n",
    "    (\"col1\",\"co12\")\n",
    ")\n",
    "\n",
    "right = spark.createDataFrame(\n",
    "    [\n",
    "        (\"A\", 20),\n",
    "        (\"Y\", 30),\n",
    "        (\"Z\", 50)\n",
    "    ],\n",
    "    (\"col1\",\"co12\")\n",
    ")\n",
    "\n",
    "left.show()\n",
    "right.show()\n",
    "\n",
    "print('left join')\n",
    "left.join(right,on='col1',how='left').show()\n",
    "print('right join')\n",
    "left.join(right,on='col1',how='right').show()\n",
    "print('inner join')\n",
    "left.join(right,on='col1',how='inner').show()\n",
    "print('outer join')\n",
    "left.join(right,on='col1',how='outer').show()\n",
    "print('cross')\n",
    "left.crossJoin(right.select('col1')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:37:33.225144Z",
     "start_time": "2021-07-31T05:37:32.675770Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n",
      "|article_id|             img_url|\n",
      "+----------+--------------------+\n",
      "|     14431|https://pic.pimg....|\n",
      "|     14431|https://pic.pimg....|\n",
      "|     14431|https://pic.pimg....|\n",
      "|     67789|https://pic.pimg....|\n",
      "|     67789|https://pic.pimg....|\n",
      "+----------+--------------------+\n",
      "\n",
      "+----------+--------------------+\n",
      "|article_id|             img_url|\n",
      "+----------+--------------------+\n",
      "|     14431|https://pic.pimg....|\n",
      "+----------+--------------------+\n",
      "\n",
      "== Physical Plan ==\n",
      "*(3) Project [img_url#2003, article_id#2002L, article_id#2024L]\n",
      "+- *(3) BroadcastHashJoin [img_url#2003], [img_url#2025], LeftOuter, BuildRight, false\n",
      "   :- *(3) Scan ExistingRDD[article_id#2002L,img_url#2003]\n",
      "   +- BroadcastExchange HashedRelationBroadcastMode(List(input[1, string, false]),false), [id=#1360]\n",
      "      +- *(2) Filter isnotnull(img_url#2025)\n",
      "         +- *(2) GlobalLimit 1\n",
      "            +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [id=#1355]\n",
      "               +- *(1) LocalLimit 1\n",
      "                  +- *(1) Scan ExistingRDD[article_id#2024L,img_url#2025]\n",
      "\n",
      "\n",
      "+--------------------+----------+----------+\n",
      "|             img_url|article_id|article_id|\n",
      "+--------------------+----------+----------+\n",
      "|https://pic.pimg....|     14431|     14431|\n",
      "|https://pic.pimg....|     14431|      null|\n",
      "|https://pic.pimg....|     14431|      null|\n",
      "|https://pic.pimg....|     67789|      null|\n",
      "|https://pic.pimg....|     67789|      null|\n",
      "+--------------------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# broadcast join\n",
    "# https://www.youtube.com/watch?v=fp53QhSfQcI\n",
    "# if we perform broadcast join 14:50 (no shuffling)\n",
    "# if we DOESNT perform broad join 05:55(shuffling a lot)\n",
    "\n",
    "data = [\n",
    "    (14431,'https://pic.pimg.tw/happy78/1528543947-685380499_n.jpg'),\n",
    "    (14431,'https://pic.pimg.tw/happy78/1528543947-362759723_n.jpg'),\n",
    "    (14431,'https://pic.pimg.tw/happy78/1528543962-2265924582_n.jpg'),\n",
    "    (67789,'https://pic.pimg.tw/happy78/1528543962-4007835890_n.jpg'),\n",
    "    (67789,'https://pic.pimg.tw/happy78/1528543962-45890_n.jpg')\n",
    "]\n",
    "\n",
    "columns = ['article_id','img_url']\n",
    "\n",
    "df_big = spark.createDataFrame(data=data, schema=columns)\n",
    "df_big.show(n=5)\n",
    "\n",
    "df_small = df_big.limit(1)\n",
    "df_small.show(n=5)\n",
    "\n",
    "# NOTE, broadcast join support only left join\n",
    "df_joined = df_big.join(F.broadcast(df_small), on=['img_url'],how='left')\n",
    "df_joined.explain()\n",
    "df_joined.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:37:34.604695Z",
     "start_time": "2021-07-31T05:37:33.227422Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|             img_url|popularity|\n",
      "+--------------------+----------+\n",
      "|https://pic.pimg....|         4|\n",
      "|https://pic.pimg....|         5|\n",
      "|https://pic.pimg....|         1|\n",
      "|https://pic.pimg....|         9|\n",
      "|https://pic.pimg....|        10|\n",
      "+--------------------+----------+\n",
      "\n",
      "+----------+--------------------+\n",
      "|article_id|             img_url|\n",
      "+----------+--------------------+\n",
      "|     14431|https://pic.pimg....|\n",
      "|       888|https://pic.pimg....|\n",
      "+----------+--------------------+\n",
      "\n",
      "+--------------------+----------+----------+\n",
      "|             img_url|popularity|article_id|\n",
      "+--------------------+----------+----------+\n",
      "|https://pic.pimg....|         4|     14431|\n",
      "+--------------------+----------+----------+\n",
      "\n",
      "+--------------------+----------+----------+\n",
      "|             img_url|article_id|popularity|\n",
      "+--------------------+----------+----------+\n",
      "|https://pic.pimg....|     14431|         4|\n",
      "|https://pic.pimg....|       888|      null|\n",
      "+--------------------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# broadcast hash join but keet the right hand keys\n",
    "\n",
    "# val small_df = sc.parallelize(List((\"Alice\", 15), (\"Bob\", 20)).toDF(\"name\", \"age\")\n",
    "# val large_df = sc.parallelize((\"Bob\", 40), (\"SomeOne\", 50)).toDF(\"name\", \"age\")\n",
    "\n",
    "# val df = large_df.withColumnRenamed(\"age\", \"large_age\").join(broadcast(small_df), Array(\"name\"), \"right_outer\")\n",
    "# val df2 = df.withColumn(\"age\", when($\"large_age\".isNotNull, $\"age\" + $\"large_age\").otherwise($\"age\")).select(\"name\", \"age\")\n",
    "# df2.show\n",
    "\n",
    "\n",
    "\n",
    "big = [\n",
    "    (14431,'https://pic.pimg.tw/happy78/1528543947-685380499_n.jpg','4'),\n",
    "    (14431,'https://pic.pimg.tw/happy78/1528543947-362759723_n.jpg','5'),\n",
    "    (14431,'https://pic.pimg.tw/happy78/1528543962-2265924582_n.jpg','1'),\n",
    "    (67789,'https://pic.pimg.tw/happy78/1528543962-4007835890_n.jpg','9'),\n",
    "    (67789,'https://pic.pimg.tw/happy78/1528543962-45890_n.jpg','10')\n",
    "]\n",
    "\n",
    "small =  [\n",
    "    (14431,'https://pic.pimg.tw/happy78/1528543947-685380499_n.jpg'),\n",
    "    (888,'https://pic.pimg.tw/happy78/6666_n.jpg'),\n",
    "    ]\n",
    "\n",
    "\n",
    "columns = ['article_id','img_url']\n",
    "\n",
    "df_big = (\n",
    "    spark.createDataFrame(data=big, schema=[*columns,'popularity'])\n",
    "    .select('img_url','popularity')\n",
    ")\n",
    "df_big.show(n=5)\n",
    "\n",
    "df_small = (\n",
    "    spark.createDataFrame(data=small, schema=columns)\n",
    ")\n",
    "df_small.show(n=5)\n",
    "\n",
    "# NOTE, broadcast join support only left join\n",
    "df_joined = (\n",
    "    df_big.join(F.broadcast(df_small), on=['img_url'],how='left')\n",
    "    .where(C(\"article_id\").isNotNull())\n",
    ")\n",
    "df_joined.show()\n",
    "\n",
    "# df2 = df.withColumn(\"age\",\n",
    "#                     when($\"large_age\".isNotNull, $\"age\" + $\"large_age\")\n",
    "#                     .otherwise($\"age\")).select(\"name\", \"age\")\n",
    "\n",
    "df_small_full = (\n",
    "    df_small\n",
    "    .join(df_joined.drop('article_id'),on=['img_url'],how='left')\n",
    ")\n",
    "\n",
    "df_small_full.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:37:35.595116Z",
     "start_time": "2021-07-31T05:37:34.606138Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------+----------+----------+\n",
      "|img_url                                               |article_id|popularity|\n",
      "+------------------------------------------------------+----------+----------+\n",
      "|https://pic.pimg.tw/happy78/1528543947-685380499_n.jpg|14431     |4         |\n",
      "|https://pic.pimg.tw/happy78/6666_n.jpg                |888       |null      |\n",
      "+------------------------------------------------------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# broadcast join and keep the right hand \n",
    "from typing import List\n",
    "\n",
    "def broadcast_right_join(\n",
    "    large_sdf : DataFrame,\n",
    "    small_sdf : DataFrame,\n",
    "    on : List[str],\n",
    "    force_broadcast : bool = False,\n",
    "    show_plan : bool = False):\n",
    "    \n",
    "    if force_broadcast:\n",
    "        small_sdf = F.broadcast(small_sdf)\n",
    "    \n",
    "    # we perform broadcast join at first pass\n",
    "    filtered_sdf = (\n",
    "        large_sdf\n",
    "        .join(small_sdf, on=on, how='left_semi')\n",
    "    )\n",
    "    \n",
    "    # Since we filter the large_sdf by small_sdf\n",
    "    # the filtered_sdf become small\n",
    "    # now we get all the rows in small_sdf with related rows in large_sdf\n",
    "    full_info_small_sdf = (\n",
    "        small_sdf\n",
    "        .join(\n",
    "            filtered_sdf,\n",
    "            on=on, how='left')\n",
    "    )\n",
    "    if show_plan:\n",
    "        print('first pass : ', filtered_sdf.explain())\n",
    "    return full_info_small_sdf\n",
    "\n",
    "df_small_full = broadcast_right_join(df_big, df_small,['img_url'], force_broadcast=False)\n",
    "df_small_full.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:37:36.358022Z",
     "start_time": "2021-07-31T05:37:35.596680Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n",
      "|article_id|             img_url|\n",
      "+----------+--------------------+\n",
      "|     14431|https://pic.pimg....|\n",
      "|     14431|https://pic.pimg....|\n",
      "|     14431|https://pic.pimg....|\n",
      "|     67789|https://pic.pimg....|\n",
      "|     67789|https://pic.pimg....|\n",
      "+----------+--------------------+\n",
      "\n",
      "+----------+--------------------+\n",
      "|article_id|             img_url|\n",
      "+----------+--------------------+\n",
      "|     14431|https://pic.pimg....|\n",
      "+----------+--------------------+\n",
      "\n",
      "left_semi : inner join but return only left dataframe column\n",
      "+--------------------+----------+\n",
      "|             img_url|article_id|\n",
      "+--------------------+----------+\n",
      "|https://pic.pimg....|     14431|\n",
      "+--------------------+----------+\n",
      "\n",
      "left_anti : difference (left - right) but return only left dataframe column\n",
      "+--------------------+----------+\n",
      "|             img_url|article_id|\n",
      "+--------------------+----------+\n",
      "|https://pic.pimg....|     14431|\n",
      "|https://pic.pimg....|     14431|\n",
      "|https://pic.pimg....|     67789|\n",
      "|https://pic.pimg....|     67789|\n",
      "+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# left_anti and ledt_semi\n",
    "# https://dzone.com/articles/pyspark-join-explained-with-examples\n",
    "\n",
    "\n",
    "data = [\n",
    "    (14431,'https://pic.pimg.tw/happy78/1528543947-685380499_n.jpg'),\n",
    "    (14431,'https://pic.pimg.tw/happy78/1528543947-362759723_n.jpg'),\n",
    "    (14431,'https://pic.pimg.tw/happy78/1528543962-2265924582_n.jpg'),\n",
    "    (67789,'https://pic.pimg.tw/happy78/1528543962-4007835890_n.jpg'),\n",
    "    (67789,'https://pic.pimg.tw/happy78/1528543962-45890_n.jpg')\n",
    "]\n",
    "\n",
    "columns = ['article_id','img_url']\n",
    "\n",
    "df_big = spark.createDataFrame(data=data, schema=columns)\n",
    "df_big.show(n=5)\n",
    "\n",
    "df_small = df_big.limit(1)\n",
    "df_small.show(n=5)\n",
    "\n",
    "print('left_semi : inner join but return only left dataframe column')\n",
    "\n",
    "df_left_semi = df_big.join(df_small,on=['img_url'],how='left_semi')\n",
    "df_left_semi.show()\n",
    "\n",
    "print('left_anti : difference (left - right) but return only left dataframe column')\n",
    "\n",
    "df_left_anti = df_big.join(df_small,on=['img_url'],how='left_anti')\n",
    "df_left_anti.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregation Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:37:36.713684Z",
     "start_time": "2021-07-31T05:37:36.361218Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+------------------------+---------------+---------+\n",
      "|store_name|food_category|food_category_popularity|        img_url|author_id|\n",
      "+----------+-------------+------------------------+---------------+---------+\n",
      "|    hotpop|         Meat|                       3|https//:123.png|   rtyg11|\n",
      "+----------+-------------+------------------------+---------------+---------+\n",
      "\n",
      "== Physical Plan ==\n",
      "*(3) Project [img_url#2175, store_name#2172, food_category#2173, food_category_popularity#2174L, author_id#2176, store_name#2203, food_category#2204, food_category_popularity#2205L, author_id#2207]\n",
      "+- *(3) BroadcastHashJoin [img_url#2175], [img_url#2206], Inner, BuildRight, false\n",
      "   :- *(3) Filter isnotnull(img_url#2175)\n",
      "   :  +- *(3) Scan ExistingRDD[store_name#2172,food_category#2173,food_category_popularity#2174L,img_url#2175,author_id#2176]\n",
      "   +- BroadcastExchange HashedRelationBroadcastMode(List(input[3, string, false]),false), [id=#1850]\n",
      "      +- *(2) Filter isnotnull(img_url#2206)\n",
      "         +- *(2) GlobalLimit 1\n",
      "            +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [id=#1845]\n",
      "               +- *(1) LocalLimit 1\n",
      "                  +- *(1) Scan ExistingRDD[store_name#2203,food_category#2204,food_category_popularity#2205L,img_url#2206,author_id#2207]\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_url</th>\n",
       "      <th>store_name</th>\n",
       "      <th>food_category</th>\n",
       "      <th>food_category_popularity</th>\n",
       "      <th>author_id</th>\n",
       "      <th>store_name</th>\n",
       "      <th>food_category</th>\n",
       "      <th>food_category_popularity</th>\n",
       "      <th>author_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https//:123.png</td>\n",
       "      <td>hotpop</td>\n",
       "      <td>Meat</td>\n",
       "      <td>3</td>\n",
       "      <td>rtyg11</td>\n",
       "      <td>hotpop</td>\n",
       "      <td>Meat</td>\n",
       "      <td>3</td>\n",
       "      <td>rtyg11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           img_url store_name food_category  food_category_popularity  \\\n",
       "0  https//:123.png     hotpop          Meat                         3   \n",
       "\n",
       "  author_id store_name food_category  food_category_popularity author_id  \n",
       "0    rtyg11     hotpop          Meat                         3    rtyg11  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2 broadcast join\n",
    "# boardcast join for big dataframe and small data frame join\n",
    "# broadcast small dataframe to each worker,\n",
    "# them the excute plan make narrow dependency instead of wide dependency\n",
    "# code - https://stackoverflow.com/questions/37487318/spark-sql-broadcast-hash-join\n",
    "# documentation - search broadcast join in doc https://spark.apache.org/docs/2.3.2/api/python/pyspark.sql.html\n",
    "# concept https://www.youtube.com/watch?v=fp53QhSfQcI 14:32 ~ 14:59\n",
    "data = [\n",
    "    (\"hotpop\",\"Meat\",3,\"https//:123.png\",\"rtyg11\"),\n",
    "    (\"hotpop\",\"Meat\",3,\"https//:456.png\",\"rtyg11\"),\n",
    "    (\"hotpop\",\"Meat\",3,\"https//:789.png\",\"rtyg11\"),\n",
    "    (\"hotpop\",\"Vegetable\",2,\"https//:111.png\",\"rtyg11\"),\n",
    "    (\"hotpop\",\"Vegetable\",2,\"https//:222.png\",\"rtyg11\"),\n",
    "    (\"branch\",\"Fried food\",1,\"https//:333.png\",\"bvc1\"),\n",
    "    (\"branch\",\"Dessert\",1,\"https//:444.png\",\"7854\"),\n",
    "  ]\n",
    "\n",
    "columns = [\"store_name\",\"food_category\",\"food_category_popularity\",\"img_url\",\"author_id\"]\n",
    "df_big = spark.createDataFrame(data = data, schema = columns)\n",
    "\n",
    "df_small = df_big.limit(1)\n",
    "\n",
    "df_small.show()\n",
    "\n",
    "df_broadcast_join = (\n",
    " df_big\n",
    "    .join(F.broadcast(df_small), on=\"img_url\")\n",
    ")\n",
    "\n",
    "df_broadcast_join.explain()\n",
    "\n",
    "df_broadcast_join.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:37:38.797009Z",
     "start_time": "2021-07-31T05:37:36.715000Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|col1|co12|\n",
      "+----+----+\n",
      "|   A|   1|\n",
      "|   B|   2|\n",
      "|   C|   3|\n",
      "+----+----+\n",
      "\n",
      "+----+----+\n",
      "|col1|co12|\n",
      "+----+----+\n",
      "|   A|  20|\n",
      "|   Y|  30|\n",
      "|   Z|  50|\n",
      "+----+----+\n",
      "\n",
      "inner - inner join and keeps columns with two tables\n",
      "which will be annoying if there are same column name or dropping column by yourself\n",
      "+----+----+----+\n",
      "|col1|co12|co12|\n",
      "+----+----+----+\n",
      "|   A|   1|  20|\n",
      "+----+----+----+\n",
      "\n",
      "left_semi - inner join but only keeps left table columns\n",
      "+----+----+\n",
      "|col1|co12|\n",
      "+----+----+\n",
      "|   A|   1|\n",
      "+----+----+\n",
      "\n",
      "left_anti - selects all rows from left that are not present in right.\n",
      "+----+----+\n",
      "|col1|co12|\n",
      "+----+----+\n",
      "|   B|   2|\n",
      "|   C|   3|\n",
      "+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3 left semi, left anti\n",
    "# right semi, right anti\n",
    "# https://dzone.com/articles/pyspark-join-explained-with-examples\n",
    "\n",
    "left = spark.createDataFrame(\n",
    "    [\n",
    "        (\"A\", 1),\n",
    "        (\"B\", 2),\n",
    "        (\"C\", 3)\n",
    "    ],\n",
    "    (\"col1\",\"co12\")\n",
    ")\n",
    "\n",
    "right = spark.createDataFrame(\n",
    "    [\n",
    "        (\"A\", 20),\n",
    "        (\"Y\", 30),\n",
    "        (\"Z\", 50)\n",
    "    ],\n",
    "    (\"col1\",\"co12\")\n",
    ")\n",
    "\n",
    "left.show()\n",
    "right.show()\n",
    "\n",
    "print('inner - inner join and keeps columns with two tables')\n",
    "print('which will be annoying if there are same column name or dropping column by yourself')\n",
    "\n",
    "left.join(right, on='col1',how='inner').show()\n",
    "\n",
    "print('left_semi - inner join but only keeps left table columns')\n",
    "\n",
    "left.join(right, on='col1',how='left_semi').show()\n",
    "\n",
    "print('left_anti - selects all rows from left that are not present in right.')\n",
    "\n",
    "left.join(right, on='col1',how='left_anti').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregation (12+)\n",
    "\n",
    "## Simple GroupBy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:37:38.834661Z",
     "start_time": "2021-07-31T05:37:38.798625Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.group.GroupedData'>\n",
      "\n",
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_df', '_jgd', 'agg', 'apply', 'applyInPandas', 'avg', 'cogroup', 'count', 'max', 'mean', 'min', 'pivot', 'sql_ctx', 'sum']\n"
     ]
    }
   ],
   "source": [
    "# 1 knowing the groupby object method\n",
    "\n",
    "data = [\n",
    "    (\"James\",\"\",\"Smith\",\"36636\",\"RD\",\"M\", 3000),\n",
    "    (\"Michael\",\"Rose\",\"\",\"40288\",\"RD\",\"M\", 4000),\n",
    "    (\"Robert\",\"\",\"Williams\",\"42114\",\"SRE\",\"M\", 4000),\n",
    "    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"SRE\",\"F\", 4000),\n",
    "    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",\"BACKEND\", -1)\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"firstname\",StringType(), True), # Nullable True\n",
    "    StructField(\"middlename\",StringType(), True),\n",
    "    StructField(\"lastname\",StringType(), True),\n",
    "    StructField(\"id\",StringType(), True),\n",
    "    StructField(\"apartment\",StringType(), True),\n",
    "    StructField(\"gender\",StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True)\n",
    "    ])\n",
    "df = spark.createDataFrame(data=data,schema=schema)\n",
    "\n",
    "df_grp = df.groupBy(\"salary\")\n",
    "\n",
    "print(type(df_grp), dir(df_grp), sep='\\n\\n')\n",
    "\n",
    "# avg, count, max, mean, sum  - Common aggregation\n",
    "# pivot - two column x, y with value in the table\n",
    "# sql_ctx - apply sql command\n",
    "# custom function - agg, apply\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:37:39.828170Z",
     "start_time": "2021-07-31T05:37:38.836488Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|deparment|avg(salary)|\n",
      "+---------+-----------+\n",
      "|        F|       -1.0|\n",
      "|       RD|     3500.0|\n",
      "|      SRE|     4000.0|\n",
      "+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2 apply single aggregation fuction on groupby object\n",
    "data = [\n",
    "    (\"James\",\"\",\"Smith\",\"36636\",\"RD\",\"M\", 3000),\n",
    "    (\"Michael\",\"Rose\",\"\",\"40288\",\"RD\",\"M\", 4000),\n",
    "    (\"Robert\",\"\",\"Williams\",\"42114\",\"SRE\",\"M\", 4000),\n",
    "    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"SRE\",\"F\", 4000),\n",
    "    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",\"BACKEND\", -1)\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"firstname\",StringType(), True), # Nullable True\n",
    "    StructField(\"middlename\",StringType(), True),\n",
    "    StructField(\"lastname\",StringType(), True),\n",
    "    StructField(\"id\",StringType(), True),\n",
    "    StructField(\"deparment\",StringType(), True),\n",
    "    StructField(\"gender\",StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True)\n",
    "    ])\n",
    "df = spark.createDataFrame(data=data,schema=schema)\n",
    "\n",
    "df_grp_department = (\n",
    "    df.groupby(\"deparment\").mean(\"salary\").alias(\"mean_salary\")\n",
    ")\n",
    "\n",
    "df_grp_department.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:37:40.418663Z",
     "start_time": "2021-07-31T05:37:39.829431Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+----------+----------+----------+----------+\n",
      "|deparment|sum_salary|avg_salary|max_salary|min_salary|group_size|\n",
      "+---------+----------+----------+----------+----------+----------+\n",
      "|        F|        -1|      -1.0|        -1|        -1|         1|\n",
      "|       RD|      7000|    3500.0|      4000|      3000|         2|\n",
      "|      SRE|      8000|    4000.0|      4000|      4000|         2|\n",
      "+---------+----------+----------+----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3 apply multiple aggregation fuction on groupby object\n",
    "data = [\n",
    "    (\"James\",\"\",\"Smith\",\"36636\",\"RD\",\"M\", 3000),\n",
    "    (\"Michael\",\"Rose\",\"\",\"40288\",\"RD\",\"M\", 4000),\n",
    "    (\"Robert\",\"\",\"Williams\",\"42114\",\"SRE\",\"M\", 4000),\n",
    "    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"SRE\",\"F\", 4000),\n",
    "    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",\"BACKEND\", -1)\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"firstname\",StringType(), True), # Nullable True\n",
    "    StructField(\"middlename\",StringType(), True),\n",
    "    StructField(\"lastname\",StringType(), True),\n",
    "    StructField(\"id\",StringType(), True),\n",
    "    StructField(\"deparment\",StringType(), True),\n",
    "    StructField(\"gender\",StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True)\n",
    "    ])\n",
    "df = spark.createDataFrame(data=data,schema=schema)\n",
    "\n",
    "df_grp_department = df.groupby(\"deparment\").agg(\n",
    "    F.sum(\"salary\").alias(\"sum_salary\"),\n",
    "    F.avg(\"salary\").alias(\"avg_salary\"),\n",
    "    F.max(\"salary\").alias(\"max_salary\"),\n",
    "    F.min(\"salary\").alias(\"min_salary\"),\n",
    "    F.count(\"salary\").alias(\"group_size\")\n",
    ")\n",
    "\n",
    "df_grp_department.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:37:40.847412Z",
     "start_time": "2021-07-31T05:37:40.420683Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>deparment</th>\n",
       "      <th>sum_salary</th>\n",
       "      <th>avg_salary</th>\n",
       "      <th>max_salary</th>\n",
       "      <th>min_salary</th>\n",
       "      <th>count_rows</th>\n",
       "      <th>all_rows</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>F</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>[-1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RD</td>\n",
       "      <td>7000</td>\n",
       "      <td>3500.0</td>\n",
       "      <td>4000</td>\n",
       "      <td>3000</td>\n",
       "      <td>2</td>\n",
       "      <td>[3000, 4000]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SRE</td>\n",
       "      <td>8000</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>4000</td>\n",
       "      <td>4000</td>\n",
       "      <td>2</td>\n",
       "      <td>[4000, 4000]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  deparment  sum_salary  avg_salary  max_salary  min_salary  count_rows  \\\n",
       "0         F          -1        -1.0          -1          -1           1   \n",
       "1        RD        7000      3500.0        4000        3000           2   \n",
       "2       SRE        8000      4000.0        4000        4000           2   \n",
       "\n",
       "       all_rows  \n",
       "0          [-1]  \n",
       "1  [3000, 4000]  \n",
       "2  [4000, 4000]  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4 collect data point for each group with the stats(min, max, sum, avg, count)\n",
    "\n",
    "\n",
    "# apply multiple aggregation fuction on groupby object\n",
    "data = [\n",
    "    (\"James\",\"\",\"Smith\",\"36636\",\"RD\",\"M\", 3000),\n",
    "    (\"Michael\",\"Rose\",\"\",\"40288\",\"RD\",\"M\", 4000),\n",
    "    (\"Robert\",\"\",\"Williams\",\"42114\",\"SRE\",\"M\", 4000),\n",
    "    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"SRE\",\"F\", 4000),\n",
    "    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",\"BACKEND\", -1)\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"firstname\",StringType(), True), # Nullable True\n",
    "    StructField(\"middlename\",StringType(), True),\n",
    "    StructField(\"lastname\",StringType(), True),\n",
    "    StructField(\"id\",StringType(), True),\n",
    "    StructField(\"deparment\",StringType(), True),\n",
    "    StructField(\"gender\",StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True)\n",
    "    ])\n",
    "df = spark.createDataFrame(data=data,schema=schema)\n",
    "\n",
    "df_grp_department = df.groupby(\"deparment\").agg(\n",
    "    F.sum(\"salary\").alias(\"sum_salary\"),\n",
    "    F.avg(\"salary\").alias(\"avg_salary\"),\n",
    "    F.max(\"salary\").alias(\"max_salary\"),\n",
    "    F.min(\"salary\").alias(\"min_salary\"),\n",
    "    F.count(\"salary\").alias(\"count_rows\"),\n",
    "    F.collect_list(\"salary\").alias(\"all_rows\")\n",
    ")\n",
    "\n",
    "df_grp_department.toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:37:41.473802Z",
     "start_time": "2021-07-31T05:37:40.849003Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+-----+---------+-------+------+\n",
      "|firstname|middlename|lastname|   id|deparment| gender|salary|\n",
      "+---------+----------+--------+-----+---------+-------+------+\n",
      "|      Jen|      Mary|   Brown|     |        F|BACKEND|    -1|\n",
      "|  Michael|      Rose|        |40288|       RD|      M|  8000|\n",
      "|    Maria|      Anne|   Jones|39192|      SRE|      F|  6000|\n",
      "+---------+----------+--------+-----+---------+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5 get first one row in each group\n",
    "# We use Window Function here\n",
    "# Key to think about this, we rank the data in each group, then \n",
    "# filtering\n",
    "# no nothing is groupby\n",
    "# which is different in pandas\n",
    "\n",
    "data = [\n",
    "    (\"James\",\"\",\"Smith\",\"36636\",\"RD\",\"M\", 3000),\n",
    "    (\"Michael\",\"Rose\",\"\",\"40288\",\"RD\",\"M\", 8000),\n",
    "    (\"Robert\",\"\",\"Williams\",\"42114\",\"SRE\",\"M\", 4000),\n",
    "    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"SRE\",\"F\", 6000),\n",
    "    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",\"BACKEND\", -1)\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"firstname\",StringType(), True), # Nullable True\n",
    "    StructField(\"middlename\",StringType(), True),\n",
    "    StructField(\"lastname\",StringType(), True),\n",
    "    StructField(\"id\",StringType(), True),\n",
    "    StructField(\"deparment\",StringType(), True),\n",
    "    StructField(\"gender\",StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True)\n",
    "    ])\n",
    "\n",
    "\n",
    "df = spark.createDataFrame(data=data,schema=schema)\n",
    "\n",
    "df = (\n",
    "    df.withColumn(\"rank_salary_by_deparment\",\n",
    "                  F.row_number().over(\n",
    "                  W.partitionBy(\"deparment\")\\\n",
    "                      .orderBy(F.desc(\"salary\"))\n",
    "                  )\n",
    "                 )\\\n",
    "    .filter(F.col(\"rank_salary_by_deparment\") == 1)\\\n",
    "    .drop('rank_salary_by_deparment')\n",
    ")\n",
    "\n",
    "df.show(n=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:37:42.070990Z",
     "start_time": "2021-07-31T05:37:41.475458Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+----------+----------+----------+----------+\n",
      "|deparment|sum_salary|avg_salary|max_salary|min_salary|count_rows|\n",
      "+---------+----------+----------+----------+----------+----------+\n",
      "|       RD|      7000|    3500.0|      4000|      3000|         2|\n",
      "|      SRE|      8000|    4000.0|      4000|      4000|         2|\n",
      "+---------+----------+----------+----------+----------+----------+\n",
      "\n",
      "root\n",
      " |-- deparment: string (nullable = true)\n",
      " |-- sum_salary: long (nullable = true)\n",
      " |-- avg_salary: double (nullable = true)\n",
      " |-- max_salary: integer (nullable = true)\n",
      " |-- min_salary: integer (nullable = true)\n",
      " |-- count_rows: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6 groupby and filtering\n",
    "\n",
    "data = [\n",
    "    (\"James\",\"\",\"Smith\",\"36636\",\"RD\",\"M\", 3000),\n",
    "    (\"Michael\",\"Rose\",\"\",\"40288\",\"RD\",\"M\", 4000),\n",
    "    (\"Robert\",\"\",\"Williams\",\"42114\",\"SRE\",\"M\", 4000),\n",
    "    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"SRE\",\"F\", 4000),\n",
    "    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",\"BACKEND\", -1)\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"firstname\",StringType(), True), # Nullable True\n",
    "    StructField(\"middlename\",StringType(), True),\n",
    "    StructField(\"lastname\",StringType(), True),\n",
    "    StructField(\"id\",StringType(), True),\n",
    "    StructField(\"deparment\",StringType(), True),\n",
    "    StructField(\"gender\",StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True)\n",
    "    ])\n",
    "df = spark.createDataFrame(data=data,schema=schema)\n",
    "\n",
    "df_grp_department = (\n",
    "    df.groupby(\"deparment\").agg(\n",
    "        F.sum(\"salary\").alias(\"sum_salary\"),\n",
    "        F.avg(\"salary\").alias(\"avg_salary\"),\n",
    "        F.max(\"salary\").alias(\"max_salary\"),\n",
    "        F.min(\"salary\").alias(\"min_salary\"),\n",
    "        F.count(\"salary\").alias(\"count_rows\"))\n",
    "    .filter(C(\"sum_salary\") > 0)\n",
    "    \n",
    ")\n",
    "\n",
    "df_grp_department.show(n=5)\n",
    "df_grp_department.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:37:42.968617Z",
     "start_time": "2021-07-31T05:37:42.073576Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|item|score|\n",
      "+----+-----+\n",
      "|   a|   10|\n",
      "|   a|   10|\n",
      "|   a|   15|\n",
      "|   a|   20|\n",
      "|   a|   20|\n",
      "+----+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----+-----+----+----------+----------+\n",
      "|item|score|rank|dense_rank|row_number|\n",
      "+----+-----+----+----------+----------+\n",
      "|   a|   20|   1|         1|         1|\n",
      "|   a|   20|   1|         1|         2|\n",
      "|   a|   20|   1|         1|         3|\n",
      "|   a|   15|   4|         2|         4|\n",
      "|   a|   10|   5|         3|         5|\n",
      "|   a|   10|   5|         3|         6|\n",
      "|   a|    5|   7|         4|         7|\n",
      "|   a|    0|   8|         5|         8|\n",
      "|   a|    0|   8|         5|         9|\n",
      "|   a|    0|   8|         5|        10|\n",
      "|   a|    0|   8|         5|        11|\n",
      "|   a|    0|   8|         5|        12|\n",
      "+----+-----+----+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 7 rank, dense_rank, and row_number\n",
    "# https://stackoverflow.com/questions/44968912/difference-in-dense-rank-and-row-number-in-spark\n",
    "# The window functions\n",
    "\n",
    "data = [\n",
    "    (\"a\",10),\n",
    "    ('a',10),\n",
    "    ('a',15),\n",
    "    ('a',20),\n",
    "    ('a',20),\n",
    "    ('a',20),\n",
    "    ('a',5),\n",
    "    ('a',0),\n",
    "    ('a',0),\n",
    "    ('a',0),\n",
    "    ('a',0),\n",
    "    ('a',0)\n",
    "]\n",
    "\n",
    "columns = ['item','score']\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show(n=5)\n",
    "\n",
    "window_spec = W.partitionBy(\"item\").orderBy(C(\"score\").desc())\n",
    "df = (\n",
    "    df.withColumn(\"rank\", F.rank().over(window_spec))\\\n",
    "    .withColumn(\"dense_rank\", F.dense_rank().over(window_spec))\\\n",
    "    .withColumn(\"row_number\", F.row_number().over(window_spec))\n",
    ")\n",
    "\n",
    "df.show(n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:37:43.601071Z",
     "start_time": "2021-07-31T05:37:42.970481Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-----+--------------+\n",
      "|store_name|food_category|  url|food_cat_count|\n",
      "+----------+-------------+-----+--------------+\n",
      "|    branch|      Dessert|ulr_7|             1|\n",
      "|    hotpop|         Meat|url_1|             3|\n",
      "|    hotpop|         Meat|url_2|             3|\n",
      "|    hotpop|         Meat|url_3|             3|\n",
      "|    hotpop|    Vegetable|url_4|             2|\n",
      "|    hotpop|    Vegetable|url_5|             2|\n",
      "|    branch|   Fried food|url_6|             1|\n",
      "+----------+-------------+-----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 11 groupby and sum by a window function\n",
    "data = [\n",
    "    (\"hotpop\",\"Meat\",\"url_1\",),\n",
    "    (\"hotpop\",\"Meat\",\"url_2\"),\n",
    "    (\"hotpop\",\"Meat\",\"url_3\"),\n",
    "    (\"hotpop\",\"Vegetable\",\"url_4\"),\n",
    "    (\"hotpop\",\"Vegetable\",\"url_5\"),\n",
    "    (\"branch\",\"Fried food\",\"url_6\"),\n",
    "    (\"branch\",\"Dessert\",\"ulr_7\"),\n",
    "  ]\n",
    "\n",
    "columns = [\"store_name\",\"food_category\",\"url\"]\n",
    "window_spec = w.partitionBy(\"store_name\",\"food_category\")\n",
    "\n",
    "df = spark.createDataFrame(data = data, schema = columns)\n",
    "df = (\n",
    "    df.withColumn(\"food_cat_count\", F.count(\"food_category\").over(window_spec))\n",
    ")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:37:44.274979Z",
     "start_time": "2021-07-31T05:37:43.603208Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+-------+\n",
      "|time|value|class|cum_sum|\n",
      "+----+-----+-----+-------+\n",
      "|   1|    3|    b|      3|\n",
      "|   2|    3|    b|      6|\n",
      "|   1|    2|    a|      2|\n",
      "|   2|    2|    a|      4|\n",
      "|   3|    2|    a|      6|\n",
      "+----+-----+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# cumsum by windown function\n",
    "\n",
    "\n",
    "df = spark.createDataFrame(\n",
    "    [(1,2,\"a\"),(3,2,\"a\"),(1,3,\"b\"),(2,2,\"a\"),(2,3,\"b\")], \n",
    "    [\"time\", \"value\", \"class\"] )\n",
    "\n",
    "windowval = (\n",
    "    W.partitionBy('class').orderBy('time')\n",
    "     .rangeBetween(W.unboundedPreceding, 0)\n",
    ")\n",
    "df_w_cumsum = df.withColumn('cum_sum', F.sum('value').over(windowval))\n",
    "df_w_cumsum.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:37:45.618378Z",
     "start_time": "2021-07-31T05:37:44.281821Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+-----+----------+-------+------+\n",
      "|firstname|middlename|lastname|   id|department| gender|salary|\n",
      "+---------+----------+--------+-----+----------+-------+------+\n",
      "|    James|          |   Smith|36636|        RD|      M|  3000|\n",
      "|  Michael|      Rose|        |40288|        RD|      M|  4000|\n",
      "|   Robert|          |Williams|42114|       SRE|      M|  4000|\n",
      "|    Maria|      Anne|   Jones|39192|       SRE|      F|  4000|\n",
      "|      Jen|      Mary|   Brown|     |         F|BACKEND|    -1|\n",
      "+---------+----------+--------+-----+----------+-------+------+\n",
      "\n",
      "+----------+-------+----+----+\n",
      "|department|BACKEND|   F|   M|\n",
      "+----------+-------+----+----+\n",
      "|         F|     -1|null|null|\n",
      "|        RD|   null|null|7000|\n",
      "|       SRE|   null|4000|4000|\n",
      "+----------+-------+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 12 pivot the datafrmae when groupby\n",
    "\n",
    "data = [\n",
    "    (\"James\",\"\",\"Smith\",\"36636\",\"RD\",\"M\", 3000),\n",
    "    (\"Michael\",\"Rose\",\"\",\"40288\",\"RD\",\"M\", 4000),\n",
    "    (\"Robert\",\"\",\"Williams\",\"42114\",\"SRE\",\"M\", 4000),\n",
    "    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"SRE\",\"F\", 4000),\n",
    "    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",\"BACKEND\", -1)\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"firstname\",StringType(), True), # Nullable True\n",
    "    StructField(\"middlename\",StringType(), True),\n",
    "    StructField(\"lastname\",StringType(), True),\n",
    "    StructField(\"id\",StringType(), True),\n",
    "    StructField(\"department\",StringType(), True),\n",
    "    StructField(\"gender\",StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True)\n",
    "    ])\n",
    "df = spark.createDataFrame(data=data,schema=schema)\n",
    "\n",
    "df.show()\n",
    "\n",
    "\n",
    "# groupBy as index\n",
    "# pivot as column\n",
    "\n",
    "(\n",
    "    df.groupBy(\"department\").pivot(\"gender\")\n",
    "      .sum(\"salary\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:37:49.396726Z",
     "start_time": "2021-07-31T05:37:45.621306Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----+------+----+\n",
      "| id|type|cost|  date|ship|\n",
      "+---+----+----+------+----+\n",
      "|  0|   A| 223|201603|PORT|\n",
      "|  0|   A|  22|201602|PORT|\n",
      "|  0|   A| 422|201601|DOCK|\n",
      "|  1|   B|3213|201602|DOCK|\n",
      "|  1|   B|3213|201601|PORT|\n",
      "|  2|   C|2321|201601|DOCK|\n",
      "+---+----+----+------+----+\n",
      "\n",
      "+---+----+------+------+------+\n",
      "| id|type|201601|201602|201603|\n",
      "+---+----+------+------+------+\n",
      "|  1|   B|3213.0|3213.0|  null|\n",
      "|  2|   C|2321.0|  null|  null|\n",
      "|  0|   A| 422.0|  22.0| 223.0|\n",
      "+---+----+------+------+------+\n",
      "\n",
      "+---+----+------+------+------+\n",
      "| id|type|201601|201602|201603|\n",
      "+---+----+------+------+------+\n",
      "|  1|   B|  PORT|  DOCK|  null|\n",
      "|  2|   C|  DOCK|  null|  null|\n",
      "|  0|   A|  DOCK|  PORT|  PORT|\n",
      "+---+----+------+------+------+\n",
      "\n",
      "+---+----+---------+---------+---------+\n",
      "| id|type|   201601|   201602|   201603|\n",
      "+---+----+---------+---------+---------+\n",
      "|  1|   B|{1, PORT}|{1, DOCK}|     null|\n",
      "|  2|   C|{1, DOCK}|     null|     null|\n",
      "|  0|   A|{1, DOCK}|{1, PORT}|{1, PORT}|\n",
      "+---+----+---------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pivot the non-numerical column\n",
    "\n",
    "\n",
    "rdd = spark.sparkContext.parallelize(\n",
    "    [\n",
    "        (0, \"A\", 223,\"201603\", \"PORT\"), \n",
    "        (0, \"A\", 22,\"201602\", \"PORT\"), \n",
    "        (0, \"A\", 422,\"201601\", \"DOCK\"), \n",
    "        (1,\"B\", 3213,\"201602\", \"DOCK\"), \n",
    "        (1,\"B\", 3213,\"201601\", \"PORT\"), \n",
    "        (2,\"C\", 2321,\"201601\", \"DOCK\")\n",
    "    ]\n",
    ")\n",
    "df_data = spark.createDataFrame(rdd, [\"id\",\"type\", \"cost\", \"date\", \"ship\"])\n",
    "\n",
    "df_data.show()\n",
    "\n",
    "\n",
    "# numerical\n",
    "df_data.groupby(df_data.id, df_data.type).pivot(\"date\").avg(\"cost\").show()\n",
    "\n",
    "# non-numerical\n",
    "\n",
    "(df_data\n",
    "    .groupby(df_data.id, df_data.type)\n",
    "    .pivot(\"date\")\n",
    "    .agg(F.first(\"ship\"))\n",
    "    .show())\n",
    "\n",
    "# perform another aggregation\n",
    "(df_data\n",
    "    .groupby(\"id\", \"type\", \"date\", \"ship\")\n",
    "    .count()\n",
    "    .groupby(\"id\", \"type\")\n",
    "    .pivot(\"date\")\n",
    "    .agg(F.max(F.struct(\"count\", \"ship\")))\n",
    "    .show())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cube and rolling up\n",
    "\n",
    "https://stackoverflow.com/questions/37975227/what-is-the-difference-between-cube-rollup-and-groupby-operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:37:50.386234Z",
     "start_time": "2021-07-31T05:37:49.398149Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  x|  y|\n",
      "+---+---+\n",
      "|foo| 1L|\n",
      "|foo| 2L|\n",
      "|bar| 2L|\n",
      "|bar| 2L|\n",
      "+---+---+\n",
      "\n",
      "<class 'pyspark.sql.group.GroupedData'>\n",
      "+----+----+-----+\n",
      "|   x|   y|count|\n",
      "+----+----+-----+\n",
      "|null|  2L|    3|\n",
      "|null|  1L|    1|\n",
      "|null|null|    4|\n",
      "| foo|  2L|    1|\n",
      "| foo|  1L|    1|\n",
      "| bar|null|    2|\n",
      "| foo|null|    2|\n",
      "| bar|  2L|    2|\n",
      "+----+----+-----+\n",
      "\n",
      "+---+---+-----+\n",
      "|  x|  y|count|\n",
      "+---+---+-----+\n",
      "|foo| 2L|    1|\n",
      "|bar| 2L|    2|\n",
      "|foo| 1L|    1|\n",
      "+---+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# cube\n",
    "# cube is an extensin to groupBy\n",
    "# It takes a list of columns and applies aggregation expressions \n",
    "# to all possible combinations of thr grouping columns\n",
    "\n",
    "data = [\n",
    "    (\"foo\",\"1L\"),\n",
    "    (\"foo\",\"2L\"),\n",
    "    (\"bar\",\"2L\"),\n",
    "    (\"bar\",\"2L\"),\n",
    "]\n",
    "\n",
    "col = [\"x\", \"y\"]\n",
    "\n",
    "df = spark.createDataFrame(data, col)\n",
    "\n",
    "df.show()\n",
    "\n",
    "print(type(df.cube(\"x\",\"y\")))\n",
    "\n",
    "df.cube(\"x\",\"y\").count().show()\n",
    "\n",
    "\n",
    "# // +----+----+-----+     \n",
    "# // |   x|   y|count|\n",
    "# // +----+----+-----+\n",
    "# // |null|   1|    1|   <- count of records where y = 1\n",
    "# // |null|   2|    3|   <- count of records where y = 2\n",
    "# // | foo|null|    2|   <- count of records where x = foo\n",
    "# // | bar|   2|    2|   <- count of records where x = bar AND y = 2\n",
    "# // | foo|   1|    1|   <- count of records where x = foo AND y = 1\n",
    "# // | foo|   2|    1|   <- count of records where x = foo AND y = 2\n",
    "# // |null|null|    4|   <- total count of records\n",
    "# // | bar|null|    2|   <- count of records where x = bar\n",
    "# // +----+----+-----+\n",
    "\n",
    "\n",
    "# compare\n",
    "df.groupBy(\"x\",\"y\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:37:50.812510Z",
     "start_time": "2021-07-31T05:37:50.387688Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-----+\n",
      "|   x|   y|count|\n",
      "+----+----+-----+\n",
      "|null|null|    4|\n",
      "| foo|  2L|    1|\n",
      "| foo|  1L|    1|\n",
      "| bar|null|    2|\n",
      "| foo|null|    2|\n",
      "| bar|  2L|    2|\n",
      "+----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## rolling up\n",
    "\n",
    "df.rollup(\"x\",\"y\").count().show()\n",
    "\n",
    "\n",
    "# // +----+----+-----+\n",
    "# // |   x|   y|count|\n",
    "# // +----+----+-----+\n",
    "# // | foo|null|    2|   <- count where x is fixed to foo\n",
    "# // | bar|   2|    2|   <- count where x is fixed to bar and y is fixed to  2\n",
    "# // | foo|   1|    1|   ...\n",
    "# // | foo|   2|    1|   ...\n",
    "# // |null|null|    4|   <- count where no column is fixed\n",
    "# // | bar|null|    2|   <- count where x is fixed to bar\n",
    "# // +----+----+-----+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## melt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:37:51.229491Z",
     "start_time": "2021-07-31T05:37:50.815428Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------------------------------------------------------------------------\n",
      " poi_name           | 丹丹漢堡                                                               \n",
      " img_url            | https://pic.pimg.tw/ksdelicacy/1438450572-4270481322.jpg               \n",
      " food_cat_pop_score | 318                                                                    \n",
      " img_article_url    | http://ksdelicacy.pixnet.net/blog/post/55774905                        \n",
      " img_author_id      | ksdelicacy                                                             \n",
      " food_cat           | Fried food                                                             \n",
      "-RECORD 1------------------------------------------------------------------------------------\n",
      " poi_name           | 拿坡里                                                                 \n",
      " img_url            | https://rmfoodie.com/wp-content/uploads/rm/1446140572-1157042739_n.jpg \n",
      " food_cat_pop_score | 333                                                                    \n",
      " img_article_url    | http://rmlove30.pixnet.net/blog/post/61986505                          \n",
      " img_author_id      | RMlove30                                                               \n",
      " food_cat           | Bread                                                                  \n",
      "\n",
      "root\n",
      " |-- poi_name: string (nullable = true)\n",
      " |-- img_url: string (nullable = true)\n",
      " |-- food_cat_pop_score: string (nullable = true)\n",
      " |-- img_article_url: string (nullable = true)\n",
      " |-- img_author_id: string (nullable = true)\n",
      " |-- food_cat: string (nullable = true)\n",
      "\n",
      "-RECORD 0--------------------------\n",
      " poi_name   | 丹丹漢堡             \n",
      " menu_key   | img_url              \n",
      " menu_value | https://pic.pimg.... \n",
      "-RECORD 1--------------------------\n",
      " poi_name   | 丹丹漢堡             \n",
      " menu_key   | food_cat_pop_score   \n",
      " menu_value | 318                  \n",
      "-RECORD 2--------------------------\n",
      " poi_name   | 丹丹漢堡             \n",
      " menu_key   | img_article_url      \n",
      " menu_value | http://ksdelicacy... \n",
      "-RECORD 3--------------------------\n",
      " poi_name   | 丹丹漢堡             \n",
      " menu_key   | img_author_id        \n",
      " menu_value | ksdelicacy           \n",
      "-RECORD 4--------------------------\n",
      " poi_name   | 丹丹漢堡             \n",
      " menu_key   | food_cat             \n",
      " menu_value | Fried food           \n",
      "-RECORD 5--------------------------\n",
      " poi_name   | 拿坡里               \n",
      " menu_key   | img_url              \n",
      " menu_value | https://rmfoodie.... \n",
      "-RECORD 6--------------------------\n",
      " poi_name   | 拿坡里               \n",
      " menu_key   | food_cat_pop_score   \n",
      " menu_value | 333                  \n",
      "-RECORD 7--------------------------\n",
      " poi_name   | 拿坡里               \n",
      " menu_key   | img_article_url      \n",
      " menu_value | http://rmlove30.p... \n",
      "-RECORD 8--------------------------\n",
      " poi_name   | 拿坡里               \n",
      " menu_key   | img_author_id        \n",
      " menu_value | RMlove30             \n",
      "-RECORD 9--------------------------\n",
      " poi_name   | 拿坡里               \n",
      " menu_key   | food_cat             \n",
      " menu_value | Bread                \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 9 melt the dataframe (wide dataframe to long dataframe)\n",
    "from typing import Iterable\n",
    "import pandas as pd\n",
    "from IPython.core.display import display\n",
    "# https://stackoverflow.com/questions/41670103/how-to-melt-spark-dataframe\n",
    "\n",
    "\n",
    "def melt(\n",
    "        df: DataFrame, \n",
    "        id_vars: Iterable[str], value_vars: Iterable[str], \n",
    "        var_name: str=\"variable\", value_name: str=\"value\") -> DataFrame:\n",
    "    \"\"\"Convert :class:`DataFrame` from wide to long format.\"\"\"\n",
    "\n",
    "    value_names_dtype = dict(df.select(value_vars).dtypes)\n",
    "    unique_dtype = set(value_names_dtype.values())\n",
    "    assert len(unique_dtype) == 1, f\"value_vars should be the same dtype, your dype fo columns : {value_names_dtype}\"\n",
    "\n",
    "    # Create array<struct<variable: str, value: ...>>\n",
    "    _vars_and_vals = F.array(*(\n",
    "        F.struct(F.lit(c).alias(var_name), C(c).alias(value_name)) \n",
    "        for c in value_vars))\n",
    "\n",
    "    # Add to the DataFrame and explode\n",
    "    _tmp = df.withColumn(\"_vars_and_vals\", F.explode(_vars_and_vals))\n",
    "\n",
    "    cols = id_vars + [\n",
    "            C(\"_vars_and_vals\")[x].alias(x) for x in [var_name, value_name]]\n",
    "    return _tmp.select(*cols)\n",
    "\n",
    "# pdf = pd.DataFrame({'A': {0: 'a', 1: 'b', 2: 'c', 4 : 'a'},\n",
    "#                     'B': {0: 1, 1: 3, 2: 5, 4 : 11},\n",
    "#                     'C': {0: 2, 1: 4, 2: 6, 4 : 12}\n",
    "#                    })\n",
    "\n",
    "# pdf_result = pd.melt(pdf, id_vars=['A'], value_vars=['B', 'C']).sort_values(by=['A'])\n",
    "\n",
    "\n",
    "# display(\n",
    "#     \"Pandas\",\n",
    "#     pdf,\n",
    "#     pdf_result,\n",
    "#     \"PySpark\",\n",
    "#        )\n",
    "\n",
    "# # Case 1\n",
    "# # pdf['C'] = pdf['C'].astype(str) # then you can convert to spark df\n",
    "# sdf = spark.createDataFrame(pdf)\n",
    "# sdf.show()\n",
    "# sdf.printSchema()\n",
    "# melt(sdf, id_vars=['A'], value_vars=['B', 'C']).show()\n",
    "\n",
    "############### Case 2 ##############\n",
    "\n",
    "data = [\n",
    "    (\"丹丹漢堡\",                                                                \n",
    "     \"https://pic.pimg.tw/ksdelicacy/1438450572-4270481322.jpg\",               \n",
    "     \"318\",                                                                    \n",
    "     \"http://ksdelicacy.pixnet.net/blog/post/55774905\",\n",
    "     \"ksdelicacy\",                                                             \n",
    "     \"Fried food\"\n",
    "    ),\n",
    "    (\"拿坡里\"   ,                                                                 \n",
    "\"https://rmfoodie.com/wp-content/uploads/rm/1446140572-1157042739_n.jpg\",\n",
    "\"333\",                                                                    \n",
    "\"http://rmlove30.pixnet.net/blog/post/61986505\",                          \n",
    "\"RMlove30\",                                                               \n",
    "    \"Bread\")\n",
    "]\n",
    "\n",
    "cols = ['poi_name',\n",
    "        'img_url',\n",
    "        'food_cat_pop_score',\n",
    "        'img_article_url',\n",
    "        'img_author_id',\n",
    "        'food_cat']\n",
    "\n",
    "df = spark.createDataFrame(data, cols)\n",
    "df.show(vertical=True, truncate=False)\n",
    "df.printSchema()\n",
    "melt(\n",
    "        df,\n",
    "        id_vars=['poi_name'],\n",
    "        value_vars=[\"img_url\",\n",
    "                    'food_cat_pop_score',\n",
    "                    \"img_article_url\",\n",
    "                    \"img_author_id\",\n",
    "                    'food_cat'\n",
    "                    ],\n",
    "        var_name = 'menu_key',value_name = 'menu_value'\n",
    "        ).show(vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect into complex type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:37:53.089518Z",
     "start_time": "2021-07-31T05:37:51.232467Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----+\n",
      "| id|key|value|\n",
      "+---+---+-----+\n",
      "|  1|  a|  123|\n",
      "|  1|  b|  234|\n",
      "|  1|  c|  345|\n",
      "|  2|  a|   12|\n",
      "|  2|  x|   23|\n",
      "+---+---+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- collections: array (nullable = false)\n",
      " |    |-- element: map (containsNull = false)\n",
      " |    |    |-- key: string\n",
      " |    |    |-- value: long (valueContainsNull = true)\n",
      "\n",
      "+---+------------------------------------+\n",
      "|id |collections                         |\n",
      "+---+------------------------------------+\n",
      "|1  |[{a -> 123}, {b -> 234}, {c -> 345}]|\n",
      "|2  |[{a -> 12}, {x -> 23}, {y -> 123}]  |\n",
      "+---+------------------------------------+\n",
      "\n",
      "[Row(id=1, collections=[{'a': 123}, {'b': 234}, {'c': 345}]), Row(id=2, collections=[{'a': 12}, {'x': 23}, {'y': 123}])]\n",
      "\n",
      "\n",
      "\n",
      "----------------- Case 2 -------------------\n",
      "\n",
      "\n",
      "\n",
      "+----------+-------------+------------------------+---------------+---------+\n",
      "|store_name|food_category|food_category_popularity|        img_url|author_id|\n",
      "+----------+-------------+------------------------+---------------+---------+\n",
      "|    hotpop|         Meat|                       3|https//:123.png|   rtyg11|\n",
      "|    hotpop|         Meat|                       3|https//:456.png|   rtyg11|\n",
      "|    hotpop|         Meat|                       3|https//:789.png|   rtyg11|\n",
      "|    hotpop|    Vegetable|                       2|https//:111.png|   rtyg11|\n",
      "|    hotpop|    Vegetable|                       2|https//:222.png|   rtyg11|\n",
      "|    branch|   Fried food|                       1|https//:333.png|     bvc1|\n",
      "|    branch|      Dessert|                       1|https//:444.png|     7854|\n",
      "+----------+-------------+------------------------+---------------+---------+\n",
      "\n",
      "+----------+--------------------+\n",
      "|store_name|                menu|\n",
      "+----------+--------------------+\n",
      "|    hotpop|[{food_category -...|\n",
      "|    branch|[{food_category -...|\n",
      "+----------+--------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(store_name='hotpop', menu=[{'food_category': 'Meat'}, {'food_category_popularity': '3'}, {'img_url': 'https//:123.png'}, {'author_id': 'rtyg11'}, {'food_category': 'Meat'}, {'food_category_popularity': '3'}, {'img_url': 'https//:456.png'}, {'author_id': 'rtyg11'}, {'food_category': 'Meat'}, {'food_category_popularity': '3'}, {'img_url': 'https//:789.png'}, {'author_id': 'rtyg11'}, {'food_category': 'Vegetable'}, {'food_category_popularity': '2'}, {'img_url': 'https//:111.png'}, {'author_id': 'rtyg11'}, {'food_category': 'Vegetable'}, {'food_category_popularity': '2'}, {'img_url': 'https//:222.png'}, {'author_id': 'rtyg11'}]),\n",
       " Row(store_name='branch', menu=[{'food_category': 'Fried food'}, {'food_category_popularity': '1'}, {'img_url': 'https//:333.png'}, {'author_id': 'bvc1'}, {'food_category': 'Dessert'}, {'food_category_popularity': '1'}, {'img_url': 'https//:444.png'}, {'author_id': '7854'}])]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 8 collect dict (map) with a group\n",
    "# https://stackoverflow.com/questions/55308482/pyspark-create-dictionary-within-groupby\n",
    "\n",
    "# collect_list : return a list of objects with duplicated\n",
    "# collect_set : return a set of objects without duplicated\n",
    "# struct : create a new struct column\n",
    "# ( > 2.4.0)map_from_entries : returns a map created from the given array of entries\n",
    "# create_map\n",
    "\n",
    "######### pyspark < 2.4.0\n",
    "data = [\n",
    "    (1,'a',123),\n",
    "    (1,'b',234),\n",
    "    (1,'c',345),\n",
    "    (2,'a',12),\n",
    "    (2,'x',23),\n",
    "    (2,'y',123)\n",
    "]\n",
    "\n",
    "columns = ['id','key','value']\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show(n=5)\n",
    "\n",
    "######## pyspark < 2.4.0\n",
    "\n",
    "df_agg = df.groupBy(\"id\").agg(\n",
    "    F.collect_list(F.create_map(C(\"key\"),C(\"value\"))).alias('collections')\n",
    ")\n",
    "\n",
    "df_agg.printSchema()\n",
    "df_agg.show(n=10, truncate=False)\n",
    "print(df_agg.collect())\n",
    "df_agg.toPandas().to_json('output/tmp.json',\n",
    "                          orient='records',\n",
    "                          force_ascii=False,\n",
    "                          lines=True)\n",
    "\n",
    "\n",
    "# to_json(join(SERVING_POI_FOOD_IMG_FOLDER,serving_fname),\n",
    "#                                        orient='records',\n",
    "#                                        force_ascii=False,\n",
    "#                                        lines=True)\n",
    "######### pyspark > 2.4.0\n",
    "# df.groupBy(\"id\").agg(\n",
    "#     F.map_from_entries(\n",
    "#         F.collect_list(\n",
    "#             F.struct(\"key\",\"value\"))).alias(\"key_value\")\n",
    "# ).show()\n",
    "\n",
    "############## Case 2 #######################\n",
    "for i in range(3):\n",
    "    print()\n",
    "print('----------------- Case 2 -------------------')\n",
    "for i in range(3):\n",
    "    print()\n",
    "    \n",
    "data = [\n",
    "    (\"hotpop\",\"Meat\",3,\"https//:123.png\",\"rtyg11\"),\n",
    "    (\"hotpop\",\"Meat\",3,\"https//:456.png\",\"rtyg11\"),\n",
    "    (\"hotpop\",\"Meat\",3,\"https//:789.png\",\"rtyg11\"),\n",
    "    (\"hotpop\",\"Vegetable\",2,\"https//:111.png\",\"rtyg11\"),\n",
    "    (\"hotpop\",\"Vegetable\",2,\"https//:222.png\",\"rtyg11\"),\n",
    "    (\"branch\",\"Fried food\",1,\"https//:333.png\",\"bvc1\"),\n",
    "    (\"branch\",\"Dessert\",1,\"https//:444.png\",\"7854\"),\n",
    "  ]\n",
    "\n",
    "columns = [\"store_name\",\"food_category\",\"food_category_popularity\",\"img_url\",\"author_id\"]\n",
    "df = spark.createDataFrame(data = data, schema = columns)\n",
    "\n",
    "df.show(n=10)\n",
    "\n",
    "\n",
    "# convert the melt column to string\n",
    "# because the column you wanna melt should be the same dtype\n",
    "df = (\n",
    "    df.withColumn(\"food_category_popularity\", C(\"food_category_popularity\").cast(StringType()))\n",
    ")\n",
    "df_complex = (\n",
    "    melt(df, id_vars=['store_name'],\n",
    "             value_vars=['food_category','food_category_popularity','img_url','author_id'],\n",
    "             var_name = 'menu_key',value_name = 'menu_value'\n",
    "        ).groupBy(\"store_name\").agg(\n",
    "        F.collect_list(F.create_map(C(\"menu_key\"), C(\"menu_value\"))).alias(\"menu\")\n",
    "    )\n",
    ")\n",
    "\n",
    "df_complex.show()\n",
    "df_complex.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:37:53.103035Z",
     "start_time": "2021-07-31T05:37:53.091057Z"
    }
   },
   "outputs": [],
   "source": [
    "# group by key, create a complexy json format like\n",
    "\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:37:53.744029Z",
     "start_time": "2021-07-31T05:37:53.104404Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- group: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- languagesAtSchool: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- currentState: string (nullable = true)\n",
      "\n",
      "+-----+----------------+------------------+------------+\n",
      "|group|name            |languagesAtSchool |currentState|\n",
      "+-----+----------------+------------------+------------+\n",
      "|1    |James,,Smith    |[Java, Scala, C++]|CA          |\n",
      "|1    |Michael,Rose,   |[Spark, Java, C++]|NJ          |\n",
      "|2    |Robert,,Williams|[CSharp, VB]      |NV          |\n",
      "|2    |Robert,,Williams|null              |NV          |\n",
      "+-----+----------------+------------------+------------+\n",
      "\n",
      "+-----+------------------------------------+\n",
      "|group|languagesAtSchool                   |\n",
      "+-----+------------------------------------+\n",
      "|1    |[Java, Scala, C++, Spark, Java, C++]|\n",
      "|2    |[CSharp, VB]                        |\n",
      "+-----+------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 12 groupby , concat the element in array\n",
    "\n",
    "# collect in array list, and flatern then\n",
    "\n",
    "columns = [\"group\",\"name\",\"languagesAtSchool\",\"currentState\"]\n",
    "data = [\n",
    "    ('1',\"James,,Smith\",[\"Java\",\"Scala\",\"C++\"],\"CA\"), \\\n",
    "    ('1',\"Michael,Rose,\",[\"Spark\",\"Java\",\"C++\"],\"NJ\"), \\\n",
    "    ('2',\"Robert,,Williams\",[\"CSharp\",\"VB\"],\"NV\"),\n",
    "    ('2',\"Robert,,Williams\",None,\"NV\")\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data=data,schema=columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n",
    "\n",
    "# collect list with collect list\n",
    "# we need to explode it first\n",
    "\n",
    "df_agg = (\n",
    "    df\n",
    "    .withColumn(\"flattern_tags\",F.explode(C(\"languagesAtSchool\")))\n",
    "    .drop('languagesAtSchool')\n",
    "    .groupBy(\"group\")\n",
    "    .agg(\n",
    "        F.collect_list(\"flattern_tags\").alias('languagesAtSchool')\n",
    "    )\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:37:54.641055Z",
     "start_time": "2021-07-31T05:37:53.745968Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- group: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- languagesAtSchool: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- currentState: string (nullable = true)\n",
      "\n",
      "+-----+----------------+------------------+------------+\n",
      "|group|name            |languagesAtSchool |currentState|\n",
      "+-----+----------------+------------------+------------+\n",
      "|1    |James,,Smith    |[Java, Scala, C++]|CA          |\n",
      "|1    |Michael,Rose,   |[Spark, Java, C++]|NJ          |\n",
      "|2    |Robert,,Williams|[CSharp, VB]      |NV          |\n",
      "|2    |Robert,,Williams|null              |NV          |\n",
      "+-----+----------------+------------------+------------+\n",
      "\n",
      "+-----+----------------------------------------+\n",
      "|group|tags                                    |\n",
      "+-----+----------------------------------------+\n",
      "|1    |[[Java, Scala, C++], [Spark, Java, C++]]|\n",
      "|2    |[[CSharp, VB]]                          |\n",
      "+-----+----------------------------------------+\n",
      "\n",
      "+-----+----------------------------------------+------------------+\n",
      "|group|tags                                    |explode_tags      |\n",
      "+-----+----------------------------------------+------------------+\n",
      "|1    |[[Java, Scala, C++], [Spark, Java, C++]]|[Java, Scala, C++]|\n",
      "|1    |[[Java, Scala, C++], [Spark, Java, C++]]|[Spark, Java, C++]|\n",
      "|2    |[[CSharp, VB]]                          |[CSharp, VB]      |\n",
      "+-----+----------------------------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 13 groupby and collect arrays into arrays\n",
    "# Yes we can\n",
    "\n",
    "columns = [\"group\",\"name\",\"languagesAtSchool\",\"currentState\"]\n",
    "data = [\n",
    "    ('1',\"James,,Smith\",[\"Java\",\"Scala\",\"C++\"],\"CA\"), \\\n",
    "    ('1',\"Michael,Rose,\",[\"Spark\",\"Java\",\"C++\"],\"NJ\"), \\\n",
    "    ('2',\"Robert,,Williams\",[\"CSharp\",\"VB\"],\"NV\"),\n",
    "    ('2',\"Robert,,Williams\",None,\"NV\")\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data=data,schema=columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n",
    "\n",
    "df_agg = (\n",
    " df.groupBy(\"group\")\n",
    "    .agg(\n",
    "        F.collect_list(C(\"languagesAtSchool\")).alias(\"tags\")\n",
    "    )\n",
    ")\n",
    "\n",
    "df_agg.show(truncate=False)\n",
    "\n",
    "\n",
    "(\n",
    "    df_agg\n",
    "    .withColumn(\"explode_tags\",F.explode(C(\"tags\")))\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:37:55.196152Z",
     "start_time": "2021-07-31T05:37:54.643474Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+---+\n",
      "|age|      dept| id|\n",
      "+---+----------+---+\n",
      "| 38|  medicine|  1|\n",
      "| 41|  medicine|  2|\n",
      "| 55|  medicine|  3|\n",
      "| 15|technology|  4|\n",
      "| 88|technology|  5|\n",
      "| 88|technology|  6|\n",
      "| 75|technology|  7|\n",
      "| 75|       mba|  8|\n",
      "| 75|       mba|  9|\n",
      "| 75|       mba| 10|\n",
      "+---+----------+---+\n",
      "\n",
      "This is wrong answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/07/31 13:37:54 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "21/07/31 13:37:54 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+---+-------+------+\n",
      "|age|      dept| id|firstID|lastID|\n",
      "+---+----------+---+-------+------+\n",
      "| 75|       mba|  8|      8|    10|\n",
      "| 75|       mba|  9|      8|    10|\n",
      "| 75|       mba| 10|      8|    10|\n",
      "| 38|  medicine|  1|      1|     1|\n",
      "| 41|  medicine|  2|      1|     2|\n",
      "| 55|  medicine|  3|      1|     3|\n",
      "| 15|technology|  4|      4|     4|\n",
      "| 75|technology|  7|      4|     7|\n",
      "| 88|technology|  5|      4|     6|\n",
      "| 88|technology|  6|      4|     6|\n",
      "+---+----------+---+-------+------+\n",
      "\n",
      "Because we prodive a orderBy clause, default frame is \n",
      "RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW\n",
      "== Physical Plan ==\n",
      "Window [first(id#4195, false) windowspecdefinition(dept#4191, age#4190L ASC NULLS FIRST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS firstID#4214, last(id#4195, false) windowspecdefinition(dept#4191, age#4190L ASC NULLS FIRST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS lastID#4220], [dept#4191], [age#4190L ASC NULLS FIRST]\n",
      "+- *(3) Sort [dept#4191 ASC NULLS FIRST, age#4190L ASC NULLS FIRST], false, 0\n",
      "   +- *(3) Project [age#4190L, dept#4191, id#4195]\n",
      "      +- Window [row_number() windowspecdefinition(_w0#4196L ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS id#4195], [_w0#4196L ASC NULLS FIRST]\n",
      "         +- *(2) Sort [_w0#4196L ASC NULLS FIRST], false, 0\n",
      "            +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [id=#3254]\n",
      "               +- *(1) Project [age#4190L, dept#4191, (monotonically_increasing_id() - 1) AS _w0#4196L]\n",
      "                  +- *(1) Scan ExistingRDD[age#4190L,dept#4191]\n",
      "\n",
      "\n",
      "So we need to re-define the wibndow\n",
      "== Physical Plan ==\n",
      "Window [nth_value(id#4195, 1, false) windowspecdefinition(dept#4191, age#4190L ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS firstID#4252, last(id#4195, false) windowspecdefinition(dept#4191, age#4190L ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS lastID#4258, row_number() windowspecdefinition(dept#4191, age#4190L ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS age_rank_in_dept#4265], [dept#4191], [age#4190L ASC NULLS FIRST]\n",
      "+- *(3) Sort [dept#4191 ASC NULLS FIRST, age#4190L ASC NULLS FIRST], false, 0\n",
      "   +- *(3) Project [age#4190L, dept#4191, id#4195]\n",
      "      +- Window [row_number() windowspecdefinition(_w0#4196L ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS id#4195], [_w0#4196L ASC NULLS FIRST]\n",
      "         +- *(2) Sort [_w0#4196L ASC NULLS FIRST], false, 0\n",
      "            +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [id=#3291]\n",
      "               +- *(1) Project [age#4190L, dept#4191, (monotonically_increasing_id() - 1) AS _w0#4196L]\n",
      "                  +- *(1) Scan ExistingRDD[age#4190L,dept#4191]\n",
      "\n",
      "\n",
      "+---+----------+---+-------+------+----------------+\n",
      "|age|      dept| id|firstID|lastID|age_rank_in_dept|\n",
      "+---+----------+---+-------+------+----------------+\n",
      "| 75|       mba|  8|      8|    10|               1|\n",
      "| 75|       mba|  9|      8|    10|               2|\n",
      "| 75|       mba| 10|      8|    10|               3|\n",
      "| 38|  medicine|  1|      1|     3|               1|\n",
      "| 41|  medicine|  2|      1|     3|               2|\n",
      "| 55|  medicine|  3|      1|     3|               3|\n",
      "| 15|technology|  4|      4|     6|               1|\n",
      "| 75|technology|  7|      4|     6|               2|\n",
      "| 88|technology|  5|      4|     6|               3|\n",
      "| 88|technology|  6|      4|     6|               4|\n",
      "+---+----------+---+-------+------+----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/07/31 13:37:54 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "21/07/31 13:37:55 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "21/07/31 13:37:55 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "# 14 groupby column C1 , get first row and last row once, order by column C2\n",
    "\n",
    "\n",
    "data = [\n",
    "    (38,\"medicine\"),\n",
    "    (41,\"medicine\"),\n",
    "    (55,\"medicine\"),\n",
    "    (15,\"technology\"),\n",
    "    (88,\"technology\"),\n",
    "    (88,\"technology\"),\n",
    "    (75,\"technology\"),\n",
    "    (75,\"mba\"),\n",
    "    (75,\"mba\"),\n",
    "    (75,\"mba\")\n",
    "    ]\n",
    "\n",
    "\n",
    "columns = ['age','dept']\n",
    "\n",
    "df = (\n",
    "    spark.createDataFrame(data=data, schema=columns)\n",
    "    .withColumn(\"id\",\n",
    "                F.row_number()\n",
    "                .over(\n",
    "                    W.orderBy(F.monotonically_increasing_id() - 1)\n",
    "                ))\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "df.show()\n",
    "\n",
    "# https://stackoverflow.com/questions/52273186/pyspark-spark-window-function-first-last-issue\n",
    "print(\"This is wrong answer\")\n",
    "\n",
    "age_in_dept = W.partitionBy('dept').orderBy(\"age\")\n",
    "df_res = (\n",
    "    df\n",
    "    .withColumn(\"firstID\", F.first('id').over(age_in_dept))\n",
    "    .withColumn(\"lastID\", F.last('id').over(age_in_dept))\n",
    ")\n",
    "\n",
    "df_res.show()\n",
    "\n",
    "print('Because we prodive a orderBy clause, default frame is ')\n",
    "print(\"RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW\")\n",
    "\n",
    "df_res.explain()\n",
    "\n",
    "print(\"So we need to re-define the wibndow\")\n",
    "\n",
    "age_in_dept = (\n",
    "    W.partitionBy('dept')\n",
    "    .orderBy(\"age\")\n",
    "    .rowsBetween(W.unboundedPreceding, W.unboundedFollowing)\n",
    ")\n",
    "\n",
    "\n",
    "##### withcolumn, select gives you the same answer, you need to do your own filtering\n",
    "df_res = (\n",
    "    df\n",
    "    .withColumn(\"firstID\", F.first('id').over(age_in_dept))\n",
    "    .withColumn(\"lastID\", F.last('id').over(age_in_dept))\n",
    "    .withColumn(\"age_rank_in_dept\",\n",
    "                F.row_number()\n",
    "                .over(\n",
    "                W.partitionBy('dept')\n",
    "                 .orderBy(\"age\")\n",
    "                ))\n",
    ")\n",
    "\n",
    "df_res.explain()\n",
    "df_res.show()\n",
    "\n",
    "\n",
    "# df_res = (\n",
    "#     df.select(\n",
    "#         \"*\",\n",
    "#         F.first('id').over(age_in_dept).alias('first_id'),\n",
    "#         F.last('id').over(age_in_dept).alias('last_id'),\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# df_res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:37:56.105011Z",
     "start_time": "2021-07-31T05:37:55.197771Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|item|score|\n",
      "+----+-----+\n",
      "|   a|   10|\n",
      "|   a|   10|\n",
      "|   a|   20|\n",
      "|   b|    5|\n",
      "|   b|    3|\n",
      "+----+-----+\n",
      "\n",
      "-RECORD 0-------------------------------------\n",
      " item                          | b            \n",
      " score                         | 3            \n",
      " collect_score_list_to_current | [3]          \n",
      " collect_score_set_to_current  | [3]          \n",
      " collect_score_list            | [3, 5]       \n",
      " collect_score_set             | [5, 3]       \n",
      "-RECORD 1-------------------------------------\n",
      " item                          | b            \n",
      " score                         | 5            \n",
      " collect_score_list_to_current | [3, 5]       \n",
      " collect_score_set_to_current  | [5, 3]       \n",
      " collect_score_list            | [3, 5]       \n",
      " collect_score_set             | [5, 3]       \n",
      "-RECORD 2-------------------------------------\n",
      " item                          | a            \n",
      " score                         | 10           \n",
      " collect_score_list_to_current | [10, 10]     \n",
      " collect_score_set_to_current  | [10]         \n",
      " collect_score_list            | [10, 10, 20] \n",
      " collect_score_set             | [20, 10]     \n",
      "-RECORD 3-------------------------------------\n",
      " item                          | a            \n",
      " score                         | 10           \n",
      " collect_score_list_to_current | [10, 10]     \n",
      " collect_score_set_to_current  | [10]         \n",
      " collect_score_list            | [10, 10, 20] \n",
      " collect_score_set             | [20, 10]     \n",
      "-RECORD 4-------------------------------------\n",
      " item                          | a            \n",
      " score                         | 20           \n",
      " collect_score_list_to_current | [10, 10, 20] \n",
      " collect_score_set_to_current  | [20, 10]     \n",
      " collect_score_list            | [10, 10, 20] \n",
      " collect_score_set             | [20, 10]     \n",
      "\n",
      "== Physical Plan ==\n",
      "Window [collect_list(score#4300L, 0, 0) windowspecdefinition(item#4299, score#4300L ASC NULLS FIRST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS collect_score_list_to_current#4313, collect_set(score#4300L, 0, 0) windowspecdefinition(item#4299, score#4300L ASC NULLS FIRST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS collect_score_set_to_current#4318, collect_list(score#4300L, 0, 0) windowspecdefinition(item#4299, score#4300L ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS collect_score_list#4324, collect_set(score#4300L, 0, 0) windowspecdefinition(item#4299, score#4300L ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS collect_score_set#4331], [item#4299], [score#4300L ASC NULLS FIRST]\n",
      "+- *(2) Sort [item#4299 ASC NULLS FIRST, score#4300L ASC NULLS FIRST], false, 0\n",
      "   +- Exchange hashpartitioning(item#4299, 200), ENSURE_REQUIREMENTS, [id=#3403]\n",
      "      +- *(1) Scan ExistingRDD[item#4299,score#4300L]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 15. Perform collect_list, colect set in a wondow\n",
    "data = [\n",
    "    (\"a\",10),\n",
    "    ('a',10),\n",
    "    ('a',20),\n",
    "    ('b',5),\n",
    "    ('b',3)\n",
    "]\n",
    "\n",
    "columns = ['item','score']\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show(n=5)\n",
    "\n",
    "score_in_item_from_unbounded_to_curr = (\n",
    "                W.partitionBy(\"item\")\n",
    "                 .orderBy(\"score\"))\n",
    "score_in_item = (\n",
    "                W.partitionBy('item')\n",
    "                 .orderBy(\"score\")\n",
    "                 .rowsBetween(W.unboundedPreceding, W.unboundedFollowing)\n",
    ")\n",
    "df_window = (\n",
    "    df\n",
    "    .withColumn(\"collect_score_list_to_current\",\n",
    "                F.collect_list(\"score\").over(score_in_item_from_unbounded_to_curr))\n",
    "    .withColumn(\"collect_score_set_to_current\",\n",
    "                F.collect_set(\"score\").over(score_in_item_from_unbounded_to_curr))\n",
    "    .withColumn(\"collect_score_list\",\n",
    "                F.collect_list(\"score\").over(score_in_item)\n",
    "               )\n",
    "    .withColumn(\"collect_score_set\",\n",
    "                F.collect_set(\"score\").over(score_in_item)\n",
    "               )\n",
    ")\n",
    "\n",
    "df_window.show(n=5, vertical=True, truncate=False)\n",
    "\n",
    "df_window.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:37:56.761613Z",
     "start_time": "2021-07-31T05:37:56.106392Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|item|score|\n",
      "+----+-----+\n",
      "|   a|   10|\n",
      "|   a|    0|\n",
      "|   a|   20|\n",
      "|   b|    5|\n",
      "|   b|    3|\n",
      "+----+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "-RECORD 0----------------------\n",
      " item               | c        \n",
      " score              | 0        \n",
      " collect_score_list | []       \n",
      "-RECORD 1----------------------\n",
      " item               | b        \n",
      " score              | 5        \n",
      " collect_score_list | [5, 3]   \n",
      "-RECORD 2----------------------\n",
      " item               | b        \n",
      " score              | 3        \n",
      " collect_score_list | [5, 3]   \n",
      "-RECORD 3----------------------\n",
      " item               | a        \n",
      " score              | 10       \n",
      " collect_score_list | [10, 20] \n",
      "-RECORD 4----------------------\n",
      " item               | a        \n",
      " score              | 0        \n",
      " collect_score_list | [10, 20] \n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 15. Perform collect_list with a filter\n",
    "# collect only the score > 0\n",
    "# https://stackoverflow.com/questions/61468705/pyspark-using-collect-list-over-window-with-condition\n",
    "data = [\n",
    "    (\"a\",10),\n",
    "    ('a',0),\n",
    "    ('a',20),\n",
    "    ('b',5),\n",
    "    ('b',3),\n",
    "    ('c',0)\n",
    "]\n",
    "\n",
    "columns = ['item','score']\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show(n=5)\n",
    "\n",
    "item = (\n",
    "        W.partitionBy(\"item\")\n",
    ")\n",
    "\n",
    "\n",
    "df_window = (\n",
    "    df\n",
    "    .withColumn(\"collect_score_list\",\n",
    "                F.collect_list(\n",
    "                    F.when(C(\"score\") > 0, C(\"score\"))\n",
    "                     .otherwise(F.lit(None))\n",
    "                    ).over(item)\n",
    "               )\n",
    "\n",
    ")\n",
    "\n",
    "df_window.show(n=5, vertical=True, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# row and columns (1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:37:56.859766Z",
     "start_time": "2021-07-31T05:37:56.765155Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attribute and method in Row :  ['__add__', '__call__', '__class__', '__contains__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getitem__', '__getnewargs__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__module__', '__mul__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__rmul__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', 'asDict', 'count', 'index']\n",
      "{'id': 14431, 'img_url': 'https://pic.pimg.tw/happy78/1528543947-685380499_n.jpg'}\n"
     ]
    }
   ],
   "source": [
    "# 1. changing rows and python dictionary \n",
    "data = [\n",
    "    (14431,'https://pic.pimg.tw/happy78/1528543947-685380499_n.jpg'),\n",
    "    (14431,'https://pic.pimg.tw/happy78/1528543947-362759723_n.jpg'),\n",
    "    (14431,'https://pic.pimg.tw/happy78/1528543962-2265924582_n.jpg'),\n",
    "    (67789,'https://pic.pimg.tw/happy78/1528543962-4007835890_n.jpg'),\n",
    "    (67789,'https://pic.pimg.tw/happy78/1528543962-45890_n.jpg')\n",
    "]\n",
    "\n",
    "cols = ['id','img_url']\n",
    "\n",
    "df = spark.createDataFrame(data, cols)\n",
    "\n",
    "rows = df.collect()\n",
    "\n",
    "r = rows[0]\n",
    "\n",
    "print('attribute and method in Row : ', dir(r))\n",
    "\n",
    "print(r.asDict(), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# udf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:37:57.731847Z",
     "start_time": "2021-07-31T05:37:56.861604Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before\n",
      "+----------+--------------------+\n",
      "|article_id|             img_url|\n",
      "+----------+--------------------+\n",
      "|     14431|https://pic.pimg....|\n",
      "|     14431|https://pic.pimg....|\n",
      "|     14431|https://pic.pimg....|\n",
      "|     67789|https://pic.pimg....|\n",
      "|     67789|https://pic.pimg....|\n",
      "+----------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23463/1917393511.py:24: DeprecationWarning: encodestring() is a deprecated alias since 3.1, use encodebytes()\n",
      "/tmp/ipykernel_23463/1917393511.py:24: DeprecationWarning: encodestring() is a deprecated alias since 3.1, use encodebytes()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-----------+\n",
      "|article_id|             img_url|img_b64_str|\n",
      "+----------+--------------------+-----------+\n",
      "|     14431|https://pic.pimg....|[B@1fd7d261|\n",
      "|     14431|https://pic.pimg....|[B@75dd770a|\n",
      "|     14431|https://pic.pimg....|[B@39576ed2|\n",
      "|     67789|https://pic.pimg....|[B@63f8c989|\n",
      "|     67789|https://pic.pimg....|       null|\n",
      "+----------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Use Pyspark to send request, get image and store as b64string \n",
    "# https://stackoverflow.com/questions/49353752/use-requests-module-and-return-response-to-pyspark-dataframe\n",
    "\n",
    "data = [\n",
    "    (14431,'https://pic.pimg.tw/happy78/1528543947-685380499_n.jpg'),\n",
    "    (14431,'https://pic.pimg.tw/happy78/1528543947-362759723_n.jpg'),\n",
    "    (14431,'https://pic.pimg.tw/happy78/1528543962-2265924582_n.jpg'),\n",
    "    (67789,'https://pic.pimg.tw/happy78/1528543962-4007835890_n.jpg'),\n",
    "    (67789,'https://pic.pimg.tw/happy78/1528543962-45890_n.jpg')\n",
    "]\n",
    "\n",
    "columns = ['article_id','img_url']\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=columns)\n",
    "print('before')\n",
    "df.show(n=5)\n",
    "\n",
    "@F.udf(returnType=StringType())\n",
    "def get_img_binary(url : str) -> str:\n",
    "    import requests\n",
    "    import base64\n",
    "    resp = requests.get(url)\n",
    "    if resp.status_code == 200:\n",
    "        return base64.encodestring(resp.content)\n",
    "    else:\n",
    "        return None\n",
    "df = (\n",
    "    df.withColumn(\"img_b64_str\", get_img_binary(C(\"img_url\")))\n",
    ")\n",
    "\n",
    "df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:37:58.027928Z",
     "start_time": "2021-07-31T05:37:57.734655Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before : \n",
      "+---+--------+\n",
      "| id|features|\n",
      "+---+--------+\n",
      "|  1|      64|\n",
      "|  2|      76|\n",
      "|  3|      54|\n",
      "|  4|      11|\n",
      "|  5|     100|\n",
      "+---+--------+\n",
      "\n",
      "[('id', 'bigint'), ('features', 'bigint'), ('pred', 'struct<category:string,prob:float>')]\n",
      "+---+--------+-------------------+\n",
      "|id |features|pred               |\n",
      "+---+--------+-------------------+\n",
      "|1  |64      |{drink, 0.8981407} |\n",
      "|2  |76      |{drink, 0.16260353}|\n",
      "|3  |54      |{food, 0.3933934}  |\n",
      "|4  |11      |{drink, 0.08618167}|\n",
      "|5  |100     |{env, 0.4585576}   |\n",
      "+---+--------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2 udf return two column values, e.g. model prediction with label and probability\n",
    "data = [\n",
    "    (1,64),\n",
    "    (2,76),\n",
    "    (3,54),\n",
    "    (4,11),\n",
    "    (5,100),\n",
    "]\n",
    "columns = ['id','features']\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=columns)\n",
    "print(\"Before : \")\n",
    "df.show(n=5)\n",
    "\n",
    "############# sol #################\n",
    "# using Row object to return multiple column\n",
    "from pyspark.sql import Row\n",
    "\n",
    "model_pred = StructType([\n",
    "    StructField(\"category\", StringType(), False),\n",
    "    StructField(\"prob\", T.FloatType(), False)\n",
    "])\n",
    "\n",
    "@F.udf(returnType=model_pred)\n",
    "def model_pred(n):\n",
    "    import random\n",
    "    category = random.choice(['food','env','compose','drink'])\n",
    "    prob = random.random()\n",
    "    return Row('category', 'prob')(category, prob)\n",
    "\n",
    "\n",
    "\n",
    "newDF = df.withColumn(\"pred\", model_pred(df[\"features\"]))\n",
    "\n",
    "print(newDF.dtypes)\n",
    "\n",
    "# newDF = newDF.select(\"id\", \"features\", \"pred.*\")\n",
    "\n",
    "newDF.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:37:58.818327Z",
     "start_time": "2021-07-31T05:37:58.030577Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cols: struct (nullable = true)\n",
      " |    |-- error: string (nullable = true)\n",
      " |    |-- message: string (nullable = true)\n",
      " |-- cols_2: struct (nullable = true)\n",
      " |    |-- error: string (nullable = true)\n",
      " |    |-- message: string (nullable = true)\n",
      "\n",
      "+----------+----------+\n",
      "|cols      |cols_2    |\n",
      "+----------+----------+\n",
      "|{abc, def}|{abc, def}|\n",
      "|{abc, def}|{abc, def}|\n",
      "|{abc, def}|{abc, def}|\n",
      "|{abc, def}|{abc, def}|\n",
      "|{abc, def}|{abc, def}|\n",
      "+----------+----------+\n",
      "\n",
      "+-----+-------+\n",
      "|error|message|\n",
      "+-----+-------+\n",
      "|  abc|    def|\n",
      "|  abc|    def|\n",
      "|  abc|    def|\n",
      "|  abc|    def|\n",
      "|  abc|    def|\n",
      "+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# return array in udf\n",
    "# 3 using udf return struct in pixlake\n",
    "\n",
    "from pixlake.etl.user.pixtree.export_cookie_to_pixinterest import PixInterestApiLog\n",
    "PixInterestApiLog.infer_spark_schema()\n",
    "\n",
    "\n",
    "# 1 return dict as stuct column by udf\n",
    "\n",
    "\n",
    "# any reference for this?\n",
    "\n",
    "\n",
    "data = [\n",
    "    (14431,'https://pic.pimg.tw/happy78/1528543947-685380499_n.jpg'),\n",
    "    (14431,'https://pic.pimg.tw/happy78/1528543947-362759723_n.jpg'),\n",
    "    (14431,'https://pic.pimg.tw/happy78/1528543962-2265924582_n.jpg'),\n",
    "    (67789,'https://pic.pimg.tw/happy78/1528543962-4007835890_n.jpg'),\n",
    "    (67789,'https://pic.pimg.tw/happy78/1528543962-45890_n.jpg')\n",
    "]\n",
    "\n",
    "cols = ['id','img_url']\n",
    "\n",
    "df = spark.createDataFrame(data, cols)\n",
    "\n",
    "def get_two_more(col) -> dict:\n",
    "    return {'error' : 'abc', 'message' : 'def'}\n",
    "\n",
    "str_schema = 'error string, message string'\n",
    "struct_schema = 'struct<\\n error: string,\\n message: string \\n>'\n",
    "api_log_sdf = (\n",
    "    df.select(\n",
    "        F.udf(get_two_more, returnType=str_schema)('img_url').alias('cols'),\n",
    "        F.udf(get_two_more, returnType=struct_schema)('img_url').alias('cols_2')\n",
    "        \n",
    "    )\n",
    ")\n",
    "\n",
    "api_log_sdf.printSchema()\n",
    "api_log_sdf.show(truncate=False)\n",
    "\n",
    "api_log_expand = (\n",
    "    api_log_sdf\n",
    "    .select('cols_2.*')\n",
    ")\n",
    "\n",
    "api_log_expand.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:37:58.833389Z",
     "start_time": "2021-07-31T05:37:58.820074Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'http://...imglink': 'http://pure_link'}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from random import choice\n",
    "candidate = [\n",
    "    {'':'http://pure_link'},\n",
    "    {'text_display':'http://pure_link'},\n",
    "    {'http://...imglink':'http://pure_link'},\n",
    "]\n",
    "\n",
    "choice(candidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:37:59.138364Z",
     "start_time": "2021-07-31T05:37:58.834486Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- list_of_string: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n",
      "-RECORD 0-------------------------------\n",
      " list_of_string | [http:// imglink.jpg] \n",
      "-RECORD 1-------------------------------\n",
      " list_of_string | [http:// imglink.jpg] \n",
      "-RECORD 2-------------------------------\n",
      " list_of_string | [http:// imglink.jpg] \n",
      "-RECORD 3-------------------------------\n",
      " list_of_string | [http:// imglink.jpg] \n",
      "-RECORD 4-------------------------------\n",
      " list_of_string | [http:// imglink.jpg] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# return array of string type\n",
    "\n",
    "from typing import List, Dict\n",
    "from random import choice\n",
    "\n",
    "def get_list_of_string(anything) -> List[str]:\n",
    "    return ['http:// imglink.jpg']\n",
    "\n",
    "\n",
    "# This is is too complicated\n",
    "# def get_list_of_dict(anything) -> List[Dict[str, str]]:\n",
    "#     # Is this work?\n",
    "#     candidate = [\n",
    "#         {'':'http://pure_link'},\n",
    "#         {'text_display':'http://pure_link'},\n",
    "#         {'http://...imglink':'http://pure_link'},\n",
    "#     ]\n",
    "#     return [choice(candidate), choice(candidate)]\n",
    "\n",
    "data = [\n",
    "    (14431,'https://pic.pimg.tw/happy78/1528543947-685380499_n.jpg'),\n",
    "    (14431,'https://pic.pimg.tw/happy78/1528543947-362759723_n.jpg'),\n",
    "    (14431,'https://pic.pimg.tw/happy78/1528543962-2265924582_n.jpg'),\n",
    "    (67789,'https://pic.pimg.tw/happy78/1528543962-4007835890_n.jpg'),\n",
    "    (67789,'https://pic.pimg.tw/happy78/1528543962-45890_n.jpg')\n",
    "]\n",
    "\n",
    "cols = ['id','img_url']\n",
    "\n",
    "df = spark.createDataFrame(data, cols)\n",
    "\n",
    "df_res = (\n",
    "    df.select(\n",
    "        F.udf(get_list_of_string, 'array<string>')('id').alias('list_of_string')\n",
    "#         F.udf(get_list_of_string, 'array<struct<>>')('id').alias('list_of_string')\n",
    "    )\n",
    ")\n",
    "\n",
    "df_res.printSchema()\n",
    "\n",
    "df_res.show(n=5, vertical=True, truncate=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pandas_udf\n",
    "\n",
    "\n",
    "* pandas_udf return maximum 2G\n",
    "\n",
    "* https://issues.apache.org/jira/browse/ARROW-1907\n",
    "\n",
    "* [pandas_udf in classmethod, you need to write a new wrapper! which is not easy](https://stackoverflow.com/questions/58170261/how-to-use-pandas-udf-in-class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:37:59.669724Z",
     "start_time": "2021-07-31T05:37:59.140401Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|multiply_func(x, x)|\n",
      "+-------------------+\n",
      "|                  1|\n",
      "|                  4|\n",
      "|                  9|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4. Pandas udf\n",
    "# documentation and concept\n",
    "# user defined function, but vectorlized by Arrow\n",
    "# 2.3.0\n",
    "# https://spark.apache.org/docs/2.3.0/sql-programming-guide.html#pandas-udfs-aka-vectorized-udfs\n",
    "# 3.0 support more!\n",
    "# https://spark.apache.org/docs/3.0.0/sql-pyspark-pandas-with-arrow.html#pandas-udfs-aka-vectorized-udfs\n",
    "\n",
    "\n",
    "# for 2.3.0\n",
    "#  Currently, there are two types of Pandas UDF: Scalar and Grouped Map.\n",
    "#  Input pd.Series, Output pd.Series\n",
    "\n",
    "# Scalar type\n",
    "@F.pandas_udf(returnType=T.LongType())\n",
    "def multiply_func(a, b):\n",
    "    return a * b\n",
    "\n",
    "\n",
    "x = pd.Series([1, 2, 3])\n",
    "\n",
    "df = spark.createDataFrame(pd.DataFrame(x), schema=[\"x\"])\n",
    "df.select(multiply_func(C(\"x\"), C(\"x\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:37:59.690551Z",
     "start_time": "2021-07-31T05:37:59.672218Z"
    }
   },
   "outputs": [],
   "source": [
    "# 5 from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "\n",
    "# # Use pandas_udf to define a Pandas UDF\n",
    "# @pandas_udf('double', PandasUDFType.SCALAR)\n",
    "# # Input/output are both a pandas.Series of doubles\n",
    "\n",
    "# def pandas_plus_one(x):\n",
    "#     return x + 1\n",
    "\n",
    "# df.withColumn('v2', pandas_plus_one(df.x))\n",
    "# df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:38:00.610356Z",
     "start_time": "2021-07-31T05:37:59.692070Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| ID|   v|\n",
      "+---+----+\n",
      "|  1| 1.0|\n",
      "|  1| 2.0|\n",
      "|  2| 3.0|\n",
      "|  2| 5.0|\n",
      "|  2|10.0|\n",
      "+---+----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joetsai/work/pixlake_spark_3/.venv/lib/python3.8/site-packages/pyspark/sql/pandas/group_ops.py:81: UserWarning: It is preferred to use 'applyInPandas' over this API. This API will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---------+-------------------+\n",
      "| ID|   v|new_col_1|          new_col_2|\n",
      "+---+----+---------+-------------------+\n",
      "|  1| 1.0|      0.0| 0.2549250844475971|\n",
      "|  1| 2.0|      1.0| 0.9105621163239953|\n",
      "|  2| 3.0|      0.0| 0.5849192992146328|\n",
      "|  2| 5.0|      1.0| 0.5600872898055889|\n",
      "|  2|10.0|      2.0|0.35722943932155393|\n",
      "+---+----+---------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6 pandasUDF return a dataframe\n",
    "# like a model prediction\n",
    "# # predict label and probability\n",
    "# A grouped map UDF defines transformation:\n",
    "# A pandas.DataFrame -> A pandas.DataFrame The returnType \n",
    "# should be a StructType describing the schema of the returned pandas.DataFrame.\n",
    "# The length of the returned pandas.DataFrame can be arbitrary and the columns must be indexed so that their position matches the corresponding field in the schema.\n",
    "# Grouped map UDFs are used with pyspark.sql.GroupedData.apply().\n",
    "# https://databricks.com/blog/2017/10/30/introducing-vectorized-udfs-for-pyspark.html\n",
    "\n",
    "# We can use BucketID for this ID\n",
    "\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "\n",
    "df = spark.createDataFrame(\n",
    "    [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)],\n",
    "    (\"ID\", \"v\"))\n",
    "\n",
    "df.show()\n",
    "########## case 1 ##############\n",
    "\n",
    "# @pandas_udf(\"id long, v double, n_rows long\", PandasUDFType.GROUPED_MAP)\n",
    "# def substract_mean(pdf):\n",
    "#     # pdf is a pandas.DataFrame\n",
    "#     n_rows = len(pdf)\n",
    "#     v = pdf.v\n",
    "#     return pdf.assign(\n",
    "#         v=v - v.mean(),\n",
    "#         n_rows=n_rows\n",
    "#     )\n",
    "\n",
    "# df.groupby(\"ID\").apply(substract_mean).show()\n",
    "\n",
    "######### case 2 ##############\n",
    "\n",
    "@pandas_udf(\"ID long, v double, new_col_1 double, new_col_2 double\", PandasUDFType.GROUPED_MAP)\n",
    "def get_more_col(pdf):\n",
    "    # pdf is a pandas.DataFrame\n",
    "    import random\n",
    "    n_counts = len(pdf)\n",
    "    \n",
    "    return pdf.assign(\n",
    "        new_col_1 = [i for i in range(n_counts)],\n",
    "        new_col_2 = [random.random() for i in range(n_counts)]\n",
    "    )\n",
    "# sdf groupby its bucket_id and apply\n",
    "\n",
    "df.groupby(\"ID\").apply(get_more_col).show()\n",
    "\n",
    "\n",
    "######## case 3 #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:38:02.038497Z",
     "start_time": "2021-07-31T05:38:00.613260Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joetsai/work/pixlake_spark_3/.venv/lib/python3.8/site-packages/pyspark/sql/pandas/functions.py:389: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  warnings.warn(\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------------+\n",
      "|article_id|             img_url|         img_b64_str|\n",
      "+----------+--------------------+--------------------+\n",
      "|     14431|https://pic.pimg....|/9j/2wCEAAoHBwgHB...|\n",
      "|     14431|https://pic.pimg....|/9j/2wCEAAoHBwgHB...|\n",
      "|     67789|https://pic.pimg....|/9j/4AAQSkZJRgABA...|\n",
      "+----------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 7 pandas udf return Null used for sending request to get images\n",
    "# return string for pandas_udf\n",
    "# https://stackoverflow.com/questions/65694026/spark-exception-error-using-pandas-udf-with-logical-statement\n",
    "from typing import Union\n",
    "import requests\n",
    "import base64\n",
    "\n",
    "\n",
    "@F.udf(returnType=StringType())\n",
    "def get_img_binary(url : str) -> str:\n",
    "    import requests\n",
    "    import base64\n",
    "    resp = requests.get(url, timeout=10)\n",
    "    if resp.status_code == 200:\n",
    "        return base64.encodestring(resp.content)\n",
    "    else:\n",
    "        return None\n",
    "# df = (\n",
    "#     df.withColumn(\"img_b64_str\", get_img_binary(C(\"img_url\")))\n",
    "# )\n",
    "\n",
    "\n",
    "# If you wanna dealing with a lot of data, use the rep\n",
    "# @F.pandas_udf('string', PandasUDFType.SCALAR)\n",
    "def download_img_to_b64_pd(img_url : pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    get image in the response and save to base64 string\n",
    "    data type flow : \n",
    "    resp.content - (bytes) \n",
    "    -> base64.encode - (bytes) \n",
    "    -> decode('utf-8') - str\n",
    "    \"\"\"\n",
    "    b64_str_list = []\n",
    "    for link in img_url:   \n",
    "        try:\n",
    "            resp = requests.get(link, timeout=10)\n",
    "            if resp.status_code == 200:\n",
    "                b64_str_list.append(base64.encodebytes(resp.content).decode('utf-8'))\n",
    "            else:\n",
    "                b64_str_list.append(None)\n",
    "        except Exception as e:\n",
    "            b64_str_list.append(None)\n",
    "    return pd.Series(b64_str_list)\n",
    "\n",
    "\n",
    "\n",
    "data = [\n",
    "    (14431,'https://pic.pimg.tw/happy78/1528543947-685380499_n.jpg'),\n",
    "    (14431,'https://pic.pimg.tw/happy78/1528543947-3623_n.jpg'), # This one will be not found\n",
    "    (14431,'https://pic.pimg.tw/happy78/1528543962-2265924582_n.jpg'),\n",
    "    (67789,'https://pic.pimg.tw/happy78/1528543962-4007835890_n.jpg'),\n",
    "    (67789,'https://pic.pimg.tw/happy78/1528543962-45890_n.jpg')\n",
    "]\n",
    "\n",
    "columns = ['article_id','img_url']\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=columns)\n",
    "# df.show(vertical=True, truncate=False)\n",
    "\n",
    "#             .withColumn(\n",
    "#                 \"bucket_id\",\n",
    "#                 F.udf(self.simple_random, returnType=\"integer\")(\"bucket_size\"),\n",
    "#             )\n",
    "\n",
    "df = (\n",
    "    df\\\n",
    "#     .withColumn(\"img_b64_str\", download_img_to_b64_pd(C(\"img_url\")))\n",
    "    .withColumn(\"img_b64_str\",F.pandas_udf(download_img_to_b64_pd, \"string\",PandasUDFType.SCALAR)(\"img_url\"))\n",
    "    .filter(C(\"img_b64_str\").isNotNull())\n",
    ")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:38:02.164183Z",
     "start_time": "2021-07-31T05:38:02.040946Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>img_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14431</td>\n",
       "      <td>https://pic.pimg.tw/happy78/1528543947-6853804...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14431</td>\n",
       "      <td>https://pic.pimg.tw/happy78/1528543947-3623_n.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14431</td>\n",
       "      <td>https://pic.pimg.tw/happy78/1528543962-2265924...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>67789</td>\n",
       "      <td>https://pic.pimg.tw/happy78/1528543962-4007835...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>67789</td>\n",
       "      <td>https://pic.pimg.tw/happy78/1528543962-45890_n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   article_id                                            img_url\n",
       "0       14431  https://pic.pimg.tw/happy78/1528543947-6853804...\n",
       "1       14431  https://pic.pimg.tw/happy78/1528543947-3623_n.jpg\n",
       "2       14431  https://pic.pimg.tw/happy78/1528543962-2265924...\n",
       "3       67789  https://pic.pimg.tw/happy78/1528543962-4007835...\n",
       "4       67789  https://pic.pimg.tw/happy78/1528543962-45890_n..."
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 8 pandas udf using generator\n",
    "# due to pd.Series return should under 2G\n",
    "# we're using another apporoach (GroupMap)\n",
    "\n",
    "def get_img_b64_generator(img_url : pd.Series):\n",
    "    '''\n",
    "    we can move this out of the udf\n",
    "    '''\n",
    "    for link in img_url:\n",
    "        try:\n",
    "            resp = requests.get(link, timeout=10)\n",
    "            if resp.status_code == 200:\n",
    "                yield base64.encodebytes(resp.content).decode('utf-8')\n",
    "            else:\n",
    "                yield None\n",
    "        except Exception as e:\n",
    "            yield None\n",
    "\n",
    "            \n",
    "return_type = \"article_id long, img_url string\"\n",
    "@F.pandas_udf(return_type,\n",
    "            F.PandasUDFType.GROUPED_MAP)\n",
    "def download_img_to_b64(df_with_url : pd.DataFrame) -> pd.DataFrame:\n",
    "    import requests\n",
    "    import base64\n",
    "    \n",
    "    img_url_series = df_with_img_b64.img_url\n",
    "    \n",
    "    # Using generator to avoid big series OOM/Serilization error\n",
    "    \n",
    "    return df_with_url.assign(\n",
    "        img_b64_str = img_type_series,\n",
    "        )\n",
    "\n",
    "data = [\n",
    "    (14431,'https://pic.pimg.tw/happy78/1528543947-685380499_n.jpg'),\n",
    "    (14431,'https://pic.pimg.tw/happy78/1528543947-3623_n.jpg'), # This one will be not found\n",
    "    (14431,'https://pic.pimg.tw/happy78/1528543962-2265924582_n.jpg'),\n",
    "    (67789,'https://pic.pimg.tw/happy78/1528543962-4007835890_n.jpg'),\n",
    "    (67789,'https://pic.pimg.tw/happy78/1528543962-45890_n.jpg')\n",
    "]\n",
    "\n",
    "columns = ['article_id','img_url']\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=columns)\n",
    "pdf = df.toPandas()\n",
    "pdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:38:02.201059Z",
     "start_time": "2021-07-31T05:38:02.166215Z"
    }
   },
   "outputs": [],
   "source": [
    "# find_all( name , attrs , recursive , string , **kwargs )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# broadcasting (2+)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:38:02.806690Z",
     "start_time": "2021-07-31T05:38:02.203388Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-------+-----+\n",
      "|firstname|lastname|country|state|\n",
      "+---------+--------+-------+-----+\n",
      "|    James|   Smith|    USA|   CA|\n",
      "|  Michael|    Rose|    USA|   NY|\n",
      "|   Robert|Williams|    USA|   CA|\n",
      "|    Maria|   Jones|    USA|   FL|\n",
      "+---------+--------+-------+-----+\n",
      "\n",
      "+---------+--------+-------+----------+\n",
      "|firstname|lastname|country|     state|\n",
      "+---------+--------+-------+----------+\n",
      "|    James|   Smith|    USA|California|\n",
      "|  Michael|    Rose|    USA|  New York|\n",
      "|   Robert|Williams|    USA|California|\n",
      "|    Maria|   Jones|    USA|   Florida|\n",
      "+---------+--------+-------+----------+\n",
      "\n",
      "+---------+--------+-------+-----+---------------+\n",
      "|firstname|lastname|country|state|converted_state|\n",
      "+---------+--------+-------+-----+---------------+\n",
      "|    James|   Smith|    USA|   CA|     California|\n",
      "|  Michael|    Rose|    USA|   NY|       New York|\n",
      "|   Robert|Williams|    USA|   CA|     California|\n",
      "|    Maria|   Jones|    USA|   FL|        Florida|\n",
      "+---------+--------+-------+-----+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1\n",
    "# broadcast the dictionary to spark \n",
    "# (which is a way that enhance multi-processing cross machine using your python code)\n",
    "# the broadcast variable should be serializable\n",
    "\n",
    "states = {\"NY\":\"New York\", \"CA\":\"California\", \"FL\":\"Florida\"}\n",
    "broadcastStates = spark.sparkContext.broadcast(states)\n",
    "\n",
    "data = [(\"James\",\"Smith\",\"USA\",\"CA\"),\n",
    "    (\"Michael\",\"Rose\",\"USA\",\"NY\"),\n",
    "    (\"Robert\",\"Williams\",\"USA\",\"CA\"),\n",
    "    (\"Maria\",\"Jones\",\"USA\",\"FL\")\n",
    "  ]\n",
    "\n",
    "columns = [\"firstname\",\"lastname\",\"country\",\"state\"]\n",
    "df = spark.createDataFrame(data = data, schema = columns)\n",
    "df.show(n=5)\n",
    "\n",
    "def state_convert(code):\n",
    "    return broadcastStates.value[code]\n",
    "\n",
    "# case 1, using rdd\n",
    "result_rdd = df.rdd.map(lambda x: (x[0],x[1],x[2],state_convert(x[3]))).toDF(columns)\n",
    "result_rdd.show(n=5)\n",
    "\n",
    "\n",
    "# case 2, using pdf\n",
    "\n",
    "@F.udf(returnType=StringType())\n",
    "def state_convert_udf(code : str) -> str:\n",
    "    return broadcastStates.value[code]\n",
    "\n",
    "result_df = (\n",
    "    df.withColumn(\"converted_state\", state_convert_udf(C(\"state\")))\n",
    ")\n",
    "\n",
    "result_df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:38:02.834032Z",
     "start_time": "2021-07-31T05:38:02.809796Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.broadcast.Broadcast'> ['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_jbroadcast', '_path', '_pickle_registry', '_python_broadcast', '_sc', 'destroy', 'dump', 'load', 'load_from_path', 'unpersist', 'value']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'NY': 'New York', 'CA': 'California', 'FL': 'Florida'}, dict)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2\n",
    "# Knowing broacsting object\n",
    "\n",
    "# broadcast the dictionary to spark \n",
    "# (which is a way that enhance multi-processing cross machine using your python code)\n",
    "# the broadcast variable should be serializable\n",
    "# https://spark.apache.org/docs/2.3.3/api/python/_modules/pyspark/broadcast.html\n",
    "states = {\"NY\":\"New York\", \"CA\":\"California\", \"FL\":\"Florida\"}\n",
    "broadcastStates = spark.sparkContext.broadcast(states)\n",
    "print(type(broadcastStates), dir(broadcastStates))\n",
    "\n",
    "# value to access the object\n",
    "broadcastStates.value, type(broadcastStates.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T16:45:51.194284Z",
     "start_time": "2021-06-25T16:45:51.076857Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export dataframe(3+)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:38:03.594913Z",
     "start_time": "2021-07-31T05:38:02.835484Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n",
      "|article_id|             img_url|\n",
      "+----------+--------------------+\n",
      "|     14431|https://pic.pimg....|\n",
      "|     14431|https://pic.pimg....|\n",
      "|     14431|https://pic.pimg....|\n",
      "|     67789|https://pic.pimg....|\n",
      "|     67789|https://pic.pimg....|\n",
      "+----------+--------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>img_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14431</td>\n",
       "      <td>https://pic.pimg.tw/happy78/1528543947-6853804...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14431</td>\n",
       "      <td>https://pic.pimg.tw/happy78/1528543947-3627597...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14431</td>\n",
       "      <td>https://pic.pimg.tw/happy78/1528543962-2265924...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>67789</td>\n",
       "      <td>https://pic.pimg.tw/happy78/1528543962-4007835...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>67789</td>\n",
       "      <td>https://pic.pimg.tw/happy78/1528543962-45890_n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   article_id                                            img_url\n",
       "0       14431  https://pic.pimg.tw/happy78/1528543947-6853804...\n",
       "1       14431  https://pic.pimg.tw/happy78/1528543947-3627597...\n",
       "2       14431  https://pic.pimg.tw/happy78/1528543962-2265924...\n",
       "3       67789  https://pic.pimg.tw/happy78/1528543962-4007835...\n",
       "4       67789  https://pic.pimg.tw/happy78/1528543962-45890_n..."
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1\n",
    "# write dataframe to jsonl format\n",
    "# https://stackoverflow.com/questions/43269244/pyspark-dataframe-write-to-single-json-file-with-specific-name\n",
    "# https://sparkbyexamples.com/pyspark/pyspark-read-json-file-into-dataframe/\n",
    "data = [\n",
    "    (14431,'https://pic.pimg.tw/happy78/1528543947-685380499_n.jpg'),\n",
    "    (14431,'https://pic.pimg.tw/happy78/1528543947-362759723_n.jpg'),\n",
    "    (14431,'https://pic.pimg.tw/happy78/1528543962-2265924582_n.jpg'),\n",
    "    (67789,'https://pic.pimg.tw/happy78/1528543962-4007835890_n.jpg'),\n",
    "    (67789,'https://pic.pimg.tw/happy78/1528543962-45890_n.jpg')\n",
    "]\n",
    "\n",
    "columns = ['article_id','img_url']\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "fname_folder = join('output','jsonl_format_folder.json')\n",
    "# This one will creat a folder contains part file for better multiple worker IO\n",
    "df.coalesce(1).write.format('json').save(fname_folder, mode='overwrite')\n",
    "df_new = spark.read.json(fname_folder)\n",
    "df_new.show(n=10)\n",
    "# However, if you wanna save it in a single file, use pandas\n",
    "fname = join('output','jsonl_format.json')\n",
    "# df.toPandas().to_json('path/file_name.json', orient='records', force_ascii=False, lines=True)\n",
    "df.toPandas().to_json(fname, orient='records',force_ascii=False,lines=True)\n",
    "df_new_pd = pd.read_json(fname,orient='records',lines=True)\n",
    "df_new_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:38:03.743420Z",
     "start_time": "2021-07-31T05:38:03.596334Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'to_json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_23463/3271730432.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m (\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'*'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m )\n",
      "\u001b[0;32m~/work/pixlake_spark_3/.venv/lib/python3.8/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1641\u001b[0m         \"\"\"\n\u001b[1;32m   1642\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1643\u001b[0;31m             raise AttributeError(\n\u001b[0m\u001b[1;32m   1644\u001b[0m                 \"'%s' object has no attribute '%s'\" % (self.__class__.__name__, name))\n\u001b[1;32m   1645\u001b[0m         \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'to_json'"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (14431,'https://pic.pimg.tw/happy78/1528543947-685380499_n.jpg'),\n",
    "    (14431,'https://pic.pimg.tw/happy78/1528543947-362759723_n.jpg'),\n",
    "    (14431,'https://pic.pimg.tw/happy78/1528543962-2265924582_n.jpg'),\n",
    "    (67789,'https://pic.pimg.tw/happy78/1528543962-4007835890_n.jpg'),\n",
    "    (67789,'https://pic.pimg.tw/happy78/1528543962-45890_n.jpg')\n",
    "]\n",
    "\n",
    "columns = ['article_id','img_url']\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "\n",
    "(\n",
    "    df\n",
    "    .select('*').to_json()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:38:03.745102Z",
     "start_time": "2021-07-31T05:38:03.745088Z"
    }
   },
   "outputs": [],
   "source": [
    "# 2 write parquet by date parittion\n",
    "\n",
    "\n",
    "# 1\n",
    "# write dataframe to jsonl format\n",
    "# https://sparkbyexamples.com/pyspark/pyspark-read-and-write-parquet-file/\n",
    "data_d1 = [\n",
    "    (14431,20210224,'https://pic.pimg.tw/happy78/1528543947-685380499_n.jpg'),\n",
    "    (14431,20210224,'https://pic.pimg.tw/happy78/1528543947-362759723_n.jpg'),\n",
    "    (14431,20210224,'https://pic.pimg.tw/happy78/1528543962-2265924582_n.jpg'),\n",
    "    (67789,20210224,'https://pic.pimg.tw/happy78/1528543962-4007835890_n.jpg'),\n",
    "    (67789,20210224,'https://pic.pimg.tw/happy78/1528543962-45890_n.jpg')\n",
    "]\n",
    "\n",
    "data_d2 = [\n",
    "    (86481,20210225,'https://pic.pimg.tw/happy78/1528543947-685380499_n.jpg'),\n",
    "    (45213,20210225,'https://pic.pimg.tw/happy78/1528543947-362759723_n.jpg'),\n",
    "    (24561,20210225,'https://pic.pimg.tw/happy78/1528543962-2265924582_n.jpg'),\n",
    "    (75371,20210225,'https://pic.pimg.tw/happy78/1528543962-4007835890_n.jpg'),\n",
    "    (25691,20210225,'https://pic.pimg.tw/happy78/1528543962-45890_n.jpg')\n",
    "]\n",
    "\n",
    "data_d3 = [\n",
    "    (7861,20210304,'https://pic.pimg.tw/happy78/1528543947-685380499_n.jpg'),\n",
    "    (45213,20210304,'https://pic.pimg.tw/happy78/1528543947-362759723_n.jpg'),\n",
    "    (1111,20210304,'https://pic.pimg.tw/happy78/1528543962-2265924582_n.jpg'),\n",
    "    (76661,20210304,'https://pic.pimg.tw/happy78/1528543962-4007835890_n.jpg'),\n",
    "    (8888,20210304,'https://pic.pimg.tw/happy78/1528543962-45890_n.jpg')\n",
    "]\n",
    "\n",
    "\n",
    "columns = ['article_id','date','img_url']\n",
    "\n",
    "df_d1 = spark.createDataFrame(data_d1, columns)\n",
    "df_d2 = spark.createDataFrame(data_d2, columns)\n",
    "df_d3 = spark.createDataFrame(data_d3, columns)\n",
    "\n",
    "for df in [df_d1, df_d2, df_d3]:\n",
    "    df.show(n=5)\n",
    "\n",
    "# save it\n",
    "parquet_fname = join(\"output\",\"save_by_date_partition.parquet\")\n",
    "for df in [df_d1, df_d2, df_d3]:\n",
    "    df.write.parquet(parquet_fname, mode=\"overwrite\", partitionBy=\"date\")\n",
    "\n",
    "# read by date range\n",
    "start_date = 20210224\n",
    "end_date = 20210301\n",
    "# support int and daterange, string might be problem\n",
    "\n",
    "new_df = spark.read.parquet(parquet_fname)\\\n",
    "         .where(C(\"date\").between(start_date, end_date))\n",
    "\n",
    "new_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:38:03.746060Z",
     "start_time": "2021-07-31T05:38:03.746045Z"
    }
   },
   "outputs": [],
   "source": [
    "# read it\n",
    "start_date = 20210224\n",
    "end_date = 20210304\n",
    "\n",
    "# df.filter(df.year >= myYear)\n",
    "new_df = spark.read.parquet(parquet_fname)\n",
    "date_range_cond = (new_df.date >= start_date) & (new_df.date <= end_date)\n",
    "new_df.filter(date_range_cond).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:38:03.747044Z",
     "start_time": "2021-07-31T05:38:03.747030Z"
    }
   },
   "outputs": [],
   "source": [
    "# 3 export data via api - I\n",
    "##### data sample, all of them is string type, we gonna send it by POST\n",
    "#     [\n",
    "#         {\n",
    "#             \"poi_hash\": \"aed525e93b72\",\n",
    "#             \"some-key\": \"some-data\",\n",
    "#             ...\n",
    "#         },\n",
    "#         ...\n",
    "#         {\n",
    "#             \"poi_hash\": \"aed525e93b72\",\n",
    "#             \"some-key\": \"some-data\",\n",
    "#             ...\n",
    "#         }\n",
    "#     ]\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "raw_data = [\n",
    "    (14431,20210224,'https://pic.pimg.tw/happy78/1528543947-685380499_n.jpg'),\n",
    "    (14432,20210224,'https://pic.pimg.tw/happy78/1528543947-362759723_n.jpg'),\n",
    "    (14433,20210224,'https://pic.pimg.tw/happy78/1528543962-2265924582_n.jpg'),\n",
    "    (67784,20210224,'https://pic.pimg.tw/happy78/1528543962-4007835890_n.jpg'),\n",
    "    (67785,20210224,'https://pic.pimg.tw/happy78/1528543962-45890_n.jpg')\n",
    "]\n",
    "col = ['id','date','img_url']\n",
    "raw_sdf = spark.createDataFrame(raw_data,col)\n",
    "raw_sdf.show()\n",
    "\n",
    "pack_sdf = (\n",
    "    raw_sdf\n",
    "    # Struct with build a structure like dictionary\n",
    "    .withColumn('package',F.struct('id','img_url'))\n",
    ")\n",
    "\n",
    "\n",
    "def export_data(row, key=None, endpoint=None, timeout=10):\n",
    "    # UDF will get Row object\n",
    "    row_in_list = [row.asDict()]\n",
    "#     try:\n",
    "#         resp = requests.post(\n",
    "#             url=endpoint,\n",
    "#             data=json.dumps(row_in_list),\n",
    "#             headers={\"x-authorization\" : key},\n",
    "#             timeout=timeout\n",
    "#         )\n",
    "        \n",
    "#         data = resp.json()\n",
    "#         log = ...\n",
    "#     except Exception as err:\n",
    "#         err_resp = str(resp.content, encoding='utf-8')\n",
    "#         error_msg = f\"WARNING: Got {err}. Error situation: {error_resp}\"\n",
    "        \n",
    "#         log = ...\n",
    "    return row_in_list\n",
    "\n",
    "\n",
    "api_log_sdf = (\n",
    "    pack_sdf.select(\n",
    "        'id',\n",
    "        'date',\n",
    "        F.udf(export_data, returnType='array<string>')('package')\n",
    "    )\n",
    ")\n",
    "\n",
    "api_log_sdf.show()\n",
    "api_log_sdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datetime related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:38:03.747922Z",
     "start_time": "2021-07-31T05:38:03.747908Z"
    }
   },
   "outputs": [],
   "source": [
    "# from unixtime to date\n",
    "\n",
    "data = [(2010, 10, 1285939860)]\n",
    "col = [\"_year\",\"_month\",\"upsert_at\"]\n",
    "\n",
    "df = spark.createDataFrame(data, col)\n",
    "df.show()\n",
    "\n",
    "(\n",
    "    df\n",
    "    .withColumn(\"date\",\n",
    "                F.from_unixtime(C(\"upsert_at\"),\"yyyyMMdd\").cast(\"integer\")\n",
    "               )\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:38:03.748880Z",
     "start_time": "2021-07-31T05:38:03.748867Z"
    }
   },
   "outputs": [],
   "source": [
    "# create a timestamp column\n",
    "\n",
    "data = [(2010, 10, 1285939860)]\n",
    "col = [\"_year\",\"_month\",\"upsert_at\"]\n",
    "\n",
    "df = spark.createDataFrame(data, col)\n",
    "df.show()\n",
    "\n",
    "(\n",
    "    df\n",
    "    .withColumn('update_at', F.unix_timestamp())\n",
    "    .withColumn('converted', F.from_unixtime(C('update_at'), \"yyyyMMdd\").cast('integer'))\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building pairs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:38:03.749697Z",
     "start_time": "2021-07-31T05:38:03.749684Z"
    }
   },
   "outputs": [],
   "source": [
    "# Cross join within a partition (or create pairs in partition)\n",
    "# Aka Permutation N 2 in partition\n",
    "\n",
    "# perform cross join in a window?\n",
    "# just N vs N inner join =)\n",
    "# https://stackoverflow.com/questions/53630342/sparksql-pyspark-crossjoin-over-dimension-for-a-specific-window\n",
    "\n",
    "# Permutation (3 2) = 6\n",
    "\n",
    "data = [\n",
    "    ('台北市信義區信義路20號','肉多多火鍋'),\n",
    "    ('台北市信義區信義路20號','文章牛肉湯台北信義店'),\n",
    "    ('台北市信義區信義路20號','7-11'),\n",
    "    ('台南市安平區安平路590號','文章牛肉湯總店'),\n",
    "]\n",
    "\n",
    "poi_sdf = spark.createDataFrame(data, ['addr','poi_name'])\n",
    "\n",
    "(\n",
    "     poi_sdf\n",
    "    .withColumnRenamed('poi_name','poi_name_A')\n",
    "    .join(\n",
    "        poi_sdf\n",
    "        .select(\n",
    "            'addr',\n",
    "            C('poi_name').alias('poi_name_B'),\n",
    "        ),\n",
    "        on=['addr']\n",
    "    )\n",
    "    .where(C(\"poi_name_A\") != C(\"poi_name_B\"))\n",
    ").show(truncate=False, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:38:03.750926Z",
     "start_time": "2021-07-31T05:38:03.750912Z"
    }
   },
   "outputs": [],
   "source": [
    "# Choose N, 2 in certian partition\n",
    "# Window function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:42:30.718627Z",
     "start_time": "2021-07-31T05:42:30.649964Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "field contain_score: Can not merge type <class 'pyspark.sql.types.DoubleType'> and <class 'pyspark.sql.types.LongType'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_23463/614803011.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0mraw_sdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_col\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0mpairs_sdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpairs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpairs_col\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m duplicated_hash_id = (\n",
      "\u001b[0;32m~/work/pixlake_spark_3/.venv/lib/python3.8/site-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    673\u001b[0m             return super(SparkSession, self).createDataFrame(\n\u001b[1;32m    674\u001b[0m                 data, schema, samplingRatio, verifySchema)\n\u001b[0;32m--> 675\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_create_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/pixlake_spark_3/.venv/lib/python3.8/site-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    698\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 700\u001b[0;31m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    701\u001b[0m         \u001b[0mjrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerDeUtil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoJavaArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_java_object_rdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplySchemaToPythonRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/pixlake_spark_3/.venv/lib/python3.8/site-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_createFromLocal\u001b[0;34m(self, data, schema)\u001b[0m\n\u001b[1;32m    510\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 512\u001b[0;31m             \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inferSchemaFromList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    513\u001b[0m             \u001b[0mconverter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_converter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstruct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/pixlake_spark_3/.venv/lib/python3.8/site-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_inferSchemaFromList\u001b[0;34m(self, data, names)\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"can not infer schema from empty dataset\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m         \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_merge_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_infer_schema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_has_nulltype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Some of types cannot be determined after inferring\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/pixlake_spark_3/.venv/lib/python3.8/site-packages/pyspark/sql/types.py\u001b[0m in \u001b[0;36m_merge_type\u001b[0;34m(a, b, name)\u001b[0m\n\u001b[1;32m   1105\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStructType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1106\u001b[0m         \u001b[0mnfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataType\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfields\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1107\u001b[0;31m         fields = [StructField(f.name, _merge_type(f.dataType, nfs.get(f.name, NullType()),\n\u001b[0m\u001b[1;32m   1108\u001b[0m                                                   name=new_name(f.name)))\n\u001b[1;32m   1109\u001b[0m                   for f in a.fields]\n",
      "\u001b[0;32m~/work/pixlake_spark_3/.venv/lib/python3.8/site-packages/pyspark/sql/types.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1105\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStructType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1106\u001b[0m         \u001b[0mnfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataType\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfields\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1107\u001b[0;31m         fields = [StructField(f.name, _merge_type(f.dataType, nfs.get(f.name, NullType()),\n\u001b[0m\u001b[1;32m   1108\u001b[0m                                                   name=new_name(f.name)))\n\u001b[1;32m   1109\u001b[0m                   for f in a.fields]\n",
      "\u001b[0;32m~/work/pixlake_spark_3/.venv/lib/python3.8/site-packages/pyspark/sql/types.py\u001b[0m in \u001b[0;36m_merge_type\u001b[0;34m(a, b, name)\u001b[0m\n\u001b[1;32m   1100\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1101\u001b[0m         \u001b[0;31m# TODO: type cast (such as int -> long)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1102\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Can not merge type %s and %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m     \u001b[0;31m# same type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: field contain_score: Can not merge type <class 'pyspark.sql.types.DoubleType'> and <class 'pyspark.sql.types.LongType'>"
     ]
    }
   ],
   "source": [
    "# filter duplicate poi with predict duplicates pair\n",
    "\n",
    "############## Get poi_suffix - JOIN ##############\n",
    "\n",
    "############## Build candidate pairs ##############\n",
    "\n",
    "############## Predict duplication ################\n",
    "\n",
    "############## pairs --> each hash? ###############\n",
    "\n",
    "############## Collect and filtered ###############\n",
    "\n",
    "raw = [\n",
    "    ('h1','鼎旺麻辣','a1'), # 3 same poi at a1\n",
    "    ('h2','鼎旺麻辣鴛鴦火鍋','a1'), # 3 same poi at a1\n",
    "    ('h3','海底撈','a2'), \n",
    "    ('h4','鼎旺麻辣火鍋','a1'), # 3 same poi at a1\n",
    "    ('h5','肉多多火鍋','a3'), # 2 same poi at a3\n",
    "    ('h6','肉多多火鍋文山景美店','a3'), # 2 same poi at a3\n",
    "    ('h7','婧 shupu','a2')\n",
    "]\n",
    "\n",
    "raw_col = ['hash_id','poi_name','addr']\n",
    "\n",
    "pairs = [\n",
    "    ('h1','鼎旺麻辣','h2','鼎旺麻辣鴛鴦火鍋',1.0, 1),\n",
    "    ('h1','鼎旺麻辣','h4','鼎旺麻辣火鍋',1.0,1),\n",
    "    ('h4','鼎旺麻辣火鍋','h2','鼎旺麻辣鴛鴦火鍋',0.66,1),\n",
    "    ('h5','肉多多火鍋','h6','肉多多火鍋文山景美店',1.0, 1),\n",
    "    ('h7','婧 shupu','h2','海底撈',0.0, 0) # negtive\n",
    "    # confuse sample\n",
    "]\n",
    "\n",
    "pairs_col = [\n",
    "    'hash_id_A','poi_name_A',\n",
    "     'hash_id_B','poi_name_B',\n",
    "    'contain_score',\n",
    "    'predict_contain_score'\n",
    "]\n",
    "\n",
    "raw_sdf = spark.createDataFrame(raw, raw_col)\n",
    "\n",
    "pairs_sdf = spark.createDataFrame(pairs, pairs_col)\n",
    "\n",
    "duplicated_hash_id = (\n",
    "    pairs_sdf\n",
    "    .where(C(\"predict_contain_score\") == 1)\n",
    "    .select(C('hash_id_B').alias('hash_id'))\n",
    ")\n",
    "\n",
    "\n",
    "#         # to group the duplicate poi for human judgement\n",
    "grouped_poi_sdf = (\n",
    "    pairs_sdf\n",
    "    .withColumn('similar_name',F.when(C(\"predict_contain_score\") == 1,\n",
    "                                      F.collect_list('poi_name_B')\n",
    "                                       .over(W.partitionBy(C(\"hash_id_A\"))))\n",
    "                                    .otherwise(None)\n",
    "                   )\n",
    "    .withColumn('hash_id_of_similar_name',F.when(C(\"predict_contain_score\") == 1,\n",
    "                                      F.collect_list('hash_id_B')\n",
    "                                       .over(W.partitionBy(C(\"hash_id_A\"))))\n",
    "                                    .otherwise(None)\n",
    "                   )\n",
    "    # add the explianation?\n",
    "    \n",
    "    # there are possible 1 hash_id_A with multiple similar poi_name_B\n",
    "    .drop_duplicates(subset=['hash_id_A'])\n",
    "    .select(\n",
    "            C(\"hash_id_A\").alias('hash_id'),\n",
    "            'similar_name',\n",
    "            'hash_id_of_similar_name'\n",
    "    )\n",
    "    \n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "grouped_filtered_poi = (\n",
    "    raw_sdf\n",
    "    .join(\n",
    "        duplicated_hash_id,\n",
    "        on=['hash_id'],\n",
    "        how='left_anti'\n",
    "    )\n",
    "    .join(\n",
    "        grouped_poi_sdf,\n",
    "        on=['hash_id'],\n",
    "        how='left'\n",
    "    )\n",
    ")\n",
    "\n",
    "raw_sdf.show()\n",
    "grouped_filtered_poi.show(n=20, truncate=False)\n",
    "# grouped_filtered_poi.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other (3+)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:38:03.752933Z",
     "start_time": "2021-07-31T05:38:03.752919Z"
    }
   },
   "outputs": [],
   "source": [
    "# 1\n",
    "# rank the food category popularity by store_name but crossed and rotated\n",
    "\n",
    "\n",
    "# create a mix ranking number\n",
    "# create a popularity_rank_score in each store_name\n",
    "    # Top popular in each store_name -> score 0\n",
    "    # Second popular in each store_name -> score 0.1\n",
    "# Add row number in each store_name, food_category -> cat_rank\n",
    "# Create category_popularity_mix_rank_score = cat_rank + popularity_rank_score\n",
    "# Sort the category_popularity_mix_rank_score by store_name\n",
    "\n",
    "data = [\n",
    "    (\"hotpop\",\"Meat\",3,),\n",
    "    (\"hotpop\",\"Meat\",3),\n",
    "    (\"hotpop\",\"Meat\",3),\n",
    "    (\"hotpop\",\"Vegetable\",2),\n",
    "    (\"hotpop\",\"Vegetable\",2),\n",
    "    (\"branch\",\"Fried food\",1),\n",
    "    (\"branch\",\"Dessert\",1),\n",
    "  ]\n",
    "\n",
    "columns = [\"store_name\",\"food_category\",\"food_category_popularity\"]\n",
    "df = spark.createDataFrame(data = data, schema = columns)\n",
    "\n",
    "print('before')\n",
    "df.show(n=10)\n",
    "\n",
    "################# sol #######################\n",
    "store_cat_pop_rank_score = F.when(C(\"store_cat_pop_rank\") == 1, 0)\\\n",
    "                            .when(C(\"store_cat_pop_rank\") == 2, 0.1)\n",
    "\n",
    "\n",
    "window_sotre_cat_pop = W.partitionBy('store_name').orderBy(C(\"food_category_popularity\").desc())\n",
    "window_sotre_cat = W.partitionBy(['store_name','food_category']).orderBy(C(\"food_category\"))\n",
    "window_sotr_cat_mix_rank = W.partitionBy(['store_name']).orderBy(C(\"mix_cat_pop_rank_score\"))\n",
    "df = (\n",
    "    df.withColumn(\"food_cat_pop_score\", 100 * C(\"food_category_popularity\") + 20 * F.randn(seed=42))\\\n",
    "      .withColumn(\"cat_idx\", F.row_number().over(window_sotre_cat))\\\n",
    "      .withColumn(\"store_cat_pop_rank\", F.dense_rank().over(window_sotre_cat_pop))\n",
    "      .withColumn(\"store_cat_pop_rank_score\", store_cat_pop_rank_score)\\\n",
    "      .withColumn(\"mix_cat_pop_rank_score\", C(\"cat_idx\") + C(\"store_cat_pop_rank_score\"))\\\n",
    "      .withColumn(\"mix_cat_pop_rank\", F.row_number().over(window_sotr_cat_mix_rank))\\\n",
    ")\n",
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:38:03.753992Z",
     "start_time": "2021-07-31T05:38:03.753978Z"
    }
   },
   "outputs": [],
   "source": [
    "# 2\n",
    "# create a food category popularity score\n",
    "# rank the food category popularity score but crossed and rotated\n",
    "\n",
    "\n",
    "\n",
    "# create a mix ranking number\n",
    "# create a popularity_rank_score in each store_name\n",
    "    # Top popular in each store_name -> score 0\n",
    "    # Second popular in each store_name -> score 0.1\n",
    "# Add row number in each store_name, food_category -> cat_rank\n",
    "# Create category_popularity_mix_rank_score = cat_rank + popularity_rank_score\n",
    "# Sort the category_popularity_mix_rank_score by store_name\n",
    "\n",
    "data = [\n",
    "    (\"hotpop\",\"Meat\",3,),\n",
    "    (\"hotpop\",\"Meat\",3),\n",
    "    (\"hotpop\",\"Meat\",3),\n",
    "    (\"hotpop\",\"Vegetable\",2),\n",
    "    (\"hotpop\",\"Vegetable\",2),\n",
    "    (\"branch\",\"Fried food\",1),\n",
    "    (\"branch\",\"Dessert\",1),\n",
    "  ]\n",
    "\n",
    "columns = [\"store_name\",\"food_category\",\"food_category_popularity\"]\n",
    "df = spark.createDataFrame(data = data, schema = columns)\n",
    "\n",
    "print('before')\n",
    "df.show(n=10)\n",
    "\n",
    "################# sol 1 #######################\n",
    "store_cat_pop_rank_score = F.when(C(\"store_cat_pop_rank\") == 1, 0)\\\n",
    "                            .when(C(\"store_cat_pop_rank\") == 2, 0.1)\n",
    "\n",
    "\n",
    "window_sotre_cat_pop = W.partitionBy('store_name').orderBy(C(\"food_category_popularity\").desc())\n",
    "window_sotre_cat = W.partitionBy(['store_name','food_category']).orderBy(C(\"food_cat_pop_score\").desc())\n",
    "window_sotr_cat_mix_rank = W.partitionBy(['store_name']).orderBy(C(\"mix_cat_pop_rank_score\"))\n",
    "df = (\n",
    "    df.withColumn(\"food_cat_pop_score\",\n",
    "                  F.round(100 * C(\"food_category_popularity\") + 20 * F.randn(seed=42))\n",
    "                 )\\\n",
    "      .withColumn(\"cat_idx\", F.row_number().over(window_sotre_cat))\\\n",
    "      .withColumn(\"store_cat_pop_rank\", F.dense_rank().over(window_sotre_cat_pop))\n",
    "      .withColumn(\"store_cat_pop_rank_score\", store_cat_pop_rank_score)\\\n",
    "      .withColumn(\"mix_cat_pop_rank_score\", C(\"cat_idx\") + C(\"store_cat_pop_rank_score\"))\\\n",
    "      .withColumn(\"mix_cat_pop_rank\", F.row_number().over(window_sotr_cat_mix_rank))\\\n",
    ")\n",
    "\n",
    "print('sol - 1')\n",
    "display(df.toPandas())\n",
    "\n",
    "############### sol 2 ########################\n",
    "\n",
    "# Use another way to sort it\n",
    "# sort by cat_idx and store_cat_pop_rank\n",
    "\n",
    "\n",
    "\n",
    "category_id_in_poi = W.partitionBy(['store_name','food_category']).orderBy(C(\"food_category\"))\n",
    "category_popularity_rank_in_poi = W.partitionBy('store_name').orderBy(C(\"food_category_popularity\").desc())\n",
    "window_sotr_cat_mix_rank = W.partitionBy(['store_name']).orderBy(\n",
    "    C(\"category_id_in_poi\"),\n",
    "    C(\"category_popularity_rank_in_poi\")\n",
    ")\n",
    "df_sol_2 = (\n",
    "    df.withColumn(\"food_cat_pop_score\", 100 * C(\"food_category_popularity\") + 20 * F.randn(seed=42))\\\n",
    "      .withColumn(\"category_id_in_poi\", F.row_number().over(category_id_in_poi))\\\n",
    "      .withColumn(\"category_popularity_rank_in_poi\", F.dense_rank().over(category_popularity_rank_in_poi))\n",
    "      .withColumn(\"staggered_rank\", F.row_number().over(window_sotr_cat_mix_rank))\\\n",
    ")\n",
    "\n",
    "print('so1 - 2')\n",
    "\n",
    "display(df_sol_2.toPandas())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:38:03.755145Z",
     "start_time": "2021-07-31T05:38:03.755131Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 3\n",
    "\n",
    "print('last-aware dense_rank')\n",
    "\n",
    "from typing import List\n",
    "def last_aware_dense_rank(\n",
    "    sdf,\n",
    "    partition_by : List[str],\n",
    "    order_by : List[C],\n",
    "    id_col : List[str],\n",
    "    output_col : str\n",
    "    ):\n",
    "    w = W.partitionBy(partition_by).orderBy(*order_by)\n",
    "    return (\n",
    "        sdf\n",
    "        .withColumn(\"dense_rank\", F.dense_rank().over(w))\n",
    "        .withColumn(\"last_in_partition\",\n",
    "                F.size(F.collect_set(*id_col).over(w)))\n",
    "        .withColumn('max_in_partition',\n",
    "                F.max('dense_rank')\n",
    "                .over(\n",
    "                 w\n",
    "                .rowsBetween(W.unboundedPreceding, W.unboundedFollowing)\n",
    "               ))\n",
    "        .withColumn('last-aware_dense_rank',\n",
    "                F.when(C(\"dense_rank\") >= C('max_in_partition'),\n",
    "                       C(\"last_in_partition\"))\n",
    "                 .otherwise(C(\"dense_rank\"))\n",
    "               )\n",
    "        .drop('dense_rank','last_in_partition','max_in_partition')\n",
    "        .withColumnRenamed('last-aware_dense_rank', output_col)\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "data = [\n",
    "            (\"a\",10),('a',10), ('a',15), ('a',20), ('a',5),\n",
    "            ('a',0), ('a',0), ('a',0), ('a',0), ('a',0),\n",
    "            ('b',3), ('b',2),('c',0), ('c',0),\n",
    "            ('d', 2), ('d', 2), ('d', 0)\n",
    "            ]\n",
    "\n",
    "indexed_data = [\n",
    "        (idx, *element)\n",
    "        for idx, element\n",
    "        in enumerate(data)\n",
    "]\n",
    "\n",
    "print(indexed_data)\n",
    "\n",
    "columns = ['id','segment','score']\n",
    "\n",
    "df = spark.createDataFrame(indexed_data, columns)\n",
    "\n",
    "last_aware_dense_rank(\n",
    "        df,\n",
    "        partition_by=['segment'],\n",
    "        order_by=[C('score').desc()],\n",
    "        id_col=['id'],\n",
    "        output_col='last-aware_dense_rank'\n",
    "        ).show(20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:38:03.756015Z",
     "start_time": "2021-07-31T05:38:03.756001Z"
    }
   },
   "outputs": [],
   "source": [
    "# 3 Knowing the functions of dataframe operation \n",
    "\n",
    "dir(F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:38:03.756895Z",
     "start_time": "2021-07-31T05:38:03.756880Z"
    }
   },
   "outputs": [],
   "source": [
    "# 4 explode_outer\n",
    "# https://spark.apache.org/docs/2.3.0/api/python/pyspark.sql.html#pyspark.sql.GroupedData.apply\n",
    "\n",
    "# return a new row for each element in the given array or map\n",
    "# Unlike explode, if the array/map is null or empty\n",
    "# the null is produced\n",
    "\n",
    "df = spark.createDataFrame(\n",
    "    [\n",
    "        (1, [\"foo\",\"bar\"], {\"x\" : 1.0}),\n",
    "        (2, [], {}),\n",
    "        (3, None, None)\n",
    "    ],\n",
    "    (\"id\", \"an_array\",\"a_map\")\n",
    ")\n",
    "\n",
    "df.show()\n",
    "\n",
    "df.select(\"id\", F.explode_outer(C(\"a_map\"))).show()\n",
    "\n",
    "df.select(\"id\", F.explode(C(\"a_map\"))).show() # null thing will be nothing\n",
    "\n",
    "df.select(\"id\", F.explode_outer(C(\"an_array\"))).show()\n",
    "\n",
    "df.select(\"id\", F.explode(C(\"an_array\"))).show() # null thing will be nothing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:38:03.757964Z",
     "start_time": "2021-07-31T05:38:03.757949Z"
    }
   },
   "outputs": [],
   "source": [
    "# 4 iterate your dataframe row by row\n",
    "# you will not use it in production\n",
    "# but it is useful when debugging and develop your algorithm\n",
    "\n",
    "df = spark.createDataFrame(\n",
    "    [\n",
    "        (1, [\"foo\",\"bar\"], {\"x\" : 1.0}),\n",
    "        (2, [], {}),\n",
    "        (3, None, None)\n",
    "    ],\n",
    "    (\"id\", \"an_array\",\"a_map\")\n",
    ")\n",
    "\n",
    "\n",
    "print('default you will get row object')\n",
    "print('you can convert row into dataframe')\n",
    "print('sometimes you might inidcate schema')\n",
    "print('\\n\\n\\n')\n",
    "    \n",
    "for row_idx, row in enumerate(df.rdd.toLocalIterator()):\n",
    "    print(row_idx, type(row))\n",
    "    slice_sdf = spark.createDataFrame([row],schema=df.schema)\n",
    "    slice_sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T05:38:03.759222Z",
     "start_time": "2021-07-31T05:38:03.759207Z"
    }
   },
   "outputs": [],
   "source": [
    "# cache and persistant\n",
    "\n",
    "# cache and persist, spark provides an optimization mechanism to astore thre indermediate compurtation \n",
    "# of a Spark DataFrame so they can be resued in subsequent actions\n",
    "\n",
    "# persist -> each node stores it's partitioned data in memory and reuse them in other actions on the datasaet\n",
    "\n",
    "# Cost efficient - Spark computations are very expensive hence reusing the computations are used to save cost\n",
    "\n",
    "# Time efficient - Reusing the repeated computations save lots of time.\n",
    "\n",
    "\n",
    "# SparkDataFrame.cache() storage level `MEMORY_AND_DISK`\n",
    "# RDD.cache() storage level `MEMORY_ONLY`\n",
    "\n",
    "\n",
    "data = [\n",
    "    (\"hotpop\",\"Meat\",3,),\n",
    "    (\"hotpop\",\"Meat\",3),\n",
    "    (\"hotpop\",\"Meat\",3),\n",
    "    (\"hotpop\",\"Vegetable\",2),\n",
    "    (\"hotpop\",\"Vegetable\",2),\n",
    "    (\"branch\",\"Fried food\",1),\n",
    "    (\"branch\",\"Dessert\",1),\n",
    "  ]\n",
    "\n",
    "columns = [\"store_name\",\"food_category\",\"food_category_popularity\"]\n",
    "\n",
    "# first df\n",
    "df = spark.createDataFrame(data = data, schema = columns)\n",
    "print('df1', df.explain())\n",
    "# second stage\n",
    "\n",
    "df2 = df.where(C(\"store_name\") == \"branch\").cache()\n",
    "\n",
    "print('df2', df2.explain())\n",
    "\n",
    "# third stage\n",
    "df3 = df2.where(\n",
    "    C(\"food_category\") == \"Dessert\"\n",
    ")\n",
    "\n",
    "print('df3', df3.explain())\n",
    "\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pixlake_spark3",
   "language": "python",
   "name": "pixlake_spark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "241.484px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
