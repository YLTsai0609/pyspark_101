{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-21T14:16:39.091850Z",
     "start_time": "2021-03-21T14:16:39.070315Z"
    }
   },
   "outputs": [],
   "source": [
    "# env : pixlake\n",
    "# we focuing on pyspark dataframe processing\n",
    "# documentation https://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-21T14:16:39.266534Z",
     "start_time": "2021-03-21T14:16:39.093263Z"
    }
   },
   "outputs": [],
   "source": [
    "# make you auto compeletion faster\n",
    "# https://stackoverflow.com/questions/40536560/ipython-and-jupyter-autocomplete-not-working\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-21T14:16:39.362549Z",
     "start_time": "2021-03-21T14:16:39.268812Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have pyspark version :  ['spark-2.3', 'spark-3.0', 'spark-3.0.1-bin-hadoop2.7', 'spark-2.3.4-bin-hadoop2.7', 'spark-2.4.7-bin-hadoop2.7', 'spark-2.4']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "print('You have pyspark version : ', os.listdir('/opt/spark/versions'))\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "# spark-2.3, spark-2.4\n",
    "os.environ['SPARK_HOME'] = '/opt/spark/versions/spark-2.3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-21T14:16:39.620188Z",
     "start_time": "2021-03-21T14:16:39.365425Z"
    }
   },
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "from pyspark.sql import SparkSession as Session\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark import SparkConf as Conf\n",
    "from pyspark.sql import functions as F, Window as W, types as T\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "C = F.col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-21T14:16:39.629218Z",
     "start_time": "2021-03-21T14:16:39.621575Z"
    }
   },
   "outputs": [],
   "source": [
    "conf = (Conf()\n",
    "    .set('spark.sql.sources.partitionOverwriteMode', 'dynamic')\n",
    "    .set('spark.driver.memory', '4g')\n",
    "    .set('spark.driver.maxResultSize', '1g')\n",
    "   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-21T14:16:42.160178Z",
     "start_time": "2021-03-21T14:16:39.630408Z"
    }
   },
   "outputs": [],
   "source": [
    "spark = (Session\n",
    "     .builder\n",
    "     .appName('pyspark-challenge')\n",
    "     .master('local[2]')\n",
    "     .config(conf=conf)\n",
    "     .getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-21T14:16:42.173319Z",
     "start_time": "2021-03-21T14:16:42.161740Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Builder', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__enter__', '__eq__', '__exit__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_convert_from_pandas', '_createFromLocal', '_createFromRDD', '_create_from_pandas_with_arrow', '_get_numpy_record_dtype', '_inferSchema', '_inferSchemaFromList', '_instantiatedSession', '_jsc', '_jsparkSession', '_jvm', '_jwrapped', '_repr_html_', '_sc', '_wrapped', 'builder', 'catalog', 'conf', 'createDataFrame', 'newSession', 'range', 'read', 'readStream', 'sparkContext', 'sql', 'stop', 'streams', 'table', 'udf', 'version']\n",
      "\n",
      "your spark version : 2.3.4\n"
     ]
    }
   ],
   "source": [
    "# 0. know what spark session can do and its version\n",
    "print(dir(spark), f'your spark version : {spark.version}'\n",
    "      , sep='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-21T14:16:45.527704Z",
     "start_time": "2021-03-21T14:16:42.174891Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['zipcode1.json', 'titanic_train.csv', 'small_zipcode.csv', 'Meteorite_Landings.csv', 'zipcodes.csv', 'zipcodes.json', 'webpage_1.txt', 'multiline-zipcode.json', 'simple_text.txt', 'zipcode2.json', 'titanic_test.csv']\n",
      "root\n",
      " |-- RecordNumber: integer (nullable = true)\n",
      " |-- Zipcode: integer (nullable = true)\n",
      " |-- ZipCodeType: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- LocationType: string (nullable = true)\n",
      " |-- Lat: double (nullable = true)\n",
      " |-- Long: double (nullable = true)\n",
      " |-- Xaxis: double (nullable = true)\n",
      " |-- Yaxis: double (nullable = true)\n",
      " |-- Zaxis: double (nullable = true)\n",
      " |-- WorldRegion: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- LocationText: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- Decommisioned: boolean (nullable = true)\n",
      " |-- TaxReturnsFiled: integer (nullable = true)\n",
      " |-- EstimatedPopulation: integer (nullable = true)\n",
      " |-- TotalWages: integer (nullable = true)\n",
      " |-- Notes: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RecordNumber</th>\n",
       "      <th>Zipcode</th>\n",
       "      <th>ZipCodeType</th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>LocationType</th>\n",
       "      <th>Lat</th>\n",
       "      <th>Long</th>\n",
       "      <th>Xaxis</th>\n",
       "      <th>Yaxis</th>\n",
       "      <th>Zaxis</th>\n",
       "      <th>WorldRegion</th>\n",
       "      <th>Country</th>\n",
       "      <th>LocationText</th>\n",
       "      <th>Location</th>\n",
       "      <th>Decommisioned</th>\n",
       "      <th>TaxReturnsFiled</th>\n",
       "      <th>EstimatedPopulation</th>\n",
       "      <th>TotalWages</th>\n",
       "      <th>Notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>704</td>\n",
       "      <td>STANDARD</td>\n",
       "      <td>PARC PARQUE</td>\n",
       "      <td>PR</td>\n",
       "      <td>NOT ACCEPTABLE</td>\n",
       "      <td>17.96</td>\n",
       "      <td>-66.22</td>\n",
       "      <td>0.38</td>\n",
       "      <td>-0.87</td>\n",
       "      <td>0.30</td>\n",
       "      <td>NA</td>\n",
       "      <td>US</td>\n",
       "      <td>Parc Parque, PR</td>\n",
       "      <td>NA-US-PR-PARC PARQUE</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>704</td>\n",
       "      <td>STANDARD</td>\n",
       "      <td>PASEO COSTA DEL SUR</td>\n",
       "      <td>PR</td>\n",
       "      <td>NOT ACCEPTABLE</td>\n",
       "      <td>17.96</td>\n",
       "      <td>-66.22</td>\n",
       "      <td>0.38</td>\n",
       "      <td>-0.87</td>\n",
       "      <td>0.30</td>\n",
       "      <td>NA</td>\n",
       "      <td>US</td>\n",
       "      <td>Paseo Costa Del Sur, PR</td>\n",
       "      <td>NA-US-PR-PASEO COSTA DEL SUR</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>709</td>\n",
       "      <td>STANDARD</td>\n",
       "      <td>BDA SAN LUIS</td>\n",
       "      <td>PR</td>\n",
       "      <td>NOT ACCEPTABLE</td>\n",
       "      <td>18.14</td>\n",
       "      <td>-66.26</td>\n",
       "      <td>0.38</td>\n",
       "      <td>-0.86</td>\n",
       "      <td>0.31</td>\n",
       "      <td>NA</td>\n",
       "      <td>US</td>\n",
       "      <td>Bda San Luis, PR</td>\n",
       "      <td>NA-US-PR-BDA SAN LUIS</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>61391</td>\n",
       "      <td>76166</td>\n",
       "      <td>UNIQUE</td>\n",
       "      <td>CINGULAR WIRELESS</td>\n",
       "      <td>TX</td>\n",
       "      <td>NOT ACCEPTABLE</td>\n",
       "      <td>32.72</td>\n",
       "      <td>-97.31</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>-0.83</td>\n",
       "      <td>0.54</td>\n",
       "      <td>NA</td>\n",
       "      <td>US</td>\n",
       "      <td>Cingular Wireless, TX</td>\n",
       "      <td>NA-US-TX-CINGULAR WIRELESS</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>61392</td>\n",
       "      <td>76177</td>\n",
       "      <td>STANDARD</td>\n",
       "      <td>FORT WORTH</td>\n",
       "      <td>TX</td>\n",
       "      <td>PRIMARY</td>\n",
       "      <td>32.75</td>\n",
       "      <td>-97.33</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>-0.83</td>\n",
       "      <td>0.54</td>\n",
       "      <td>NA</td>\n",
       "      <td>US</td>\n",
       "      <td>Fort Worth, TX</td>\n",
       "      <td>NA-US-TX-FORT WORTH</td>\n",
       "      <td>False</td>\n",
       "      <td>2126.0</td>\n",
       "      <td>4053.0</td>\n",
       "      <td>122396986.0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RecordNumber  Zipcode ZipCodeType                 City State  \\\n",
       "0             1      704    STANDARD          PARC PARQUE    PR   \n",
       "1             2      704    STANDARD  PASEO COSTA DEL SUR    PR   \n",
       "2            10      709    STANDARD         BDA SAN LUIS    PR   \n",
       "3         61391    76166      UNIQUE    CINGULAR WIRELESS    TX   \n",
       "4         61392    76177    STANDARD           FORT WORTH    TX   \n",
       "\n",
       "     LocationType    Lat   Long  Xaxis  Yaxis  Zaxis WorldRegion Country  \\\n",
       "0  NOT ACCEPTABLE  17.96 -66.22   0.38  -0.87   0.30          NA      US   \n",
       "1  NOT ACCEPTABLE  17.96 -66.22   0.38  -0.87   0.30          NA      US   \n",
       "2  NOT ACCEPTABLE  18.14 -66.26   0.38  -0.86   0.31          NA      US   \n",
       "3  NOT ACCEPTABLE  32.72 -97.31  -0.10  -0.83   0.54          NA      US   \n",
       "4         PRIMARY  32.75 -97.33  -0.10  -0.83   0.54          NA      US   \n",
       "\n",
       "              LocationText                      Location  Decommisioned  \\\n",
       "0          Parc Parque, PR          NA-US-PR-PARC PARQUE          False   \n",
       "1  Paseo Costa Del Sur, PR  NA-US-PR-PASEO COSTA DEL SUR          False   \n",
       "2         Bda San Luis, PR         NA-US-PR-BDA SAN LUIS          False   \n",
       "3    Cingular Wireless, TX    NA-US-TX-CINGULAR WIRELESS          False   \n",
       "4           Fort Worth, TX           NA-US-TX-FORT WORTH          False   \n",
       "\n",
       "   TaxReturnsFiled  EstimatedPopulation   TotalWages Notes  \n",
       "0              NaN                  NaN          NaN  None  \n",
       "1              NaN                  NaN          NaN  None  \n",
       "2              NaN                  NaN          NaN  None  \n",
       "3              NaN                  NaN          NaN  None  \n",
       "4           2126.0               4053.0  122396986.0  None  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. read data from csv\n",
    "print(os.listdir('../data'))\n",
    "df_from_csv_1 = spark.read.csv('../data/zipcodes.csv',\n",
    "                               header=True,\n",
    "                              inferSchema=True)\n",
    "df_from_csv_1.printSchema()\n",
    "df_from_csv_1.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-21T14:16:45.819336Z",
     "start_time": "2021-03-21T14:16:45.530221Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['zipcode1.json', 'titanic_train.csv', 'small_zipcode.csv', 'Meteorite_Landings.csv', 'zipcodes.csv', 'zipcodes.json', 'webpage_1.txt', 'multiline-zipcode.json', 'simple_text.txt', 'zipcode2.json', 'titanic_test.csv']\n",
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_df', '_jreader', '_set_opts', '_spark', 'csv', 'format', 'jdbc', 'json', 'load', 'option', 'options', 'orc', 'parquet', 'schema', 'table', 'text']\n",
      "root\n",
      " |-- City: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Decommisioned: boolean (nullable = true)\n",
      " |-- EstimatedPopulation: long (nullable = true)\n",
      " |-- Lat: double (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- LocationText: string (nullable = true)\n",
      " |-- LocationType: string (nullable = true)\n",
      " |-- Long: double (nullable = true)\n",
      " |-- Notes: string (nullable = true)\n",
      " |-- RecordNumber: long (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- TaxReturnsFiled: long (nullable = true)\n",
      " |-- TotalWages: long (nullable = true)\n",
      " |-- WorldRegion: string (nullable = true)\n",
      " |-- Xaxis: double (nullable = true)\n",
      " |-- Yaxis: double (nullable = true)\n",
      " |-- Zaxis: double (nullable = true)\n",
      " |-- ZipCodeType: string (nullable = true)\n",
      " |-- Zipcode: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>Country</th>\n",
       "      <th>Decommisioned</th>\n",
       "      <th>EstimatedPopulation</th>\n",
       "      <th>Lat</th>\n",
       "      <th>Location</th>\n",
       "      <th>LocationText</th>\n",
       "      <th>LocationType</th>\n",
       "      <th>Long</th>\n",
       "      <th>Notes</th>\n",
       "      <th>RecordNumber</th>\n",
       "      <th>State</th>\n",
       "      <th>TaxReturnsFiled</th>\n",
       "      <th>TotalWages</th>\n",
       "      <th>WorldRegion</th>\n",
       "      <th>Xaxis</th>\n",
       "      <th>Yaxis</th>\n",
       "      <th>Zaxis</th>\n",
       "      <th>ZipCodeType</th>\n",
       "      <th>Zipcode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PARC PARQUE</td>\n",
       "      <td>US</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.96</td>\n",
       "      <td>NA-US-PR-PARC PARQUE</td>\n",
       "      <td>Parc Parque, PR</td>\n",
       "      <td>NOT ACCEPTABLE</td>\n",
       "      <td>-66.22</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>PR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NA</td>\n",
       "      <td>0.38</td>\n",
       "      <td>-0.87</td>\n",
       "      <td>0.30</td>\n",
       "      <td>STANDARD</td>\n",
       "      <td>704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PASEO COSTA DEL SUR</td>\n",
       "      <td>US</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.96</td>\n",
       "      <td>NA-US-PR-PASEO COSTA DEL SUR</td>\n",
       "      <td>Paseo Costa Del Sur, PR</td>\n",
       "      <td>NOT ACCEPTABLE</td>\n",
       "      <td>-66.22</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>PR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NA</td>\n",
       "      <td>0.38</td>\n",
       "      <td>-0.87</td>\n",
       "      <td>0.30</td>\n",
       "      <td>STANDARD</td>\n",
       "      <td>704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BDA SAN LUIS</td>\n",
       "      <td>US</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.14</td>\n",
       "      <td>NA-US-PR-BDA SAN LUIS</td>\n",
       "      <td>Bda San Luis, PR</td>\n",
       "      <td>NOT ACCEPTABLE</td>\n",
       "      <td>-66.26</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>PR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NA</td>\n",
       "      <td>0.38</td>\n",
       "      <td>-0.86</td>\n",
       "      <td>0.31</td>\n",
       "      <td>STANDARD</td>\n",
       "      <td>709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CINGULAR WIRELESS</td>\n",
       "      <td>US</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32.72</td>\n",
       "      <td>NA-US-TX-CINGULAR WIRELESS</td>\n",
       "      <td>Cingular Wireless, TX</td>\n",
       "      <td>NOT ACCEPTABLE</td>\n",
       "      <td>-97.31</td>\n",
       "      <td>None</td>\n",
       "      <td>61391</td>\n",
       "      <td>TX</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NA</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>-0.83</td>\n",
       "      <td>0.54</td>\n",
       "      <td>UNIQUE</td>\n",
       "      <td>76166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FORT WORTH</td>\n",
       "      <td>US</td>\n",
       "      <td>False</td>\n",
       "      <td>4053.0</td>\n",
       "      <td>32.75</td>\n",
       "      <td>NA-US-TX-FORT WORTH</td>\n",
       "      <td>Fort Worth, TX</td>\n",
       "      <td>PRIMARY</td>\n",
       "      <td>-97.33</td>\n",
       "      <td>None</td>\n",
       "      <td>61392</td>\n",
       "      <td>TX</td>\n",
       "      <td>2126.0</td>\n",
       "      <td>122396986.0</td>\n",
       "      <td>NA</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>-0.83</td>\n",
       "      <td>0.54</td>\n",
       "      <td>STANDARD</td>\n",
       "      <td>76177</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  City Country  Decommisioned  EstimatedPopulation    Lat  \\\n",
       "0          PARC PARQUE      US          False                  NaN  17.96   \n",
       "1  PASEO COSTA DEL SUR      US          False                  NaN  17.96   \n",
       "2         BDA SAN LUIS      US          False                  NaN  18.14   \n",
       "3    CINGULAR WIRELESS      US          False                  NaN  32.72   \n",
       "4           FORT WORTH      US          False               4053.0  32.75   \n",
       "\n",
       "                       Location             LocationText    LocationType  \\\n",
       "0          NA-US-PR-PARC PARQUE          Parc Parque, PR  NOT ACCEPTABLE   \n",
       "1  NA-US-PR-PASEO COSTA DEL SUR  Paseo Costa Del Sur, PR  NOT ACCEPTABLE   \n",
       "2         NA-US-PR-BDA SAN LUIS         Bda San Luis, PR  NOT ACCEPTABLE   \n",
       "3    NA-US-TX-CINGULAR WIRELESS    Cingular Wireless, TX  NOT ACCEPTABLE   \n",
       "4           NA-US-TX-FORT WORTH           Fort Worth, TX         PRIMARY   \n",
       "\n",
       "    Long Notes  RecordNumber State  TaxReturnsFiled   TotalWages WorldRegion  \\\n",
       "0 -66.22  None             1    PR              NaN          NaN          NA   \n",
       "1 -66.22  None             2    PR              NaN          NaN          NA   \n",
       "2 -66.26  None            10    PR              NaN          NaN          NA   \n",
       "3 -97.31  None         61391    TX              NaN          NaN          NA   \n",
       "4 -97.33  None         61392    TX           2126.0  122396986.0          NA   \n",
       "\n",
       "   Xaxis  Yaxis  Zaxis ZipCodeType  Zipcode  \n",
       "0   0.38  -0.87   0.30    STANDARD      704  \n",
       "1   0.38  -0.87   0.30    STANDARD      704  \n",
       "2   0.38  -0.86   0.31    STANDARD      709  \n",
       "3  -0.10  -0.83   0.54      UNIQUE    76166  \n",
       "4  -0.10  -0.83   0.54    STANDARD    76177  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2 read data from json\n",
    "print(os.listdir('../data'))\n",
    "print(dir(spark.read))\n",
    "# 沒有infer_schema\n",
    "df_from_json = spark.read.json('../data/zipcodes.json')\n",
    "df_from_json.printSchema()\n",
    "df_from_json.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-21T14:16:46.447660Z",
     "start_time": "2021-03-21T14:16:45.821213Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+\n",
      "|language|user_counts|\n",
      "+--------+-----------+\n",
      "|    Java|      20000|\n",
      "|  Python|     100000|\n",
      "|   Scala|       3000|\n",
      "+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3 create dataframe from rdd list\n",
    "columns = [\"language\",\"user_counts\"]\n",
    "data = [\n",
    "    (\"Java\",\"20000\"),\n",
    "    (\"Python\",\"100000\"),\n",
    "    (\"Scala\",\"3000\")\n",
    "       ]\n",
    "# 先分散到rdd\n",
    "# rdd = spark.sparkContext.parallelize(data)\n",
    "# print(dir(rdd), type(rdd), sep='\\n\\n')\n",
    "# print()\n",
    "# df_from_rdd = rdd.toDF(schema=columns)\n",
    "# df_from_rdd.show(n=5)\n",
    "\n",
    "# 直接create，讓spark dataframe進行分散\n",
    "df = spark.createDataFrame(data=data,schema=columns)\n",
    "df.show(n=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-21T14:16:46.574303Z",
     "start_time": "2021-03-21T14:16:46.450052Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Builder', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__enter__', '__eq__', '__exit__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_conf', '_convert_from_pandas', '_createFromLocal', '_createFromRDD', '_create_from_pandas_with_arrow', '_get_numpy_record_dtype', '_inferSchema', '_inferSchemaFromList', '_instantiatedSession', '_jsc', '_jsparkSession', '_jvm', '_jwrapped', '_repr_html_', '_sc', '_wrapped', 'builder', 'catalog', 'conf', 'createDataFrame', 'newSession', 'range', 'read', 'readStream', 'sparkContext', 'sql', 'stop', 'streams', 'table', 'udf', 'version']\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4 create 5 row fake data using spark range\n",
    "print(dir(spark))\n",
    "print(type(spark.range(start=0,end=10)))\n",
    "columns = ['row_number']\n",
    "single_column_df = spark.range(start=0,end=10)\n",
    "single_column_df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-21T14:16:46.632448Z",
     "start_time": "2021-03-21T14:16:46.576263Z"
    }
   },
   "outputs": [],
   "source": [
    "# 6 create empty dataframe\n",
    "\n",
    "columns = [\"language\",\"user_counts\"]\n",
    "\n",
    "# empty RDD + schema won't work\n",
    "# df_1 = spark.createDataFrame(data=spark.sparkContext.emptyRDD(),\n",
    "#                              schema=columns)\n",
    "# df_1.show(n=5)\n",
    "\n",
    "# empty list -> rdd -> df won't work\n",
    "# df2 = spark.sparkContext.parallelize([]).toDF(columns)\n",
    "\n",
    "# df3 = spark.createDataFrame([]) # won't work this version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-21T14:16:46.984320Z",
     "start_time": "2021-03-21T14:16:46.634681Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 2\n"
     ]
    }
   ],
   "source": [
    "# 7 get dataframe shape\n",
    "print(df.count(), len(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-21T14:16:47.222651Z",
     "start_time": "2021-03-21T14:16:46.986608Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+----------+\n",
      "|language|user_counts|new_column|\n",
      "+--------+-----------+----------+\n",
      "|    Java|      20000|       ABC|\n",
      "|  Python|     100000|       ABC|\n",
      "|   Scala|       3000|       ABC|\n",
      "+--------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 8 add const column to a existing dataframe\n",
    "\n",
    "columns = [\"language\",\"user_counts\"]\n",
    "data = [\n",
    "    (\"Java\",\"20000\"),\n",
    "    (\"Python\",\"100000\"),\n",
    "    (\"Scala\",\"3000\")\n",
    "       ]\n",
    "df = spark.createDataFrame(data=data,schema=columns)\n",
    "df = (\n",
    "    df.withColumn(\"new_column\",F.lit(\"ABC\")) \n",
    "    # F.lit means literal, retrurn a column\n",
    ")\n",
    "df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-21T14:16:47.773683Z",
     "start_time": "2021-03-21T14:16:47.224842Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-----+\n",
      "|language|user_counts|index|\n",
      "+--------+-----------+-----+\n",
      "|    Java|      20000|    1|\n",
      "|  Python|     100000|    2|\n",
      "|   Scala|       3000|    3|\n",
      "+--------+-----------+-----+\n",
      "\n",
      "+--------+-----------+-------+\n",
      "|language|user_counts|row_num|\n",
      "+--------+-----------+-------+\n",
      "|    Java|      20000|      1|\n",
      "|  Python|     100000|      2|\n",
      "|   Scala|       3000|      3|\n",
      "+--------+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 9 add a row_id column from a exisiting dataframe\n",
    "# https://stackoverflow.com/questions/53082891/adding-a-unique-consecutive-row-number-to-dataframe-in-pyspark\n",
    "columns = [\"language\",\"user_counts\"]\n",
    "data = [\n",
    "    (\"Java\",\"20000\"),\n",
    "    (\"Python\",\"100000\"),\n",
    "    (\"Scala\",\"3000\")\n",
    "       ]\n",
    "df_1 = spark.createDataFrame(data=data,schema=columns)\n",
    "df_1 = (\n",
    "    df_1.withColumn(\"index\", \n",
    "                  F.row_number().over(\n",
    "                      W.orderBy(F.monotonically_increasing_id() - 1)\n",
    "                  )\n",
    "                 )\n",
    "    # F.monotonically_increasing_id does not give 1 ~ N\n",
    "    # So we use window function to work around\n",
    ")\n",
    "df_1.show(n=5)\n",
    "\n",
    "# Mre clear way to do that\n",
    "df_2 = spark.createDataFrame(data=data,schema=columns)\n",
    "w = W.orderBy(F.lit('A'))\n",
    "df_2 = (\n",
    "    df_2.withColumn(\"row_num\", F.row_number().over(w))\n",
    ")\n",
    "df_2.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-21T14:16:47.945983Z",
     "start_time": "2021-03-21T14:16:47.775503Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+----------+-------------------+\n",
      "|language|user_counts|new_column|      random_number|\n",
      "+--------+-----------+----------+-------------------+\n",
      "|    Java|      20000|       ABC|0.32983843609694985|\n",
      "|  Python|     100000|       ABC| 0.9598589029169691|\n",
      "|   Scala|       3000|       ABC| 0.3027997046337778|\n",
      "+--------+-----------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 10 add a random number to a exisit column\n",
    "df = (\n",
    "#     df.withColumn('random_number', F.when(F.rand() > 0.5, 1).otherwise(0))\n",
    "        df.withColumn('random_number', F.rand())\n",
    ")\n",
    "\n",
    "df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-21T14:16:48.159945Z",
     "start_time": "2021-03-21T14:16:47.947998Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>user_counts</th>\n",
       "      <th>new_column</th>\n",
       "      <th>random_number</th>\n",
       "      <th>binary_cut_05</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Java</td>\n",
       "      <td>20000</td>\n",
       "      <td>ABC</td>\n",
       "      <td>0.329838</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Python</td>\n",
       "      <td>100000</td>\n",
       "      <td>ABC</td>\n",
       "      <td>0.959859</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Scala</td>\n",
       "      <td>3000</td>\n",
       "      <td>ABC</td>\n",
       "      <td>0.302800</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  language user_counts new_column  random_number  binary_cut_05\n",
       "0     Java       20000        ABC       0.329838              0\n",
       "1   Python      100000        ABC       0.959859              1\n",
       "2    Scala        3000        ABC       0.302800              0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 11 add a binary 0, 1 based on condition to an exisit column\n",
    "df = (\n",
    "    df.withColumn('binary_cut_05',F.when(F.rand() > 0.5, 1).otherwise(0))\n",
    ")\n",
    "df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-21T14:16:48.341887Z",
     "start_time": "2021-03-21T14:16:48.162613Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-----------+\n",
      "|row_id|language|user_counts|\n",
      "+------+--------+-----------+\n",
      "|     0|    Java|      20000|\n",
      "|     1|  Python|     100000|\n",
      "|     2|   Scala|       3000|\n",
      "+------+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 12 create a dataframe contains row_index and fake data\n",
    "\n",
    "columns = [\"row_id\",\"language\",\"user_counts\"]\n",
    "data = [\n",
    "    (0, \"Java\",\"20000\"),\n",
    "    (1, \"Python\",\"100000\"),\n",
    "    (2, \"Scala\",\"3000\")\n",
    "       ]\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=columns)\n",
    "df.show(n=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-21T14:16:49.197530Z",
     "start_time": "2021-03-21T14:16:48.344975Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+-----+------+------+\n",
      "|firstname|middlename|lastname|   id|gender|salary|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|    James|          |   Smith|36636|     M|  3000|\n",
      "|  Michael|      Rose|        |40288|     M|  4000|\n",
      "|   Robert|          |Williams|42114|     M|  4000|\n",
      "|    Maria|      Anne|   Jones|39192|     F|  4000|\n",
      "|      Jen|      Mary|   Brown|     |     F|    -1|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "\n",
      "+--------------------+-----+------+------+\n",
      "|                name|   id|gender|salary|\n",
      "+--------------------+-----+------+------+\n",
      "|    [James, , Smith]|36636|     M|  3000|\n",
      "|   [Michael, Rose, ]|40288|     M|  4000|\n",
      "|[Robert, , Williams]|42114|     M|  4000|\n",
      "|[Maria, Anne, Jones]|39192|     F|  4000|\n",
      "|  [Jen, Mary, Brown]|     |     F|    -1|\n",
      "+--------------------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 13 construct a complex data for spark dataframe\n",
    "# using StructType\n",
    "\n",
    "# Case 1\n",
    "data = [\n",
    "    (\"James\",\"\",\"Smith\",\"36636\",\"M\", 3000),\n",
    "    (\"Michael\",\"Rose\",\"\",\"40288\",\"M\", 4000),\n",
    "    (\"Robert\",\"\",\"Williams\",\"42114\",\"M\", 4000),\n",
    "    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\", 4000),\n",
    "    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\", -1)\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"firstname\",StringType(), True), # Nullable True\n",
    "    StructField(\"middlename\",StringType(), True),\n",
    "    StructField(\"lastname\",StringType(), True),\n",
    "    StructField(\"id\",StringType(), True),\n",
    "    StructField(\"gender\",StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True)\n",
    "    ])\n",
    "df = spark.createDataFrame(data=data,schema=schema)\n",
    "df.show(n=5)\n",
    "\n",
    "# Case 2\n",
    "\n",
    "struct_data = [\n",
    "    ((\"James\",\"\",\"Smith\"),\"36636\",\"M\", 3000),\n",
    "    ((\"Michael\",\"Rose\",\"\"),\"40288\",\"M\", 4000),\n",
    "    ((\"Robert\",\"\",\"Williams\"),\"42114\",\"M\", 4000),\n",
    "    ((\"Maria\",\"Anne\",\"Jones\"),\"39192\",\"F\", 4000),\n",
    "    ((\"Jen\",\"Mary\",\"Brown\"),\"\",\"F\", -1)\n",
    "]\n",
    "\n",
    "structure_schema = StructType([\n",
    "    StructField('name',\n",
    "        StructType([\n",
    "            StructField(\"firstname\",StringType(), True), # Nullable True\n",
    "            StructField(\"middlename\",StringType(), True),\n",
    "            StructField(\"lastname\",StringType(), True),\n",
    "    ])),\n",
    "    StructField(\"id\",StringType(), True),\n",
    "    StructField(\"gender\",StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True)\n",
    "    ])\n",
    "\n",
    "df = spark.createDataFrame(data=struct_data, schema=structure_schema)\n",
    "df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-21T14:16:49.532832Z",
     "start_time": "2021-03-21T14:16:49.200286Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- languagesAtSchool: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- currentState: string (nullable = true)\n",
      "\n",
      "+----------------+------------------+------------+\n",
      "|name            |languagesAtSchool |currentState|\n",
      "+----------------+------------------+------------+\n",
      "|James,,Smith    |[Java, Scala, C++]|CA          |\n",
      "|Michael,Rose,   |[Spark, Java, C++]|NJ          |\n",
      "|Robert,,Williams|[CSharp, VB]      |NV          |\n",
      "+----------------+------------------+------------+\n",
      "\n",
      "+-------------+-----+\n",
      "|         name|  col|\n",
      "+-------------+-----+\n",
      "| James,,Smith| Java|\n",
      "| James,,Smith|Scala|\n",
      "| James,,Smith|  C++|\n",
      "|Michael,Rose,|Spark|\n",
      "|Michael,Rose,| Java|\n",
      "+-------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 14 construct a complex data for spark dataframe\n",
    "# using ArratyType\n",
    "# https://github.com/spark-examples/pyspark-examples/blob/master/pyspark-array-string.py\n",
    "\n",
    "columns = [\"name\",\"languagesAtSchool\",\"currentState\"]\n",
    "data = [(\"James,,Smith\",[\"Java\",\"Scala\",\"C++\"],\"CA\"), \\\n",
    "    (\"Michael,Rose,\",[\"Spark\",\"Java\",\"C++\"],\"NJ\"), \\\n",
    "    (\"Robert,,Williams\",[\"CSharp\",\"VB\"],\"NV\")]\n",
    "\n",
    "df = spark.createDataFrame(data=data,schema=columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n",
    "\n",
    "df.select(df.name, F.explode(df.languagesAtSchool)).show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-21T14:16:49.713823Z",
     "start_time": "2021-03-21T14:16:49.534775Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- language: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- peoperties: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      "\n",
      "+----------+--------------+--------------------+\n",
      "|      name|      language|          peoperties|\n",
      "+----------+--------------+--------------------+\n",
      "|     James| [Java, Scala]|[eye -> brown, ha...|\n",
      "|   Michael|[Spark, Java,]|[eye ->, hair -> ...|\n",
      "|    Robert|    [CSharp, ]|[eye -> , hair ->...|\n",
      "|Washington|          null|                null|\n",
      "| Jefferson|        [1, 2]|                  []|\n",
      "+----------+--------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 15 construct a complex data for spark dataframe\n",
    "# using MapType\n",
    "# https://github.com/spark-examples/pyspark-examples/blob/master/pyspark-explode-array-map.py\n",
    "data = [\n",
    "        ('James',['Java','Scala'],{'hair':'black','eye':'brown'}),\n",
    "        ('Michael',['Spark','Java',None],{'hair':'brown','eye':None}),\n",
    "        ('Robert',['CSharp',''],{'hair':'red','eye':''}),\n",
    "        ('Washington',None,None),\n",
    "        ('Jefferson',['1','2'],{})\n",
    "]\n",
    "\n",
    "columns = ['name','language','peoperties']\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=columns)\n",
    "\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-21T14:16:49.728283Z",
     "start_time": "2021-03-21T14:16:49.715881Z"
    }
   },
   "outputs": [],
   "source": [
    "# 16 create a nested array-type dataframe\n",
    "# https://github.com/spark-examples/pyspark-examples/blob/master/pyspark-explode-nested-array.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-21T14:16:49.831074Z",
     "start_time": "2021-03-21T14:16:49.730196Z"
    }
   },
   "outputs": [],
   "source": [
    "# 17 create a datetime column for spark dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Column Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-21T14:16:50.125835Z",
     "start_time": "2021-03-21T14:16:49.833899Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+--------------+------------------+\n",
      "|language|user_counts|user_count_100|    user_count_log|\n",
      "+--------+-----------+--------------+------------------+\n",
      "|    Java|      20000|       2000000| 4.301029995663981|\n",
      "|  Python|     100000|      10000000|               5.0|\n",
      "|   Scala|       3000|        300000|3.4771212547196626|\n",
      "+--------+-----------+--------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1 create new column based on original column\n",
    "\n",
    "columns = [\"language\", \"user_counts\"]\n",
    "data = [\n",
    "    (\"Java\",20000),\n",
    "    (\"Python\",100000),\n",
    "    (\"Scala\",3000)\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=columns)\n",
    "df = (\n",
    "    df.withColumn(\"user_count_100\", C(\"user_counts\") * 100)\\\n",
    "    .withColumn(\"user_count_log\", F.log10(C(\"user_counts\")))\n",
    ")\n",
    "\n",
    "df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-21T14:16:50.310365Z",
     "start_time": "2021-03-21T14:16:50.128512Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+\n",
      "|  lang|const_col|\n",
      "+------+---------+\n",
      "|  Java|      ABC|\n",
      "|Python|      ABC|\n",
      "| Scala|      ABC|\n",
      "+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2 rename, drop, add constant column to existing dataframe\n",
    "# https://stackoverflow.com/questions/34077353/how-to-change-dataframe-column-names-in-pyspark\n",
    "columns = [\"language\", \"user_counts\"]\n",
    "data = [\n",
    "    (\"Java\",20000),\n",
    "    (\"Python\",100000),\n",
    "    (\"Scala\",3000)\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df = (\n",
    "    df.withColumn('const_col',F.lit('ABC'))\\\n",
    "    .withColumnRenamed(\"language\",\"lang\")\\\n",
    "    .drop(\"user_counts\")\n",
    ")\n",
    "df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-21T14:16:50.333775Z",
     "start_time": "2021-03-21T14:16:50.313249Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.column.Column'>\n",
      "\n",
      "['__add__', '__and__', '__bool__', '__class__', '__contains__', '__delattr__', '__dict__', '__dir__', '__div__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__invert__', '__iter__', '__le__', '__lt__', '__mod__', '__module__', '__mul__', '__ne__', '__neg__', '__new__', '__nonzero__', '__or__', '__pow__', '__radd__', '__rand__', '__rdiv__', '__reduce__', '__reduce_ex__', '__repr__', '__rmod__', '__rmul__', '__ror__', '__rpow__', '__rsub__', '__rtruediv__', '__setattr__', '__sizeof__', '__str__', '__sub__', '__subclasshook__', '__truediv__', '__weakref__', '_asc_doc', '_bitwiseAND_doc', '_bitwiseOR_doc', '_bitwiseXOR_doc', '_contains_doc', '_desc_doc', '_endswith_doc', '_eqNullSafe_doc', '_isNotNull_doc', '_isNull_doc', '_jc', '_like_doc', '_rlike_doc', '_startswith_doc', 'alias', 'asc', 'astype', 'between', 'bitwiseAND', 'bitwiseOR', 'bitwiseXOR', 'cast', 'contains', 'desc', 'endswith', 'eqNullSafe', 'getField', 'getItem', 'isNotNull', 'isNull', 'isin', 'like', 'name', 'otherwise', 'over', 'rlike', 'startswith', 'substr', 'when']\n"
     ]
    }
   ],
   "source": [
    "# 3 know what method and attribute can be called with column object\n",
    "\n",
    "\n",
    "print(type(C(\"language\")), dir(C(\"language\")), sep='\\n\\n')\n",
    "\n",
    "# alias - 可以換名字\n",
    "# asc, desc - 可以排序\n",
    "# astype,cast - 可以轉型\n",
    "# between - 可以傳入start_date以及end_date過濾\n",
    "# bitwiseAND, bitwiseOR, bitwiseXOR - 可以做布林運算\n",
    "# contains - 可以做字串搜尋\n",
    "# endwith, startwith, rlike, substring - 可以做字串比對\n",
    "# eqNullSafe, isNotNull, isNull - 可以檢查null值，Python須以None傳入\n",
    "# isin, like - 可以做值的比對(數值，字串值)\n",
    "# name - 可以取得欄位名稱\n",
    "# when, otherwise - 可以做條件判斷\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-21T14:16:50.642287Z",
     "start_time": "2021-03-21T14:16:50.342384Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-------------+\n",
      "|language|user_counts|is_many_users|\n",
      "+--------+-----------+-------------+\n",
      "|    Java|      20000|            0|\n",
      "|  Python|     100000|            1|\n",
      "|   Scala|       3000|            0|\n",
      "+--------+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3 create a new dynamic column(if else condition based on old column)\n",
    "# Case 1\n",
    "columns = [\"language\", \"user_counts\"]\n",
    "data = [\n",
    "    (\"Java\",20000),\n",
    "    (\"Python\",100000),\n",
    "    (\"Scala\",3000)\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "df = (\n",
    "    df.withColumn(\"is_many_users\",\n",
    "                  F.when(C('user_counts') > 50000, 1).otherwise(0)\n",
    "                 )\n",
    ")\n",
    "\n",
    "df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-21T14:16:51.102378Z",
     "start_time": "2021-03-21T14:16:50.646651Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|firstname|middlename|lastname|   id|gender|salary|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|    James|          |   Smith|36636|     M|  3000|\n",
      "|  Michael|      Rose|        |40288|     M|  4000|\n",
      "|   Robert|          |Williams|42114|     M|  4000|\n",
      "|    Maria|      Anne|   Jones|39192|     F|  4000|\n",
      "|      Jen|      Mary|   Brown|     |     F|    -1|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "\n",
      "After\n",
      "+---------+----------+--------+-----+------+------+----------------+\n",
      "|firstname|middlename|lastname|   id|gender|salary|       full_name|\n",
      "+---------+----------+--------+-----+------+------+----------------+\n",
      "|    James|      null|   Smith|36636|     M|  3000|             N/A|\n",
      "|  Michael|      Rose|    null|40288|     M|  4000|             N/A|\n",
      "|   Robert|      null|Williams|42114|     M|  4000|             N/A|\n",
      "|    Maria|      Anne|   Jones|39192|     F|  4000|Maria Anne Jones|\n",
      "|      Jen|      Mary|   Brown|     |     F|    -1|  Jen Mary Brown|\n",
      "+---------+----------+--------+-----+------+------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4 create a new dynamic column(if else condition based on old column) plus empty string replacement\n",
    "\n",
    "data = [\n",
    "    (\"James\",\"\",\"Smith\",\"36636\",\"M\", 3000),\n",
    "    (\"Michael\",\"Rose\",\"\",\"40288\",\"M\", 4000),\n",
    "    (\"Robert\",\"\",\"Williams\",\"42114\",\"M\", 4000),\n",
    "    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\", 4000),\n",
    "    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\", -1)\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"firstname\",StringType(), True), # Nullable True\n",
    "    StructField(\"middlename\",StringType(), True),\n",
    "    StructField(\"lastname\",StringType(), True),\n",
    "    StructField(\"id\",StringType(), True),\n",
    "    StructField(\"gender\",StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True)\n",
    "    ])\n",
    "df = spark.createDataFrame(data=data,schema=schema)\n",
    "\n",
    "# we cannot compare column values with empty string\n",
    "# so the work-around method is replace empty string to null\n",
    "# then using isNotNull()\n",
    "is_full_name_exist = (C(\"firstname\").isNotNull() & C(\"middlename\").isNotNull() & C(\"lastname\").isNotNull())\n",
    "\n",
    "\n",
    "def blank_as_null(x):\n",
    "    \"\"\"\n",
    "    helper function for converting row value from empty string to null\n",
    "    https://stackoverflow.com/questions/33287886/replace-empty-strings-with-none-null-values-in-dataframe\n",
    "    \"\"\"\n",
    "    return F.when(C(x) != \"\", C(x)).otherwise(None)\n",
    "\n",
    "print(\"Before\")\n",
    "df.show(n=5)\n",
    "df = (\n",
    "    df.withColumn(\"firstname\", blank_as_null(\"firstname\"))\\\n",
    "    .withColumn(\"middlename\", blank_as_null(\"middlename\"))\\\n",
    "    .withColumn(\"lastname\", blank_as_null(\"lastname\"))\\\n",
    "    .withColumn(\"full_name\",\n",
    "                  F.when(\n",
    "                      is_full_name_exist,\n",
    "                      F.concat(C(\"firstname\"), F.lit(' '),\n",
    "                               C(\"middlename\"), F.lit(' '),\n",
    "                               C(\"lastname\"))\n",
    "                  ).otherwise(F.lit('N/A'))\n",
    "                 )\n",
    ")\n",
    "print(\"After\")\n",
    "df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-21T14:16:51.288958Z",
     "start_time": "2021-03-21T14:16:51.104124Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Column<b'language'>, Column<b'user_counts'>] <class 'list'> <class 'pyspark.sql.column.Column'>\n",
      "+--------+-----------+\n",
      "|language|user_counts|\n",
      "+--------+-----------+\n",
      "|    Java|      20000|\n",
      "|  Python|     100000|\n",
      "|   Scala|       3000|\n",
      "+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5 select columns which also exist on another dataframe\n",
    "\n",
    "columns_1 = [\"language\", \"user_counts\"]\n",
    "data_1 = [\n",
    "    (\"Java\",20000),\n",
    "    (\"Python\",100000),\n",
    "    (\"Scala\",3000)\n",
    "]\n",
    "\n",
    "columns_2 = [\"language\", \"user_counts\",\"note\"]\n",
    "data_2 = [\n",
    "    (\"Java\",20000,\"nothing\"),\n",
    "    (\"Python\",100000,\"nothing\"),\n",
    "    (\"Scala\",3000,\"nothing\")\n",
    "]\n",
    "\n",
    "df_1 = spark.createDataFrame(data_1, columns_1)\n",
    "df_2 = spark.createDataFrame(data_2, columns_2)\n",
    "\n",
    "# union columns\n",
    "same_cols = [F.col(c) for c in df_2.columns if c in df_1.columns]\n",
    "print(same_cols, type(same_cols), type(same_cols[0]))\n",
    "\n",
    "df_same_col = df_1.select(*same_cols)\n",
    "df_same_col.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-21T14:16:51.478043Z",
     "start_time": "2021-03-21T14:16:51.292030Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-----+-----------------+-----------------+\n",
      "|language|user_counts|const|scientific_sign_1|scientific_sign_2|\n",
      "+--------+-----------+-----+-----------------+-----------------+\n",
      "|    Java|      20000|10000|           1.0E40|          1.0E-40|\n",
      "|  Python|     100000|10000|           1.0E40|          1.0E-40|\n",
      "|   Scala|       3000|10000|           1.0E40|          1.0E-40|\n",
      "+--------+-----------+-----+-----------------+-----------------+\n",
      "\n",
      "root\n",
      " |-- language: string (nullable = true)\n",
      " |-- user_counts: long (nullable = true)\n",
      " |-- const: integer (nullable = false)\n",
      " |-- scientific_sign_1: double (nullable = false)\n",
      " |-- scientific_sign_2: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6 create a const numerical column\n",
    "\n",
    "# 3 create a new dynamic column(if else condition based on old column)\n",
    "# Case 1\n",
    "columns = [\"language\", \"user_counts\"]\n",
    "data = [\n",
    "    (\"Java\",20000),\n",
    "    (\"Python\",100000),\n",
    "    (\"Scala\",3000)\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "df = (\n",
    "    df.withColumn(\"const\",F.lit(10000))\\\n",
    "    .withColumn(\"scientific_sign_1\", F.lit(1e40))\n",
    "    .withColumn(\"scientific_sign_2\", F.lit(1e-40))\n",
    ")\n",
    "\n",
    "df.show(n=5)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-21T14:16:51.665733Z",
     "start_time": "2021-03-21T14:16:51.480760Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-------------------+------------------+-------------------+\n",
      "|language|user_counts|        uniform_0_1|     uniform_0_100|         normal_0_1|\n",
      "+--------+-----------+-------------------+------------------+-------------------+\n",
      "|    Java|      20000| 0.6661236774413726| 66.61236774413726| 0.4085363219031828|\n",
      "|  Python|     100000| 0.3856203005100328| 38.56203005100328|-0.7556247885860078|\n",
      "|   Scala|       3000|0.27636619934035966|27.636619934035966|-1.4773884185536659|\n",
      "+--------+-----------+-------------------+------------------+-------------------+\n",
      "\n",
      "root\n",
      " |-- language: string (nullable = true)\n",
      " |-- user_counts: long (nullable = true)\n",
      " |-- uniform_0_1: double (nullable = false)\n",
      " |-- uniform_0_100: double (nullable = false)\n",
      " |-- normal_0_1: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create a random variable column\n",
    "# https://spark.apache.org/docs/2.3.4/api/python/pyspark.sql.html#pyspark.sql.DataFrame\n",
    "# rand uniform [0, 1]\n",
    "# randn Normal distribution mu = 0, sigma = 1\n",
    "\n",
    "columns = [\"language\", \"user_counts\"]\n",
    "data = [\n",
    "    (\"Java\",20000),\n",
    "    (\"Python\",100000),\n",
    "    (\"Scala\",3000)\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "df = (\n",
    "    df.withColumn(\"uniform_0_1\",F.rand(seed=42))\\\n",
    "      .withColumn(\"uniform_0_100\",100 * F.rand(seed=42))\\\n",
    "      .withColumn(\"normal_0_1\", F.randn(seed=42))\n",
    ")\n",
    "\n",
    "df.show(n=5)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# String Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-21T14:16:51.681119Z",
     "start_time": "2021-03-21T14:16:51.668183Z"
    }
   },
   "outputs": [],
   "source": [
    "# 1 string concat two column values to a new column from an existing dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-21T14:16:51.788902Z",
     "start_time": "2021-03-21T14:16:51.682541Z"
    }
   },
   "outputs": [],
   "source": [
    "# 2 cut of left 3 char of specific column to a new column from an existing dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-21T14:16:51.897234Z",
     "start_time": "2021-03-21T14:16:51.791961Z"
    }
   },
   "outputs": [],
   "source": [
    "# 3 convert string type column to int/float type column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-21T14:16:52.005730Z",
     "start_time": "2021-03-21T14:16:51.899964Z"
    }
   },
   "outputs": [],
   "source": [
    "# 4 convert string type column to datetime type column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-21T14:16:52.276672Z",
     "start_time": "2021-03-21T14:16:52.007951Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+-----+------+------+\n",
      "|firstname|middlename|lastname|   id|gender|salary|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|    James|          |   Smith|36636|     M|  3000|\n",
      "|  Michael|      Rose|        |40288|     M|  4000|\n",
      "|   Robert|          |Williams|42114|     M|  4000|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. filter on equal condition\n",
    "\n",
    "data = [\n",
    "    (\"James\",\"\",\"Smith\",\"36636\",\"M\", 3000),\n",
    "    (\"Michael\",\"Rose\",\"\",\"40288\",\"M\", 4000),\n",
    "    (\"Robert\",\"\",\"Williams\",\"42114\",\"M\", 4000),\n",
    "    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\", 4000),\n",
    "    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\", -1)\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"firstname\",StringType(), True), # Nullable True\n",
    "    StructField(\"middlename\",StringType(), True),\n",
    "    StructField(\"lastname\",StringType(), True),\n",
    "    StructField(\"id\",StringType(), True),\n",
    "    StructField(\"gender\",StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True)\n",
    "    ])\n",
    "df = spark.createDataFrame(data=data,schema=schema)\n",
    "\n",
    "df = (\n",
    "    df.filter(C(\"gender\") == \"M\")\n",
    ")\n",
    "\n",
    "df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-21T14:16:52.469543Z",
     "start_time": "2021-03-21T14:16:52.278614Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+-----+------+------+\n",
      "|firstname|middlename|lastname|   id|gender|salary|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|  Michael|      Rose|        |40288|     M|  4000|\n",
      "|   Robert|          |Williams|42114|     M|  4000|\n",
      "|    Maria|      Anne|   Jones|39192|     F|  6000|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2 filter on >, <, >=, <=\n",
    "\n",
    "data = [\n",
    "    (\"James\",\"\",\"Smith\",\"36636\",\"M\", 3000),\n",
    "    (\"Michael\",\"Rose\",\"\",\"40288\",\"M\", 4000),\n",
    "    (\"Robert\",\"\",\"Williams\",\"42114\",\"M\", 4000),\n",
    "    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\", 6000),\n",
    "    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\", -1)\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"firstname\",StringType(), True), # Nullable True\n",
    "    StructField(\"middlename\",StringType(), True),\n",
    "    StructField(\"lastname\",StringType(), True),\n",
    "    StructField(\"id\",StringType(), True),\n",
    "    StructField(\"gender\",StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True)\n",
    "    ])\n",
    "df = spark.createDataFrame(data=data,schema=schema)\n",
    "\n",
    "df = (\n",
    "    df.filter(C(\"salary\") >= 4000)\n",
    ")\n",
    "\n",
    "df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-21T14:16:52.653454Z",
     "start_time": "2021-03-21T14:16:52.472809Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+-----+------+------+\n",
      "|firstname|middlename|lastname|   id|gender|salary|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|  Michael|      Rose|        |40288|     M|  4000|\n",
      "|   Robert|          |Williams|42114|     M|  4000|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3 multiple conditions require parenthese around each condition\n",
    "\n",
    "\n",
    "data = [\n",
    "    (\"James\",\"\",\"Smith\",\"36636\",\"M\", 3000),\n",
    "    (\"Michael\",\"Rose\",\"\",\"40288\",\"M\", 4000),\n",
    "    (\"Robert\",\"\",\"Williams\",\"42114\",\"M\", 4000),\n",
    "    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\", 6000),\n",
    "    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\", -1)\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"firstname\",StringType(), True), # Nullable True\n",
    "    StructField(\"middlename\",StringType(), True),\n",
    "    StructField(\"lastname\",StringType(), True),\n",
    "    StructField(\"id\",StringType(), True),\n",
    "    StructField(\"gender\",StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True)\n",
    "    ])\n",
    "df = spark.createDataFrame(data=data,schema=schema)\n",
    "\n",
    "# This is lazy computing\n",
    "rich_man_who_worth_married = (\n",
    "    (C(\"gender\") == \"M\") &\n",
    "    (C(\"salary\") >= 4000)\n",
    ")\n",
    "\n",
    "df = (\n",
    "    df.filter(rich_man_who_worth_married)\n",
    ")\n",
    "\n",
    "df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-21T14:16:52.836688Z",
     "start_time": "2021-03-21T14:16:52.656868Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+-----+------+------+\n",
      "|firstname|middlename|lastname|   id|gender|salary|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|    Maria|      Anne|   Jones|39192|     F|  6000|\n",
      "|      Jen|      Mary|   Brown|     |     F|    -1|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4 Compare against a list of allowed values\n",
    "\n",
    "\n",
    "data = [\n",
    "    (\"James\",\"\",\"Smith\",\"36636\",\"M\", 3000),\n",
    "    (\"Michael\",\"Rose\",\"\",\"40288\",\"M\", 4000),\n",
    "    (\"Robert\",\"\",\"Williams\",\"42114\",\"M\", 4000),\n",
    "    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\", 6000),\n",
    "    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\", -1)\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"firstname\",StringType(), True), # Nullable True\n",
    "    StructField(\"middlename\",StringType(), True),\n",
    "    StructField(\"lastname\",StringType(), True),\n",
    "    StructField(\"id\",StringType(), True),\n",
    "    StructField(\"gender\",StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True)\n",
    "    ])\n",
    "df = spark.createDataFrame(data=data,schema=schema)\n",
    "\n",
    "df = (\n",
    "    df.filter(C(\"gender\").isin([\"F\"]))\n",
    ")\n",
    "\n",
    "df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-21T14:16:53.197439Z",
     "start_time": "2021-03-21T14:16:52.839090Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+-----+------+------+\n",
      "|firstname|middlename|lastname|   id|gender|salary|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|    Maria|      Anne|   Jones|39192|     F|  6000|\n",
      "|  Michael|      Rose|        |40288|     M|  4000|\n",
      "|   Robert|          |Williams|42114|     M|  4000|\n",
      "|    James|          |   Smith|36636|     M|  3000|\n",
      "|      Jen|      Mary|   Brown|     |     F|    -1|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|firstname|middlename|lastname|   id|gender|salary|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|      Jen|      Mary|   Brown|     |     F|    -1|\n",
      "|    James|          |   Smith|36636|     M|  3000|\n",
      "|  Michael|      Rose|        |40288|     M|  4000|\n",
      "|   Robert|          |Williams|42114|     M|  4000|\n",
      "|    Maria|      Anne|   Jones|39192|     F|  6000|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5 Sort result\n",
    "\n",
    "data = [\n",
    "    (\"James\",\"\",\"Smith\",\"36636\",\"M\", 3000),\n",
    "    (\"Michael\",\"Rose\",\"\",\"40288\",\"M\", 4000),\n",
    "    (\"Robert\",\"\",\"Williams\",\"42114\",\"M\", 4000),\n",
    "    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\", 6000),\n",
    "    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\", -1)\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"firstname\",StringType(), True), # Nullable True\n",
    "    StructField(\"middlename\",StringType(), True),\n",
    "    StructField(\"lastname\",StringType(), True),\n",
    "    StructField(\"id\",StringType(), True),\n",
    "    StructField(\"gender\",StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True)\n",
    "    ])\n",
    "df = spark.createDataFrame(data=data,schema=schema)\n",
    "\n",
    "\n",
    "df = df.orderBy(C(\"salary\").desc())\n",
    "\n",
    "df.show(n=5)\n",
    "\n",
    "df = df.orderBy(C(\"salary\").asc())\n",
    "\n",
    "df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-21T14:16:54.524308Z",
     "start_time": "2021-03-21T14:16:53.199789Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before\n",
      "+----------+-------+--------------------+\n",
      "|article_id|   pred|             img_url|\n",
      "+----------+-------+--------------------+\n",
      "|     67789|0.93422|https://pic.pimg....|\n",
      "|     67789|0.94231|https://pic.pimg....|\n",
      "|     14431|0.97611|https://pic.pimg....|\n",
      "|     14431|0.99834|https://pic.pimg....|\n",
      "|     14431|0.99834|https://pic.pimg....|\n",
      "+----------+-------+--------------------+\n",
      "\n",
      "Sol 1 FAILED SOMETIMES WHEN PARTITION != 1\n",
      "+----------+-------+--------------------+\n",
      "|article_id|   pred|             img_url|\n",
      "+----------+-------+--------------------+\n",
      "|     67789|0.93422|https://pic.pimg....|\n",
      "|     67789|0.94231|https://pic.pimg....|\n",
      "|     14431|0.97611|https://pic.pimg....|\n",
      "|     14431|0.99834|https://pic.pimg....|\n",
      "+----------+-------+--------------------+\n",
      "\n",
      "Sol 2 WORKED WITH ANY PARTITION\n",
      "+----------+-------+--------------------+\n",
      "|article_id|   pred|             img_url|\n",
      "+----------+-------+--------------------+\n",
      "|     67789|0.93422|https://pic.pimg....|\n",
      "|     67789|0.94231|https://pic.pimg....|\n",
      "|     14431|0.97611|https://pic.pimg....|\n",
      "|     14431|0.99834|https://pic.pimg....|\n",
      "+----------+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6 select distinct rows based on certain column but keep first row\n",
    "# In this case, model prediction to filter the same images\n",
    "\n",
    "############## DROP DUPLICATED doesn't work in this case\n",
    "# https://stackoverflow.com/questions/38687212/spark-dataframe-drop-duplicates-and-keep-first\n",
    "data = [\n",
    "    (14431,0.99834,'https://pic.pimg.tw/happy78/1528543947-685380499_n.jpg'),\n",
    "    (14431,0.99834,'https://pic.pimg.tw/happy78/1528543947-362759723_n.jpg'),\n",
    "    (14431,0.97611,'https://pic.pimg.tw/happy78/1528543962-2265924582_n.jpg'),\n",
    "    (67789,0.93422,'https://pic.pimg.tw/happy78/1528543962-4007835890_n.jpg'),\n",
    "    (67789,0.94231,'https://pic.pimg.tw/happy78/1528543962-45890_n.jpg')\n",
    "]\n",
    "\n",
    "columns = ['article_id','pred','img_url']\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=columns)\n",
    "print('before')\n",
    "\n",
    "df.orderBy('pred').show(n=5)\n",
    "\n",
    "print('Sol 1 FAILED SOMETIMES WHEN PARTITION != 1')\n",
    "df_1 = (\n",
    "    df.drop_duplicates(subset=[\"pred\"])\n",
    ")\n",
    "\n",
    "df_1.orderBy('pred').show(n=5)\n",
    "\n",
    "\n",
    "############## Using Window Function and sort, rank, worked!\n",
    "# You can check the 5 th question of Aggregation, The solution is the same\n",
    "\n",
    "print('Sol 2 WORKED WITH ANY PARTITION')\n",
    "df_2 = (\n",
    "    df.withColumn(\"rank_by_pred\",\n",
    "                  F.row_number().over(\n",
    "                  W.partitionBy(\"pred\")\\\n",
    "                      .orderBy(F.desc(\"pred\"))\n",
    "                  )\n",
    "                 )\\\n",
    "    .filter(F.col(\"rank_by_pred\") == 1)\\\n",
    "    .drop('rank_by_pred')\n",
    ")\n",
    "df_2.orderBy('pred').show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## String filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-21T14:28:42.328679Z",
     "start_time": "2021-03-21T14:28:42.169956Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-------------------------------------------------------------\n",
      " article_id | 14431                                                   \n",
      " pred       | 0.99834                                                 \n",
      " img_url    | https://pic.pimg.tw/happy78/1528543947-685380499_n.jpg  \n",
      "-RECORD 1-------------------------------------------------------------\n",
      " article_id | 14431                                                   \n",
      " pred       | 0.99834                                                 \n",
      " img_url    | https://pic.pimg.tw/happy78/1528543947-362759723_n.jpg  \n",
      "-RECORD 2-------------------------------------------------------------\n",
      " article_id | 14431                                                   \n",
      " pred       | 0.97611                                                 \n",
      " img_url    | https://pic.pimg.tw/happy78/1528543962-2265924582_n.jpg \n",
      "-RECORD 3-------------------------------------------------------------\n",
      " article_id | 67789                                                   \n",
      " pred       | 0.93422                                                 \n",
      " img_url    | https://pic.pimg.tw/happy78/1528543962-4007835890_n.jpg \n",
      "-RECORD 4-------------------------------------------------------------\n",
      " article_id | 67789                                                   \n",
      " pred       | 0.94231                                                 \n",
      " img_url    | https://pic.pimg.tw/happy78/1528543962-45890_n.jpg      \n",
      "-RECORD 5-------------------------------------------------------------\n",
      " article_id | 67789                                                   \n",
      " pred       | 0.94111                                                 \n",
      " img_url    | https://pic.pimg.tw/happy78/1528543962-45890_n.png      \n",
      "-RECORD 6-------------------------------------------------------------\n",
      " article_id | 67789                                                   \n",
      " pred       | 0.94111                                                 \n",
      " img_url    |                                                         \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# regax filtering\n",
    "# pyspark regexp_extract api cannot get all the groups\n",
    "data = [\n",
    "    (14431,0.99834,'https://pic.pimg.tw/happy78/1528543947-685380499_n.jpg mfkvmdfasx'),\n",
    "    (14431,0.99834,'https://pic.pimg.tw/happy78/1528543947-362759723_n.jpg'),\n",
    "    (14431,0.97611,'https://pic.pimg.tw/happy78/1528543962-2265924582_n.jpg'),\n",
    "    (67789,0.93422,'https://pic.pimg.tw/happy78/1528543962-4007835890_n.jpg'),\n",
    "    (67789,0.94231,'https://pic.pimg.tw/happy78/1528543962-45890_n.jpg'),\n",
    "    (67789,0.94111,'https://pic.pimg.tw/happy78/1528543962-45890_n.png'),\n",
    "    (67789,0.94111,'png')\n",
    "]\n",
    "\n",
    "columns = ['article_id','pred','img_url']\n",
    "\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "\n",
    "df_filted = (\n",
    "    df.withColumn(\"img_url\", F.regexp_extract(C('img_url'),\n",
    "                                              r'(http\\S+jpg\\b)|(http\\S+png\\b)',\n",
    "                                              0))\n",
    ")\n",
    "\n",
    "df_filted.show(n=20, vertical=True, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-21T14:35:14.287892Z",
     "start_time": "2021-03-21T14:35:13.875715Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------------------------------------------------------------------\n",
      " article_id | 14431                                                             \n",
      " pred       | 0.99834                                                           \n",
      " img_url    | https://pic.pimg.tw/happy78/1528543947-685380499_n.jpg mfkvmdfasx \n",
      "\n",
      "-RECORD 0-----------------------------------------------------------------------\n",
      " article_id | 14431                                                             \n",
      " pred       | 0.99834                                                           \n",
      " img_url    | https://pic.pimg.tw/happy78/1528543947-685380499_n.jpg mfkvmdfasx \n",
      "\n",
      "-RECORD 0--------------------------------------------------------\n",
      " article_id | 67789                                              \n",
      " pred       | 0.94111                                            \n",
      " img_url    | https://pic.pimg.tw/happy78/1528543962-45890_n.png \n",
      "-RECORD 1--------------------------------------------------------\n",
      " article_id | 67789                                              \n",
      " pred       | 0.94111                                            \n",
      " img_url    | png                                                \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# regax filtering\n",
    "# contains\n",
    "# startswith\n",
    "# endwith\n",
    "\n",
    "data = [\n",
    "    (14431,0.99834,'https://pic.pimg.tw/happy78/1528543947-685380499_n.jpg mfkvmdfasx'),\n",
    "    (14431,0.99834,'https://pic.pimg.tw/happy78/1528543947-362759723_n.jpg'),\n",
    "    (14431,0.97611,'https://pic.pimg.tw/happy78/1528543962-2265924582_n.jpg'),\n",
    "    (67789,0.93422,'https://pic.pimg.tw/happy78/1528543962-4007835890_n.jpg'),\n",
    "    (67789,0.94231,'https://pic.pimg.tw/happy78/1528543962-45890_n.jpg'),\n",
    "    (67789,0.94111,'https://pic.pimg.tw/happy78/1528543962-45890_n.png'),\n",
    "    (67789,0.94111,'png')\n",
    "]\n",
    "\n",
    "columns = ['article_id','pred','img_url']\n",
    "\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "df_con = (\n",
    "    df.filter(df.img_url.contains('mfkv'))\n",
    ")\n",
    "df_con.show(vertical=True,truncate=False)\n",
    "\n",
    "df_startwith = (\n",
    "    df.filter(df.img_url.startswith(\"png\"))\n",
    ")\n",
    "\n",
    "df_con.show(vertical=True,truncate=False)\n",
    "\n",
    "df_endwith = (\n",
    "    df.filter(df.img_url.endswith(\"png\"))\n",
    ")\n",
    "\n",
    "df_endwith.show(vertical=True,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reg-like\n",
    "# is in list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregation Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-21T14:16:54.569761Z",
     "start_time": "2021-03-21T14:16:54.526356Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.group.GroupedData'>\n",
      "\n",
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_df', '_jgd', 'agg', 'apply', 'avg', 'count', 'max', 'mean', 'min', 'pivot', 'sql_ctx', 'sum']\n"
     ]
    }
   ],
   "source": [
    "# 1 knowing the groupby object method\n",
    "\n",
    "data = [\n",
    "    (\"James\",\"\",\"Smith\",\"36636\",\"RD\",\"M\", 3000),\n",
    "    (\"Michael\",\"Rose\",\"\",\"40288\",\"RD\",\"M\", 4000),\n",
    "    (\"Robert\",\"\",\"Williams\",\"42114\",\"SRE\",\"M\", 4000),\n",
    "    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"SRE\",\"F\", 4000),\n",
    "    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",\"BACKEND\", -1)\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"firstname\",StringType(), True), # Nullable True\n",
    "    StructField(\"middlename\",StringType(), True),\n",
    "    StructField(\"lastname\",StringType(), True),\n",
    "    StructField(\"id\",StringType(), True),\n",
    "    StructField(\"apartment\",StringType(), True),\n",
    "    StructField(\"gender\",StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True)\n",
    "    ])\n",
    "df = spark.createDataFrame(data=data,schema=schema)\n",
    "\n",
    "df_grp = df.groupBy(\"salary\")\n",
    "\n",
    "print(type(df_grp), dir(df_grp), sep='\\n\\n')\n",
    "\n",
    "# avg, count, max, mean, sum  - Common aggregation\n",
    "# pivot - two column x, y with value in the table\n",
    "# sql_ctx - apply sql command\n",
    "# custom function - agg, apply\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-21T14:16:55.229803Z",
     "start_time": "2021-03-21T14:16:54.571480Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|deparment|avg(salary)|\n",
      "+---------+-----------+\n",
      "|        F|       -1.0|\n",
      "|       RD|     3500.0|\n",
      "|      SRE|     4000.0|\n",
      "+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2 apply single aggregation fuction on groupby object\n",
    "data = [\n",
    "    (\"James\",\"\",\"Smith\",\"36636\",\"RD\",\"M\", 3000),\n",
    "    (\"Michael\",\"Rose\",\"\",\"40288\",\"RD\",\"M\", 4000),\n",
    "    (\"Robert\",\"\",\"Williams\",\"42114\",\"SRE\",\"M\", 4000),\n",
    "    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"SRE\",\"F\", 4000),\n",
    "    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",\"BACKEND\", -1)\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"firstname\",StringType(), True), # Nullable True\n",
    "    StructField(\"middlename\",StringType(), True),\n",
    "    StructField(\"lastname\",StringType(), True),\n",
    "    StructField(\"id\",StringType(), True),\n",
    "    StructField(\"deparment\",StringType(), True),\n",
    "    StructField(\"gender\",StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True)\n",
    "    ])\n",
    "df = spark.createDataFrame(data=data,schema=schema)\n",
    "\n",
    "df_grp_department = (\n",
    "    df.groupby(\"deparment\").mean(\"salary\").alias(\"mean_salary\")\n",
    ")\n",
    "\n",
    "df_grp_department.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-21T14:16:55.842917Z",
     "start_time": "2021-03-21T14:16:55.231726Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+----------+----------+----------+----------+\n",
      "|deparment|sum_salary|avg_salary|max_salary|min_salary|group_size|\n",
      "+---------+----------+----------+----------+----------+----------+\n",
      "|        F|        -1|      -1.0|        -1|        -1|         1|\n",
      "|       RD|      7000|    3500.0|      4000|      3000|         2|\n",
      "|      SRE|      8000|    4000.0|      4000|      4000|         2|\n",
      "+---------+----------+----------+----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3 apply multiple aggregation fuction on groupby object\n",
    "data = [\n",
    "    (\"James\",\"\",\"Smith\",\"36636\",\"RD\",\"M\", 3000),\n",
    "    (\"Michael\",\"Rose\",\"\",\"40288\",\"RD\",\"M\", 4000),\n",
    "    (\"Robert\",\"\",\"Williams\",\"42114\",\"SRE\",\"M\", 4000),\n",
    "    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"SRE\",\"F\", 4000),\n",
    "    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",\"BACKEND\", -1)\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"firstname\",StringType(), True), # Nullable True\n",
    "    StructField(\"middlename\",StringType(), True),\n",
    "    StructField(\"lastname\",StringType(), True),\n",
    "    StructField(\"id\",StringType(), True),\n",
    "    StructField(\"deparment\",StringType(), True),\n",
    "    StructField(\"gender\",StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True)\n",
    "    ])\n",
    "df = spark.createDataFrame(data=data,schema=schema)\n",
    "\n",
    "df_grp_department = df.groupby(\"deparment\").agg(\n",
    "    F.sum(\"salary\").alias(\"sum_salary\"),\n",
    "    F.avg(\"salary\").alias(\"avg_salary\"),\n",
    "    F.max(\"salary\").alias(\"max_salary\"),\n",
    "    F.min(\"salary\").alias(\"min_salary\"),\n",
    "    F.count(\"salary\").alias(\"group_size\")\n",
    ")\n",
    "\n",
    "df_grp_department.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-21T14:16:56.248545Z",
     "start_time": "2021-03-21T14:16:55.844261Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>deparment</th>\n",
       "      <th>sum_salary</th>\n",
       "      <th>avg_salary</th>\n",
       "      <th>max_salary</th>\n",
       "      <th>min_salary</th>\n",
       "      <th>count_rows</th>\n",
       "      <th>all_rows</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>F</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>[-1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RD</td>\n",
       "      <td>7000</td>\n",
       "      <td>3500.0</td>\n",
       "      <td>4000</td>\n",
       "      <td>3000</td>\n",
       "      <td>2</td>\n",
       "      <td>[3000, 4000]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SRE</td>\n",
       "      <td>8000</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>4000</td>\n",
       "      <td>4000</td>\n",
       "      <td>2</td>\n",
       "      <td>[4000, 4000]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  deparment  sum_salary  avg_salary  max_salary  min_salary  count_rows  \\\n",
       "0         F          -1        -1.0          -1          -1           1   \n",
       "1        RD        7000      3500.0        4000        3000           2   \n",
       "2       SRE        8000      4000.0        4000        4000           2   \n",
       "\n",
       "       all_rows  \n",
       "0          [-1]  \n",
       "1  [3000, 4000]  \n",
       "2  [4000, 4000]  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4 collect data point for each group with the stats(min, max, sum, avg, count)\n",
    "\n",
    "\n",
    "# apply multiple aggregation fuction on groupby object\n",
    "data = [\n",
    "    (\"James\",\"\",\"Smith\",\"36636\",\"RD\",\"M\", 3000),\n",
    "    (\"Michael\",\"Rose\",\"\",\"40288\",\"RD\",\"M\", 4000),\n",
    "    (\"Robert\",\"\",\"Williams\",\"42114\",\"SRE\",\"M\", 4000),\n",
    "    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"SRE\",\"F\", 4000),\n",
    "    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",\"BACKEND\", -1)\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"firstname\",StringType(), True), # Nullable True\n",
    "    StructField(\"middlename\",StringType(), True),\n",
    "    StructField(\"lastname\",StringType(), True),\n",
    "    StructField(\"id\",StringType(), True),\n",
    "    StructField(\"deparment\",StringType(), True),\n",
    "    StructField(\"gender\",StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True)\n",
    "    ])\n",
    "df = spark.createDataFrame(data=data,schema=schema)\n",
    "\n",
    "df_grp_department = df.groupby(\"deparment\").agg(\n",
    "    F.sum(\"salary\").alias(\"sum_salary\"),\n",
    "    F.avg(\"salary\").alias(\"avg_salary\"),\n",
    "    F.max(\"salary\").alias(\"max_salary\"),\n",
    "    F.min(\"salary\").alias(\"min_salary\"),\n",
    "    F.count(\"salary\").alias(\"count_rows\"),\n",
    "    F.collect_list(\"salary\").alias(\"all_rows\")\n",
    ")\n",
    "\n",
    "df_grp_department.toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-21T14:16:56.874394Z",
     "start_time": "2021-03-21T14:16:56.250668Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+-----+---------+-------+------+\n",
      "|firstname|middlename|lastname|   id|deparment| gender|salary|\n",
      "+---------+----------+--------+-----+---------+-------+------+\n",
      "|      Jen|      Mary|   Brown|     |        F|BACKEND|    -1|\n",
      "|  Michael|      Rose|        |40288|       RD|      M|  8000|\n",
      "|    Maria|      Anne|   Jones|39192|      SRE|      F|  6000|\n",
      "+---------+----------+--------+-----+---------+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5 get first one row in each group\n",
    "# We use Window Function here\n",
    "# Key to think about this, we rank the data in each group, then \n",
    "# filtering\n",
    "# no nothing is groupby\n",
    "# which is different in pandas\n",
    "\n",
    "data = [\n",
    "    (\"James\",\"\",\"Smith\",\"36636\",\"RD\",\"M\", 3000),\n",
    "    (\"Michael\",\"Rose\",\"\",\"40288\",\"RD\",\"M\", 8000),\n",
    "    (\"Robert\",\"\",\"Williams\",\"42114\",\"SRE\",\"M\", 4000),\n",
    "    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"SRE\",\"F\", 6000),\n",
    "    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",\"BACKEND\", -1)\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"firstname\",StringType(), True), # Nullable True\n",
    "    StructField(\"middlename\",StringType(), True),\n",
    "    StructField(\"lastname\",StringType(), True),\n",
    "    StructField(\"id\",StringType(), True),\n",
    "    StructField(\"deparment\",StringType(), True),\n",
    "    StructField(\"gender\",StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True)\n",
    "    ])\n",
    "\n",
    "\n",
    "df = spark.createDataFrame(data=data,schema=schema)\n",
    "\n",
    "df = (\n",
    "    df.withColumn(\"rank_salary_by_deparment\",\n",
    "                  F.row_number().over(\n",
    "                  W.partitionBy(\"deparment\")\\\n",
    "                      .orderBy(F.desc(\"salary\"))\n",
    "                  )\n",
    "                 )\\\n",
    "    .filter(F.col(\"rank_salary_by_deparment\") == 1)\\\n",
    "    .drop('rank_salary_by_deparment')\n",
    ")\n",
    "\n",
    "df.show(n=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-21T14:16:57.444948Z",
     "start_time": "2021-03-21T14:16:56.876605Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+----------+----------+----------+----------+\n",
      "|deparment|sum_salary|avg_salary|max_salary|min_salary|count_rows|\n",
      "+---------+----------+----------+----------+----------+----------+\n",
      "|       RD|      7000|    3500.0|      4000|      3000|         2|\n",
      "|      SRE|      8000|    4000.0|      4000|      4000|         2|\n",
      "+---------+----------+----------+----------+----------+----------+\n",
      "\n",
      "root\n",
      " |-- deparment: string (nullable = true)\n",
      " |-- sum_salary: long (nullable = true)\n",
      " |-- avg_salary: double (nullable = true)\n",
      " |-- max_salary: integer (nullable = true)\n",
      " |-- min_salary: integer (nullable = true)\n",
      " |-- count_rows: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6 groupby and filtering\n",
    "\n",
    "data = [\n",
    "    (\"James\",\"\",\"Smith\",\"36636\",\"RD\",\"M\", 3000),\n",
    "    (\"Michael\",\"Rose\",\"\",\"40288\",\"RD\",\"M\", 4000),\n",
    "    (\"Robert\",\"\",\"Williams\",\"42114\",\"SRE\",\"M\", 4000),\n",
    "    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"SRE\",\"F\", 4000),\n",
    "    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",\"BACKEND\", -1)\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"firstname\",StringType(), True), # Nullable True\n",
    "    StructField(\"middlename\",StringType(), True),\n",
    "    StructField(\"lastname\",StringType(), True),\n",
    "    StructField(\"id\",StringType(), True),\n",
    "    StructField(\"deparment\",StringType(), True),\n",
    "    StructField(\"gender\",StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True)\n",
    "    ])\n",
    "df = spark.createDataFrame(data=data,schema=schema)\n",
    "\n",
    "df_grp_department = (\n",
    "    df.groupby(\"deparment\").agg(\n",
    "        F.sum(\"salary\").alias(\"sum_salary\"),\n",
    "        F.avg(\"salary\").alias(\"avg_salary\"),\n",
    "        F.max(\"salary\").alias(\"max_salary\"),\n",
    "        F.min(\"salary\").alias(\"min_salary\"),\n",
    "        F.count(\"salary\").alias(\"count_rows\"))\n",
    "    .filter(C(\"sum_salary\") > 0)\n",
    "    \n",
    ")\n",
    "\n",
    "df_grp_department.show(n=5)\n",
    "df_grp_department.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-21T14:16:58.236393Z",
     "start_time": "2021-03-21T14:16:57.446906Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|item|score|\n",
      "+----+-----+\n",
      "|   a|   10|\n",
      "|   a|   10|\n",
      "|   a|   20|\n",
      "+----+-----+\n",
      "\n",
      "+----+-----+----+----------+----------+\n",
      "|item|score|rank|dense_rank|row_number|\n",
      "+----+-----+----+----------+----------+\n",
      "|   a|   10|   1|         1|         1|\n",
      "|   a|   10|   1|         1|         2|\n",
      "|   a|   20|   3|         2|         3|\n",
      "+----+-----+----+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 7 rank, dense_rank, and row_number\n",
    "# https://stackoverflow.com/questions/44968912/difference-in-dense-rank-and-row-number-in-spark\n",
    "# The window functions\n",
    "\n",
    "data = [\n",
    "    (\"a\",10),\n",
    "    ('a',10),\n",
    "    ('a',20)\n",
    "]\n",
    "\n",
    "columns = ['item','score']\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show(n=5)\n",
    "\n",
    "window_spec = W.partitionBy(\"item\").orderBy(\"score\")\n",
    "df = (\n",
    "    df.withColumn(\"rank\", F.rank().over(window_spec))\\\n",
    "    .withColumn(\"dense_rank\", F.dense_rank().over(window_spec))\\\n",
    "    .withColumn(\"row_number\", F.row_number().over(window_spec))\n",
    ")\n",
    "\n",
    "df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-21T14:16:59.128017Z",
     "start_time": "2021-03-21T14:16:58.238935Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----+\n",
      "| id|key|value|\n",
      "+---+---+-----+\n",
      "|  1|  a|  123|\n",
      "|  1|  b|  234|\n",
      "|  1|  c|  345|\n",
      "|  2|  a|   12|\n",
      "|  2|  x|   23|\n",
      "+---+---+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- collections: array (nullable = true)\n",
      " |    |-- element: map (containsNull = true)\n",
      " |    |    |-- key: string\n",
      " |    |    |-- value: long (valueContainsNull = true)\n",
      "\n",
      "+---+------------------------------------+\n",
      "|id |collections                         |\n",
      "+---+------------------------------------+\n",
      "|1  |[[a -> 123], [b -> 234], [c -> 345]]|\n",
      "|2  |[[a -> 12], [x -> 23], [y -> 123]]  |\n",
      "+---+------------------------------------+\n",
      "\n",
      "[Row(id=1, collections=[{'a': 123}, {'b': 234}, {'c': 345}]), Row(id=2, collections=[{'a': 12}, {'x': 23}, {'y': 123}])]\n"
     ]
    }
   ],
   "source": [
    "# 8 collect dict (map) with a group\n",
    "# https://stackoverflow.com/questions/55308482/pyspark-create-dictionary-within-groupby\n",
    "\n",
    "# collect_list : return a list of objects with duplicated\n",
    "# collect_set : return a set of objects without duplicated\n",
    "# struct : create a new struct column\n",
    "# ( > 2.4.0)map_from_entries : returns a map created from the given array of entries\n",
    "# create_map\n",
    "\n",
    "######### pyspark < 2.4.0\n",
    "data = [\n",
    "    (1,'a',123),\n",
    "    (1,'b',234),\n",
    "    (1,'c',345),\n",
    "    (2,'a',12),\n",
    "    (2,'x',23),\n",
    "    (2,'y',123)\n",
    "]\n",
    "\n",
    "columns = ['id','key','value']\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show(n=5)\n",
    "\n",
    "######## pyspark < 2.4.0\n",
    "\n",
    "df_agg = df.groupBy(\"id\").agg(\n",
    "    F.collect_list(F.create_map(C(\"key\"),C(\"value\"))).alias('collections')\n",
    ")\n",
    "\n",
    "df_agg.printSchema()\n",
    "df_agg.show(n=10, truncate=False)\n",
    "print(df_agg.collect())\n",
    "df_agg.toPandas().to_json('tmp.json',\n",
    "                          orient='records',\n",
    "                          force_ascii=False,\n",
    "                          lines=True)\n",
    "\n",
    "# to_json(join(SERVING_POI_FOOD_IMG_FOLDER,serving_fname),\n",
    "#                                        orient='records',\n",
    "#                                        force_ascii=False,\n",
    "#                                        lines=True)\n",
    "######### pyspark > 2.4.0\n",
    "# df.groupBy(\"id\").agg(\n",
    "#     F.map_from_entries(\n",
    "#         F.collect_list(\n",
    "#             F.struct(\"key\",\"value\"))).alias(\"key_value\")\n",
    "# ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-21T14:16:59.516673Z",
     "start_time": "2021-03-21T14:16:59.130547Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pandas'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   A   B   C\n",
       "0  a   1   2\n",
       "1  b   3   4\n",
       "2  c   5   6\n",
       "4  a  11  12"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>variable</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>B</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a</td>\n",
       "      <td>B</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a</td>\n",
       "      <td>C</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>a</td>\n",
       "      <td>C</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b</td>\n",
       "      <td>B</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>b</td>\n",
       "      <td>C</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c</td>\n",
       "      <td>B</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>c</td>\n",
       "      <td>C</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   A variable  value\n",
       "0  a        B      1\n",
       "3  a        B     11\n",
       "4  a        C      2\n",
       "7  a        C     12\n",
       "1  b        B      3\n",
       "5  b        C      4\n",
       "2  c        B      5\n",
       "6  c        C      6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'PySpark'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+\n",
      "|  A|  B|  C|\n",
      "+---+---+---+\n",
      "|  a|  1|  2|\n",
      "|  b|  3|  4|\n",
      "|  c|  5|  6|\n",
      "|  a| 11| 12|\n",
      "+---+---+---+\n",
      "\n",
      "root\n",
      " |-- A: string (nullable = true)\n",
      " |-- B: long (nullable = true)\n",
      " |-- C: long (nullable = true)\n",
      "\n",
      "+---+--------+-----+\n",
      "|  A|variable|value|\n",
      "+---+--------+-----+\n",
      "|  a|       B|    1|\n",
      "|  a|       C|    2|\n",
      "|  b|       B|    3|\n",
      "|  b|       C|    4|\n",
      "|  c|       B|    5|\n",
      "|  c|       C|    6|\n",
      "|  a|       B|   11|\n",
      "|  a|       C|   12|\n",
      "+---+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 9 melt the dataframe (wide dataframe to long dataframe)\n",
    "from typing import Iterable\n",
    "import pandas as pd\n",
    "from IPython.core.display import display\n",
    "# https://stackoverflow.com/questions/41670103/how-to-melt-spark-dataframe\n",
    "\n",
    "\n",
    "def melt(\n",
    "        df: DataFrame, \n",
    "        id_vars: Iterable[str], value_vars: Iterable[str], \n",
    "        var_name: str=\"variable\", value_name: str=\"value\") -> DataFrame:\n",
    "    \"\"\"Convert :class:`DataFrame` from wide to long format.\"\"\"\n",
    "\n",
    "    # Create array<struct<variable: str, value: ...>>\n",
    "    _vars_and_vals = F.array(*(\n",
    "        F.struct(F.lit(c).alias(var_name), C(c).alias(value_name)) \n",
    "        for c in value_vars))\n",
    "\n",
    "    # Add to the DataFrame and explode\n",
    "    _tmp = df.withColumn(\"_vars_and_vals\", F.explode(_vars_and_vals))\n",
    "\n",
    "    cols = id_vars + [\n",
    "            C(\"_vars_and_vals\")[x].alias(x) for x in [var_name, value_name]]\n",
    "    return _tmp.select(*cols)\n",
    "\n",
    "pdf = pd.DataFrame({'A': {0: 'a', 1: 'b', 2: 'c', 4 : 'a'},\n",
    "                    'B': {0: 1, 1: 3, 2: 5, 4 : 11},\n",
    "                    'C': {0: 2, 1: 4, 2: 6, 4 : 12}\n",
    "                   })\n",
    "\n",
    "pdf_result = pd.melt(pdf, id_vars=['A'], value_vars=['B', 'C']).sort_values(by=['A'])\n",
    "\n",
    "\n",
    "display(\n",
    "    \"Pandas\",\n",
    "    pdf,\n",
    "    pdf_result,\n",
    "    \"PySpark\",\n",
    "       )\n",
    "\n",
    "# Case 1\n",
    "# pdf['C'] = pdf['C'].astype(str) # then you can convert to spark df\n",
    "sdf = spark.createDataFrame(pdf)\n",
    "sdf.show()\n",
    "sdf.printSchema()\n",
    "melt(sdf, id_vars=['A'], value_vars=['B', 'C']).show()\n",
    "\n",
    "# Case 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-21T14:17:00.294542Z",
     "start_time": "2021-03-21T14:16:59.519555Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+------------------------+---------------+---------+\n",
      "|store_name|food_category|food_category_popularity|        img_url|author_id|\n",
      "+----------+-------------+------------------------+---------------+---------+\n",
      "|    hotpop|         Meat|                       3|https//:123.png|   rtyg11|\n",
      "|    hotpop|         Meat|                       3|https//:456.png|   rtyg11|\n",
      "|    hotpop|         Meat|                       3|https//:789.png|   rtyg11|\n",
      "|    hotpop|    Vegetable|                       2|https//:111.png|   rtyg11|\n",
      "|    hotpop|    Vegetable|                       2|https//:222.png|   rtyg11|\n",
      "|    branch|   Fried food|                       1|https//:333.png|     bvc1|\n",
      "|    branch|      Dessert|                       1|https//:444.png|     7854|\n",
      "+----------+-------------+------------------------+---------------+---------+\n",
      "\n",
      "+----------+--------------------+\n",
      "|store_name|                menu|\n",
      "+----------+--------------------+\n",
      "|    hotpop|[[food_category -...|\n",
      "|    branch|[[food_category -...|\n",
      "+----------+--------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(store_name='hotpop', menu=[{'food_category': 'Meat'}, {'food_category_popularity': '3'}, {'img_url': 'https//:123.png'}, {'author_id': 'rtyg11'}, {'food_category': 'Meat'}, {'food_category_popularity': '3'}, {'img_url': 'https//:456.png'}, {'author_id': 'rtyg11'}, {'food_category': 'Meat'}, {'food_category_popularity': '3'}, {'img_url': 'https//:789.png'}, {'author_id': 'rtyg11'}, {'food_category': 'Vegetable'}, {'food_category_popularity': '2'}, {'img_url': 'https//:111.png'}, {'author_id': 'rtyg11'}, {'food_category': 'Vegetable'}, {'food_category_popularity': '2'}, {'img_url': 'https//:222.png'}, {'author_id': 'rtyg11'}]),\n",
       " Row(store_name='branch', menu=[{'food_category': 'Fried food'}, {'food_category_popularity': '1'}, {'img_url': 'https//:333.png'}, {'author_id': 'bvc1'}, {'food_category': 'Dessert'}, {'food_category_popularity': '1'}, {'img_url': 'https//:444.png'}, {'author_id': '7854'}])]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 10 collect dict (map) within a group\n",
    "\n",
    "data = [\n",
    "    (\"hotpop\",\"Meat\",3,\"https//:123.png\",\"rtyg11\"),\n",
    "    (\"hotpop\",\"Meat\",3,\"https//:456.png\",\"rtyg11\"),\n",
    "    (\"hotpop\",\"Meat\",3,\"https//:789.png\",\"rtyg11\"),\n",
    "    (\"hotpop\",\"Vegetable\",2,\"https//:111.png\",\"rtyg11\"),\n",
    "    (\"hotpop\",\"Vegetable\",2,\"https//:222.png\",\"rtyg11\"),\n",
    "    (\"branch\",\"Fried food\",1,\"https//:333.png\",\"bvc1\"),\n",
    "    (\"branch\",\"Dessert\",1,\"https//:444.png\",\"7854\"),\n",
    "  ]\n",
    "\n",
    "columns = [\"store_name\",\"food_category\",\"food_category_popularity\",\"img_url\",\"author_id\"]\n",
    "df = spark.createDataFrame(data = data, schema = columns)\n",
    "\n",
    "df.show(n=10)\n",
    "\n",
    "# melt it first\n",
    "\n",
    "def melt(\n",
    "        df: DataFrame, \n",
    "        id_vars: Iterable[str], value_vars: Iterable[str], \n",
    "        var_name: str=\"variable\", value_name: str=\"value\") -> DataFrame:\n",
    "    \"\"\"Convert :class:`DataFrame` from wide to long format.\"\"\"\n",
    "\n",
    "    # Create array<struct<variable: str, value: ...>>\n",
    "    _vars_and_vals = F.array(*(\n",
    "        F.struct(F.lit(c).alias(var_name), C(c).alias(value_name)) \n",
    "        for c in value_vars))\n",
    "\n",
    "    # Add to the DataFrame and explode\n",
    "    _tmp = df.withColumn(\"_vars_and_vals\", F.explode(_vars_and_vals))\n",
    "\n",
    "    cols = id_vars + [\n",
    "            C(\"_vars_and_vals\")[x].alias(x) for x in [var_name, value_name]]\n",
    "    return _tmp.select(*cols)\n",
    "\n",
    "\n",
    "# convert the melt column to string\n",
    "# because the column you wanna melt should be the same dtype\n",
    "df = (\n",
    "    df.withColumn(\"food_category_popularity\", C(\"food_category_popularity\").cast(StringType()))\n",
    ")\n",
    "df_complex = (\n",
    "    melt(df, id_vars=['store_name'],\n",
    "             value_vars=['food_category','food_category_popularity','img_url','author_id'],\n",
    "             var_name = 'menu_key',value_name = 'menu_value'\n",
    "        ).groupBy(\"store_name\").agg(\n",
    "        F.collect_list(F.create_map(C(\"menu_key\"), C(\"menu_value\"))).alias(\"menu\")\n",
    "    )\n",
    ")\n",
    "\n",
    "df_complex.show()\n",
    "df_complex.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-21T14:17:00.865447Z",
     "start_time": "2021-03-21T14:17:00.296313Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-----+--------------+\n",
      "|store_name|food_category|  url|food_cat_count|\n",
      "+----------+-------------+-----+--------------+\n",
      "|    branch|      Dessert|ulr_7|             1|\n",
      "|    hotpop|         Meat|url_1|             3|\n",
      "|    hotpop|         Meat|url_2|             3|\n",
      "|    hotpop|         Meat|url_3|             3|\n",
      "|    hotpop|    Vegetable|url_4|             2|\n",
      "|    hotpop|    Vegetable|url_5|             2|\n",
      "|    branch|   Fried food|url_6|             1|\n",
      "+----------+-------------+-----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 11 groupby and sum by a window function\n",
    "data = [\n",
    "    (\"hotpop\",\"Meat\",\"url_1\",),\n",
    "    (\"hotpop\",\"Meat\",\"url_2\"),\n",
    "    (\"hotpop\",\"Meat\",\"url_3\"),\n",
    "    (\"hotpop\",\"Vegetable\",\"url_4\"),\n",
    "    (\"hotpop\",\"Vegetable\",\"url_5\"),\n",
    "    (\"branch\",\"Fried food\",\"url_6\"),\n",
    "    (\"branch\",\"Dessert\",\"ulr_7\"),\n",
    "  ]\n",
    "\n",
    "columns = [\"store_name\",\"food_category\",\"url\"]\n",
    "window_spec = w.partitionBy(\"store_name\",\"food_category\")\n",
    "\n",
    "df = spark.createDataFrame(data = data, schema = columns)\n",
    "df = (\n",
    "    df.withColumn(\"food_cat_count\", F.count(\"food_category\").over(window_spec))\n",
    ")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-21T14:17:01.912685Z",
     "start_time": "2021-03-21T14:17:00.866931Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before\n",
      "+----------+--------------------+\n",
      "|article_id|             img_url|\n",
      "+----------+--------------------+\n",
      "|     14431|https://pic.pimg....|\n",
      "|     14431|https://pic.pimg....|\n",
      "|     14431|https://pic.pimg....|\n",
      "|     67789|https://pic.pimg....|\n",
      "|     67789|https://pic.pimg....|\n",
      "+----------+--------------------+\n",
      "\n",
      "+----------+--------------------+-----------+\n",
      "|article_id|             img_url|img_b64_str|\n",
      "+----------+--------------------+-----------+\n",
      "|     14431|https://pic.pimg....|[B@57192075|\n",
      "|     14431|https://pic.pimg....|[B@231b4e6f|\n",
      "|     14431|https://pic.pimg....|[B@19afc22c|\n",
      "|     67789|https://pic.pimg....|[B@18275710|\n",
      "|     67789|https://pic.pimg....|       null|\n",
      "+----------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Use Pyspark to send request, get image and store as b64string \n",
    "# https://stackoverflow.com/questions/49353752/use-requests-module-and-return-response-to-pyspark-dataframe\n",
    "\n",
    "data = [\n",
    "    (14431,'https://pic.pimg.tw/happy78/1528543947-685380499_n.jpg'),\n",
    "    (14431,'https://pic.pimg.tw/happy78/1528543947-362759723_n.jpg'),\n",
    "    (14431,'https://pic.pimg.tw/happy78/1528543962-2265924582_n.jpg'),\n",
    "    (67789,'https://pic.pimg.tw/happy78/1528543962-4007835890_n.jpg'),\n",
    "    (67789,'https://pic.pimg.tw/happy78/1528543962-45890_n.jpg')\n",
    "]\n",
    "\n",
    "columns = ['article_id','img_url']\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=columns)\n",
    "print('before')\n",
    "df.show(n=5)\n",
    "\n",
    "@F.udf(returnType=StringType())\n",
    "def get_img_binary(url : str) -> str:\n",
    "    import requests\n",
    "    import base64\n",
    "    resp = requests.get(url)\n",
    "    if resp.status_code == 200:\n",
    "        return base64.encodestring(resp.content)\n",
    "    else:\n",
    "        return None\n",
    "df = (\n",
    "    df.withColumn(\"img_b64_str\", get_img_binary(C(\"img_url\")))\n",
    ")\n",
    "\n",
    "df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-21T14:55:13.313495Z",
     "start_time": "2021-03-21T14:55:13.000448Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before : \n",
      "+---+--------+\n",
      "| id|features|\n",
      "+---+--------+\n",
      "|  1|      64|\n",
      "|  2|      76|\n",
      "|  3|      54|\n",
      "|  4|      11|\n",
      "|  5|     100|\n",
      "+---+--------+\n",
      "\n",
      "compose 0.5205074918080084 <class 'str'> <class 'float'>\n",
      "+---+--------+--------+----------+\n",
      "|id |features|category|prob      |\n",
      "+---+--------+--------+----------+\n",
      "|1  |64      |food    |0.5874553 |\n",
      "|2  |76      |drink   |0.1357918 |\n",
      "|3  |54      |drink   |0.6265335 |\n",
      "|4  |11      |drink   |0.7386318 |\n",
      "|5  |100     |drink   |0.11627066|\n",
      "+---+--------+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2 udf return two column values, e.g. model prediction with label and probability\n",
    "import random\n",
    "data = [\n",
    "    (1,64),\n",
    "    (2,76),\n",
    "    (3,54),\n",
    "    (4,11),\n",
    "    (5,100),\n",
    "]\n",
    "columns = ['id','features']\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=columns)\n",
    "print(\"Before : \")\n",
    "df.show(n=5)\n",
    "\n",
    "############# sol #################\n",
    "# using Row object to return multiple column\n",
    "from pyspark.sql import Row\n",
    "\n",
    "model_pred = StructType([\n",
    "    StructField(\"category\", StringType(), False),\n",
    "    StructField(\"prob\", T.FloatType(), False)])\n",
    "\n",
    "@F.udf(returnType=model_pred)\n",
    "def model_pred(n):\n",
    "    import random\n",
    "    category = random.choice(['food','env','compose','drink'])\n",
    "    prob = random.random()\n",
    "    return Row('category', 'prob')(category, prob)\n",
    "\n",
    "cat = random.choice(['food','env','compose','drink'])\n",
    "p = random.random()\n",
    "\n",
    "print(cat, p, type(cat), type(p))\n",
    "\n",
    "newDF = df.withColumn(\"pred\", model_pred(df[\"features\"]))\n",
    "newDF = newDF.select(\"id\", \"features\", \"pred.*\")\n",
    "\n",
    "newDF.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-21T14:17:02.236675Z",
     "start_time": "2021-03-21T14:17:02.224785Z"
    }
   },
   "outputs": [],
   "source": [
    "# 3. Use Pyspark to load a tf.keras model\n",
    "# serieslize the model and make prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-21T14:17:02.354587Z",
     "start_time": "2021-03-21T14:17:02.238615Z"
    }
   },
   "outputs": [],
   "source": [
    "# 4. Pandas udf\n",
    "# documentation and concept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# broadcasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-21T14:17:02.988355Z",
     "start_time": "2021-03-21T14:17:02.357074Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-------+-----+\n",
      "|firstname|lastname|country|state|\n",
      "+---------+--------+-------+-----+\n",
      "|    James|   Smith|    USA|   CA|\n",
      "|  Michael|    Rose|    USA|   NY|\n",
      "|   Robert|Williams|    USA|   CA|\n",
      "|    Maria|   Jones|    USA|   FL|\n",
      "+---------+--------+-------+-----+\n",
      "\n",
      "+---------+--------+-------+----------+\n",
      "|firstname|lastname|country|     state|\n",
      "+---------+--------+-------+----------+\n",
      "|    James|   Smith|    USA|California|\n",
      "|  Michael|    Rose|    USA|  New York|\n",
      "|   Robert|Williams|    USA|California|\n",
      "|    Maria|   Jones|    USA|   Florida|\n",
      "+---------+--------+-------+----------+\n",
      "\n",
      "+---------+--------+-------+-----+---------------+\n",
      "|firstname|lastname|country|state|converted_state|\n",
      "+---------+--------+-------+-----+---------------+\n",
      "|    James|   Smith|    USA|   CA|     California|\n",
      "|  Michael|    Rose|    USA|   NY|       New York|\n",
      "|   Robert|Williams|    USA|   CA|     California|\n",
      "|    Maria|   Jones|    USA|   FL|        Florida|\n",
      "+---------+--------+-------+-----+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1\n",
    "# broadcast the dictionary to spark \n",
    "# (which is a way that enhance multi-processing cross machine using your python code)\n",
    "# the broadcast variable should be serializable\n",
    "\n",
    "states = {\"NY\":\"New York\", \"CA\":\"California\", \"FL\":\"Florida\"}\n",
    "broadcastStates = spark.sparkContext.broadcast(states)\n",
    "\n",
    "data = [(\"James\",\"Smith\",\"USA\",\"CA\"),\n",
    "    (\"Michael\",\"Rose\",\"USA\",\"NY\"),\n",
    "    (\"Robert\",\"Williams\",\"USA\",\"CA\"),\n",
    "    (\"Maria\",\"Jones\",\"USA\",\"FL\")\n",
    "  ]\n",
    "\n",
    "columns = [\"firstname\",\"lastname\",\"country\",\"state\"]\n",
    "df = spark.createDataFrame(data = data, schema = columns)\n",
    "df.show(n=5)\n",
    "\n",
    "def state_convert(code):\n",
    "    return broadcastStates.value[code]\n",
    "\n",
    "# case 1, using rdd\n",
    "result_rdd = df.rdd.map(lambda x: (x[0],x[1],x[2],state_convert(x[3]))).toDF(columns)\n",
    "result_rdd.show(n=5)\n",
    "\n",
    "\n",
    "# case 2, using pdf\n",
    "\n",
    "@F.udf(returnType=StringType())\n",
    "def state_convert_udf(code : str) -> str:\n",
    "    return broadcastStates.value[code]\n",
    "\n",
    "result_df = (\n",
    "    df.withColumn(\"converted_state\", state_convert_udf(C(\"state\")))\n",
    ")\n",
    "\n",
    "result_df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-21T14:17:03.017801Z",
     "start_time": "2021-03-21T14:17:02.991174Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.broadcast.Broadcast'> ['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_jbroadcast', '_path', '_pickle_registry', '_python_broadcast', '_sc', 'destroy', 'dump', 'load', 'load_from_path', 'unpersist', 'value']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'NY': 'New York', 'CA': 'California', 'FL': 'Florida'}, dict)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2\n",
    "# Knowing broacsting object\n",
    "\n",
    "# broadcast the dictionary to spark \n",
    "# (which is a way that enhance multi-processing cross machine using your python code)\n",
    "# the broadcast variable should be serializable\n",
    "# https://spark.apache.org/docs/2.3.3/api/python/_modules/pyspark/broadcast.html\n",
    "states = {\"NY\":\"New York\", \"CA\":\"California\", \"FL\":\"Florida\"}\n",
    "broadcastStates = spark.sparkContext.broadcast(states)\n",
    "print(type(broadcastStates), dir(broadcastStates))\n",
    "\n",
    "# value to access the object\n",
    "broadcastStates.value, type(broadcastStates.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-21T14:17:03.715371Z",
     "start_time": "2021-03-21T14:17:03.019311Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n",
      "|article_id|             img_url|\n",
      "+----------+--------------------+\n",
      "|     14431|https://pic.pimg....|\n",
      "|     14431|https://pic.pimg....|\n",
      "|     14431|https://pic.pimg....|\n",
      "|     67789|https://pic.pimg....|\n",
      "|     67789|https://pic.pimg....|\n",
      "+----------+--------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>img_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14431</td>\n",
       "      <td>https://pic.pimg.tw/happy78/1528543947-6853804...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14431</td>\n",
       "      <td>https://pic.pimg.tw/happy78/1528543947-3627597...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14431</td>\n",
       "      <td>https://pic.pimg.tw/happy78/1528543962-2265924...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>67789</td>\n",
       "      <td>https://pic.pimg.tw/happy78/1528543962-4007835...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>67789</td>\n",
       "      <td>https://pic.pimg.tw/happy78/1528543962-45890_n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   article_id                                            img_url\n",
       "0       14431  https://pic.pimg.tw/happy78/1528543947-6853804...\n",
       "1       14431  https://pic.pimg.tw/happy78/1528543947-3627597...\n",
       "2       14431  https://pic.pimg.tw/happy78/1528543962-2265924...\n",
       "3       67789  https://pic.pimg.tw/happy78/1528543962-4007835...\n",
       "4       67789  https://pic.pimg.tw/happy78/1528543962-45890_n..."
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1\n",
    "# write dataframe to jsonl format\n",
    "# https://stackoverflow.com/questions/43269244/pyspark-dataframe-write-to-single-json-file-with-specific-name\n",
    "# https://sparkbyexamples.com/pyspark/pyspark-read-json-file-into-dataframe/\n",
    "data = [\n",
    "    (14431,'https://pic.pimg.tw/happy78/1528543947-685380499_n.jpg'),\n",
    "    (14431,'https://pic.pimg.tw/happy78/1528543947-362759723_n.jpg'),\n",
    "    (14431,'https://pic.pimg.tw/happy78/1528543962-2265924582_n.jpg'),\n",
    "    (67789,'https://pic.pimg.tw/happy78/1528543962-4007835890_n.jpg'),\n",
    "    (67789,'https://pic.pimg.tw/happy78/1528543962-45890_n.jpg')\n",
    "]\n",
    "\n",
    "columns = ['article_id','img_url']\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "fname_folder = join('output','jsonl_format_folder.json')\n",
    "# This one will creat a folder contains part file for better multiple worker IO\n",
    "df.coalesce(1).write.format('json').save(fname_folder, mode='overwrite')\n",
    "df_new = spark.read.json(fname_folder)\n",
    "df_new.show(n=10)\n",
    "# However, if you wanna save it in a single file, use pandas\n",
    "fname = join('output','jsonl_format.json')\n",
    "# df.toPandas().to_json('path/file_name.json', orient='records', force_ascii=False, lines=True)\n",
    "df.toPandas().to_json(fname, orient='records',force_ascii=False,lines=True)\n",
    "df_new_pd = pd.read_json(fname,orient='records',lines=True)\n",
    "df_new_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-21T14:17:04.965814Z",
     "start_time": "2021-03-21T14:17:03.718099Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+--------------------+\n",
      "|article_id|    date|             img_url|\n",
      "+----------+--------+--------------------+\n",
      "|     14431|20210224|https://pic.pimg....|\n",
      "|     14431|20210224|https://pic.pimg....|\n",
      "|     14431|20210224|https://pic.pimg....|\n",
      "|     67789|20210224|https://pic.pimg....|\n",
      "|     67789|20210224|https://pic.pimg....|\n",
      "+----------+--------+--------------------+\n",
      "\n",
      "+----------+--------+--------------------+\n",
      "|article_id|    date|             img_url|\n",
      "+----------+--------+--------------------+\n",
      "|     86481|20210225|https://pic.pimg....|\n",
      "|     45213|20210225|https://pic.pimg....|\n",
      "|     24561|20210225|https://pic.pimg....|\n",
      "|     75371|20210225|https://pic.pimg....|\n",
      "|     25691|20210225|https://pic.pimg....|\n",
      "+----------+--------+--------------------+\n",
      "\n",
      "+----------+--------+--------------------+\n",
      "|article_id|    date|             img_url|\n",
      "+----------+--------+--------------------+\n",
      "|      7861|20210304|https://pic.pimg....|\n",
      "|     45213|20210304|https://pic.pimg....|\n",
      "|      1111|20210304|https://pic.pimg....|\n",
      "|     76661|20210304|https://pic.pimg....|\n",
      "|      8888|20210304|https://pic.pimg....|\n",
      "+----------+--------+--------------------+\n",
      "\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1150.parquet.\n: java.lang.AssertionError: assertion failed: Conflicting partition column names detected:\n\n\tPartition column name list #0: article_id\n\tPartition column name list #1: date\n\nFor partitioned table directories, data files should only live in leaf directories.\nAnd directories at the same level should have the same partition column name.\nPlease check the following directories for unexpected files or inconsistent partition column names:\n\n\tfile:/home/joetsai/work/yulong/pyspark_101/notebook/output/save_by_date_partition.parquet/article_id=99999\n\tfile:/home/joetsai/work/yulong/pyspark_101/notebook/output/save_by_date_partition.parquet/article_id=branch\n\tfile:/home/joetsai/work/yulong/pyspark_101/notebook/output/save_by_date_partition.parquet/article_id=hotpop\n\tfile:/home/joetsai/work/yulong/pyspark_101/notebook/output/save_by_date_partition.parquet/date=20210225\n\tfile:/home/joetsai/work/yulong/pyspark_101/notebook/output/save_by_date_partition.parquet/date=20210224\n\tfile:/home/joetsai/work/yulong/pyspark_101/notebook/output/save_by_date_partition.parquet/date=20210304\n\tat scala.Predef$.assert(Predef.scala:170)\n\tat org.apache.spark.sql.execution.datasources.PartitioningUtils$.resolvePartitions(PartitioningUtils.scala:324)\n\tat org.apache.spark.sql.execution.datasources.PartitioningUtils$.parsePartitions(PartitioningUtils.scala:142)\n\tat org.apache.spark.sql.execution.datasources.PartitioningUtils$.parsePartitions(PartitioningUtils.scala:98)\n\tat org.apache.spark.sql.execution.datasources.PartitioningAwareFileIndex.inferPartitioning(PartitioningAwareFileIndex.scala:153)\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.partitionSpec(InMemoryFileIndex.scala:71)\n\tat org.apache.spark.sql.execution.datasources.PartitioningAwareFileIndex.partitionSchema(PartitioningAwareFileIndex.scala:50)\n\tat org.apache.spark.sql.execution.datasources.DataSource.combineInferredAndUserSpecifiedPartitionSchema(DataSource.scala:116)\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:167)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:393)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:239)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:227)\n\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:622)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-843b9840bd54>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;31m# support int and daterange, string might be problem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0mnew_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparquet_fname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m          \u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"date\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbetween\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_date\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_date\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/pixlake/.venv/lib/python3.7/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, *paths)\u001b[0m\n\u001b[1;32m    301\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'string'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'year'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'int'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'month'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'int'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'day'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'int'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m         \"\"\"\n\u001b[0;32m--> 303\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/pixlake/.venv/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/pixlake/.venv/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/pixlake/.venv/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1150.parquet.\n: java.lang.AssertionError: assertion failed: Conflicting partition column names detected:\n\n\tPartition column name list #0: article_id\n\tPartition column name list #1: date\n\nFor partitioned table directories, data files should only live in leaf directories.\nAnd directories at the same level should have the same partition column name.\nPlease check the following directories for unexpected files or inconsistent partition column names:\n\n\tfile:/home/joetsai/work/yulong/pyspark_101/notebook/output/save_by_date_partition.parquet/article_id=99999\n\tfile:/home/joetsai/work/yulong/pyspark_101/notebook/output/save_by_date_partition.parquet/article_id=branch\n\tfile:/home/joetsai/work/yulong/pyspark_101/notebook/output/save_by_date_partition.parquet/article_id=hotpop\n\tfile:/home/joetsai/work/yulong/pyspark_101/notebook/output/save_by_date_partition.parquet/date=20210225\n\tfile:/home/joetsai/work/yulong/pyspark_101/notebook/output/save_by_date_partition.parquet/date=20210224\n\tfile:/home/joetsai/work/yulong/pyspark_101/notebook/output/save_by_date_partition.parquet/date=20210304\n\tat scala.Predef$.assert(Predef.scala:170)\n\tat org.apache.spark.sql.execution.datasources.PartitioningUtils$.resolvePartitions(PartitioningUtils.scala:324)\n\tat org.apache.spark.sql.execution.datasources.PartitioningUtils$.parsePartitions(PartitioningUtils.scala:142)\n\tat org.apache.spark.sql.execution.datasources.PartitioningUtils$.parsePartitions(PartitioningUtils.scala:98)\n\tat org.apache.spark.sql.execution.datasources.PartitioningAwareFileIndex.inferPartitioning(PartitioningAwareFileIndex.scala:153)\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.partitionSpec(InMemoryFileIndex.scala:71)\n\tat org.apache.spark.sql.execution.datasources.PartitioningAwareFileIndex.partitionSchema(PartitioningAwareFileIndex.scala:50)\n\tat org.apache.spark.sql.execution.datasources.DataSource.combineInferredAndUserSpecifiedPartitionSchema(DataSource.scala:116)\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:167)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:393)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:239)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:227)\n\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:622)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "# 2 write parquet by date parittion\n",
    "\n",
    "\n",
    "# 1\n",
    "# write dataframe to jsonl format\n",
    "# https://sparkbyexamples.com/pyspark/pyspark-read-and-write-parquet-file/\n",
    "data_d1 = [\n",
    "    (14431,20210224,'https://pic.pimg.tw/happy78/1528543947-685380499_n.jpg'),\n",
    "    (14431,20210224,'https://pic.pimg.tw/happy78/1528543947-362759723_n.jpg'),\n",
    "    (14431,20210224,'https://pic.pimg.tw/happy78/1528543962-2265924582_n.jpg'),\n",
    "    (67789,20210224,'https://pic.pimg.tw/happy78/1528543962-4007835890_n.jpg'),\n",
    "    (67789,20210224,'https://pic.pimg.tw/happy78/1528543962-45890_n.jpg')\n",
    "]\n",
    "\n",
    "data_d2 = [\n",
    "    (86481,20210225,'https://pic.pimg.tw/happy78/1528543947-685380499_n.jpg'),\n",
    "    (45213,20210225,'https://pic.pimg.tw/happy78/1528543947-362759723_n.jpg'),\n",
    "    (24561,20210225,'https://pic.pimg.tw/happy78/1528543962-2265924582_n.jpg'),\n",
    "    (75371,20210225,'https://pic.pimg.tw/happy78/1528543962-4007835890_n.jpg'),\n",
    "    (25691,20210225,'https://pic.pimg.tw/happy78/1528543962-45890_n.jpg')\n",
    "]\n",
    "\n",
    "data_d3 = [\n",
    "    (7861,20210304,'https://pic.pimg.tw/happy78/1528543947-685380499_n.jpg'),\n",
    "    (45213,20210304,'https://pic.pimg.tw/happy78/1528543947-362759723_n.jpg'),\n",
    "    (1111,20210304,'https://pic.pimg.tw/happy78/1528543962-2265924582_n.jpg'),\n",
    "    (76661,20210304,'https://pic.pimg.tw/happy78/1528543962-4007835890_n.jpg'),\n",
    "    (8888,20210304,'https://pic.pimg.tw/happy78/1528543962-45890_n.jpg')\n",
    "]\n",
    "\n",
    "\n",
    "columns = ['article_id','date','img_url']\n",
    "\n",
    "df_d1 = spark.createDataFrame(data_d1, columns)\n",
    "df_d2 = spark.createDataFrame(data_d2, columns)\n",
    "df_d3 = spark.createDataFrame(data_d3, columns)\n",
    "\n",
    "for df in [df_d1, df_d2, df_d3]:\n",
    "    df.show(n=5)\n",
    "\n",
    "# save it\n",
    "parquet_fname = join(\"output\",\"save_by_date_partition.parquet\")\n",
    "for df in [df_d1, df_d2, df_d3]:\n",
    "    df.write.parquet(parquet_fname, mode=\"overwrite\", partitionBy=\"date\")\n",
    "\n",
    "# read by date range\n",
    "start_date = 20210224\n",
    "end_date = 20210301\n",
    "# support int and daterange, string might be problem\n",
    "\n",
    "new_df = spark.read.parquet(parquet_fname)\\\n",
    "         .where(C(\"date\").between(start_date, end_date))\n",
    "\n",
    "new_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-21T14:17:04.968042Z",
     "start_time": "2021-03-21T14:16:38.955Z"
    }
   },
   "outputs": [],
   "source": [
    "# read it\n",
    "start_date = 20210224\n",
    "end_date = 20210304\n",
    "\n",
    "# df.filter(df.year >= myYear)\n",
    "new_df = spark.read.parquet(parquet_fname)\n",
    "date_range_cond = (new_df.date >= start_date) & (new_df.date <= end_date)\n",
    "new_df.filter(date_range_cond).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-21T14:17:04.969560Z",
     "start_time": "2021-03-21T14:16:38.958Z"
    }
   },
   "outputs": [],
   "source": [
    "# different partition\n",
    "# can be read at the same time?\n",
    "data_d4 = [\n",
    "    (99999,20210224,'https://pic.pimg.tw/happy78/1528543947-685380499_n.jpg'),\n",
    "    (99999,20210225,'https://pic.pimg.tw/happy78/1528543947-362759723_n.jpg'),\n",
    "    (99999,20210226,'https://pic.pimg.tw/happy78/1528543962-2265924582_n.jpg'),\n",
    "    (99999,20210301,'https://pic.pimg.tw/happy78/1528543962-4007835890_n.jpg'),\n",
    "    (99999,20210303,'https://pic.pimg.tw/happy78/1528543962-45890_n.jpg')\n",
    "]\n",
    "\n",
    "columns = ['article_id','date','img_url']\n",
    "\n",
    "df = spark.createDataFrame(data_d4, columns)\n",
    "\n",
    "df.show(n=5)\n",
    "# now we partition by article_id\n",
    "parquet_fname = join(\"output\",\"save_by_date_partition.parquet\")\n",
    "df.write.parquet(parquet_fname, mode=\"overwrite\", partitionBy=\"article_id\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-21T14:17:04.971178Z",
     "start_time": "2021-03-21T14:16:38.961Z"
    }
   },
   "outputs": [],
   "source": [
    "# read by date range\n",
    "start_date = 20210224\n",
    "end_date = 20210301\n",
    "# support int and daterange, string might be problem\n",
    "\n",
    "new_df = spark.read.parquet(parquet_fname)\\\n",
    "         .where(C(\"date\").between(start_date, end_date))\n",
    "\n",
    "new_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-21T14:17:04.973182Z",
     "start_time": "2021-03-21T14:16:38.968Z"
    }
   },
   "outputs": [],
   "source": [
    "# 1\n",
    "# rank the food category popularity by store_name but crossed and rotated\n",
    "\n",
    "\n",
    "# create a mix ranking number\n",
    "# create a popularity_rank_score in each store_name\n",
    "    # Top popular in each store_name -> score 0\n",
    "    # Second popular in each store_name -> score 0.1\n",
    "# Add row number in each store_name, food_category -> cat_rank\n",
    "# Create category_popularity_mix_rank_score = cat_rank + popularity_rank_score\n",
    "# Sort the category_popularity_mix_rank_score by store_name\n",
    "\n",
    "data = [\n",
    "    (\"hotpop\",\"Meat\",3,),\n",
    "    (\"hotpop\",\"Meat\",3),\n",
    "    (\"hotpop\",\"Meat\",3),\n",
    "    (\"hotpop\",\"Vegetable\",2),\n",
    "    (\"hotpop\",\"Vegetable\",2),\n",
    "    (\"branch\",\"Fried food\",1),\n",
    "    (\"branch\",\"Dessert\",1),\n",
    "  ]\n",
    "\n",
    "columns = [\"store_name\",\"food_category\",\"food_category_popularity\"]\n",
    "df = spark.createDataFrame(data = data, schema = columns)\n",
    "\n",
    "print('before')\n",
    "df.show(n=10)\n",
    "\n",
    "################# sol #######################\n",
    "store_cat_pop_rank_score = F.when(C(\"store_cat_pop_rank\") == 1, 0)\\\n",
    "                            .when(C(\"store_cat_pop_rank\") == 2, 0.1)\n",
    "\n",
    "\n",
    "window_sotre_cat_pop = W.partitionBy('store_name').orderBy(C(\"food_category_popularity\").desc())\n",
    "window_sotre_cat = W.partitionBy(['store_name','food_category']).orderBy(C(\"food_category\"))\n",
    "window_sotr_cat_mix_rank = W.partitionBy(['store_name']).orderBy(C(\"mix_cat_pop_rank_score\"))\n",
    "df = (\n",
    "    df.withColumn(\"food_cat_pop_score\", 100 * C(\"food_category_popularity\") + 20 * F.randn(seed=42))\\\n",
    "      .withColumn(\"cat_idx\", F.row_number().over(window_sotre_cat))\\\n",
    "      .withColumn(\"store_cat_pop_rank\", F.dense_rank().over(window_sotre_cat_pop))\n",
    "      .withColumn(\"store_cat_pop_rank_score\", store_cat_pop_rank_score)\\\n",
    "      .withColumn(\"mix_cat_pop_rank_score\", C(\"cat_idx\") + C(\"store_cat_pop_rank_score\"))\\\n",
    "      .withColumn(\"mix_cat_pop_rank\", F.row_number().over(window_sotr_cat_mix_rank))\\\n",
    ")\n",
    "df.toPandas()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-21T14:17:04.975006Z",
     "start_time": "2021-03-21T14:16:38.971Z"
    }
   },
   "outputs": [],
   "source": [
    "# 2\n",
    "# create a food category popularity score\n",
    "# rank the food category popularity score but crossed and rotated\n",
    "\n",
    "\n",
    "\n",
    "# create a mix ranking number\n",
    "# create a popularity_rank_score in each store_name\n",
    "    # Top popular in each store_name -> score 0\n",
    "    # Second popular in each store_name -> score 0.1\n",
    "# Add row number in each store_name, food_category -> cat_rank\n",
    "# Create category_popularity_mix_rank_score = cat_rank + popularity_rank_score\n",
    "# Sort the category_popularity_mix_rank_score by store_name\n",
    "\n",
    "data = [\n",
    "    (\"hotpop\",\"Meat\",3,),\n",
    "    (\"hotpop\",\"Meat\",3),\n",
    "    (\"hotpop\",\"Meat\",3),\n",
    "    (\"hotpop\",\"Vegetable\",2),\n",
    "    (\"hotpop\",\"Vegetable\",2),\n",
    "    (\"branch\",\"Fried food\",1),\n",
    "    (\"branch\",\"Dessert\",1),\n",
    "  ]\n",
    "\n",
    "columns = [\"store_name\",\"food_category\",\"food_category_popularity\"]\n",
    "df = spark.createDataFrame(data = data, schema = columns)\n",
    "\n",
    "print('before')\n",
    "df.show(n=10)\n",
    "\n",
    "################# sol #######################\n",
    "store_cat_pop_rank_score = F.when(C(\"store_cat_pop_rank\") == 1, 0)\\\n",
    "                            .when(C(\"store_cat_pop_rank\") == 2, 0.1)\n",
    "\n",
    "# TODO we create a popularity score based in food_category_popularity first, then sort the rows based on it\n",
    "\n",
    "window_sotre_cat_pop = W.partitionBy('store_name').orderBy(C(\"food_category_popularity\").desc())\n",
    "window_sotre_cat = W.partitionBy(['store_name','food_category']).orderBy(C(\"food_cat_pop_score\").desc())\n",
    "window_sotr_cat_mix_rank = W.partitionBy(['store_name']).orderBy(C(\"mix_cat_pop_rank_score\"))\n",
    "df = (\n",
    "    df.withColumn(\"food_cat_pop_score\",\n",
    "                  F.round(100 * C(\"food_category_popularity\") + 20 * F.randn(seed=42))\n",
    "                 )\\\n",
    "      .withColumn(\"cat_idx\", F.row_number().over(window_sotre_cat))\\\n",
    "      .withColumn(\"store_cat_pop_rank\", F.dense_rank().over(window_sotre_cat_pop))\n",
    "      .withColumn(\"store_cat_pop_rank_score\", store_cat_pop_rank_score)\\\n",
    "      .withColumn(\"mix_cat_pop_rank_score\", C(\"cat_idx\") + C(\"store_cat_pop_rank_score\"))\\\n",
    "      .withColumn(\"mix_cat_pop_rank\", F.row_number().over(window_sotr_cat_mix_rank))\\\n",
    ")\n",
    "df.toPandas()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-21T14:17:04.976733Z",
     "start_time": "2021-03-21T14:16:38.974Z"
    }
   },
   "outputs": [],
   "source": [
    "# 2 Knowing the functions of dataframe operation \n",
    "\n",
    "dir(F)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pixlake",
   "language": "python",
   "name": "pixlake"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "247px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
