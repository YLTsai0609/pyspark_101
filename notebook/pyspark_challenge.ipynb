{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T12:17:25.364407Z",
     "start_time": "2021-03-03T12:17:25.341537Z"
    }
   },
   "outputs": [],
   "source": [
    "# env : pixlake\n",
    "# we focuing on pyspark dataframe processing\n",
    "# documentation https://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T12:17:25.494018Z",
     "start_time": "2021-03-03T12:17:25.366041Z"
    }
   },
   "outputs": [],
   "source": [
    "# make you auto compeletion faster\n",
    "# https://stackoverflow.com/questions/40536560/ipython-and-jupyter-autocomplete-not-working\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T12:17:25.590644Z",
     "start_time": "2021-03-03T12:17:25.496142Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['SPARK_HOME'] = '/opt/spark/versions/spark-2.3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T12:17:25.852160Z",
     "start_time": "2021-03-03T12:17:25.594129Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession as Session\n",
    "from pyspark import SparkConf as Conf\n",
    "from pyspark.sql import functions as F, Window as W\n",
    "C = F.col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T12:17:25.860585Z",
     "start_time": "2021-03-03T12:17:25.853588Z"
    }
   },
   "outputs": [],
   "source": [
    "conf = (Conf()\n",
    "    .set('spark.sql.sources.partitionOverwriteMode', 'dynamic')\n",
    "    .set('spark.driver.memory', '5g')\n",
    "    .set('spark.driver.maxResultSize', '2g')\n",
    "   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T12:17:28.400749Z",
     "start_time": "2021-03-03T12:17:25.861841Z"
    }
   },
   "outputs": [],
   "source": [
    "spark = (Session\n",
    "     .builder\n",
    "     .appName('pyspark-challenge')\n",
    "     .master('local[2]')\n",
    "     .config(conf=conf)\n",
    "     .getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T12:17:28.414181Z",
     "start_time": "2021-03-03T12:17:28.402956Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Builder', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__enter__', '__eq__', '__exit__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_convert_from_pandas', '_createFromLocal', '_createFromRDD', '_create_from_pandas_with_arrow', '_get_numpy_record_dtype', '_inferSchema', '_inferSchemaFromList', '_instantiatedSession', '_jsc', '_jsparkSession', '_jvm', '_jwrapped', '_repr_html_', '_sc', '_wrapped', 'builder', 'catalog', 'conf', 'createDataFrame', 'newSession', 'range', 'read', 'readStream', 'sparkContext', 'sql', 'stop', 'streams', 'table', 'udf', 'version']\n",
      "\n",
      "your spark version : 2.3.4\n"
     ]
    }
   ],
   "source": [
    "# 0. knowing what spark session can do and its version\n",
    "print(dir(spark), f'your spark version : {spark.version}'\n",
    "      , sep='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T12:17:31.791193Z",
     "start_time": "2021-03-03T12:17:28.416756Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['zipcode1.json', 'titanic_train.csv', 'small_zipcode.csv', 'Meteorite_Landings.csv', 'zipcodes.csv', 'zipcodes.json', 'webpage_1.txt', 'multiline-zipcode.json', 'simple_text.txt', 'zipcode2.json', 'titanic_test.csv']\n",
      "root\n",
      " |-- RecordNumber: integer (nullable = true)\n",
      " |-- Zipcode: integer (nullable = true)\n",
      " |-- ZipCodeType: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- LocationType: string (nullable = true)\n",
      " |-- Lat: double (nullable = true)\n",
      " |-- Long: double (nullable = true)\n",
      " |-- Xaxis: double (nullable = true)\n",
      " |-- Yaxis: double (nullable = true)\n",
      " |-- Zaxis: double (nullable = true)\n",
      " |-- WorldRegion: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- LocationText: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- Decommisioned: boolean (nullable = true)\n",
      " |-- TaxReturnsFiled: integer (nullable = true)\n",
      " |-- EstimatedPopulation: integer (nullable = true)\n",
      " |-- TotalWages: integer (nullable = true)\n",
      " |-- Notes: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RecordNumber</th>\n",
       "      <th>Zipcode</th>\n",
       "      <th>ZipCodeType</th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>LocationType</th>\n",
       "      <th>Lat</th>\n",
       "      <th>Long</th>\n",
       "      <th>Xaxis</th>\n",
       "      <th>Yaxis</th>\n",
       "      <th>Zaxis</th>\n",
       "      <th>WorldRegion</th>\n",
       "      <th>Country</th>\n",
       "      <th>LocationText</th>\n",
       "      <th>Location</th>\n",
       "      <th>Decommisioned</th>\n",
       "      <th>TaxReturnsFiled</th>\n",
       "      <th>EstimatedPopulation</th>\n",
       "      <th>TotalWages</th>\n",
       "      <th>Notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>704</td>\n",
       "      <td>STANDARD</td>\n",
       "      <td>PARC PARQUE</td>\n",
       "      <td>PR</td>\n",
       "      <td>NOT ACCEPTABLE</td>\n",
       "      <td>17.96</td>\n",
       "      <td>-66.22</td>\n",
       "      <td>0.38</td>\n",
       "      <td>-0.87</td>\n",
       "      <td>0.30</td>\n",
       "      <td>NA</td>\n",
       "      <td>US</td>\n",
       "      <td>Parc Parque, PR</td>\n",
       "      <td>NA-US-PR-PARC PARQUE</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>704</td>\n",
       "      <td>STANDARD</td>\n",
       "      <td>PASEO COSTA DEL SUR</td>\n",
       "      <td>PR</td>\n",
       "      <td>NOT ACCEPTABLE</td>\n",
       "      <td>17.96</td>\n",
       "      <td>-66.22</td>\n",
       "      <td>0.38</td>\n",
       "      <td>-0.87</td>\n",
       "      <td>0.30</td>\n",
       "      <td>NA</td>\n",
       "      <td>US</td>\n",
       "      <td>Paseo Costa Del Sur, PR</td>\n",
       "      <td>NA-US-PR-PASEO COSTA DEL SUR</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>709</td>\n",
       "      <td>STANDARD</td>\n",
       "      <td>BDA SAN LUIS</td>\n",
       "      <td>PR</td>\n",
       "      <td>NOT ACCEPTABLE</td>\n",
       "      <td>18.14</td>\n",
       "      <td>-66.26</td>\n",
       "      <td>0.38</td>\n",
       "      <td>-0.86</td>\n",
       "      <td>0.31</td>\n",
       "      <td>NA</td>\n",
       "      <td>US</td>\n",
       "      <td>Bda San Luis, PR</td>\n",
       "      <td>NA-US-PR-BDA SAN LUIS</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>61391</td>\n",
       "      <td>76166</td>\n",
       "      <td>UNIQUE</td>\n",
       "      <td>CINGULAR WIRELESS</td>\n",
       "      <td>TX</td>\n",
       "      <td>NOT ACCEPTABLE</td>\n",
       "      <td>32.72</td>\n",
       "      <td>-97.31</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>-0.83</td>\n",
       "      <td>0.54</td>\n",
       "      <td>NA</td>\n",
       "      <td>US</td>\n",
       "      <td>Cingular Wireless, TX</td>\n",
       "      <td>NA-US-TX-CINGULAR WIRELESS</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>61392</td>\n",
       "      <td>76177</td>\n",
       "      <td>STANDARD</td>\n",
       "      <td>FORT WORTH</td>\n",
       "      <td>TX</td>\n",
       "      <td>PRIMARY</td>\n",
       "      <td>32.75</td>\n",
       "      <td>-97.33</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>-0.83</td>\n",
       "      <td>0.54</td>\n",
       "      <td>NA</td>\n",
       "      <td>US</td>\n",
       "      <td>Fort Worth, TX</td>\n",
       "      <td>NA-US-TX-FORT WORTH</td>\n",
       "      <td>False</td>\n",
       "      <td>2126.0</td>\n",
       "      <td>4053.0</td>\n",
       "      <td>122396986.0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RecordNumber  Zipcode ZipCodeType                 City State  \\\n",
       "0             1      704    STANDARD          PARC PARQUE    PR   \n",
       "1             2      704    STANDARD  PASEO COSTA DEL SUR    PR   \n",
       "2            10      709    STANDARD         BDA SAN LUIS    PR   \n",
       "3         61391    76166      UNIQUE    CINGULAR WIRELESS    TX   \n",
       "4         61392    76177    STANDARD           FORT WORTH    TX   \n",
       "\n",
       "     LocationType    Lat   Long  Xaxis  Yaxis  Zaxis WorldRegion Country  \\\n",
       "0  NOT ACCEPTABLE  17.96 -66.22   0.38  -0.87   0.30          NA      US   \n",
       "1  NOT ACCEPTABLE  17.96 -66.22   0.38  -0.87   0.30          NA      US   \n",
       "2  NOT ACCEPTABLE  18.14 -66.26   0.38  -0.86   0.31          NA      US   \n",
       "3  NOT ACCEPTABLE  32.72 -97.31  -0.10  -0.83   0.54          NA      US   \n",
       "4         PRIMARY  32.75 -97.33  -0.10  -0.83   0.54          NA      US   \n",
       "\n",
       "              LocationText                      Location  Decommisioned  \\\n",
       "0          Parc Parque, PR          NA-US-PR-PARC PARQUE          False   \n",
       "1  Paseo Costa Del Sur, PR  NA-US-PR-PASEO COSTA DEL SUR          False   \n",
       "2         Bda San Luis, PR         NA-US-PR-BDA SAN LUIS          False   \n",
       "3    Cingular Wireless, TX    NA-US-TX-CINGULAR WIRELESS          False   \n",
       "4           Fort Worth, TX           NA-US-TX-FORT WORTH          False   \n",
       "\n",
       "   TaxReturnsFiled  EstimatedPopulation   TotalWages Notes  \n",
       "0              NaN                  NaN          NaN  None  \n",
       "1              NaN                  NaN          NaN  None  \n",
       "2              NaN                  NaN          NaN  None  \n",
       "3              NaN                  NaN          NaN  None  \n",
       "4           2126.0               4053.0  122396986.0  None  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. read data from csv\n",
    "print(os.listdir('../data'))\n",
    "df_from_csv_1 = spark.read.csv('../data/zipcodes.csv',\n",
    "                               header=True,\n",
    "                              inferSchema=True)\n",
    "df_from_csv_1.printSchema()\n",
    "df_from_csv_1.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T12:17:32.073674Z",
     "start_time": "2021-03-03T12:17:31.793289Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['zipcode1.json', 'titanic_train.csv', 'small_zipcode.csv', 'Meteorite_Landings.csv', 'zipcodes.csv', 'zipcodes.json', 'webpage_1.txt', 'multiline-zipcode.json', 'simple_text.txt', 'zipcode2.json', 'titanic_test.csv']\n",
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_df', '_jreader', '_set_opts', '_spark', 'csv', 'format', 'jdbc', 'json', 'load', 'option', 'options', 'orc', 'parquet', 'schema', 'table', 'text']\n",
      "root\n",
      " |-- City: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Decommisioned: boolean (nullable = true)\n",
      " |-- EstimatedPopulation: long (nullable = true)\n",
      " |-- Lat: double (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- LocationText: string (nullable = true)\n",
      " |-- LocationType: string (nullable = true)\n",
      " |-- Long: double (nullable = true)\n",
      " |-- Notes: string (nullable = true)\n",
      " |-- RecordNumber: long (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- TaxReturnsFiled: long (nullable = true)\n",
      " |-- TotalWages: long (nullable = true)\n",
      " |-- WorldRegion: string (nullable = true)\n",
      " |-- Xaxis: double (nullable = true)\n",
      " |-- Yaxis: double (nullable = true)\n",
      " |-- Zaxis: double (nullable = true)\n",
      " |-- ZipCodeType: string (nullable = true)\n",
      " |-- Zipcode: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>Country</th>\n",
       "      <th>Decommisioned</th>\n",
       "      <th>EstimatedPopulation</th>\n",
       "      <th>Lat</th>\n",
       "      <th>Location</th>\n",
       "      <th>LocationText</th>\n",
       "      <th>LocationType</th>\n",
       "      <th>Long</th>\n",
       "      <th>Notes</th>\n",
       "      <th>RecordNumber</th>\n",
       "      <th>State</th>\n",
       "      <th>TaxReturnsFiled</th>\n",
       "      <th>TotalWages</th>\n",
       "      <th>WorldRegion</th>\n",
       "      <th>Xaxis</th>\n",
       "      <th>Yaxis</th>\n",
       "      <th>Zaxis</th>\n",
       "      <th>ZipCodeType</th>\n",
       "      <th>Zipcode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PARC PARQUE</td>\n",
       "      <td>US</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.96</td>\n",
       "      <td>NA-US-PR-PARC PARQUE</td>\n",
       "      <td>Parc Parque, PR</td>\n",
       "      <td>NOT ACCEPTABLE</td>\n",
       "      <td>-66.22</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>PR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NA</td>\n",
       "      <td>0.38</td>\n",
       "      <td>-0.87</td>\n",
       "      <td>0.30</td>\n",
       "      <td>STANDARD</td>\n",
       "      <td>704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PASEO COSTA DEL SUR</td>\n",
       "      <td>US</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.96</td>\n",
       "      <td>NA-US-PR-PASEO COSTA DEL SUR</td>\n",
       "      <td>Paseo Costa Del Sur, PR</td>\n",
       "      <td>NOT ACCEPTABLE</td>\n",
       "      <td>-66.22</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>PR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NA</td>\n",
       "      <td>0.38</td>\n",
       "      <td>-0.87</td>\n",
       "      <td>0.30</td>\n",
       "      <td>STANDARD</td>\n",
       "      <td>704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BDA SAN LUIS</td>\n",
       "      <td>US</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.14</td>\n",
       "      <td>NA-US-PR-BDA SAN LUIS</td>\n",
       "      <td>Bda San Luis, PR</td>\n",
       "      <td>NOT ACCEPTABLE</td>\n",
       "      <td>-66.26</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>PR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NA</td>\n",
       "      <td>0.38</td>\n",
       "      <td>-0.86</td>\n",
       "      <td>0.31</td>\n",
       "      <td>STANDARD</td>\n",
       "      <td>709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CINGULAR WIRELESS</td>\n",
       "      <td>US</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32.72</td>\n",
       "      <td>NA-US-TX-CINGULAR WIRELESS</td>\n",
       "      <td>Cingular Wireless, TX</td>\n",
       "      <td>NOT ACCEPTABLE</td>\n",
       "      <td>-97.31</td>\n",
       "      <td>None</td>\n",
       "      <td>61391</td>\n",
       "      <td>TX</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NA</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>-0.83</td>\n",
       "      <td>0.54</td>\n",
       "      <td>UNIQUE</td>\n",
       "      <td>76166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FORT WORTH</td>\n",
       "      <td>US</td>\n",
       "      <td>False</td>\n",
       "      <td>4053.0</td>\n",
       "      <td>32.75</td>\n",
       "      <td>NA-US-TX-FORT WORTH</td>\n",
       "      <td>Fort Worth, TX</td>\n",
       "      <td>PRIMARY</td>\n",
       "      <td>-97.33</td>\n",
       "      <td>None</td>\n",
       "      <td>61392</td>\n",
       "      <td>TX</td>\n",
       "      <td>2126.0</td>\n",
       "      <td>122396986.0</td>\n",
       "      <td>NA</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>-0.83</td>\n",
       "      <td>0.54</td>\n",
       "      <td>STANDARD</td>\n",
       "      <td>76177</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  City Country  Decommisioned  EstimatedPopulation    Lat  \\\n",
       "0          PARC PARQUE      US          False                  NaN  17.96   \n",
       "1  PASEO COSTA DEL SUR      US          False                  NaN  17.96   \n",
       "2         BDA SAN LUIS      US          False                  NaN  18.14   \n",
       "3    CINGULAR WIRELESS      US          False                  NaN  32.72   \n",
       "4           FORT WORTH      US          False               4053.0  32.75   \n",
       "\n",
       "                       Location             LocationText    LocationType  \\\n",
       "0          NA-US-PR-PARC PARQUE          Parc Parque, PR  NOT ACCEPTABLE   \n",
       "1  NA-US-PR-PASEO COSTA DEL SUR  Paseo Costa Del Sur, PR  NOT ACCEPTABLE   \n",
       "2         NA-US-PR-BDA SAN LUIS         Bda San Luis, PR  NOT ACCEPTABLE   \n",
       "3    NA-US-TX-CINGULAR WIRELESS    Cingular Wireless, TX  NOT ACCEPTABLE   \n",
       "4           NA-US-TX-FORT WORTH           Fort Worth, TX         PRIMARY   \n",
       "\n",
       "    Long Notes  RecordNumber State  TaxReturnsFiled   TotalWages WorldRegion  \\\n",
       "0 -66.22  None             1    PR              NaN          NaN          NA   \n",
       "1 -66.22  None             2    PR              NaN          NaN          NA   \n",
       "2 -66.26  None            10    PR              NaN          NaN          NA   \n",
       "3 -97.31  None         61391    TX              NaN          NaN          NA   \n",
       "4 -97.33  None         61392    TX           2126.0  122396986.0          NA   \n",
       "\n",
       "   Xaxis  Yaxis  Zaxis ZipCodeType  Zipcode  \n",
       "0   0.38  -0.87   0.30    STANDARD      704  \n",
       "1   0.38  -0.87   0.30    STANDARD      704  \n",
       "2   0.38  -0.86   0.31    STANDARD      709  \n",
       "3  -0.10  -0.83   0.54      UNIQUE    76166  \n",
       "4  -0.10  -0.83   0.54    STANDARD    76177  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2 read data from json\n",
    "print(os.listdir('../data'))\n",
    "print(dir(spark.read))\n",
    "# 沒有infer_schema\n",
    "df_from_json = spark.read.json('../data/zipcodes.json')\n",
    "df_from_json.printSchema()\n",
    "df_from_json.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T12:17:32.953262Z",
     "start_time": "2021-03-03T12:17:32.076096Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__add__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getnewargs__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_computeFractionForSampleSize', '_defaultReducePartitions', '_id', '_jrdd', '_jrdd_deserializer', '_memory_limit', '_pickled', '_reserialize', '_to_java_object_rdd', 'aggregate', 'aggregateByKey', 'cache', 'cartesian', 'checkpoint', 'coalesce', 'cogroup', 'collect', 'collectAsMap', 'combineByKey', 'context', 'count', 'countApprox', 'countApproxDistinct', 'countByKey', 'countByValue', 'ctx', 'distinct', 'filter', 'first', 'flatMap', 'flatMapValues', 'fold', 'foldByKey', 'foreach', 'foreachPartition', 'fullOuterJoin', 'getCheckpointFile', 'getNumPartitions', 'getStorageLevel', 'glom', 'groupBy', 'groupByKey', 'groupWith', 'histogram', 'id', 'intersection', 'isCheckpointed', 'isEmpty', 'isLocallyCheckpointed', 'is_cached', 'is_checkpointed', 'join', 'keyBy', 'keys', 'leftOuterJoin', 'localCheckpoint', 'lookup', 'map', 'mapPartitions', 'mapPartitionsWithIndex', 'mapPartitionsWithSplit', 'mapValues', 'max', 'mean', 'meanApprox', 'min', 'name', 'partitionBy', 'partitioner', 'persist', 'pipe', 'randomSplit', 'reduce', 'reduceByKey', 'reduceByKeyLocally', 'repartition', 'repartitionAndSortWithinPartitions', 'rightOuterJoin', 'sample', 'sampleByKey', 'sampleStdev', 'sampleVariance', 'saveAsHadoopDataset', 'saveAsHadoopFile', 'saveAsNewAPIHadoopDataset', 'saveAsNewAPIHadoopFile', 'saveAsPickleFile', 'saveAsSequenceFile', 'saveAsTextFile', 'setName', 'sortBy', 'sortByKey', 'stats', 'stdev', 'subtract', 'subtractByKey', 'sum', 'sumApprox', 'take', 'takeOrdered', 'takeSample', 'toDF', 'toDebugString', 'toLocalIterator', 'top', 'treeAggregate', 'treeReduce', 'union', 'unpersist', 'values', 'variance', 'zip', 'zipWithIndex', 'zipWithUniqueId']\n",
      "\n",
      "<class 'pyspark.rdd.RDD'>\n",
      "\n",
      "+--------+-----------+\n",
      "|language|user_counts|\n",
      "+--------+-----------+\n",
      "|    Java|      20000|\n",
      "|  Python|     100000|\n",
      "|   Scala|       3000|\n",
      "+--------+-----------+\n",
      "\n",
      "+--------+-----------+\n",
      "|language|user_counts|\n",
      "+--------+-----------+\n",
      "|    Java|      20000|\n",
      "|  Python|     100000|\n",
      "|   Scala|       3000|\n",
      "+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3 create dataframe from rdd list\n",
    "columns = [\"language\",\"user_counts\"]\n",
    "data = [\n",
    "    (\"Java\",\"20000\"),\n",
    "    (\"Python\",\"100000\"),\n",
    "    (\"Scala\",\"3000\")\n",
    "       ]\n",
    "# 先分散到rdd\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "print(dir(rdd), type(rdd), sep='\\n\\n')\n",
    "print()\n",
    "df_from_rdd = rdd.toDF(schema=columns)\n",
    "df_from_rdd.show(n=5)\n",
    "\n",
    "# 直接create，讓spark dataframe進行分散\n",
    "df = spark.createDataFrame(data=data,schema=columns)\n",
    "df.show(n=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T12:17:33.107498Z",
     "start_time": "2021-03-03T12:17:32.956656Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Builder', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__enter__', '__eq__', '__exit__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_conf', '_convert_from_pandas', '_createFromLocal', '_createFromRDD', '_create_from_pandas_with_arrow', '_get_numpy_record_dtype', '_inferSchema', '_inferSchemaFromList', '_instantiatedSession', '_jsc', '_jsparkSession', '_jvm', '_jwrapped', '_repr_html_', '_sc', '_wrapped', 'builder', 'catalog', 'conf', 'createDataFrame', 'newSession', 'range', 'read', 'readStream', 'sparkContext', 'sql', 'stop', 'streams', 'table', 'udf', 'version']\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4 create 5 row fake data using spark range\n",
    "print(dir(spark))\n",
    "print(type(spark.range(start=0,end=10)))\n",
    "columns = ['row_number']\n",
    "single_column_df = spark.range(start=0,end=10)\n",
    "single_column_df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T12:17:33.117806Z",
     "start_time": "2021-03-03T12:17:33.109019Z"
    }
   },
   "outputs": [],
   "source": [
    "# 6 create empty dataframe\n",
    "\n",
    "columns = [\"language\",\"user_counts\"]\n",
    "\n",
    "# empty RDD + schema won't work\n",
    "# df_1 = spark.createDataFrame(data=spark.sparkContext.emptyRDD(),\n",
    "#                              schema=columns)\n",
    "# df_1.show(n=5)\n",
    "\n",
    "# empty list -> rdd -> df won't work\n",
    "# df2 = spark.sparkContext.parallelize([]).toDF(columns)\n",
    "\n",
    "# df3 = spark.createDataFrame([]) # won't work this version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T12:17:33.415063Z",
     "start_time": "2021-03-03T12:17:33.119004Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 2\n"
     ]
    }
   ],
   "source": [
    "# 7 get dataframe shape\n",
    "print(df.count(), len(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T12:17:33.427086Z",
     "start_time": "2021-03-03T12:17:33.416689Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AutoBatchedSerializer', 'Column', 'DataFrame', 'DataType', 'PandasUDFType', 'PickleSerializer', 'PythonEvalType', 'SparkContext', 'StringType', 'UserDefinedFunction', '__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_binary_mathfunctions', '_collect_list_doc', '_collect_set_doc', '_create_binary_mathfunction', '_create_function', '_create_udf', '_create_window_function', '_functions', '_functions_1_4', '_functions_1_6', '_functions_2_1', '_functions_deprecated', '_lit_doc', '_message', '_string_functions', '_test', '_to_java_column', '_to_seq', '_window_functions', '_wrap_deprecated_function', 'abs', 'acos', 'add_months', 'approxCountDistinct', 'approx_count_distinct', 'array', 'array_contains', 'asc', 'ascii', 'asin', 'atan', 'atan2', 'avg', 'base64', 'bin', 'bitwiseNOT', 'blacklist', 'broadcast', 'bround', 'cbrt', 'ceil', 'coalesce', 'col', 'collect_list', 'collect_set', 'column', 'concat', 'concat_ws', 'conv', 'corr', 'cos', 'cosh', 'count', 'countDistinct', 'covar_pop', 'covar_samp', 'crc32', 'create_map', 'cume_dist', 'current_date', 'current_timestamp', 'date_add', 'date_format', 'date_sub', 'date_trunc', 'datediff', 'dayofmonth', 'dayofweek', 'dayofyear', 'decode', 'degrees', 'dense_rank', 'desc', 'encode', 'exp', 'explode', 'explode_outer', 'expm1', 'expr', 'factorial', 'first', 'floor', 'format_number', 'format_string', 'from_json', 'from_unixtime', 'from_utc_timestamp', 'functools', 'get_json_object', 'greatest', 'grouping', 'grouping_id', 'hash', 'hex', 'hour', 'hypot', 'ignore_unicode_prefix', 'initcap', 'input_file_name', 'instr', 'isnan', 'isnull', 'json_tuple', 'kurtosis', 'lag', 'last', 'last_day', 'lead', 'least', 'length', 'levenshtein', 'lit', 'locate', 'log', 'log10', 'log1p', 'log2', 'lower', 'lpad', 'ltrim', 'map_keys', 'map_values', 'math', 'max', 'md5', 'mean', 'min', 'minute', 'monotonically_increasing_id', 'month', 'months_between', 'nanvl', 'next_day', 'ntile', 'pandas_udf', 'percent_rank', 'posexplode', 'posexplode_outer', 'pow', 'quarter', 'radians', 'rand', 'randn', 'rank', 'regexp_extract', 'regexp_replace', 'repeat', 'reverse', 'rint', 'round', 'row_number', 'rpad', 'rtrim', 'second', 'sha1', 'sha2', 'shiftLeft', 'shiftRight', 'shiftRightUnsigned', 'signum', 'sin', 'since', 'sinh', 'size', 'skewness', 'sort_array', 'soundex', 'spark_partition_id', 'split', 'sqrt', 'stddev', 'stddev_pop', 'stddev_samp', 'struct', 'substring', 'substring_index', 'sum', 'sumDistinct', 'sys', 'tan', 'tanh', 'toDegrees', 'toRadians', 'to_date', 'to_json', 'to_timestamp', 'to_utc_timestamp', 'translate', 'trim', 'trunc', 'udf', 'unbase64', 'unhex', 'unix_timestamp', 'upper', 'var_pop', 'var_samp', 'variance', 'warnings', 'weekofyear', 'when', 'window', 'year']\n"
     ]
    }
   ],
   "source": [
    "# 8 knowing what methods are supported by sql.function\n",
    "print(dir(F))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T12:17:34.376021Z",
     "start_time": "2021-03-03T12:17:33.428898Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+----------+\n",
      "|language|user_counts|new_column|\n",
      "+--------+-----------+----------+\n",
      "|    Java|      20000|       ABC|\n",
      "|  Python|     100000|       ABC|\n",
      "|   Scala|       3000|       ABC|\n",
      "+--------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 9 add const column to a existing dataframe\n",
    "\n",
    "columns = [\"language\",\"user_counts\"]\n",
    "data = [\n",
    "    (\"Java\",\"20000\"),\n",
    "    (\"Python\",\"100000\"),\n",
    "    (\"Scala\",\"3000\")\n",
    "       ]\n",
    "df = spark.createDataFrame(data=data,schema=columns)\n",
    "df = (\n",
    "    df.withColumn(\"new_column\",F.lit(\"ABC\")) \n",
    "    # F.lit means literal, retrurn a column\n",
    ")\n",
    "df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T12:17:34.915696Z",
     "start_time": "2021-03-03T12:17:34.378671Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-----+\n",
      "|language|user_counts|index|\n",
      "+--------+-----------+-----+\n",
      "|    Java|      20000|    1|\n",
      "|  Python|     100000|    2|\n",
      "|   Scala|       3000|    3|\n",
      "+--------+-----------+-----+\n",
      "\n",
      "+--------+-----------+-------+\n",
      "|language|user_counts|row_num|\n",
      "+--------+-----------+-------+\n",
      "|    Java|      20000|      1|\n",
      "|  Python|     100000|      2|\n",
      "|   Scala|       3000|      3|\n",
      "+--------+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 10 add a row_id column from a exisiting dataframe\n",
    "# https://stackoverflow.com/questions/53082891/adding-a-unique-consecutive-row-number-to-dataframe-in-pyspark\n",
    "columns = [\"language\",\"user_counts\"]\n",
    "data = [\n",
    "    (\"Java\",\"20000\"),\n",
    "    (\"Python\",\"100000\"),\n",
    "    (\"Scala\",\"3000\")\n",
    "       ]\n",
    "df_1 = spark.createDataFrame(data=data,schema=columns)\n",
    "df_1 = (\n",
    "    df_1.withColumn(\"index\", \n",
    "                  F.row_number().over(\n",
    "                      W.orderBy(F.monotonically_increasing_id() - 1)\n",
    "                  )\n",
    "                 )\n",
    "    # F.monotonically_increasing_id does not give 1 ~ N\n",
    "    # So we use window function to work around\n",
    ")\n",
    "df_1.show(n=5)\n",
    "\n",
    "# Mre clear way to do that\n",
    "df_2 = spark.createDataFrame(data=data,schema=columns)\n",
    "w = W.orderBy(F.lit('A'))\n",
    "df_2 = (\n",
    "    df_2.withColumn(\"row_num\", F.row_number().over(w))\n",
    ")\n",
    "df_2.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T12:17:35.091774Z",
     "start_time": "2021-03-03T12:17:34.917297Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+----------+--------------------+\n",
      "|language|user_counts|new_column|       random_number|\n",
      "+--------+-----------+----------+--------------------+\n",
      "|    Java|      20000|       ABC|  0.8694520619364656|\n",
      "|  Python|     100000|       ABC|  0.7817434263538586|\n",
      "|   Scala|       3000|       ABC|0.033087345331351936|\n",
      "+--------+-----------+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 10 add a random number to a exisit column\n",
    "df = (\n",
    "#     df.withColumn('random_number', F.when(F.rand() > 0.5, 1).otherwise(0))\n",
    "        df.withColumn('random_number', F.rand())\n",
    ")\n",
    "\n",
    "df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T12:17:35.305473Z",
     "start_time": "2021-03-03T12:17:35.093600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>user_counts</th>\n",
       "      <th>new_column</th>\n",
       "      <th>random_number</th>\n",
       "      <th>binary_cut_05</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Java</td>\n",
       "      <td>20000</td>\n",
       "      <td>ABC</td>\n",
       "      <td>0.869452</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Python</td>\n",
       "      <td>100000</td>\n",
       "      <td>ABC</td>\n",
       "      <td>0.781743</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Scala</td>\n",
       "      <td>3000</td>\n",
       "      <td>ABC</td>\n",
       "      <td>0.033087</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  language user_counts new_column  random_number  binary_cut_05\n",
       "0     Java       20000        ABC       0.869452              1\n",
       "1   Python      100000        ABC       0.781743              1\n",
       "2    Scala        3000        ABC       0.033087              1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 11 add a binary 0, 1 based on condition to an exisit column\n",
    "df = (\n",
    "    df.withColumn('binary_cut_05',F.when(F.rand() > 0.5, 1).otherwise(0))\n",
    ")\n",
    "df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T12:17:35.321464Z",
     "start_time": "2021-03-03T12:17:35.306877Z"
    }
   },
   "outputs": [],
   "source": [
    "# 12 create a dataframe contains row_index and fake data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T12:17:35.390128Z",
     "start_time": "2021-03-03T12:17:35.323388Z"
    }
   },
   "outputs": [],
   "source": [
    "# 13 create new column based on original column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pixlake",
   "language": "python",
   "name": "pixlake"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
