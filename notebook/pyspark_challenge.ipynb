{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T09:57:05.580670Z",
     "start_time": "2021-06-19T09:57:05.577866Z"
    }
   },
   "outputs": [],
   "source": [
    "# total : 58 problem and solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T09:57:05.703639Z",
     "start_time": "2021-06-19T09:57:05.593720Z"
    }
   },
   "outputs": [],
   "source": [
    "# env : pixlake\n",
    "# we focuing on pyspark dataframe processing\n",
    "# documentation https://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T09:57:05.811450Z",
     "start_time": "2021-06-19T09:57:05.705611Z"
    }
   },
   "outputs": [],
   "source": [
    "# make you auto compeletion faster\n",
    "# https://stackoverflow.com/questions/40536560/ipython-and-jupyter-autocomplete-not-working\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T09:57:05.916705Z",
     "start_time": "2021-06-19T09:57:05.813796Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have pyspark version :  ['spark-2.3', 'spark-3.0', 'spark-3.0.1-bin-hadoop2.7', 'spark-2.3.4-bin-hadoop2.7', 'spark-2.4.7-bin-hadoop2.7', 'spark-2.4']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "def get_workstation_spark_path(where_are_you : str) -> str:\n",
    "    if where_are_you == 'titan':\n",
    "        return '/home/data/ryanchao2012/lib'\n",
    "    elif where_are_you == 'thor':\n",
    "        return '/opt/spark/versions'\n",
    "    else:\n",
    "        raise ValueError(\"wrong work station name\")\n",
    "\n",
    "spark_path = get_workstation_spark_path('thor')\n",
    "\n",
    "print('You have pyspark version : ', os.listdir(spark_path))\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "# spark-2.3, spark-2.4\n",
    "os.environ['SPARK_HOME'] = os.path.join(spark_path,'spark-2.3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T09:57:06.035002Z",
     "start_time": "2021-06-19T09:57:05.919270Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/opt/spark/versions/spark-2.3'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ['SPARK_HOME']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T09:57:06.510479Z",
     "start_time": "2021-06-19T09:57:06.036691Z"
    }
   },
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession as Session\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark import SparkConf as Conf\n",
    "from pyspark.sql import functions as F, Window as W, types as T\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "C = F.col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T09:57:06.521604Z",
     "start_time": "2021-06-19T09:57:06.512017Z"
    }
   },
   "outputs": [],
   "source": [
    "conf = (Conf()\n",
    "    .set('spark.sql.sources.partitionOverwriteMode', 'dynamic')\n",
    "    .set('spark.driver.memory', '4g')\n",
    "    .set('spark.driver.maxResultSize', '1g')\n",
    "   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T09:57:09.126914Z",
     "start_time": "2021-06-19T09:57:06.522722Z"
    }
   },
   "outputs": [],
   "source": [
    "spark = (Session\n",
    "     .builder\n",
    "     .appName('pyspark-challenge')\n",
    "     .master('local[2]')\n",
    "     .config(conf=conf)\n",
    "     .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T09:57:09.145613Z",
     "start_time": "2021-06-19T09:57:09.130288Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://thor:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[2]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-challenge</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f9a40b4a290>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating DataFrame (16+)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T09:57:09.264511Z",
     "start_time": "2021-06-19T09:57:09.149056Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Builder', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__enter__', '__eq__', '__exit__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_conf', '_convert_from_pandas', '_createFromLocal', '_createFromRDD', '_create_from_pandas_with_arrow', '_get_numpy_record_dtype', '_inferSchema', '_inferSchemaFromList', '_instantiatedSession', '_jsc', '_jsparkSession', '_jvm', '_jwrapped', '_repr_html_', '_sc', '_wrapped', 'builder', 'catalog', 'conf', 'createDataFrame', 'newSession', 'range', 'read', 'readStream', 'sparkContext', 'sql', 'stop', 'streams', 'table', 'udf', 'version']\n",
      "\n",
      "your spark version : 2.3.4\n"
     ]
    }
   ],
   "source": [
    "# 0. know what spark session can do and its version\n",
    "print(dir(spark), f'your spark version : {spark.version}'\n",
    "      , sep='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T09:57:09.362715Z",
     "start_time": "2021-06-19T09:57:09.267228Z"
    }
   },
   "outputs": [],
   "source": [
    "# # 1. read data from csv\n",
    "# print(os.listdir('../data'))\n",
    "# df_from_csv_1 = spark.read.csv('../data/zipcodes.csv',\n",
    "#                                header=True,\n",
    "#                               inferSchema=True)\n",
    "# df_from_csv_1.printSchema()\n",
    "# df_from_csv_1.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T09:57:09.479313Z",
     "start_time": "2021-06-19T09:57:09.365274Z"
    }
   },
   "outputs": [],
   "source": [
    "# # 2 read data from json\n",
    "# print(os.listdir('../data'))\n",
    "# print(dir(spark.read))\n",
    "# # 沒有infer_schema\n",
    "# df_from_json = spark.read.json('../data/zipcodes.json')\n",
    "# df_from_json.printSchema()\n",
    "# df_from_json.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T09:57:12.144149Z",
     "start_time": "2021-06-19T09:57:09.482170Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+\n",
      "|language|user_counts|\n",
      "+--------+-----------+\n",
      "|    Java|      20000|\n",
      "|  Python|     100000|\n",
      "|   Scala|       3000|\n",
      "+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3 create dataframe from rdd list\n",
    "columns = [\"language\",\"user_counts\"]\n",
    "data = [\n",
    "    (\"Java\",\"20000\"),\n",
    "    (\"Python\",\"100000\"),\n",
    "    (\"Scala\",\"3000\")\n",
    "       ]\n",
    "# 先分散到rdd\n",
    "# rdd = spark.sparkContext.parallelize(data)\n",
    "# print(dir(rdd), type(rdd), sep='\\n\\n')\n",
    "# print()\n",
    "# df_from_rdd = rdd.toDF(schema=columns)\n",
    "# df_from_rdd.show(n=5)\n",
    "\n",
    "# 直接create，讓spark dataframe進行分散\n",
    "df = spark.createDataFrame(data=data,schema=columns)\n",
    "df.show(n=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T09:57:12.524256Z",
     "start_time": "2021-06-19T09:57:12.145576Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Builder', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__enter__', '__eq__', '__exit__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_conf', '_convert_from_pandas', '_createFromLocal', '_createFromRDD', '_create_from_pandas_with_arrow', '_get_numpy_record_dtype', '_inferSchema', '_inferSchemaFromList', '_instantiatedSession', '_jsc', '_jsparkSession', '_jvm', '_jwrapped', '_repr_html_', '_sc', '_wrapped', 'builder', 'catalog', 'conf', 'createDataFrame', 'newSession', 'range', 'read', 'readStream', 'sparkContext', 'sql', 'stop', 'streams', 'table', 'udf', 'version']\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4 create 5 row fake data using spark range\n",
    "print(dir(spark))\n",
    "print(type(spark.range(start=0,end=10)))\n",
    "columns = ['row_number']\n",
    "single_column_df = spark.range(start=0,end=10)\n",
    "single_column_df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T09:57:12.538117Z",
     "start_time": "2021-06-19T09:57:12.526562Z"
    }
   },
   "outputs": [],
   "source": [
    "# 6 create empty dataframe\n",
    "\n",
    "columns = [\"language\",\"user_counts\"]\n",
    "\n",
    "# empty RDD + schema won't work\n",
    "# df_1 = spark.createDataFrame(data=spark.sparkContext.emptyRDD(),\n",
    "#                              schema=columns)\n",
    "# df_1.show(n=5)\n",
    "\n",
    "# empty list -> rdd -> df won't work\n",
    "# df2 = spark.sparkContext.parallelize([]).toDF(columns)\n",
    "\n",
    "# df3 = spark.createDataFrame([]) # won't work this version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T09:57:13.018387Z",
     "start_time": "2021-06-19T09:57:12.539588Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 2\n"
     ]
    }
   ],
   "source": [
    "# 7 get dataframe shape\n",
    "print(df.count(), len(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T09:57:13.274307Z",
     "start_time": "2021-06-19T09:57:13.020498Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+----------+\n",
      "|language|user_counts|new_column|\n",
      "+--------+-----------+----------+\n",
      "|    Java|      20000|       ABC|\n",
      "|  Python|     100000|       ABC|\n",
      "|   Scala|       3000|       ABC|\n",
      "+--------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 8 add const column to a existing dataframe\n",
    "\n",
    "columns = [\"language\",\"user_counts\"]\n",
    "data = [\n",
    "    (\"Java\",\"20000\"),\n",
    "    (\"Python\",\"100000\"),\n",
    "    (\"Scala\",\"3000\")\n",
    "       ]\n",
    "df = spark.createDataFrame(data=data,schema=columns)\n",
    "df = (\n",
    "    df.withColumn(\"new_column\",F.lit(\"ABC\")) \n",
    "    # F.lit means literal, retrurn a column\n",
    ")\n",
    "df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T09:57:13.930037Z",
     "start_time": "2021-06-19T09:57:13.277428Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-----+\n",
      "|language|user_counts|index|\n",
      "+--------+-----------+-----+\n",
      "|    Java|      20000|    1|\n",
      "|  Python|     100000|    2|\n",
      "|   Scala|       3000|    3|\n",
      "+--------+-----------+-----+\n",
      "\n",
      "+--------+-----------+-------+\n",
      "|language|user_counts|row_num|\n",
      "+--------+-----------+-------+\n",
      "|    Java|      20000|      1|\n",
      "|  Python|     100000|      2|\n",
      "|   Scala|       3000|      3|\n",
      "+--------+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 9 add a row_id column from a exisiting dataframe\n",
    "# https://stackoverflow.com/questions/53082891/adding-a-unique-consecutive-row-number-to-dataframe-in-pyspark\n",
    "columns = [\"language\",\"user_counts\"]\n",
    "data = [\n",
    "    (\"Java\",\"20000\"),\n",
    "    (\"Python\",\"100000\"),\n",
    "    (\"Scala\",\"3000\")\n",
    "       ]\n",
    "df_1 = spark.createDataFrame(data=data,schema=columns)\n",
    "df_1 = (\n",
    "    df_1.withColumn(\"index\", \n",
    "                  F.row_number().over(\n",
    "                      W.orderBy(F.monotonically_increasing_id() - 1)\n",
    "                  )\n",
    "                 )\n",
    "    # F.monotonically_increasing_id does not give 1 ~ N\n",
    "    # So we use window function to work around\n",
    ")\n",
    "df_1.show(n=5)\n",
    "\n",
    "# Mre clear way to do that\n",
    "df_2 = spark.createDataFrame(data=data,schema=columns)\n",
    "w = W.orderBy(F.lit('A'))\n",
    "df_2 = (\n",
    "    df_2.withColumn(\"row_num\", F.row_number().over(w))\n",
    ")\n",
    "df_2.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T09:57:14.121334Z",
     "start_time": "2021-06-19T09:57:13.932168Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+----------+-------------------+\n",
      "|language|user_counts|new_column|      random_number|\n",
      "+--------+-----------+----------+-------------------+\n",
      "|    Java|      20000|       ABC|0.45733416683599093|\n",
      "|  Python|     100000|       ABC| 0.9843752039051096|\n",
      "|   Scala|       3000|       ABC| 0.9477803309870161|\n",
      "+--------+-----------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 10 add a random number to a exisit column\n",
    "df = (\n",
    "#     df.withColumn('random_number', F.when(F.rand() > 0.5, 1).otherwise(0))\n",
    "        df.withColumn('random_number', F.rand())\n",
    ")\n",
    "\n",
    "df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T09:57:14.436651Z",
     "start_time": "2021-06-19T09:57:14.124133Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>user_counts</th>\n",
       "      <th>new_column</th>\n",
       "      <th>random_number</th>\n",
       "      <th>binary_cut_05</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Java</td>\n",
       "      <td>20000</td>\n",
       "      <td>ABC</td>\n",
       "      <td>0.457334</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Python</td>\n",
       "      <td>100000</td>\n",
       "      <td>ABC</td>\n",
       "      <td>0.984375</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Scala</td>\n",
       "      <td>3000</td>\n",
       "      <td>ABC</td>\n",
       "      <td>0.947780</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  language user_counts new_column  random_number  binary_cut_05\n",
       "0     Java       20000        ABC       0.457334              0\n",
       "1   Python      100000        ABC       0.984375              0\n",
       "2    Scala        3000        ABC       0.947780              1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 11 add a binary 0, 1 based on condition to an exisit column\n",
    "df = (\n",
    "    df.withColumn('binary_cut_05',F.when(F.rand() > 0.5, 1).otherwise(0))\n",
    ")\n",
    "df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T09:57:14.614361Z",
     "start_time": "2021-06-19T09:57:14.438136Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-----------+\n",
      "|row_id|language|user_counts|\n",
      "+------+--------+-----------+\n",
      "|     0|    Java|      20000|\n",
      "|     1|  Python|     100000|\n",
      "|     2|   Scala|       3000|\n",
      "+------+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 12 create a dataframe contains row_index and fake data\n",
    "\n",
    "columns = [\"row_id\",\"language\",\"user_counts\"]\n",
    "data = [\n",
    "    (0, \"Java\",\"20000\"),\n",
    "    (1, \"Python\",\"100000\"),\n",
    "    (2, \"Scala\",\"3000\")\n",
    "       ]\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=columns)\n",
    "df.show(n=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T09:57:14.982010Z",
     "start_time": "2021-06-19T09:57:14.617142Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+-----+------+------+\n",
      "|firstname|middlename|lastname|   id|gender|salary|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|    James|          |   Smith|36636|     M|  3000|\n",
      "|  Michael|      Rose|        |40288|     M|  4000|\n",
      "|   Robert|          |Williams|42114|     M|  4000|\n",
      "|    Maria|      Anne|   Jones|39192|     F|  4000|\n",
      "|      Jen|      Mary|   Brown|     |     F|    -1|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "\n",
      "+--------------------+-----+------+------+\n",
      "|                name|   id|gender|salary|\n",
      "+--------------------+-----+------+------+\n",
      "|    [James, , Smith]|36636|     M|  3000|\n",
      "|   [Michael, Rose, ]|40288|     M|  4000|\n",
      "|[Robert, , Williams]|42114|     M|  4000|\n",
      "|[Maria, Anne, Jones]|39192|     F|  4000|\n",
      "|  [Jen, Mary, Brown]|     |     F|    -1|\n",
      "+--------------------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 13 construct a complex data for spark dataframe\n",
    "# using StructType\n",
    "\n",
    "# Case 1\n",
    "data = [\n",
    "    (\"James\",\"\",\"Smith\",\"36636\",\"M\", 3000),\n",
    "    (\"Michael\",\"Rose\",\"\",\"40288\",\"M\", 4000),\n",
    "    (\"Robert\",\"\",\"Williams\",\"42114\",\"M\", 4000),\n",
    "    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\", 4000),\n",
    "    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\", -1)\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"firstname\",StringType(), True), # Nullable True\n",
    "    StructField(\"middlename\",StringType(), True),\n",
    "    StructField(\"lastname\",StringType(), True),\n",
    "    StructField(\"id\",StringType(), True),\n",
    "    StructField(\"gender\",StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True)\n",
    "    ])\n",
    "df = spark.createDataFrame(data=data,schema=schema)\n",
    "df.show(n=5)\n",
    "\n",
    "# Case 2\n",
    "\n",
    "struct_data = [\n",
    "    ((\"James\",\"\",\"Smith\"),\"36636\",\"M\", 3000),\n",
    "    ((\"Michael\",\"Rose\",\"\"),\"40288\",\"M\", 4000),\n",
    "    ((\"Robert\",\"\",\"Williams\"),\"42114\",\"M\", 4000),\n",
    "    ((\"Maria\",\"Anne\",\"Jones\"),\"39192\",\"F\", 4000),\n",
    "    ((\"Jen\",\"Mary\",\"Brown\"),\"\",\"F\", -1)\n",
    "]\n",
    "\n",
    "structure_schema = StructType([\n",
    "    StructField('name',\n",
    "        StructType([\n",
    "            StructField(\"firstname\",StringType(), True), # Nullable True\n",
    "            StructField(\"middlename\",StringType(), True),\n",
    "            StructField(\"lastname\",StringType(), True),\n",
    "    ])),\n",
    "    StructField(\"id\",StringType(), True),\n",
    "    StructField(\"gender\",StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True)\n",
    "    ])\n",
    "\n",
    "df = spark.createDataFrame(data=struct_data, schema=structure_schema)\n",
    "df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T09:57:15.333107Z",
     "start_time": "2021-06-19T09:57:14.983537Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- languagesAtSchool: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- currentState: string (nullable = true)\n",
      "\n",
      "+----------------+------------------+------------+\n",
      "|name            |languagesAtSchool |currentState|\n",
      "+----------------+------------------+------------+\n",
      "|James,,Smith    |[Java, Scala, C++]|CA          |\n",
      "|Michael,Rose,   |[Spark, Java, C++]|NJ          |\n",
      "|Robert,,Williams|[CSharp, VB]      |NV          |\n",
      "+----------------+------------------+------------+\n",
      "\n",
      "+-------------+-----+\n",
      "|         name|  col|\n",
      "+-------------+-----+\n",
      "| James,,Smith| Java|\n",
      "| James,,Smith|Scala|\n",
      "| James,,Smith|  C++|\n",
      "|Michael,Rose,|Spark|\n",
      "|Michael,Rose,| Java|\n",
      "+-------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 14 construct a complex data for spark dataframe\n",
    "# using ArratyType\n",
    "# https://github.com/spark-examples/pyspark-examples/blob/master/pyspark-array-string.py\n",
    "\n",
    "columns = [\"name\",\"languagesAtSchool\",\"currentState\"]\n",
    "data = [(\"James,,Smith\",[\"Java\",\"Scala\",\"C++\"],\"CA\"), \\\n",
    "    (\"Michael,Rose,\",[\"Spark\",\"Java\",\"C++\"],\"NJ\"), \\\n",
    "    (\"Robert,,Williams\",[\"CSharp\",\"VB\"],\"NV\")]\n",
    "\n",
    "df = spark.createDataFrame(data=data,schema=columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n",
    "\n",
    "df.select(df.name, F.explode(df.languagesAtSchool)).show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T09:57:15.634241Z",
     "start_time": "2021-06-19T09:57:15.341744Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- language: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- peoperties: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      "\n",
      "+----------+--------------+--------------------+\n",
      "|      name|      language|          peoperties|\n",
      "+----------+--------------+--------------------+\n",
      "|     James| [Java, Scala]|[eye -> brown, ha...|\n",
      "|   Michael|[Spark, Java,]|[eye ->, hair -> ...|\n",
      "|    Robert|    [CSharp, ]|[eye -> , hair ->...|\n",
      "|Washington|          null|                null|\n",
      "| Jefferson|        [1, 2]|                  []|\n",
      "+----------+--------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 15 construct a complex data for spark dataframe\n",
    "# using MapType\n",
    "# https://github.com/spark-examples/pyspark-examples/blob/master/pyspark-explode-array-map.py\n",
    "data = [\n",
    "        ('James',['Java','Scala'],{'hair':'black','eye':'brown'}),\n",
    "        ('Michael',['Spark','Java',None],{'hair':'brown','eye':None}),\n",
    "        ('Robert',['CSharp',''],{'hair':'red','eye':''}),\n",
    "        ('Washington',None,None),\n",
    "        ('Jefferson',['1','2'],{})\n",
    "]\n",
    "\n",
    "columns = ['name','language','peoperties']\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=columns)\n",
    "\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T09:57:15.653793Z",
     "start_time": "2021-06-19T09:57:15.641868Z"
    }
   },
   "outputs": [],
   "source": [
    "# 16 create a nested array-type dataframe\n",
    "# https://github.com/spark-examples/pyspark-examples/blob/master/pyspark-explode-nested-array.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T09:57:15.753755Z",
     "start_time": "2021-06-19T09:57:15.655759Z"
    }
   },
   "outputs": [],
   "source": [
    "# 17 create a datetime column for spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T09:57:17.005526Z",
     "start_time": "2021-06-19T09:57:15.757178Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------+\n",
      "| languagesAtSchool|currentState|\n",
      "+------------------+------------+\n",
      "|[Java, Scala, C++]|          CA|\n",
      "|[Spark, Java, C++]|          NJ|\n",
      "|      [CSharp, VB]|          NV|\n",
      "+------------------+------------+\n",
      "\n",
      "root\n",
      " |-- languagesAtSchool: string (nullable = true)\n",
      " |-- currentState: string (nullable = true)\n",
      "\n",
      "java.lang.UnsupportedOperationException: CSV data source does not support array<string> data type.\n",
      "java.lang.UnsupportedOperationException: CSV data source does not support array<string> data type.\n",
      "+------------+--------------------+\n",
      "|currentState|   languagesAtSchool|\n",
      "+------------+--------------------+\n",
      "|          CA|['Java', 'Scala',...|\n",
      "|          NJ|['Spark', 'Java',...|\n",
      "|          NV|    ['CSharp', 'VB']|\n",
      "+------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 18 read a csv with array-of-string schema (fake issue)\n",
    "# you should read it from json\n",
    "\n",
    "\n",
    "columns = [\"languagesAtSchool\",\"currentState\"]\n",
    "data = [\n",
    "    ([\"Java\",\"Scala\",\"C++\"],\"CA\"), \n",
    "    ([\"Spark\",\"Java\",\"C++\"],\"NJ\"),\n",
    "    ([\"CSharp\",\"VB\"],\"NV\")\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data=data,schema=columns)\n",
    "\n",
    "df.show()\n",
    "\n",
    "# we write the data into csv\n",
    "SAVE = True\n",
    "if SAVE:\n",
    "    df.toPandas().to_csv('tmp.csv',index=False)\n",
    "\n",
    "# You will get sting\n",
    "(\n",
    "    spark.read.csv('tmp.csv',inferSchema=True, header=True).printSchema()\n",
    ")\n",
    "\n",
    "# Convert it by schema? - No, not supported...\n",
    "\n",
    "schema = (\n",
    "    T.StructType()\n",
    "    .add(\"languagesAtSchool\", T.ArrayType(T.StringType()), True)\n",
    "    .add(\"currentState\", T.StringType(), True)\n",
    ")\n",
    "try:\n",
    "    (\n",
    "        spark.read.csv('tmp.csv',schema=schema, header=True).show()\n",
    "    )\n",
    "except Exception as e:\n",
    "#     print(e)\n",
    "    # short answer\n",
    "    print('java.lang.UnsupportedOperationException: CSV data source does not support array<string> data type.')\n",
    "\n",
    "# How about use F.json to convert that?\n",
    "\n",
    "try:\n",
    "    (\n",
    "        spark.read.csv('tmp.csv',inferSchema=True, header=True)\n",
    "        .cache()\n",
    "        .withColumn('languagesAtSchool',F.from_json(C(\"languagesAtSchool\"),'array<string>'))\n",
    "    ).show()\n",
    "except Exception as e:\n",
    "#     print(e)\n",
    "    # ashort answer\n",
    "    print(\"java.lang.UnsupportedOperationException: CSV data source does not support array<string> data type.\")\n",
    "    \n",
    "    \n",
    "# convert it into json, make your life easiler\n",
    "\n",
    "SAVE_JSON = True\n",
    "if SAVE_JSON:\n",
    "    pd.read_csv('tmp.csv').to_json('tmp.json',orient='records',force_ascii=False)\n",
    "\n",
    "# Amazing!\n",
    "spark.read.json('tmp.json').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Column Operations (8+)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T09:57:17.220103Z",
     "start_time": "2021-06-19T09:57:17.007355Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+--------------+------------------+\n",
      "|language|user_counts|user_count_100|    user_count_log|\n",
      "+--------+-----------+--------------+------------------+\n",
      "|    Java|      20000|       2000000| 4.301029995663981|\n",
      "|  Python|     100000|      10000000|               5.0|\n",
      "|   Scala|       3000|        300000|3.4771212547196626|\n",
      "+--------+-----------+--------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1 create new column based on original column\n",
    "\n",
    "columns = [\"language\", \"user_counts\"]\n",
    "data = [\n",
    "    (\"Java\",20000),\n",
    "    (\"Python\",100000),\n",
    "    (\"Scala\",3000)\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=columns)\n",
    "df = (\n",
    "    df.withColumn(\"user_count_100\", C(\"user_counts\") * 100)\\\n",
    "    .withColumn(\"user_count_log\", F.log10(C(\"user_counts\")))\n",
    ")\n",
    "\n",
    "df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T09:57:17.396682Z",
     "start_time": "2021-06-19T09:57:17.222255Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+\n",
      "|  lang|const_col|\n",
      "+------+---------+\n",
      "|  Java|      ABC|\n",
      "|Python|      ABC|\n",
      "| Scala|      ABC|\n",
      "+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2 rename, drop, add constant column to existing dataframe\n",
    "# https://stackoverflow.com/questions/34077353/how-to-change-dataframe-column-names-in-pyspark\n",
    "columns = [\"language\", \"user_counts\"]\n",
    "data = [\n",
    "    (\"Java\",20000),\n",
    "    (\"Python\",100000),\n",
    "    (\"Scala\",3000)\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df = (\n",
    "    df.withColumn('const_col',F.lit('ABC'))\\\n",
    "    .withColumnRenamed(\"language\",\"lang\")\\\n",
    "    .drop(\"user_counts\")\n",
    ")\n",
    "df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T09:57:17.423517Z",
     "start_time": "2021-06-19T09:57:17.398923Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.column.Column'>\n",
      "\n",
      "['__add__', '__and__', '__bool__', '__class__', '__contains__', '__delattr__', '__dict__', '__dir__', '__div__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__invert__', '__iter__', '__le__', '__lt__', '__mod__', '__module__', '__mul__', '__ne__', '__neg__', '__new__', '__nonzero__', '__or__', '__pow__', '__radd__', '__rand__', '__rdiv__', '__reduce__', '__reduce_ex__', '__repr__', '__rmod__', '__rmul__', '__ror__', '__rpow__', '__rsub__', '__rtruediv__', '__setattr__', '__sizeof__', '__str__', '__sub__', '__subclasshook__', '__truediv__', '__weakref__', '_asc_doc', '_bitwiseAND_doc', '_bitwiseOR_doc', '_bitwiseXOR_doc', '_contains_doc', '_desc_doc', '_endswith_doc', '_eqNullSafe_doc', '_isNotNull_doc', '_isNull_doc', '_jc', '_like_doc', '_rlike_doc', '_startswith_doc', 'alias', 'asc', 'astype', 'between', 'bitwiseAND', 'bitwiseOR', 'bitwiseXOR', 'cast', 'contains', 'desc', 'endswith', 'eqNullSafe', 'getField', 'getItem', 'isNotNull', 'isNull', 'isin', 'like', 'name', 'otherwise', 'over', 'rlike', 'startswith', 'substr', 'when']\n"
     ]
    }
   ],
   "source": [
    "# 3 know what method and attribute can be called with column object\n",
    "\n",
    "\n",
    "print(type(C(\"language\")), dir(C(\"language\")), sep='\\n\\n')\n",
    "\n",
    "# alias - 可以換名字\n",
    "# asc, desc - 可以排序\n",
    "# astype,cast - 可以轉型\n",
    "# between - 可以傳入start_date以及end_date過濾\n",
    "# bitwiseAND, bitwiseOR, bitwiseXOR - 可以做布林運算\n",
    "# contains - 可以做字串搜尋\n",
    "# endwith, startwith, rlike, substring - 可以做字串比對\n",
    "# eqNullSafe, isNotNull, isNull - 可以檢查null值，Python須以None傳入\n",
    "# isin, like - 可以做值的比對(數值，字串值)\n",
    "# name - 可以取得欄位名稱\n",
    "# when, otherwise - 可以做條件判斷\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T09:57:17.504792Z",
     "start_time": "2021-06-19T09:57:17.425748Z"
    }
   },
   "outputs": [],
   "source": [
    "# +=*/\n",
    "\n",
    "# df = spark.createDataFrame([(6,3), (7, 3), (13,6), (5, 0)], [\"x\", \"y\"])\n",
    "\n",
    "# df = (\n",
    "#     df.withColumn(\"mod_cross_col\", C(\"x\") % C(\"y\"))\n",
    "#     df.withColumn(\"mod_contant\", C(\"x\") )\n",
    "#      )\n",
    "# df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T09:57:17.747408Z",
     "start_time": "2021-06-19T09:57:17.507232Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-------------+\n",
      "|language|user_counts|is_many_users|\n",
      "+--------+-----------+-------------+\n",
      "|    Java|      20000|            0|\n",
      "|  Python|     100000|            1|\n",
      "|   Scala|       3000|            0|\n",
      "+--------+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4 create a new dynamic column(if else condition based on old column)\n",
    "columns = [\"language\", \"user_counts\"]\n",
    "data = [\n",
    "    (\"Java\",20000),\n",
    "    (\"Python\",100000),\n",
    "    (\"Scala\",3000)\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "df = (\n",
    "    df.withColumn(\"is_many_users\",\n",
    "                  F.when(C('user_counts') > 50000, 1).otherwise(0)\n",
    "                 )\n",
    ")\n",
    "\n",
    "df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T09:57:18.168581Z",
     "start_time": "2021-06-19T09:57:17.750212Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|firstname|middlename|lastname|   id|gender|salary|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|    James|          |   Smith|36636|     M|  3000|\n",
      "|  Michael|      Rose|        |40288|     M|  4000|\n",
      "|   Robert|          |Williams|42114|     M|  4000|\n",
      "|    Maria|      Anne|   Jones|39192|     F|  4000|\n",
      "|      Jen|      Mary|   Brown|     |     F|    -1|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "\n",
      "After\n",
      "+---------+----------+--------+-----+------+------+----------------+\n",
      "|firstname|middlename|lastname|   id|gender|salary|       full_name|\n",
      "+---------+----------+--------+-----+------+------+----------------+\n",
      "|    James|      null|   Smith|36636|     M|  3000|             N/A|\n",
      "|  Michael|      Rose|    null|40288|     M|  4000|             N/A|\n",
      "|   Robert|      null|Williams|42114|     M|  4000|             N/A|\n",
      "|    Maria|      Anne|   Jones|39192|     F|  4000|Maria Anne Jones|\n",
      "|      Jen|      Mary|   Brown|     |     F|    -1|  Jen Mary Brown|\n",
      "+---------+----------+--------+-----+------+------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5 create a new dynamic column(if else condition based on old column) plus empty string replacement\n",
    "\n",
    "data = [\n",
    "    (\"James\",\"\",\"Smith\",\"36636\",\"M\", 3000),\n",
    "    (\"Michael\",\"Rose\",\"\",\"40288\",\"M\", 4000),\n",
    "    (\"Robert\",\"\",\"Williams\",\"42114\",\"M\", 4000),\n",
    "    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\", 4000),\n",
    "    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\", -1)\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"firstname\",StringType(), True), # Nullable True\n",
    "    StructField(\"middlename\",StringType(), True),\n",
    "    StructField(\"lastname\",StringType(), True),\n",
    "    StructField(\"id\",StringType(), True),\n",
    "    StructField(\"gender\",StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True)\n",
    "    ])\n",
    "df = spark.createDataFrame(data=data,schema=schema)\n",
    "\n",
    "# we cannot compare column values with empty string\n",
    "# so the work-around method is replace empty string to null\n",
    "# then using isNotNull()\n",
    "is_full_name_exist = (C(\"firstname\").isNotNull() & C(\"middlename\").isNotNull() & C(\"lastname\").isNotNull())\n",
    "\n",
    "\n",
    "def blank_as_null(x):\n",
    "    \"\"\"\n",
    "    helper function for converting row value from empty string to null\n",
    "    https://stackoverflow.com/questions/33287886/replace-empty-strings-with-none-null-values-in-dataframe\n",
    "    \"\"\"\n",
    "    return F.when(C(x) != \"\", C(x)).otherwise(None)\n",
    "\n",
    "print(\"Before\")\n",
    "df.show(n=5)\n",
    "df = (\n",
    "    df.withColumn(\"firstname\", blank_as_null(\"firstname\"))\\\n",
    "    .withColumn(\"middlename\", blank_as_null(\"middlename\"))\\\n",
    "    .withColumn(\"lastname\", blank_as_null(\"lastname\"))\\\n",
    "    .withColumn(\"full_name\",\n",
    "                  F.when(\n",
    "                      is_full_name_exist,\n",
    "                      F.concat(C(\"firstname\"), F.lit(' '),\n",
    "                               C(\"middlename\"), F.lit(' '),\n",
    "                               C(\"lastname\"))\n",
    "                  ).otherwise(F.lit('N/A'))\n",
    "                 )\n",
    ")\n",
    "print(\"After\")\n",
    "df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T09:57:18.352067Z",
     "start_time": "2021-06-19T09:57:18.171528Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Column<b'language'>, Column<b'user_counts'>] <class 'list'> <class 'pyspark.sql.column.Column'>\n",
      "+--------+-----------+\n",
      "|language|user_counts|\n",
      "+--------+-----------+\n",
      "|    Java|      20000|\n",
      "|  Python|     100000|\n",
      "|   Scala|       3000|\n",
      "+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6 select columns which also exist on another dataframe\n",
    "\n",
    "columns_1 = [\"language\", \"user_counts\"]\n",
    "data_1 = [\n",
    "    (\"Java\",20000),\n",
    "    (\"Python\",100000),\n",
    "    (\"Scala\",3000)\n",
    "]\n",
    "\n",
    "columns_2 = [\"language\", \"user_counts\",\"note\"]\n",
    "data_2 = [\n",
    "    (\"Java\",20000,\"nothing\"),\n",
    "    (\"Python\",100000,\"nothing\"),\n",
    "    (\"Scala\",3000,\"nothing\")\n",
    "]\n",
    "\n",
    "df_1 = spark.createDataFrame(data_1, columns_1)\n",
    "df_2 = spark.createDataFrame(data_2, columns_2)\n",
    "\n",
    "# union columns\n",
    "same_cols = [F.col(c) for c in df_2.columns if c in df_1.columns]\n",
    "print(same_cols, type(same_cols), type(same_cols[0]))\n",
    "\n",
    "df_same_col = df_1.select(*same_cols)\n",
    "df_same_col.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T09:57:18.568670Z",
     "start_time": "2021-06-19T09:57:18.355134Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-----+-----------------+-----------------+\n",
      "|language|user_counts|const|scientific_sign_1|scientific_sign_2|\n",
      "+--------+-----------+-----+-----------------+-----------------+\n",
      "|    Java|      20000|10000|           1.0E40|          1.0E-40|\n",
      "|  Python|     100000|10000|           1.0E40|          1.0E-40|\n",
      "|   Scala|       3000|10000|           1.0E40|          1.0E-40|\n",
      "+--------+-----------+-----+-----------------+-----------------+\n",
      "\n",
      "root\n",
      " |-- language: string (nullable = true)\n",
      " |-- user_counts: long (nullable = true)\n",
      " |-- const: integer (nullable = false)\n",
      " |-- scientific_sign_1: double (nullable = false)\n",
      " |-- scientific_sign_2: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 7 create a const numerical column\n",
    "\n",
    "\n",
    "columns = [\"language\", \"user_counts\"]\n",
    "data = [\n",
    "    (\"Java\",20000),\n",
    "    (\"Python\",100000),\n",
    "    (\"Scala\",3000)\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "df = (\n",
    "    df.withColumn(\"const\",F.lit(10000))\\\n",
    "    .withColumn(\"scientific_sign_1\", F.lit(1e40))\n",
    "    .withColumn(\"scientific_sign_2\", F.lit(1e-40))\n",
    ")\n",
    "\n",
    "df.show(n=5)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T09:57:18.786412Z",
     "start_time": "2021-06-19T09:57:18.571086Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-------------------+------------------+-------------------+\n",
      "|language|user_counts|        uniform_0_1|     uniform_0_100|         normal_0_1|\n",
      "+--------+-----------+-------------------+------------------+-------------------+\n",
      "|    Java|      20000| 0.6661236774413726| 66.61236774413726| 0.4085363219031828|\n",
      "|  Python|     100000| 0.3856203005100328| 38.56203005100328|-0.7556247885860078|\n",
      "|   Scala|       3000|0.27636619934035966|27.636619934035966|-1.4773884185536659|\n",
      "+--------+-----------+-------------------+------------------+-------------------+\n",
      "\n",
      "root\n",
      " |-- language: string (nullable = true)\n",
      " |-- user_counts: long (nullable = true)\n",
      " |-- uniform_0_1: double (nullable = false)\n",
      " |-- uniform_0_100: double (nullable = false)\n",
      " |-- normal_0_1: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 8 create a random variable column\n",
    "# https://spark.apache.org/docs/2.3.4/api/python/pyspark.sql.html#pyspark.sql.DataFrame\n",
    "# rand uniform [0, 1]\n",
    "# randn Normal distribution mu = 0, sigma = 1\n",
    "\n",
    "columns = [\"language\", \"user_counts\"]\n",
    "data = [\n",
    "    (\"Java\",20000),\n",
    "    (\"Python\",100000),\n",
    "    (\"Scala\",3000)\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "df = (\n",
    "    df.withColumn(\"uniform_0_1\",F.rand(seed=42))\\\n",
    "      .withColumn(\"uniform_0_100\",100 * F.rand(seed=42))\\\n",
    "      .withColumn(\"normal_0_1\", F.randn(seed=42))\n",
    ")\n",
    "\n",
    "df.show(n=5)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T09:57:19.031520Z",
     "start_time": "2021-06-19T09:57:18.789734Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+\n",
      "|language|user_counts|\n",
      "+--------+-----------+\n",
      "|    Java|      20000|\n",
      "|  Python|     100000|\n",
      "|   Scala|       3000|\n",
      "+--------+-----------+\n",
      "\n",
      "['Java', 'Python', 'Scala']\n"
     ]
    }
   ],
   "source": [
    "# 9 convert pyspark dataframe column to a python list\n",
    "# write a def func\n",
    "\n",
    "columns = [\"language\", \"user_counts\"]\n",
    "data = [\n",
    "    (\"Java\",20000),\n",
    "    (\"Python\",100000),\n",
    "    (\"Scala\",3000)\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()\n",
    "\n",
    "row_list = df.select(\"language\").collect()\n",
    "language_list = [row.language for row in row_list]\n",
    "print(language_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T09:57:19.358470Z",
     "start_time": "2021-06-19T09:57:19.034286Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+\n",
      "|language|user_counts|\n",
      "+--------+-----------+\n",
      "|    Java|      20000|\n",
      "|  Python|     100000|\n",
      "|   Scala|       3000|\n",
      "+--------+-----------+\n",
      "\n",
      "+--------+-----------+\n",
      "|language|user_counts|\n",
      "+--------+-----------+\n",
      "|       0|          0|\n",
      "+--------+-----------+\n",
      "\n",
      "root\n",
      " |-- language: long (nullable = false)\n",
      " |-- user_counts: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 10 check NA for all columns\n",
    "def calculate_null(sdf):\n",
    "    return sdf.select([F.count(F.when(F.isnan(c), c)).alias(c)\n",
    "                for c in sdf.columns])\n",
    "    \n",
    "\n",
    "\n",
    "columns = [\"language\", \"user_counts\"]\n",
    "data = [\n",
    "    (\"Java\",20000),\n",
    "    (\"Python\",100000),\n",
    "    (\"Scala\",3000)\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()\n",
    "\n",
    "calculate_null(df).show()\n",
    "\n",
    "calculate_null(df).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T09:57:19.548670Z",
     "start_time": "2021-06-19T09:57:19.360332Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-------------+\n",
      "|language|user_counts|       fillme|\n",
      "+--------+-----------+-------------+\n",
      "|    Java|         50|FillingString|\n",
      "|  Python|         50|FillingString|\n",
      "|   Scala|         50|FillingString|\n",
      "+--------+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 11 fillna in columns\n",
    "\n",
    "columns = [\"language\", \"user_counts\"]\n",
    "data = [\n",
    "    (\"Java\",20000),\n",
    "    (\"Python\",100000),\n",
    "    (\"Scala\",3000)\n",
    "]\n",
    "\n",
    "# df.withColumn('new_column', lit(None).cast(StringType()))\n",
    "df = (\n",
    "    spark.createDataFrame(data,columns)\n",
    "    .withColumn(\"user_counts\", F.lit(None).cast(T.IntegerType()))\n",
    "    .withColumn(\"fillme\", F.lit(None).cast(T.StringType()))\n",
    ").na.fill({\n",
    "        'user_counts' : 50,\n",
    "        'fillme' : \"FillingString\" \n",
    "    })\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# String Operation (0+)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T09:57:19.564689Z",
     "start_time": "2021-06-19T09:57:19.551365Z"
    }
   },
   "outputs": [],
   "source": [
    "# 1 string concat two column values to a new column from an existing dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T09:57:19.670043Z",
     "start_time": "2021-06-19T09:57:19.566197Z"
    }
   },
   "outputs": [],
   "source": [
    "# 2 cut of left 3 char of specific column to a new column from an existing dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T09:57:19.770020Z",
     "start_time": "2021-06-19T09:57:19.672103Z"
    }
   },
   "outputs": [],
   "source": [
    "# 3 convert string type column to int/float type column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T09:57:19.870175Z",
     "start_time": "2021-06-19T09:57:19.772320Z"
    }
   },
   "outputs": [],
   "source": [
    "# 4 convert string type column to datetime type column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical filtering (6+)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T09:57:20.135584Z",
     "start_time": "2021-06-19T09:57:19.873124Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+-----+------+------+\n",
      "|firstname|middlename|lastname|   id|gender|salary|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|    James|          |   Smith|36636|     M|  3000|\n",
      "|  Michael|      Rose|        |40288|     M|  4000|\n",
      "|   Robert|          |Williams|42114|     M|  4000|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. filter on equal condition\n",
    "\n",
    "data = [\n",
    "    (\"James\",\"\",\"Smith\",\"36636\",\"M\", 3000),\n",
    "    (\"Michael\",\"Rose\",\"\",\"40288\",\"M\", 4000),\n",
    "    (\"Robert\",\"\",\"Williams\",\"42114\",\"M\", 4000),\n",
    "    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\", 4000),\n",
    "    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\", -1)\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"firstname\",StringType(), True), # Nullable True\n",
    "    StructField(\"middlename\",StringType(), True),\n",
    "    StructField(\"lastname\",StringType(), True),\n",
    "    StructField(\"id\",StringType(), True),\n",
    "    StructField(\"gender\",StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True)\n",
    "    ])\n",
    "df = spark.createDataFrame(data=data,schema=schema)\n",
    "\n",
    "df = (\n",
    "    df.filter(C(\"gender\") == \"M\")\n",
    ")\n",
    "\n",
    "df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T09:57:20.312741Z",
     "start_time": "2021-06-19T09:57:20.138416Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+-----+------+------+\n",
      "|firstname|middlename|lastname|   id|gender|salary|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|  Michael|      Rose|        |40288|     M|  4000|\n",
      "|   Robert|          |Williams|42114|     M|  4000|\n",
      "|    Maria|      Anne|   Jones|39192|     F|  6000|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2 filter on >, <, >=, <=\n",
    "\n",
    "data = [\n",
    "    (\"James\",\"\",\"Smith\",\"36636\",\"M\", 3000),\n",
    "    (\"Michael\",\"Rose\",\"\",\"40288\",\"M\", 4000),\n",
    "    (\"Robert\",\"\",\"Williams\",\"42114\",\"M\", 4000),\n",
    "    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\", 6000),\n",
    "    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\", -1)\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"firstname\",StringType(), True), # Nullable True\n",
    "    StructField(\"middlename\",StringType(), True),\n",
    "    StructField(\"lastname\",StringType(), True),\n",
    "    StructField(\"id\",StringType(), True),\n",
    "    StructField(\"gender\",StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True)\n",
    "    ])\n",
    "df = spark.createDataFrame(data=data,schema=schema)\n",
    "\n",
    "df = (\n",
    "    df.filter(C(\"salary\") >= 4000)\n",
    ")\n",
    "\n",
    "df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T09:57:20.496377Z",
     "start_time": "2021-06-19T09:57:20.315863Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+-----+------+------+\n",
      "|firstname|middlename|lastname|   id|gender|salary|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|  Michael|      Rose|        |40288|     M|  4000|\n",
      "|   Robert|          |Williams|42114|     M|  4000|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3 multiple conditions require parenthese around each condition\n",
    "\n",
    "\n",
    "data = [\n",
    "    (\"James\",\"\",\"Smith\",\"36636\",\"M\", 3000),\n",
    "    (\"Michael\",\"Rose\",\"\",\"40288\",\"M\", 4000),\n",
    "    (\"Robert\",\"\",\"Williams\",\"42114\",\"M\", 4000),\n",
    "    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\", 6000),\n",
    "    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\", -1)\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"firstname\",StringType(), True), # Nullable True\n",
    "    StructField(\"middlename\",StringType(), True),\n",
    "    StructField(\"lastname\",StringType(), True),\n",
    "    StructField(\"id\",StringType(), True),\n",
    "    StructField(\"gender\",StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True)\n",
    "    ])\n",
    "df = spark.createDataFrame(data=data,schema=schema)\n",
    "\n",
    "# This is lazy computing\n",
    "rich_man_who_worth_married = (\n",
    "    (C(\"gender\") == \"M\") &\n",
    "    (C(\"salary\") >= 4000)\n",
    ")\n",
    "\n",
    "df = (\n",
    "    df.filter(rich_man_who_worth_married)\n",
    ")\n",
    "\n",
    "df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T09:57:20.680706Z",
     "start_time": "2021-06-19T09:57:20.499777Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+-----+------+------+\n",
      "|firstname|middlename|lastname|   id|gender|salary|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|    Maria|      Anne|   Jones|39192|     F|  6000|\n",
      "|      Jen|      Mary|   Brown|     |     F|    -1|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4 Compare against a list of allowed values\n",
    "\n",
    "\n",
    "data = [\n",
    "    (\"James\",\"\",\"Smith\",\"36636\",\"M\", 3000),\n",
    "    (\"Michael\",\"Rose\",\"\",\"40288\",\"M\", 4000),\n",
    "    (\"Robert\",\"\",\"Williams\",\"42114\",\"M\", 4000),\n",
    "    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\", 6000),\n",
    "    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\", -1)\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"firstname\",StringType(), True), # Nullable True\n",
    "    StructField(\"middlename\",StringType(), True),\n",
    "    StructField(\"lastname\",StringType(), True),\n",
    "    StructField(\"id\",StringType(), True),\n",
    "    StructField(\"gender\",StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True)\n",
    "    ])\n",
    "df = spark.createDataFrame(data=data,schema=schema)\n",
    "\n",
    "df = (\n",
    "    df.filter(C(\"gender\").isin([\"F\"]))\n",
    ")\n",
    "\n",
    "df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T09:57:21.042780Z",
     "start_time": "2021-06-19T09:57:20.683315Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+-----+------+------+\n",
      "|firstname|middlename|lastname|   id|gender|salary|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|    Maria|      Anne|   Jones|39192|     F|  6000|\n",
      "|   Robert|          |Williams|42114|     M|  4000|\n",
      "|  Michael|      Rose|        |40288|     M|  4000|\n",
      "|    James|          |   Smith|36636|     M|  3000|\n",
      "|      Jen|      Mary|   Brown|     |     F|    -1|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|firstname|middlename|lastname|   id|gender|salary|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|      Jen|      Mary|   Brown|     |     F|    -1|\n",
      "|    James|          |   Smith|36636|     M|  3000|\n",
      "|  Michael|      Rose|        |40288|     M|  4000|\n",
      "|   Robert|          |Williams|42114|     M|  4000|\n",
      "|    Maria|      Anne|   Jones|39192|     F|  6000|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5 Sort result\n",
    "\n",
    "data = [\n",
    "    (\"James\",\"\",\"Smith\",\"36636\",\"M\", 3000),\n",
    "    (\"Michael\",\"Rose\",\"\",\"40288\",\"M\", 4000),\n",
    "    (\"Robert\",\"\",\"Williams\",\"42114\",\"M\", 4000),\n",
    "    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\", 6000),\n",
    "    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\", -1)\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"firstname\",StringType(), True), # Nullable True\n",
    "    StructField(\"middlename\",StringType(), True),\n",
    "    StructField(\"lastname\",StringType(), True),\n",
    "    StructField(\"id\",StringType(), True),\n",
    "    StructField(\"gender\",StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True)\n",
    "    ])\n",
    "df = spark.createDataFrame(data=data,schema=schema)\n",
    "\n",
    "\n",
    "df = df.orderBy(C(\"salary\").desc())\n",
    "\n",
    "df.show(n=5)\n",
    "\n",
    "df = df.orderBy(C(\"salary\").asc())\n",
    "\n",
    "df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T09:57:22.384267Z",
     "start_time": "2021-06-19T09:57:21.044548Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before\n",
      "+----------+-------+--------------------+\n",
      "|article_id|   pred|             img_url|\n",
      "+----------+-------+--------------------+\n",
      "|     67789|0.93422|https://pic.pimg....|\n",
      "|     67789|0.94231|https://pic.pimg....|\n",
      "|     14431|0.97611|https://pic.pimg....|\n",
      "|     14431|0.99834|https://pic.pimg....|\n",
      "|     14431|0.99834|https://pic.pimg....|\n",
      "+----------+-------+--------------------+\n",
      "\n",
      "Sol 1 FAILED SOMETIMES WHEN PARTITION != 1\n",
      "+----------+-------+--------------------+\n",
      "|article_id|   pred|             img_url|\n",
      "+----------+-------+--------------------+\n",
      "|     67789|0.93422|https://pic.pimg....|\n",
      "|     67789|0.94231|https://pic.pimg....|\n",
      "|     14431|0.97611|https://pic.pimg....|\n",
      "|     14431|0.99834|https://pic.pimg....|\n",
      "+----------+-------+--------------------+\n",
      "\n",
      "Sol 2 WORKED WITH ANY PARTITION\n",
      "+----------+-------+--------------------+\n",
      "|article_id|   pred|             img_url|\n",
      "+----------+-------+--------------------+\n",
      "|     67789|0.93422|https://pic.pimg....|\n",
      "|     67789|0.94231|https://pic.pimg....|\n",
      "|     14431|0.97611|https://pic.pimg....|\n",
      "|     14431|0.99834|https://pic.pimg....|\n",
      "+----------+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6 select distinct rows based on certain column but keep first row\n",
    "# In this case, model prediction to filter the same images\n",
    "\n",
    "############## DROP DUPLICATED doesn't work in this case\n",
    "# https://stackoverflow.com/questions/38687212/spark-dataframe-drop-duplicates-and-keep-first\n",
    "data = [\n",
    "    (14431,0.99834,'https://pic.pimg.tw/happy78/1528543947-685380499_n.jpg'),\n",
    "    (14431,0.99834,'https://pic.pimg.tw/happy78/1528543947-362759723_n.jpg'),\n",
    "    (14431,0.97611,'https://pic.pimg.tw/happy78/1528543962-2265924582_n.jpg'),\n",
    "    (67789,0.93422,'https://pic.pimg.tw/happy78/1528543962-4007835890_n.jpg'),\n",
    "    (67789,0.94231,'https://pic.pimg.tw/happy78/1528543962-45890_n.jpg')\n",
    "]\n",
    "\n",
    "columns = ['article_id','pred','img_url']\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=columns)\n",
    "print('before')\n",
    "\n",
    "df.orderBy('pred').show(n=5)\n",
    "\n",
    "print('Sol 1 FAILED SOMETIMES WHEN PARTITION != 1')\n",
    "df_1 = (\n",
    "    df.drop_duplicates(subset=[\"pred\"])\n",
    ")\n",
    "\n",
    "df_1.orderBy('pred').show(n=5)\n",
    "\n",
    "\n",
    "############## Using Window Function and sort, rank, worked!\n",
    "# You can check the 5 th question of Aggregation, The solution is the same\n",
    "\n",
    "print('Sol 2 WORKED WITH ANY PARTITION')\n",
    "df_2 = (\n",
    "    df.withColumn(\"rank_by_pred\",\n",
    "                  F.row_number().over(\n",
    "                  W.partitionBy(\"pred\")\\\n",
    "                      .orderBy(F.desc(\"pred\"))\n",
    "                  )\n",
    "                 )\\\n",
    "    .filter(F.col(\"rank_by_pred\") == 1)\\\n",
    "    .drop('rank_by_pred')\n",
    ")\n",
    "df_2.orderBy('pred').show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## String filtering (2+)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T09:57:22.592927Z",
     "start_time": "2021-06-19T09:57:22.386324Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " article_id | 14431                                                                                                                                                                                   \n",
      " pred       | 0.99834                                                                                                                                                                                 \n",
      " img_url    | https://pic.pimg.tw/happy78/1528543947-685380499_n.jpg                                                                                                                                  \n",
      "-RECORD 1---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " article_id | 14431                                                                                                                                                                                   \n",
      " pred       | 0.99834                                                                                                                                                                                 \n",
      " img_url    |                                                                                                                                                                                         \n",
      "-RECORD 2---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " article_id | 0                                                                                                                                                                                       \n",
      " pred       | 0.78                                                                                                                                                                                    \n",
      " img_url    | https://pic.pimg.tw/happy78/1528543947-2503982521_n.jpg                                                                                                                                 \n",
      "-RECORD 3---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " article_id | 1                                                                                                                                                                                       \n",
      " pred       | 0.45                                                                                                                                                                                    \n",
      " img_url    | https://s.yimg.com/bt/api/res/1.2/OzjFf8Ov8yUBECUAWKMcDw--/YXBwaWQ9eW5ld3NfbGVnbztxPTg1O3c9NjAw/http://media.zenfs.com/zh_hant_tw/News/stormmedia/20160601-093346_U720_M161728_0c0b.jpg \n",
      "-RECORD 4---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " article_id | 14431                                                                                                                                                                                   \n",
      " pred       | 0.99834                                                                                                                                                                                 \n",
      " img_url    | https://pic.pimg.tw/happy78/1528543947-362759723_n.jpg                                                                                                                                  \n",
      "-RECORD 5---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " article_id | 14431                                                                                                                                                                                   \n",
      " pred       | 0.97611                                                                                                                                                                                 \n",
      " img_url    | https://pic.pimg.tw/happy78/1528543962-2265924582_n.jpg                                                                                                                                 \n",
      "-RECORD 6---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " article_id | 67789                                                                                                                                                                                   \n",
      " pred       | 0.93422                                                                                                                                                                                 \n",
      " img_url    | https://pic.pimg.tw/happy78/1528543962-4007835890_n.jpg                                                                                                                                 \n",
      "-RECORD 7---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " article_id | 67789                                                                                                                                                                                   \n",
      " pred       | 0.94231                                                                                                                                                                                 \n",
      " img_url    | https://pic.pimg.tw/happy78/1528543962-45890_n.jpg                                                                                                                                      \n",
      "-RECORD 8---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " article_id | 67789                                                                                                                                                                                   \n",
      " pred       | 0.94111                                                                                                                                                                                 \n",
      " img_url    | https://pic.pimg.tw/happy78/1528543962-45890_n.png                                                                                                                                      \n",
      "-RECORD 9---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " article_id | 67789                                                                                                                                                                                   \n",
      " pred       | 0.94111                                                                                                                                                                                 \n",
      " img_url    |                                                                                                                                                                                         \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. regax filtering\n",
    "# pyspark regexp_extract api cannot get all the groups\n",
    "data = [\n",
    "    (14431,0.99834,'https://pic.pimg.tw/happy78/1528543947-685380499_n.jpg mfkvmdfasx'),\n",
    "    (14431,0.99834,'https://pic.pimg.tw/happy78/1528543947-685380499_n.jpgsd,clsd,cluah'),\n",
    "    (0,0.78,'src=\"https://pic.pimg.tw/happy78/1528543947-2503982521_n.jpg\" title=\"IMG_5216.jpg\"'),\n",
    "    (1,0.45,'https://s.yimg.com/bt/api/res/1.2/OzjFf8Ov8yUBECUAWKMcDw--/YXBwaWQ9eW5ld3NfbGVnbztxPTg1O3c9NjAw/http://media.zenfs.com/zh_hant_tw/News/stormmedia/20160601-093346_U720_M161728_0c0b.jpg'),\n",
    "    (14431,0.99834,'https://pic.pimg.tw/happy78/1528543947-362759723_n.jpg'),\n",
    "    (14431,0.97611,'https://pic.pimg.tw/happy78/1528543962-2265924582_n.jpg'),\n",
    "    (67789,0.93422,'https://pic.pimg.tw/happy78/1528543962-4007835890_n.jpg'),\n",
    "    (67789,0.94231,'https://pic.pimg.tw/happy78/1528543962-45890_n.jpg'),\n",
    "    (67789,0.94111,'https://pic.pimg.tw/happy78/1528543962-45890_n.png'),\n",
    "    (67789,0.94111,'png')\n",
    "]\n",
    "\n",
    "columns = ['article_id','pred','img_url']\n",
    "\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "\n",
    "df_filted = (\n",
    "    df.withColumn(\"img_url\", F.regexp_extract(C('img_url'),\n",
    "                                              r'(http\\S+jpg\\b)|(http\\S+png\\b)',\n",
    "                                              0))\n",
    ")\n",
    "\n",
    "df_filted.show(n=20, vertical=True, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T09:57:22.965855Z",
     "start_time": "2021-06-19T09:57:22.596704Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " article_id  | 14431                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
      " raw_content | <p>今天要帶大家去吃吃台中有名的這一鍋皇室秘藏鍋物，據說會有著「帝王般」的享受（是有沒有這麼誇張），話說這一鍋北中南都有分店，但是不知道是從哪裡起家的？我們這次吃的是「台中朝富店」喔</p>\n",
      "\n",
      "<p>地點：<span style=\"-webkit-text-stroke-width:0px; background-color:rgba(0, 0, 0, 0.03); color:rgba(0, 0, 0, 0.87); display:inline !important; float:none; font-family:roboto,noto sans tc,arial,sans-serif; font-size:13px; font-style:normal; font-variant-caps:normal; font-variant-ligatures:normal; font-weight:400; letter-spacing:normal; orphans:2; text-align:left; text-decoration-color:initial; text-decoration-style:initial; text-indent:0px; text-transform:none; white-space:normal; widows:2; word-spacing:0px\">台中市西屯區朝富路36號</span></p>\n",
      "\n",
      "<p>電話：<span style=\"-webkit-text-stroke-width:0px; background-color:rgba(0, 0, 0, 0.03); color:rgba(0, 0, 0, 0.87); display:inline !important; float:none; font-family:roboto,noto sans tc,arial,sans-serif; font-size:13px; font-style:normal; font-variant-caps:normal; font-variant-ligatures:normal; font-weight:400; letter-spacing:normal; orphans:2; text-align:left; text-decoration-color:initial; text-decoration-style:initial; text-indent:0px; text-transform:none; white-space:normal; widows:2; word-spacing:0px\">04 3609 0088</span></p>\n",
      "\n",
      "<p>營業時間：11:30~03:00</p>\n",
      "\n",
      "<p><iframe class=\"\" frameborder=\"0\" height=\"350\" marginheight=\"0\" marginwidth=\"0\" scrolling=\"no\" src=\"//maps.google.com/?ie=UTF8&amp;f=q&amp;source=s_q&amp;q=loc:24.166896,+120.63769000000002+(%E5%8F%B0%E7%81%A3%E5%8F%B0%E4%B8%AD%E5%B8%82%E8%A5%BF%E5%B1%AF%E5%8D%80%E6%9C%9D%E5%AF%8C%E8%B7%AF%E9%80%99%E4%B8%80%E9%8D%8B%E5%8F%B0%E4%B8%AD%E6%9C%9D%E5%AF%8C%E6%AE%BF)&amp;sll=24.160044,120.62857&amp;ll=24.166896,120.63769&amp;marker=24.166896,120.63769&amp;mrt=loc&amp;z=15&amp;t=m&amp;output=embed\" width=\"425\"></iframe><br>\n",
      "</small></p>\n",
      "\n",
      "</span></strong></p>\n",
      "\n",
      "<p><small>朝富店的外觀非常壯闊氣派，不禁讓人點點頭「嗯，果然是台中的餐廳，建築物都要超。大」</small></p>\n",
      "\n",
      "<p><img alt=\"IMG_5208.jpg\" src=\"https://pic.pimg.tw/happy78/1528543947-685380499_n.jpg\" title=\"IMG_5208.jpg\"></p>\n",
      "\n",
      "<p>進去唄～</p>\n",
      "\n",
      "<p><img alt=\"IMG_5217.jpg\" src=\"https://pic.pimg.tw/happy78/1528543947-362759723_n.jpg\" title=\"IMG_5217.jpg\"></p>\n",
      "\n",
      "<p>一樓有個大水池，看起來很富麗堂皇</p>\n",
      "\n",
      "<p><img alt=\"IMG_5232.jpg\" src=\"https://pic.pimg.tw/happy78/1528543962-2265924582_n.jpg\" title=\"IMG_5232.jpg\"></p>\n",
      "\n",
      "</p>\n",
      "\n",
      "<p>最讓我讚嘆的就是這個牡丹（？中國風的樓梯了！！好美喔～～</p>\n",
      "\n",
      "<p><img alt=\"IMG_5230.jpg\" src=\"https://pic.pimg.tw/happy78/1528543962-4007835890_n.jpg\" title=\"IMG_5230.jpg\"></p>\n",
      "\n",
      "<p>整棟建築物看起來用餐環境很棒</p>\n",
      "\n",
      "<p><img alt=\"IMG_5229.jpg\" src=\"https://pic.pimg.tw/happy78/1528543962-1785144547_n.jpg\" title=\"IMG_5229.jpg\"></p>\n",
      "\n",
      "<p>我看到一二樓都有這些宮廷風服飾，我沒問服務生是不是可以穿（因為我也不想穿XD而且整家店我也沒看到有人在穿XD）不過如果你想穿的話我想應該是沒人會阻止</p>\n",
      "\n",
      "<p><img alt=\"IMG_5228.jpg\" src=\"https://pic.pimg.tw/happy78/1528543960-3197815885_n.jpg\" title=\"IMG_5228.jpg\"></p>\n",
      "\n",
      "<p>醬料也有滿多選擇的</p>\n",
      "\n",
      "<p><img alt=\"IMG_5216.jpg\" src=\"https://pic.pimg.tw/happy78/1528543947-2503982521_n.jpg\" title=\"IMG_5216.jpg\"></p>\n",
      "\n",
      "</p>\n",
      "\n",
      "<p>我們點了麻辣鍋跟老火湯的鴛鴦鍋</p>\n",
      "\n",
      "<p><img alt=\"IMG_5221.jpg\" src=\"https://pic.pimg.tw/happy78/1528543950-1832601825_n.jpg\" title=\"IMG_5221.jpg\"></p>\n",
      "\n",
      "看喔）</p>\n",
      "\n",
      "<p>這是梅花豬268$</p>\n",
      "\n",
      "<p><img alt=\"IMG_5219.jpg\" src=\"https://pic.pimg.tw/happy78/1528543949-4214686345_n.jpg\" title=\"IMG_5219.jpg\"></p>\n",
      "\n",
      "<p>和牛梅花598$</p>\n",
      "\n",
      "<p><img alt=\"IMG_5222.jpg\" src=\"https://pic.pimg.tw/happy78/1528543955-3093638684_n.jpg\" title=\"IMG_5222.jpg\"></p>\n",
      "\n",
      "<p>因為跟長輩吃比較不好意思慢慢拍，所以照片沒有很多，照片裡還有玉米筍跟盛味綜合丸238$，另外右上角是店家招待的香蔥油條，我覺得滿特別的，乾吃的話會很有嚼勁，泡湯吃也很美味</p>\n",
      "\n",
      "<p>&nbsp;</p>\n",
      "\n",
      "<p><img alt=\"IMG_5224.jpg\" src=\"https://pic.pimg.tw/happy78/1528543955-714147528_n.jpg\" title=\"IMG_5224.jpg\"></p>\n",
      "\n",
      "</p>\n",
      "\n",
      "<p>要營養均衡，所以當然要吃青菜啦，這是五彩鮮蔬盤 \n",
      "-RECORD 1-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " article_id  | 55444                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
      " raw_content | </p><divstyle=\"text-align:center;\">                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \n",
      "\n",
      "-RECORD 0-----------------\n",
      " article_id  | 0          \n",
      " raw_content | ['</p>\\n'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1.2 regax replace (filtering)\n",
    "# example <a href=\"url\">link text</a> --> \"\"\n",
    "# <a href=\"https://www.w3schools.com/\">Visit W3Schools.com!</a>\n",
    "\n",
    "############### Case I #############################\n",
    "data = [\n",
    "    (14431,'<p>今天要帶大家去吃吃台中有名的這一鍋皇室秘藏鍋物，據說會有著「帝王般」的享受（是有沒有這麼誇張），話說這一鍋北中南都有分店，但是不知道是從哪裡起家的？我們這次吃的是「台中朝富店」喔</p>\\n\\n<p>地點：<span style=\"-webkit-text-stroke-width:0px; background-color:rgba(0, 0, 0, 0.03); color:rgba(0, 0, 0, 0.87); display:inline !important; float:none; font-family:roboto,noto sans tc,arial,sans-serif; font-size:13px; font-style:normal; font-variant-caps:normal; font-variant-ligatures:normal; font-weight:400; letter-spacing:normal; orphans:2; text-align:left; text-decoration-color:initial; text-decoration-style:initial; text-indent:0px; text-transform:none; white-space:normal; widows:2; word-spacing:0px\">台中市西屯區朝富路36號</span></p>\\n\\n<p>電話：<span style=\"-webkit-text-stroke-width:0px; background-color:rgba(0, 0, 0, 0.03); color:rgba(0, 0, 0, 0.87); display:inline !important; float:none; font-family:roboto,noto sans tc,arial,sans-serif; font-size:13px; font-style:normal; font-variant-caps:normal; font-variant-ligatures:normal; font-weight:400; letter-spacing:normal; orphans:2; text-align:left; text-decoration-color:initial; text-decoration-style:initial; text-indent:0px; text-transform:none; white-space:normal; widows:2; word-spacing:0px\">04 3609 0088</span></p>\\n\\n<p>營業時間：11:30~03:00</p>\\n\\n<p><iframe class=\"\" frameborder=\"0\" height=\"350\" marginheight=\"0\" marginwidth=\"0\" scrolling=\"no\" src=\"//maps.google.com/?ie=UTF8&amp;f=q&amp;source=s_q&amp;q=loc:24.166896,+120.63769000000002+(%E5%8F%B0%E7%81%A3%E5%8F%B0%E4%B8%AD%E5%B8%82%E8%A5%BF%E5%B1%AF%E5%8D%80%E6%9C%9D%E5%AF%8C%E8%B7%AF%E9%80%99%E4%B8%80%E9%8D%8B%E5%8F%B0%E4%B8%AD%E6%9C%9D%E5%AF%8C%E6%AE%BF)&amp;sll=24.160044,120.62857&amp;ll=24.166896,120.63769&amp;marker=24.166896,120.63769&amp;mrt=loc&amp;z=15&amp;t=m&amp;output=embed\" width=\"425\"></iframe><br>\\n<small><a href=\"//maps.google.com/?ie=UTF8&amp;f=q&amp;source=s_q&amp;q=loc:24.166896,+120.63769000000002+(%E5%8F%B0%E7%81%A3%E5%8F%B0%E4%B8%AD%E5%B8%82%E8%A5%BF%E5%B1%AF%E5%8D%80%E6%9C%9D%E5%AF%8C%E8%B7%AF%E9%80%99%E4%B8%80%E9%8D%8B%E5%8F%B0%E4%B8%AD%E6%9C%9D%E5%AF%8C%E6%AE%BF)&amp;sll=24.160044,120.62857&amp;ll=24.166896,120.63769&amp;marker=24.166896,120.63769&amp;mrt=loc&amp;z=15&amp;t=m&amp;source=embed\" style=\"color:#0000FF;text-align:left\">檢視較大的地圖</a></small></p>\\n\\n<p><strong><span style=\"font-size:14px\"><a href=\"https://www.instagram.com/p/BhtcYEmhi9U/\" target=\"_blank\"><img alt=\"\" height=\"20\" src=\"//s.pixfs.net/f.pixnet.net/images/emotions/032.gif\" title=\"\" width=\"20\">想看IG介紹請點我<img alt=\"\" height=\"20\" src=\"//s.pixfs.net/f.pixnet.net/images/emotions/019.gif\" title=\"\" width=\"20\"></a></span></strong></p>\\n\\n<p><small>朝富店的外觀非常壯闊氣派，不禁讓人點點頭「嗯，果然是台中的餐廳，建築物都要超。大」</small></p>\\n\\n<p><img alt=\"IMG_5208.jpg\" src=\"https://pic.pimg.tw/happy78/1528543947-685380499_n.jpg\" title=\"IMG_5208.jpg\"></p>\\n\\n<p>進去唄～</p>\\n\\n<p><img alt=\"IMG_5217.jpg\" src=\"https://pic.pimg.tw/happy78/1528543947-362759723_n.jpg\" title=\"IMG_5217.jpg\"></p>\\n\\n<p>一樓有個大水池，看起來很富麗堂皇</p>\\n\\n<p><img alt=\"IMG_5232.jpg\" src=\"https://pic.pimg.tw/happy78/1528543962-2265924582_n.jpg\" title=\"IMG_5232.jpg\"></p>\\n\\n<p><a href=\"https://vbtrax.com/track/clicks/1006/ce2bc2bd9f093698a587f264d175924223674fdf2aabebf50760b4?subid_1=&amp;subid_2=&amp;subid_3=&amp;subid_4=&amp;subid_5=\"><img alt=\"\" border=\"0\" src=\"https://vbtrax.com/track/imp/img/368/ce2bc2bd9f093698a587f264d175924223674fdf2aabebf50760b4?subid_1=&amp;subid_2=&amp;subid_3=&amp;subid_4=&amp;subid_5=\" title=\"\"></a></p>\\n\\n<p>最讓我讚嘆的就是這個牡丹（？中國風的樓梯了！！好美喔～～</p>\\n\\n<p><img alt=\"IMG_5230.jpg\" src=\"https://pic.pimg.tw/happy78/1528543962-4007835890_n.jpg\" title=\"IMG_5230.jpg\"></p>\\n\\n<p>整棟建築物看起來用餐環境很棒</p>\\n\\n<p><img alt=\"IMG_5229.jpg\" src=\"https://pic.pimg.tw/happy78/1528543962-1785144547_n.jpg\" title=\"IMG_5229.jpg\"></p>\\n\\n<p>我看到一二樓都有這些宮廷風服飾，我沒問服務生是不是可以穿（因為我也不想穿XD而且整家店我也沒看到有人在穿XD）不過如果你想穿的話我想應該是沒人會阻止</p>\\n\\n<p><img alt=\"IMG_5228.jpg\" src=\"https://pic.pimg.tw/happy78/1528543960-3197815885_n.jpg\" title=\"IMG_5228.jpg\"></p>\\n\\n<p>醬料也有滿多選擇的</p>\\n\\n<p><img alt=\"IMG_5216.jpg\" src=\"https://pic.pimg.tw/happy78/1528543947-2503982521_n.jpg\" title=\"IMG_5216.jpg\"></p>\\n\\n<p><a href=\"https://vbtrax.com/track/clicks/1096/ce2bc2b79e073698a587f264d175924223674fdf2aabebf50769b4?subid_1=&amp;subid_2=&amp;subid_3=&amp;subid_4=&amp;subid_5=\"><img alt=\"\" border=\"0\" src=\"https://vbtrax.com/track/imp/img/976/ce2bc2b79e073698a587f264d175924223674fdf2aabebf50769b4?subid_1=&amp;subid_2=&amp;subid_3=&amp;subid_4=&amp;subid_5=\" title=\"\"></a></p>\\n\\n<p>我們點了麻辣鍋跟老火湯的鴛鴦鍋</p>\\n\\n<p><img alt=\"IMG_5221.jpg\" src=\"https://pic.pimg.tw/happy78/1528543950-1832601825_n.jpg\" title=\"IMG_5221.jpg\"></p>\\n\\n<p>因為我們沒有很餓，所以沒有點很多，以下就一一介紹：（對了，這次忘記拍菜單，想看菜單的人可以去它<a href=\"http://www.toponepot.com/product_detail.php\" target=\"_blank\"><strong>官網</strong></a>看喔）</p>\\n\\n<p>這是梅花豬268$</p>\\n\\n<p><img alt=\"IMG_5219.jpg\" src=\"https://pic.pimg.tw/happy78/1528543949-4214686345_n.jpg\" title=\"IMG_5219.jpg\"></p>\\n\\n<p>和牛梅花598$</p>\\n\\n<p><img alt=\"IMG_5222.jpg\" src=\"https://pic.pimg.tw/happy78/1528543955-3093638684_n.jpg\" title=\"IMG_5222.jpg\"></p>\\n\\n<p>因為跟長輩吃比較不好意思慢慢拍，所以照片沒有很多，照片裡還有玉米筍跟盛味綜合丸238$，另外右上角是店家招待的香蔥油條，我覺得滿特別的，乾吃的話會很有嚼勁，泡湯吃也很美味</p>\\n\\n<p>&nbsp;</p>\\n\\n<p><img alt=\"IMG_5224.jpg\" src=\"https://pic.pimg.tw/happy78/1528543955-714147528_n.jpg\" title=\"IMG_5224.jpg\"></p>\\n\\n<p><a href=\"https://vbtrax.com/track/clicks/3456/ce2bc2bd9c0123daefcda67f8835ce1328684bc177fbb9b20a63b60061?subid_1=&amp;subid_2=&amp;subid_3=&amp;subid_4=&amp;subid_5=\"><img alt=\"\" border=\"0\" src=\"https://vbtrax.com/track/imp/img/35035/ce2bc2bd9c0123daefcda67f8835ce1328684bc177fbb9b20a63b60061?subid_1=&amp;subid_2=&amp;subid_3=&amp;subid_4=&amp;subid_5=\" title=\"\"></a></p>\\n\\n<p>要營養均衡，所以當然要吃青菜啦，這是五彩鮮蔬盤'),\n",
    "    (55444,'<pstyle=\"text-align:left;\"><imgsrc=\"https://pic.pimg.tw/peko721/1555405696-3356669416.jpg\"alt=\"麥食達韓式料理.麥食達菜單.麥食達.台北車站美食.北車美食.石鍋拌飯.\"/></p><pstyle=\"text-align:left;\"><strong><spanstyle=\"color:#0000ff;font-size:14pt;\">麥食達</span></strong></p><pstyle=\"text-align:left;\">地址：台北市中正區懷寧街86號</p><pstyle=\"text-align:left;\">這裡不是裝潢華麗的韓式料理餐廳，而是一家有點像家庭食堂的小店，紅蔘茶、冬粉、味噌湯都是無限量供應。</p><pstyle=\"text-align:left;\">相關文章：<atitle=\"【台北車站美食】麥食達韓式料理｜228公園旁的平價石鍋拌飯、鍋物｜附麥食達菜單\"href=\"https://peko721.pixnet.net/blog/post/46669347\"target=\"_blank\">麥食達韓式料理｜228公園旁的平價石鍋拌飯、鍋物</a></p><divstyle=\"text-align:center;\">')\n",
    "    \n",
    "]\n",
    "\n",
    "columns = ['article_id','raw_content']\n",
    "\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "\n",
    "df_filted = (\n",
    "    df.withColumn(\"raw_content\", F.regexp_replace(C('raw_content'),\n",
    "#                                               r'(<a href.*</a>)|(<atitle.*href.*</a>)',\n",
    "                                              r'(<.*href.*</a>)',\n",
    "                                              ''))\n",
    ")\n",
    "\n",
    "df_filted.show(n=20, vertical=True, truncate=False)\n",
    "############### Case II ###########################\n",
    "\n",
    "\n",
    "with open(\"../data/webpage_1.txt\", \"r\") as f:\n",
    "    text = [f.read()]\n",
    "\n",
    "schema = StructType(\n",
    "    [\n",
    "    StructField(\"article_id\", IntegerType(), True),\n",
    "    StructField(\"raw_content\", StringType(), True)\n",
    "    ]\n",
    ")\n",
    "# data format\n",
    "# [(r1_col1, r1_col2, ...),\n",
    "#  (r2_col1, r2_col2, ...),\n",
    "# ]\n",
    "df = spark.createDataFrame([(0, text)], schema=schema)\n",
    "# df.show(truncate=False)\n",
    "\n",
    "df_filted = (\n",
    "    df.withColumn(\"raw_content\", F.regexp_replace(C('raw_content'),\n",
    "#                                               r'(<a href.*</a>)|(<atitle.*href.*</a>)',\n",
    "                                              r'(<.*href.*</a>)',\n",
    "                                              ''))\n",
    ")\n",
    "\n",
    "df_filted.show(n=20, vertical=True, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T09:57:23.400875Z",
     "start_time": "2021-06-19T09:57:22.969201Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------------------------------------------------------------------\n",
      " article_id | 14431                                                             \n",
      " pred       | 0.99834                                                           \n",
      " img_url    | https://pic.pimg.tw/happy78/1528543947-685380499_n.jpg mfkvmdfasx \n",
      "\n",
      "-RECORD 0-----------------------------------------------------------------------\n",
      " article_id | 14431                                                             \n",
      " pred       | 0.99834                                                           \n",
      " img_url    | https://pic.pimg.tw/happy78/1528543947-685380499_n.jpg mfkvmdfasx \n",
      "\n",
      "-RECORD 0--------------------------------------------------------\n",
      " article_id | 67789                                              \n",
      " pred       | 0.94111                                            \n",
      " img_url    | https://pic.pimg.tw/happy78/1528543962-45890_n.png \n",
      "-RECORD 1--------------------------------------------------------\n",
      " article_id | 67789                                              \n",
      " pred       | 0.94111                                            \n",
      " img_url    | png                                                \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2. regax filtering\n",
    "# contains\n",
    "# startswith\n",
    "# endwith\n",
    "\n",
    "data = [\n",
    "    (14431,0.99834,'https://pic.pimg.tw/happy78/1528543947-685380499_n.jpg mfkvmdfasx'),\n",
    "    (14431,0.99834,'https://pic.pimg.tw/happy78/1528543947-362759723_n.jpg'),\n",
    "    (14431,0.97611,'https://pic.pimg.tw/happy78/1528543962-2265924582_n.jpg'),\n",
    "    (67789,0.93422,'https://pic.pimg.tw/happy78/1528543962-4007835890_n.jpg'),\n",
    "    (67789,0.94231,'https://pic.pimg.tw/happy78/1528543962-45890_n.jpg'),\n",
    "    (67789,0.94111,'https://pic.pimg.tw/happy78/1528543962-45890_n.png'),\n",
    "    (67789,0.94111,'png')\n",
    "]\n",
    "\n",
    "columns = ['article_id','pred','img_url']\n",
    "\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "df_con = (\n",
    "    df.filter(df.img_url.contains('mfkv'))\n",
    ")\n",
    "df_con.show(vertical=True,truncate=False)\n",
    "\n",
    "df_startwith = (\n",
    "    df.filter(df.img_url.startswith(\"png\"))\n",
    ")\n",
    "\n",
    "df_con.show(vertical=True,truncate=False)\n",
    "\n",
    "df_endwith = (\n",
    "    df.filter(df.img_url.endswith(\"png\"))\n",
    ")\n",
    "\n",
    "df_endwith.show(vertical=True,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T09:57:23.665496Z",
     "start_time": "2021-06-19T09:57:23.404496Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------------------------------------------------------------------------------------------\n",
      " article_id | 14431                                                                                     \n",
      " cleaned    | 今天要帶大家去吃吃台中有名的這一鍋皇室秘藏鍋物，據說會有著「帝王般」的享受（是有沒有這麼誇張），話說這一鍋北中南都有分店，但是不知道是從哪裡起家的？我們這次吃的是「台中朝富店」喔 \n",
      " length     | 89                                                                                        \n",
      "-RECORD 1-----------------------------------------------------------------------------------------------\n",
      " article_id | 14431                                                                                     \n",
      " cleaned    | 地點：                                                                                       \n",
      " length     | 3                                                                                         \n",
      "-RECORD 2-----------------------------------------------------------------------------------------------\n",
      " article_id | 14431                                                                                     \n",
      " cleaned    | 台中市西屯區朝富路36號                                                                              \n",
      " length     | 12                                                                                        \n",
      "-RECORD 3-----------------------------------------------------------------------------------------------\n",
      " article_id | 14431                                                                                     \n",
      " cleaned    | 電話：                                                                                       \n",
      " length     | 3                                                                                         \n",
      "-RECORD 4-----------------------------------------------------------------------------------------------\n",
      " article_id | 14431                                                                                     \n",
      " cleaned    | 04                                                                                        \n",
      " length     | 2                                                                                         \n",
      "-RECORD 5-----------------------------------------------------------------------------------------------\n",
      " article_id | 14431                                                                                     \n",
      " cleaned    | 3609                                                                                      \n",
      " length     | 4                                                                                         \n",
      "-RECORD 6-----------------------------------------------------------------------------------------------\n",
      " article_id | 14431                                                                                     \n",
      " cleaned    | 0088                                                                                      \n",
      " length     | 4                                                                                         \n",
      "-RECORD 7-----------------------------------------------------------------------------------------------\n",
      " article_id | 14431                                                                                     \n",
      " cleaned    | 營業時間：11:30~03:00                                                                          \n",
      " length     | 16                                                                                        \n",
      "-RECORD 8-----------------------------------------------------------------------------------------------\n",
      " article_id | 14431                                                                                     \n",
      " cleaned    | 檢視較大的地圖                                                                                   \n",
      " length     | 7                                                                                         \n",
      "-RECORD 9-----------------------------------------------------------------------------------------------\n",
      " article_id | 14431                                                                                     \n",
      " cleaned    | 想看IG介紹請點我                                                                                 \n",
      " length     | 9                                                                                         \n",
      "-RECORD 10----------------------------------------------------------------------------------------------\n",
      " article_id | 14431                                                                                     \n",
      " cleaned    | 朝富店的外觀非常壯闊氣派，不禁讓人點點頭「嗯，果然是台中的餐廳，建築物都要超。大」                                                 \n",
      " length     | 41                                                                                        \n",
      "-RECORD 11----------------------------------------------------------------------------------------------\n",
      " article_id | 14431                                                                                     \n",
      " cleaned    | 進去唄～                                                                                      \n",
      " length     | 4                                                                                         \n",
      "-RECORD 12----------------------------------------------------------------------------------------------\n",
      " article_id | 14431                                                                                     \n",
      " cleaned    | 一樓有個大水池，看起來很富麗堂皇                                                                          \n",
      " length     | 16                                                                                        \n",
      "-RECORD 13----------------------------------------------------------------------------------------------\n",
      " article_id | 14431                                                                                     \n",
      " cleaned    | 最讓我讚嘆的就是這個牡丹（？中國風的樓梯了！！好美喔～～                                                              \n",
      " length     | 28                                                                                        \n",
      "-RECORD 14----------------------------------------------------------------------------------------------\n",
      " article_id | 14431                                                                                     \n",
      " cleaned    | 整棟建築物看起來用餐環境很棒                                                                            \n",
      " length     | 14                                                                                        \n",
      "-RECORD 15----------------------------------------------------------------------------------------------\n",
      " article_id | 14431                                                                                     \n",
      " cleaned    | 我看到一二樓都有這些宮廷風服飾，我沒問服務生是不是可以穿（因為我也不想穿XD而且整家店我也沒看到有人在穿XD）不過如果你想穿的話我想應該是沒人會阻止                \n",
      " length     | 74                                                                                        \n",
      "-RECORD 16----------------------------------------------------------------------------------------------\n",
      " article_id | 14431                                                                                     \n",
      " cleaned    | 醬料也有滿多選擇的                                                                                 \n",
      " length     | 9                                                                                         \n",
      "-RECORD 17----------------------------------------------------------------------------------------------\n",
      " article_id | 14431                                                                                     \n",
      " cleaned    | 我們點了麻辣鍋跟老火湯的鴛鴦鍋                                                                           \n",
      " length     | 15                                                                                        \n",
      "-RECORD 18----------------------------------------------------------------------------------------------\n",
      " article_id | 14431                                                                                     \n",
      " cleaned    | 因為我們沒有很餓，所以沒有點很多，以下就一一介紹：（對了，這次忘記拍菜單，想看菜單的人可以去它                                           \n",
      " length     | 47                                                                                        \n",
      "-RECORD 19----------------------------------------------------------------------------------------------\n",
      " article_id | 14431                                                                                     \n",
      " cleaned    | 官網                                                                                        \n",
      " length     | 2                                                                                         \n",
      "-RECORD 20----------------------------------------------------------------------------------------------\n",
      " article_id | 14431                                                                                     \n",
      " cleaned    | 看喔）                                                                                       \n",
      " length     | 3                                                                                         \n",
      "-RECORD 21----------------------------------------------------------------------------------------------\n",
      " article_id | 14431                                                                                     \n",
      " cleaned    | 這是梅花豬268$                                                                                 \n",
      " length     | 9                                                                                         \n",
      "-RECORD 22----------------------------------------------------------------------------------------------\n",
      " article_id | 14431                                                                                     \n",
      " cleaned    | 和牛梅花598$                                                                                  \n",
      " length     | 8                                                                                         \n",
      "-RECORD 23----------------------------------------------------------------------------------------------\n",
      " article_id | 14431                                                                                     \n",
      " cleaned    | 因為跟長輩吃比較不好意思慢慢拍，所以照片沒有很多，照片裡還有玉米筍跟盛味綜合丸238$，另外右上角是店家招待的香蔥油條，我覺得滿特別的，乾吃的話會很有嚼勁，泡湯吃也很美味     \n",
      " length     | 85                                                                                        \n",
      "-RECORD 24----------------------------------------------------------------------------------------------\n",
      " article_id | 14431                                                                                     \n",
      " cleaned    | &nbsp;                                                                                    \n",
      " length     | 6                                                                                         \n",
      "-RECORD 25----------------------------------------------------------------------------------------------\n",
      " article_id | 14431                                                                                     \n",
      " cleaned    | 要營養均衡，所以當然要吃青菜啦，這是五彩鮮蔬盤                                                                   \n",
      " length     | 23                                                                                        \n",
      "-RECORD 26----------------------------------------------------------------------------------------------\n",
      " article_id | 55444                                                                                     \n",
      " cleaned    | 麥食達                                                                                       \n",
      " length     | 3                                                                                         \n",
      "-RECORD 27----------------------------------------------------------------------------------------------\n",
      " article_id | 55444                                                                                     \n",
      " cleaned    | 地址：台北市中正區懷寧街86號                                                                           \n",
      " length     | 15                                                                                        \n",
      "-RECORD 28----------------------------------------------------------------------------------------------\n",
      " article_id | 55444                                                                                     \n",
      " cleaned    | 這裡不是裝潢華麗的韓式料理餐廳，而是一家有點像家庭食堂的小店，紅蔘茶、冬粉、味噌湯都是無限量供應。                                         \n",
      " length     | 49                                                                                        \n",
      "-RECORD 29----------------------------------------------------------------------------------------------\n",
      " article_id | 55444                                                                                     \n",
      " cleaned    | 相關文章：                                                                                     \n",
      " length     | 5                                                                                         \n",
      "-RECORD 30----------------------------------------------------------------------------------------------\n",
      " article_id | 55444                                                                                     \n",
      " cleaned    | 麥食達韓式料理｜228公園旁的平價石鍋拌飯、鍋物                                                                  \n",
      " length     | 24                                                                                        \n",
      "-RECORD 31----------------------------------------------------------------------------------------------\n",
      " article_id | 666                                                                                       \n",
      " cleaned    | This                                                                                      \n",
      " length     | 4                                                                                         \n",
      "-RECORD 32----------------------------------------------------------------------------------------------\n",
      " article_id | 666                                                                                       \n",
      " cleaned    | is                                                                                        \n",
      " length     | 2                                                                                         \n",
      "-RECORD 33----------------------------------------------------------------------------------------------\n",
      " article_id | 666                                                                                       \n",
      " cleaned    | python                                                                                    \n",
      " length     | 6                                                                                         \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# html - related, separate each part of html tag\n",
    "\n",
    "\n",
    "data = [\n",
    "    (14431,'<p>今天要帶大家去吃吃台中有名的這一鍋皇室秘藏鍋物，據說會有著「帝王般」的享受（是有沒有這麼誇張），話說這一鍋北中南都有分店，但是不知道是從哪裡起家的？我們這次吃的是「台中朝富店」喔</p>\\n\\n<p>地點：<span style=\"-webkit-text-stroke-width:0px; background-color:rgba(0, 0, 0, 0.03); color:rgba(0, 0, 0, 0.87); display:inline !important; float:none; font-family:roboto,noto sans tc,arial,sans-serif; font-size:13px; font-style:normal; font-variant-caps:normal; font-variant-ligatures:normal; font-weight:400; letter-spacing:normal; orphans:2; text-align:left; text-decoration-color:initial; text-decoration-style:initial; text-indent:0px; text-transform:none; white-space:normal; widows:2; word-spacing:0px\">台中市西屯區朝富路36號</span></p>\\n\\n<p>電話：<span style=\"-webkit-text-stroke-width:0px; background-color:rgba(0, 0, 0, 0.03); color:rgba(0, 0, 0, 0.87); display:inline !important; float:none; font-family:roboto,noto sans tc,arial,sans-serif; font-size:13px; font-style:normal; font-variant-caps:normal; font-variant-ligatures:normal; font-weight:400; letter-spacing:normal; orphans:2; text-align:left; text-decoration-color:initial; text-decoration-style:initial; text-indent:0px; text-transform:none; white-space:normal; widows:2; word-spacing:0px\">04 3609 0088</span></p>\\n\\n<p>營業時間：11:30~03:00</p>\\n\\n<p><iframe class=\"\" frameborder=\"0\" height=\"350\" marginheight=\"0\" marginwidth=\"0\" scrolling=\"no\" src=\"//maps.google.com/?ie=UTF8&amp;f=q&amp;source=s_q&amp;q=loc:24.166896,+120.63769000000002+(%E5%8F%B0%E7%81%A3%E5%8F%B0%E4%B8%AD%E5%B8%82%E8%A5%BF%E5%B1%AF%E5%8D%80%E6%9C%9D%E5%AF%8C%E8%B7%AF%E9%80%99%E4%B8%80%E9%8D%8B%E5%8F%B0%E4%B8%AD%E6%9C%9D%E5%AF%8C%E6%AE%BF)&amp;sll=24.160044,120.62857&amp;ll=24.166896,120.63769&amp;marker=24.166896,120.63769&amp;mrt=loc&amp;z=15&amp;t=m&amp;output=embed\" width=\"425\"></iframe><br>\\n<small><a href=\"//maps.google.com/?ie=UTF8&amp;f=q&amp;source=s_q&amp;q=loc:24.166896,+120.63769000000002+(%E5%8F%B0%E7%81%A3%E5%8F%B0%E4%B8%AD%E5%B8%82%E8%A5%BF%E5%B1%AF%E5%8D%80%E6%9C%9D%E5%AF%8C%E8%B7%AF%E9%80%99%E4%B8%80%E9%8D%8B%E5%8F%B0%E4%B8%AD%E6%9C%9D%E5%AF%8C%E6%AE%BF)&amp;sll=24.160044,120.62857&amp;ll=24.166896,120.63769&amp;marker=24.166896,120.63769&amp;mrt=loc&amp;z=15&amp;t=m&amp;source=embed\" style=\"color:#0000FF;text-align:left\">檢視較大的地圖</a></small></p>\\n\\n<p><strong><span style=\"font-size:14px\"><a href=\"https://www.instagram.com/p/BhtcYEmhi9U/\" target=\"_blank\"><img alt=\"\" height=\"20\" src=\"//s.pixfs.net/f.pixnet.net/images/emotions/032.gif\" title=\"\" width=\"20\">想看IG介紹請點我<img alt=\"\" height=\"20\" src=\"//s.pixfs.net/f.pixnet.net/images/emotions/019.gif\" title=\"\" width=\"20\"></a></span></strong></p>\\n\\n<p><small>朝富店的外觀非常壯闊氣派，不禁讓人點點頭「嗯，果然是台中的餐廳，建築物都要超。大」</small></p>\\n\\n<p><img alt=\"IMG_5208.jpg\" src=\"https://pic.pimg.tw/happy78/1528543947-685380499_n.jpg\" title=\"IMG_5208.jpg\"></p>\\n\\n<p>進去唄～</p>\\n\\n<p><img alt=\"IMG_5217.jpg\" src=\"https://pic.pimg.tw/happy78/1528543947-362759723_n.jpg\" title=\"IMG_5217.jpg\"></p>\\n\\n<p>一樓有個大水池，看起來很富麗堂皇</p>\\n\\n<p><img alt=\"IMG_5232.jpg\" src=\"https://pic.pimg.tw/happy78/1528543962-2265924582_n.jpg\" title=\"IMG_5232.jpg\"></p>\\n\\n<p><a href=\"https://vbtrax.com/track/clicks/1006/ce2bc2bd9f093698a587f264d175924223674fdf2aabebf50760b4?subid_1=&amp;subid_2=&amp;subid_3=&amp;subid_4=&amp;subid_5=\"><img alt=\"\" border=\"0\" src=\"https://vbtrax.com/track/imp/img/368/ce2bc2bd9f093698a587f264d175924223674fdf2aabebf50760b4?subid_1=&amp;subid_2=&amp;subid_3=&amp;subid_4=&amp;subid_5=\" title=\"\"></a></p>\\n\\n<p>最讓我讚嘆的就是這個牡丹（？中國風的樓梯了！！好美喔～～</p>\\n\\n<p><img alt=\"IMG_5230.jpg\" src=\"https://pic.pimg.tw/happy78/1528543962-4007835890_n.jpg\" title=\"IMG_5230.jpg\"></p>\\n\\n<p>整棟建築物看起來用餐環境很棒</p>\\n\\n<p><img alt=\"IMG_5229.jpg\" src=\"https://pic.pimg.tw/happy78/1528543962-1785144547_n.jpg\" title=\"IMG_5229.jpg\"></p>\\n\\n<p>我看到一二樓都有這些宮廷風服飾，我沒問服務生是不是可以穿（因為我也不想穿XD而且整家店我也沒看到有人在穿XD）不過如果你想穿的話我想應該是沒人會阻止</p>\\n\\n<p><img alt=\"IMG_5228.jpg\" src=\"https://pic.pimg.tw/happy78/1528543960-3197815885_n.jpg\" title=\"IMG_5228.jpg\"></p>\\n\\n<p>醬料也有滿多選擇的</p>\\n\\n<p><img alt=\"IMG_5216.jpg\" src=\"https://pic.pimg.tw/happy78/1528543947-2503982521_n.jpg\" title=\"IMG_5216.jpg\"></p>\\n\\n<p><a href=\"https://vbtrax.com/track/clicks/1096/ce2bc2b79e073698a587f264d175924223674fdf2aabebf50769b4?subid_1=&amp;subid_2=&amp;subid_3=&amp;subid_4=&amp;subid_5=\"><img alt=\"\" border=\"0\" src=\"https://vbtrax.com/track/imp/img/976/ce2bc2b79e073698a587f264d175924223674fdf2aabebf50769b4?subid_1=&amp;subid_2=&amp;subid_3=&amp;subid_4=&amp;subid_5=\" title=\"\"></a></p>\\n\\n<p>我們點了麻辣鍋跟老火湯的鴛鴦鍋</p>\\n\\n<p><img alt=\"IMG_5221.jpg\" src=\"https://pic.pimg.tw/happy78/1528543950-1832601825_n.jpg\" title=\"IMG_5221.jpg\"></p>\\n\\n<p>因為我們沒有很餓，所以沒有點很多，以下就一一介紹：（對了，這次忘記拍菜單，想看菜單的人可以去它<a href=\"http://www.toponepot.com/product_detail.php\" target=\"_blank\"><strong>官網</strong></a>看喔）</p>\\n\\n<p>這是梅花豬268$</p>\\n\\n<p><img alt=\"IMG_5219.jpg\" src=\"https://pic.pimg.tw/happy78/1528543949-4214686345_n.jpg\" title=\"IMG_5219.jpg\"></p>\\n\\n<p>和牛梅花598$</p>\\n\\n<p><img alt=\"IMG_5222.jpg\" src=\"https://pic.pimg.tw/happy78/1528543955-3093638684_n.jpg\" title=\"IMG_5222.jpg\"></p>\\n\\n<p>因為跟長輩吃比較不好意思慢慢拍，所以照片沒有很多，照片裡還有玉米筍跟盛味綜合丸238$，另外右上角是店家招待的香蔥油條，我覺得滿特別的，乾吃的話會很有嚼勁，泡湯吃也很美味</p>\\n\\n<p>&nbsp;</p>\\n\\n<p><img alt=\"IMG_5224.jpg\" src=\"https://pic.pimg.tw/happy78/1528543955-714147528_n.jpg\" title=\"IMG_5224.jpg\"></p>\\n\\n<p><a href=\"https://vbtrax.com/track/clicks/3456/ce2bc2bd9c0123daefcda67f8835ce1328684bc177fbb9b20a63b60061?subid_1=&amp;subid_2=&amp;subid_3=&amp;subid_4=&amp;subid_5=\"><img alt=\"\" border=\"0\" src=\"https://vbtrax.com/track/imp/img/35035/ce2bc2bd9c0123daefcda67f8835ce1328684bc177fbb9b20a63b60061?subid_1=&amp;subid_2=&amp;subid_3=&amp;subid_4=&amp;subid_5=\" title=\"\"></a></p>\\n\\n<p>要營養均衡，所以當然要吃青菜啦，這是五彩鮮蔬盤'),\n",
    "    (55444,'<pstyle=\"text-align:left;\"><imgsrc=\"https://pic.pimg.tw/peko721/1555405696-3356669416.jpg\"alt=\"麥食達韓式料理.麥食達菜單.麥食達.台北車站美食.北車美食.石鍋拌飯.\"/></p><pstyle=\"text-align:left;\"><strong><spanstyle=\"color:#0000ff;font-size:14pt;\">麥食達</span></strong></p><pstyle=\"text-align:left;\">地址：台北市中正區懷寧街86號</p><pstyle=\"text-align:left;\">這裡不是裝潢華麗的韓式料理餐廳，而是一家有點像家庭食堂的小店，紅蔘茶、冬粉、味噌湯都是無限量供應。</p><pstyle=\"text-align:left;\">相關文章：<atitle=\"【台北車站美食】麥食達韓式料理｜228公園旁的平價石鍋拌飯、鍋物｜附麥食達菜單\"href=\"https://peko721.pixnet.net/blog/post/46669347\"target=\"_blank\">麥食達韓式料理｜228公園旁的平價石鍋拌飯、鍋物</a></p><divstyle=\"text-align:center;\">'),\n",
    "    (666,'<html><head></head><body><h1>This is python</h1></body></html>')\n",
    "]\n",
    "\n",
    "columns = ['article_id','raw_content']\n",
    "\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "\n",
    "\n",
    "df_filted = (\n",
    "    df\n",
    "    .withColumn(\"cleaned\",\n",
    "                  F.explode_outer(\n",
    "                      F.split(C(\"raw_content\"),r'<[^>]*>|\\p{Z}+|\\s+'))\n",
    "                 )\n",
    "    .withColumn(\"cleaned\", F.regexp_replace(C(\"cleaned\"), r\"\\p{Z}*\", \"\"))\n",
    "    .withColumn(\"cleaned\",F.trim(C(\"cleaned\")))\n",
    "    .withColumn(\"length\", F.length(C(\"cleaned\")))\n",
    "    .where(F.length(C(\"cleaned\")) > 0)\n",
    "    .drop('raw_content')\n",
    ")\n",
    "\n",
    "df_filted.show(n=200,vertical=True, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T09:57:23.969350Z",
     "start_time": "2021-06-19T09:57:23.668202Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- languagesAtSchool: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- major_use: string (nullable = true)\n",
      " |-- raw_content: string (nullable = true)\n",
      "\n",
      "+----------------+------------------+---------+---------------------------------------------+\n",
      "|name            |languagesAtSchool |major_use|raw_content                                  |\n",
      "+----------------+------------------+---------+---------------------------------------------+\n",
      "|James,,Smith    |[Java, Scala, C++]|Java     |sdmcsmcmpkla,pxozas,pxmasmxpJavasodmcasmdcpso|\n",
      "|Michael,Rose,   |[Spark, Java, C++]|Python   |sd,mcmsdcopmsocmpsmdcpython                  |\n",
      "|Robert,,Williams|[CSharp, VB]      |         |smdkcadpmcpowmcpmspdcmpsdc                   |\n",
      "|ABC,ss,Williams |[]                |Scala    |smdoicmsocomamsodmcosdcoms                   |\n",
      "|ABC,ss,Williams |[]                |Scala    |smdoicmsocomamsodmcosdcomsSca                |\n",
      "+----------------+------------------+---------+---------------------------------------------+\n",
      "\n",
      "+----------------+------------------+---------+---------------------------------------------+-------+\n",
      "|name            |languagesAtSchool |major_use|raw_content                                  |matched|\n",
      "+----------------+------------------+---------+---------------------------------------------+-------+\n",
      "|James,,Smith    |[Java, Scala, C++]|Java     |sdmcsmcmpkla,pxozas,pxmasmxpJavasodmcasmdcpso|true   |\n",
      "|Michael,Rose,   |[Spark, Java, C++]|Python   |sd,mcmsdcopmsocmpsmdcpython                  |false  |\n",
      "|Robert,,Williams|[CSharp, VB]      |         |smdkcadpmcpowmcpmspdcmpsdc                   |true   |\n",
      "|ABC,ss,Williams |[]                |Scala    |smdoicmsocomamsodmcosdcoms                   |false  |\n",
      "|ABC,ss,Williams |[]                |Scala    |smdoicmsocomamsodmcosdcomsSca                |false  |\n",
      "+----------------+------------------+---------+---------------------------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3 reg-like\n",
    "# Search all the pattern mahor_use in raw_content\n",
    "\n",
    "\n",
    "columns = [\"name\",\"languagesAtSchool\",\"major_use\",\"raw_content\"]\n",
    "data = [\n",
    "    (\"James,,Smith\",[\"Java\",\"Scala\",\"C++\"],\"Java\",\"sdmcsmcmpkla,pxozas,pxmasmxpJavasodmcasmdcpso\"), \\\n",
    "    (\"Michael,Rose,\",[\"Spark\",\"Java\",\"C++\"],\"Python\",\"sd,mcmsdcopmsocmpsmdcpython\"), \\\n",
    "    (\"Robert,,Williams\",[\"CSharp\",\"VB\"],\"\",\"smdkcadpmcpowmcpmspdcmpsdc\"),\n",
    "    (\"ABC,ss,Williams\",[],\"Scala\",\"smdoicmsocomamsodmcosdcoms\"),\n",
    "    (\"ABC,ss,Williams\",[],\"Scala\",\"smdoicmsocomamsodmcosdcomsSca\")\n",
    "       ]\n",
    "\n",
    "\n",
    "df = spark.createDataFrame(data=data,schema=columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n",
    "\n",
    "(\n",
    "    df\n",
    "    .withColumn(\"matched\",C(\"raw_content\").contains(C(\"major_use\")))\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## filtering in complex type (1+)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T09:57:24.285081Z",
     "start_time": "2021-06-19T09:57:23.972831Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- languagesAtSchool: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- currentState: string (nullable = true)\n",
      "\n",
      "+----------------+------------------+------------+\n",
      "|name            |languagesAtSchool |currentState|\n",
      "+----------------+------------------+------------+\n",
      "|James,,Smith    |[Java, Scala, C++]|CA          |\n",
      "|Michael,Rose,   |[Spark, Java, C++]|NJ          |\n",
      "|Robert,,Williams|[CSharp, VB]      |NV          |\n",
      "+----------------+------------------+------------+\n",
      "\n",
      "+----------------+------------------+------------+------------------------+\n",
      "|name            |languagesAtSchool |currentState|filter_languagesAtSchool|\n",
      "+----------------+------------------+------------+------------------------+\n",
      "|James,,Smith    |[Java, Scala, C++]|CA          |true                    |\n",
      "|Michael,Rose,   |[Spark, Java, C++]|NJ          |true                    |\n",
      "|Robert,,Williams|[CSharp, VB]      |NV          |false                   |\n",
      "+----------------+------------------+------------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. filter in array type\n",
    "columns = [\"name\",\"languagesAtSchool\",\"currentState\"]\n",
    "data = [(\"James,,Smith\",[\"Java\",\"Scala\",\"C++\"],\"CA\"), \\\n",
    "    (\"Michael,Rose,\",[\"Spark\",\"Java\",\"C++\"],\"NJ\"), \\\n",
    "    (\"Robert,,Williams\",[\"CSharp\",\"VB\"],\"NV\")]\n",
    "\n",
    "df = spark.createDataFrame(data=data,schema=columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n",
    "\n",
    "######### Case 1, single value #############\n",
    "\n",
    "df = (\n",
    "    df.withColumn(\"filter_languagesAtSchool\",F.array_contains(C(\"languagesAtSchool\"), \"Java\"))\n",
    ")\n",
    "df.show(truncate=False)\n",
    "\n",
    "\n",
    "######### Case 2, multiple values ?\n",
    " \n",
    "# must_language = [\"Spark\",\"Java\"]\n",
    "# must_language_lit = [F.lit(i) for i in must_language]\n",
    "# print(must_language_lit)\n",
    "# df = (\n",
    "#     df.withColumn(\"filter_languagesAtSchool\",F.array_contains(C(\"languagesAtSchool\"), must_language_lit))\n",
    "# )\n",
    "# df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T09:57:24.621576Z",
     "start_time": "2021-06-19T09:57:24.288831Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- languagesAtSchool: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- major_use: string (nullable = true)\n",
      "\n",
      "+----------------+------------------+---------+\n",
      "|name            |languagesAtSchool |major_use|\n",
      "+----------------+------------------+---------+\n",
      "|James,,Smith    |[Java, Scala, C++]|Java     |\n",
      "|Michael,Rose,   |[Spark, Java, C++]|Python   |\n",
      "|Robert,,Williams|[CSharp, VB]      |         |\n",
      "|ABC,ss,Williams |[]                |Scala    |\n",
      "+----------------+------------------+---------+\n",
      "\n",
      "+----------------+------------------+---------+--------------------+\n",
      "|name            |languagesAtSchool |major_use|is_major_been_taught|\n",
      "+----------------+------------------+---------+--------------------+\n",
      "|James,,Smith    |[Java, Scala, C++]|Java     |1                   |\n",
      "|Michael,Rose,   |[Spark, Java, C++]|Python   |0                   |\n",
      "|Robert,,Williams|[CSharp, VB]      |         |0                   |\n",
      "|ABC,ss,Williams |[]                |Scala    |0                   |\n",
      "+----------------+------------------+---------+--------------------+\n",
      "\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- languagesAtSchool: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- major_use: string (nullable = true)\n",
      " |-- is_major_been_taught: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 2 column A array contains column B\n",
    "# https://stackoverflow.com/questions/48488463/use-is-in-between-2-spark-dataframe-columns\n",
    "\n",
    "columns = [\"name\",\"languagesAtSchool\",\"major_use\"]\n",
    "data = [(\"James,,Smith\",[\"Java\",\"Scala\",\"C++\"],\"Java\"), \\\n",
    "    (\"Michael,Rose,\",[\"Spark\",\"Java\",\"C++\"],\"Python\"), \\\n",
    "    (\"Robert,,Williams\",[\"CSharp\",\"VB\"],\"\"),\n",
    "    (\"ABC,ss,Williams\",[],\"Scala\")]\n",
    "\n",
    "\n",
    "df = spark.createDataFrame(data=data,schema=columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n",
    "\n",
    "######### Case 1, single value #############\n",
    "\n",
    "df = (\n",
    "    df\n",
    "    .withColumn(\"is_major_been_taught\",F.expr(\"array_contains(languagesAtSchool, major_use)\").cast(\"integer\"))\n",
    "#     .withColumn(\"is_major_been_taught\",F.expr(\"array_contains(languagesAtSchool, major_use)\"))\n",
    ")\n",
    "df.show(truncate=False)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JOIN (3+)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T09:57:27.640160Z",
     "start_time": "2021-06-19T09:57:24.624475Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|col1|co12|\n",
      "+----+----+\n",
      "|   A|   1|\n",
      "|   B|   2|\n",
      "|   C|   3|\n",
      "+----+----+\n",
      "\n",
      "+----+----+\n",
      "|col1|co12|\n",
      "+----+----+\n",
      "|   A|  20|\n",
      "|   Y|  30|\n",
      "|   Z|  50|\n",
      "+----+----+\n",
      "\n",
      "left join\n",
      "+----+----+----+\n",
      "|col1|co12|co12|\n",
      "+----+----+----+\n",
      "|   B|   2|null|\n",
      "|   C|   3|null|\n",
      "|   A|   1|  20|\n",
      "+----+----+----+\n",
      "\n",
      "right join\n",
      "+----+----+----+\n",
      "|col1|co12|co12|\n",
      "+----+----+----+\n",
      "|   Y|null|  30|\n",
      "|   Z|null|  50|\n",
      "|   A|   1|  20|\n",
      "+----+----+----+\n",
      "\n",
      "inner join\n",
      "+----+----+----+\n",
      "|col1|co12|co12|\n",
      "+----+----+----+\n",
      "|   A|   1|  20|\n",
      "+----+----+----+\n",
      "\n",
      "outer join\n",
      "+----+----+----+\n",
      "|col1|co12|co12|\n",
      "+----+----+----+\n",
      "|   B|   2|null|\n",
      "|   Y|null|  30|\n",
      "|   C|   3|null|\n",
      "|   Z|null|  50|\n",
      "|   A|   1|  20|\n",
      "+----+----+----+\n",
      "\n",
      "cross\n",
      "+----+----+----+\n",
      "|col1|co12|col1|\n",
      "+----+----+----+\n",
      "|   A|   1|   A|\n",
      "|   A|   1|   Y|\n",
      "|   A|   1|   Z|\n",
      "|   B|   2|   A|\n",
      "|   C|   3|   A|\n",
      "|   B|   2|   Y|\n",
      "|   B|   2|   Z|\n",
      "|   C|   3|   Y|\n",
      "|   C|   3|   Z|\n",
      "+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1 left, right, inner, outer, cross\n",
    "# cross join ( cartesian product with another DataFrame )\n",
    "# cross join usually use high computational cost which we should avoid to use it \n",
    "# https://spark.apache.org/docs/2.3.0/api/python/pyspark.sql.html#pyspark.sql.GroupedData.apply\n",
    "\n",
    "\n",
    "# https://www.youtube.com/watch?v=fp53QhSfQcI\n",
    "# check the viodeo 05:55 ~ 9.08\n",
    "# The shuffle hash join works best when\n",
    "# distribute evenly with the key we are joining on\n",
    "# have an adequate number of keys for parallesim\n",
    "# The problem often happens when the table you wanna join is unevenly distribute (8 : 00)\n",
    "left = spark.createDataFrame(\n",
    "    [\n",
    "        (\"A\", 1),\n",
    "        (\"B\", 2),\n",
    "        (\"C\", 3)\n",
    "    ],\n",
    "    (\"col1\",\"co12\")\n",
    ")\n",
    "\n",
    "right = spark.createDataFrame(\n",
    "    [\n",
    "        (\"A\", 20),\n",
    "        (\"Y\", 30),\n",
    "        (\"Z\", 50)\n",
    "    ],\n",
    "    (\"col1\",\"co12\")\n",
    ")\n",
    "\n",
    "left.show()\n",
    "right.show()\n",
    "\n",
    "print('left join')\n",
    "left.join(right,on='col1',how='left').show()\n",
    "print('right join')\n",
    "left.join(right,on='col1',how='right').show()\n",
    "print('inner join')\n",
    "left.join(right,on='col1',how='inner').show()\n",
    "print('outer join')\n",
    "left.join(right,on='col1',how='outer').show()\n",
    "print('cross')\n",
    "left.crossJoin(right.select('col1')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T09:57:28.184627Z",
     "start_time": "2021-06-19T09:57:27.643200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n",
      "|article_id|             img_url|\n",
      "+----------+--------------------+\n",
      "|     14431|https://pic.pimg....|\n",
      "|     14431|https://pic.pimg....|\n",
      "|     14431|https://pic.pimg....|\n",
      "|     67789|https://pic.pimg....|\n",
      "|     67789|https://pic.pimg....|\n",
      "+----------+--------------------+\n",
      "\n",
      "+----------+--------------------+\n",
      "|article_id|             img_url|\n",
      "+----------+--------------------+\n",
      "|     14431|https://pic.pimg....|\n",
      "+----------+--------------------+\n",
      "\n",
      "== Physical Plan ==\n",
      "*(3) Project [img_url#1230, article_id#1229L, article_id#1251L]\n",
      "+- *(3) BroadcastHashJoin [img_url#1230], [img_url#1252], LeftOuter, BuildRight\n",
      "   :- Scan ExistingRDD[article_id#1229L,img_url#1230]\n",
      "   +- BroadcastExchange HashedRelationBroadcastMode(List(input[1, string, true]))\n",
      "      +- *(2) GlobalLimit 1\n",
      "         +- Exchange SinglePartition\n",
      "            +- *(1) LocalLimit 1\n",
      "               +- Scan ExistingRDD[article_id#1251L,img_url#1252]\n",
      "+--------------------+----------+----------+\n",
      "|             img_url|article_id|article_id|\n",
      "+--------------------+----------+----------+\n",
      "|https://pic.pimg....|     14431|     14431|\n",
      "|https://pic.pimg....|     14431|      null|\n",
      "|https://pic.pimg....|     14431|      null|\n",
      "|https://pic.pimg....|     67789|      null|\n",
      "|https://pic.pimg....|     67789|      null|\n",
      "+--------------------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# broadcast join\n",
    "# https://www.youtube.com/watch?v=fp53QhSfQcI\n",
    "# if we perform broadcast join 14:50 (no shuffling)\n",
    "# if we DOESNT perform broad join 05:55(shuffling a lot)\n",
    "\n",
    "data = [\n",
    "    (14431,'https://pic.pimg.tw/happy78/1528543947-685380499_n.jpg'),\n",
    "    (14431,'https://pic.pimg.tw/happy78/1528543947-362759723_n.jpg'),\n",
    "    (14431,'https://pic.pimg.tw/happy78/1528543962-2265924582_n.jpg'),\n",
    "    (67789,'https://pic.pimg.tw/happy78/1528543962-4007835890_n.jpg'),\n",
    "    (67789,'https://pic.pimg.tw/happy78/1528543962-45890_n.jpg')\n",
    "]\n",
    "\n",
    "columns = ['article_id','img_url']\n",
    "\n",
    "df_big = spark.createDataFrame(data=data, schema=columns)\n",
    "df_big.show(n=5)\n",
    "\n",
    "df_small = df_big.limit(1)\n",
    "df_small.show(n=5)\n",
    "\n",
    "# NOTE, broadcast join support only left join\n",
    "df_joined = df_big.join(F.broadcast(df_small), on=['img_url'],how='left')\n",
    "df_joined.explain()\n",
    "df_joined.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T09:57:28.912040Z",
     "start_time": "2021-06-19T09:57:28.188280Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n",
      "|article_id|             img_url|\n",
      "+----------+--------------------+\n",
      "|     14431|https://pic.pimg....|\n",
      "|     14431|https://pic.pimg....|\n",
      "|     14431|https://pic.pimg....|\n",
      "|     67789|https://pic.pimg....|\n",
      "|     67789|https://pic.pimg....|\n",
      "+----------+--------------------+\n",
      "\n",
      "+----------+--------------------+\n",
      "|article_id|             img_url|\n",
      "+----------+--------------------+\n",
      "|     14431|https://pic.pimg....|\n",
      "+----------+--------------------+\n",
      "\n",
      "left_semi : inner join but return only left dataframe column\n",
      "+--------------------+----------+\n",
      "|             img_url|article_id|\n",
      "+--------------------+----------+\n",
      "|https://pic.pimg....|     14431|\n",
      "+--------------------+----------+\n",
      "\n",
      "left_anti : difference left - right but return only left dataframe column\n",
      "+--------------------+----------+\n",
      "|             img_url|article_id|\n",
      "+--------------------+----------+\n",
      "|https://pic.pimg....|     14431|\n",
      "|https://pic.pimg....|     14431|\n",
      "|https://pic.pimg....|     67789|\n",
      "|https://pic.pimg....|     67789|\n",
      "+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# left_anti and ledt_semi\n",
    "# https://dzone.com/articles/pyspark-join-explained-with-examples\n",
    "\n",
    "\n",
    "data = [\n",
    "    (14431,'https://pic.pimg.tw/happy78/1528543947-685380499_n.jpg'),\n",
    "    (14431,'https://pic.pimg.tw/happy78/1528543947-362759723_n.jpg'),\n",
    "    (14431,'https://pic.pimg.tw/happy78/1528543962-2265924582_n.jpg'),\n",
    "    (67789,'https://pic.pimg.tw/happy78/1528543962-4007835890_n.jpg'),\n",
    "    (67789,'https://pic.pimg.tw/happy78/1528543962-45890_n.jpg')\n",
    "]\n",
    "\n",
    "columns = ['article_id','img_url']\n",
    "\n",
    "df_big = spark.createDataFrame(data=data, schema=columns)\n",
    "df_big.show(n=5)\n",
    "\n",
    "df_small = df_big.limit(1)\n",
    "df_small.show(n=5)\n",
    "\n",
    "print('left_semi : inner join but return only left dataframe column')\n",
    "\n",
    "df_left_semi = df_big.join(df_small,on=['img_url'],how='left_semi')\n",
    "df_left_semi.show()\n",
    "\n",
    "print('left_anti : difference left - right but return only left dataframe column')\n",
    "\n",
    "df_left_anti = df_big.join(df_small,on=['img_url'],how='left_anti')\n",
    "df_left_anti.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregation Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T09:57:29.260488Z",
     "start_time": "2021-06-19T09:57:28.915147Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+------------------------+---------------+---------+\n",
      "|store_name|food_category|food_category_popularity|        img_url|author_id|\n",
      "+----------+-------------+------------------------+---------------+---------+\n",
      "|    hotpop|         Meat|                       3|https//:123.png|   rtyg11|\n",
      "+----------+-------------+------------------------+---------------+---------+\n",
      "\n",
      "== Physical Plan ==\n",
      "*(3) Project [img_url#1315, store_name#1312, food_category#1313, food_category_popularity#1314L, author_id#1316, store_name#1341, food_category#1342, food_category_popularity#1343L, author_id#1345]\n",
      "+- *(3) BroadcastHashJoin [img_url#1315], [img_url#1344], Inner, BuildRight\n",
      "   :- *(3) Filter isnotnull(img_url#1315)\n",
      "   :  +- Scan ExistingRDD[store_name#1312,food_category#1313,food_category_popularity#1314L,img_url#1315,author_id#1316]\n",
      "   +- BroadcastExchange HashedRelationBroadcastMode(List(input[3, string, false]))\n",
      "      +- *(2) Filter isnotnull(img_url#1344)\n",
      "         +- *(2) GlobalLimit 1\n",
      "            +- Exchange SinglePartition\n",
      "               +- *(1) LocalLimit 1\n",
      "                  +- Scan ExistingRDD[store_name#1341,food_category#1342,food_category_popularity#1343L,img_url#1344,author_id#1345]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_url</th>\n",
       "      <th>store_name</th>\n",
       "      <th>food_category</th>\n",
       "      <th>food_category_popularity</th>\n",
       "      <th>author_id</th>\n",
       "      <th>store_name</th>\n",
       "      <th>food_category</th>\n",
       "      <th>food_category_popularity</th>\n",
       "      <th>author_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https//:123.png</td>\n",
       "      <td>hotpop</td>\n",
       "      <td>Meat</td>\n",
       "      <td>3</td>\n",
       "      <td>rtyg11</td>\n",
       "      <td>hotpop</td>\n",
       "      <td>Meat</td>\n",
       "      <td>3</td>\n",
       "      <td>rtyg11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           img_url store_name food_category  food_category_popularity  \\\n",
       "0  https//:123.png     hotpop          Meat                         3   \n",
       "\n",
       "  author_id store_name food_category  food_category_popularity author_id  \n",
       "0    rtyg11     hotpop          Meat                         3    rtyg11  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2 broadcast join\n",
    "# boardcast join for big dataframe and small data frame join\n",
    "# broadcast small dataframe to each worker,\n",
    "# them the excute plan make narrow dependency instead of wide dependency\n",
    "# code - https://stackoverflow.com/questions/37487318/spark-sql-broadcast-hash-join\n",
    "# documentation - search broadcast join in doc https://spark.apache.org/docs/2.3.2/api/python/pyspark.sql.html\n",
    "# concept https://www.youtube.com/watch?v=fp53QhSfQcI 14:32 ~ 14:59\n",
    "data = [\n",
    "    (\"hotpop\",\"Meat\",3,\"https//:123.png\",\"rtyg11\"),\n",
    "    (\"hotpop\",\"Meat\",3,\"https//:456.png\",\"rtyg11\"),\n",
    "    (\"hotpop\",\"Meat\",3,\"https//:789.png\",\"rtyg11\"),\n",
    "    (\"hotpop\",\"Vegetable\",2,\"https//:111.png\",\"rtyg11\"),\n",
    "    (\"hotpop\",\"Vegetable\",2,\"https//:222.png\",\"rtyg11\"),\n",
    "    (\"branch\",\"Fried food\",1,\"https//:333.png\",\"bvc1\"),\n",
    "    (\"branch\",\"Dessert\",1,\"https//:444.png\",\"7854\"),\n",
    "  ]\n",
    "\n",
    "columns = [\"store_name\",\"food_category\",\"food_category_popularity\",\"img_url\",\"author_id\"]\n",
    "df_big = spark.createDataFrame(data = data, schema = columns)\n",
    "\n",
    "df_small = df_big.limit(1)\n",
    "\n",
    "df_small.show()\n",
    "\n",
    "df_broadcast_join = (\n",
    " df_big\n",
    "    .join(F.broadcast(df_small), on=\"img_url\")\n",
    ")\n",
    "\n",
    "df_broadcast_join.explain()\n",
    "\n",
    "df_broadcast_join.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T09:57:31.096962Z",
     "start_time": "2021-06-19T09:57:29.262159Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|col1|co12|\n",
      "+----+----+\n",
      "|   A|   1|\n",
      "|   B|   2|\n",
      "|   C|   3|\n",
      "+----+----+\n",
      "\n",
      "+----+----+\n",
      "|col1|co12|\n",
      "+----+----+\n",
      "|   A|  20|\n",
      "|   Y|  30|\n",
      "|   Z|  50|\n",
      "+----+----+\n",
      "\n",
      "inner - inner join and keeps columns with two tables\n",
      "which will be annoying if there are same column name or dropping column by yourself\n",
      "+----+----+----+\n",
      "|col1|co12|co12|\n",
      "+----+----+----+\n",
      "|   A|   1|  20|\n",
      "+----+----+----+\n",
      "\n",
      "left_semi - inner join but only keeps left table columns\n",
      "+----+----+\n",
      "|col1|co12|\n",
      "+----+----+\n",
      "|   A|   1|\n",
      "+----+----+\n",
      "\n",
      "left_anti - selects all rows from left that are not present in right.\n",
      "+----+----+\n",
      "|col1|co12|\n",
      "+----+----+\n",
      "|   B|   2|\n",
      "|   C|   3|\n",
      "+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3 left semi, left anti\n",
    "# right semi, right anti\n",
    "# https://dzone.com/articles/pyspark-join-explained-with-examples\n",
    "\n",
    "left = spark.createDataFrame(\n",
    "    [\n",
    "        (\"A\", 1),\n",
    "        (\"B\", 2),\n",
    "        (\"C\", 3)\n",
    "    ],\n",
    "    (\"col1\",\"co12\")\n",
    ")\n",
    "\n",
    "right = spark.createDataFrame(\n",
    "    [\n",
    "        (\"A\", 20),\n",
    "        (\"Y\", 30),\n",
    "        (\"Z\", 50)\n",
    "    ],\n",
    "    (\"col1\",\"co12\")\n",
    ")\n",
    "\n",
    "left.show()\n",
    "right.show()\n",
    "\n",
    "print('inner - inner join and keeps columns with two tables')\n",
    "print('which will be annoying if there are same column name or dropping column by yourself')\n",
    "\n",
    "left.join(right, on='col1',how='inner').show()\n",
    "\n",
    "print('left_semi - inner join but only keeps left table columns')\n",
    "\n",
    "left.join(right, on='col1',how='left_semi').show()\n",
    "\n",
    "print('left_anti - selects all rows from left that are not present in right.')\n",
    "\n",
    "left.join(right, on='col1',how='left_anti').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregation (12+)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T09:57:31.132523Z",
     "start_time": "2021-06-19T09:57:31.099041Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.group.GroupedData'>\n",
      "\n",
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_df', '_jgd', 'agg', 'apply', 'avg', 'count', 'max', 'mean', 'min', 'pivot', 'sql_ctx', 'sum']\n"
     ]
    }
   ],
   "source": [
    "# 1 knowing the groupby object method\n",
    "\n",
    "data = [\n",
    "    (\"James\",\"\",\"Smith\",\"36636\",\"RD\",\"M\", 3000),\n",
    "    (\"Michael\",\"Rose\",\"\",\"40288\",\"RD\",\"M\", 4000),\n",
    "    (\"Robert\",\"\",\"Williams\",\"42114\",\"SRE\",\"M\", 4000),\n",
    "    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"SRE\",\"F\", 4000),\n",
    "    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",\"BACKEND\", -1)\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"firstname\",StringType(), True), # Nullable True\n",
    "    StructField(\"middlename\",StringType(), True),\n",
    "    StructField(\"lastname\",StringType(), True),\n",
    "    StructField(\"id\",StringType(), True),\n",
    "    StructField(\"apartment\",StringType(), True),\n",
    "    StructField(\"gender\",StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True)\n",
    "    ])\n",
    "df = spark.createDataFrame(data=data,schema=schema)\n",
    "\n",
    "df_grp = df.groupBy(\"salary\")\n",
    "\n",
    "print(type(df_grp), dir(df_grp), sep='\\n\\n')\n",
    "\n",
    "# avg, count, max, mean, sum  - Common aggregation\n",
    "# pivot - two column x, y with value in the table\n",
    "# sql_ctx - apply sql command\n",
    "# custom function - agg, apply\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T09:57:31.755092Z",
     "start_time": "2021-06-19T09:57:31.134336Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|deparment|avg(salary)|\n",
      "+---------+-----------+\n",
      "|        F|       -1.0|\n",
      "|       RD|     3500.0|\n",
      "|      SRE|     4000.0|\n",
      "+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2 apply single aggregation fuction on groupby object\n",
    "data = [\n",
    "    (\"James\",\"\",\"Smith\",\"36636\",\"RD\",\"M\", 3000),\n",
    "    (\"Michael\",\"Rose\",\"\",\"40288\",\"RD\",\"M\", 4000),\n",
    "    (\"Robert\",\"\",\"Williams\",\"42114\",\"SRE\",\"M\", 4000),\n",
    "    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"SRE\",\"F\", 4000),\n",
    "    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",\"BACKEND\", -1)\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"firstname\",StringType(), True), # Nullable True\n",
    "    StructField(\"middlename\",StringType(), True),\n",
    "    StructField(\"lastname\",StringType(), True),\n",
    "    StructField(\"id\",StringType(), True),\n",
    "    StructField(\"deparment\",StringType(), True),\n",
    "    StructField(\"gender\",StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True)\n",
    "    ])\n",
    "df = spark.createDataFrame(data=data,schema=schema)\n",
    "\n",
    "df_grp_department = (\n",
    "    df.groupby(\"deparment\").mean(\"salary\").alias(\"mean_salary\")\n",
    ")\n",
    "\n",
    "df_grp_department.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T09:57:32.243030Z",
     "start_time": "2021-06-19T09:57:31.756954Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+----------+----------+----------+----------+\n",
      "|deparment|sum_salary|avg_salary|max_salary|min_salary|group_size|\n",
      "+---------+----------+----------+----------+----------+----------+\n",
      "|        F|        -1|      -1.0|        -1|        -1|         1|\n",
      "|       RD|      7000|    3500.0|      4000|      3000|         2|\n",
      "|      SRE|      8000|    4000.0|      4000|      4000|         2|\n",
      "+---------+----------+----------+----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3 apply multiple aggregation fuction on groupby object\n",
    "data = [\n",
    "    (\"James\",\"\",\"Smith\",\"36636\",\"RD\",\"M\", 3000),\n",
    "    (\"Michael\",\"Rose\",\"\",\"40288\",\"RD\",\"M\", 4000),\n",
    "    (\"Robert\",\"\",\"Williams\",\"42114\",\"SRE\",\"M\", 4000),\n",
    "    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"SRE\",\"F\", 4000),\n",
    "    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",\"BACKEND\", -1)\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"firstname\",StringType(), True), # Nullable True\n",
    "    StructField(\"middlename\",StringType(), True),\n",
    "    StructField(\"lastname\",StringType(), True),\n",
    "    StructField(\"id\",StringType(), True),\n",
    "    StructField(\"deparment\",StringType(), True),\n",
    "    StructField(\"gender\",StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True)\n",
    "    ])\n",
    "df = spark.createDataFrame(data=data,schema=schema)\n",
    "\n",
    "df_grp_department = df.groupby(\"deparment\").agg(\n",
    "    F.sum(\"salary\").alias(\"sum_salary\"),\n",
    "    F.avg(\"salary\").alias(\"avg_salary\"),\n",
    "    F.max(\"salary\").alias(\"max_salary\"),\n",
    "    F.min(\"salary\").alias(\"min_salary\"),\n",
    "    F.count(\"salary\").alias(\"group_size\")\n",
    ")\n",
    "\n",
    "df_grp_department.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T09:57:32.603071Z",
     "start_time": "2021-06-19T09:57:32.245004Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>deparment</th>\n",
       "      <th>sum_salary</th>\n",
       "      <th>avg_salary</th>\n",
       "      <th>max_salary</th>\n",
       "      <th>min_salary</th>\n",
       "      <th>count_rows</th>\n",
       "      <th>all_rows</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>F</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>[-1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RD</td>\n",
       "      <td>7000</td>\n",
       "      <td>3500.0</td>\n",
       "      <td>4000</td>\n",
       "      <td>3000</td>\n",
       "      <td>2</td>\n",
       "      <td>[3000, 4000]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SRE</td>\n",
       "      <td>8000</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>4000</td>\n",
       "      <td>4000</td>\n",
       "      <td>2</td>\n",
       "      <td>[4000, 4000]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  deparment  sum_salary  avg_salary  max_salary  min_salary  count_rows  \\\n",
       "0         F          -1        -1.0          -1          -1           1   \n",
       "1        RD        7000      3500.0        4000        3000           2   \n",
       "2       SRE        8000      4000.0        4000        4000           2   \n",
       "\n",
       "       all_rows  \n",
       "0          [-1]  \n",
       "1  [3000, 4000]  \n",
       "2  [4000, 4000]  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4 collect data point for each group with the stats(min, max, sum, avg, count)\n",
    "\n",
    "\n",
    "# apply multiple aggregation fuction on groupby object\n",
    "data = [\n",
    "    (\"James\",\"\",\"Smith\",\"36636\",\"RD\",\"M\", 3000),\n",
    "    (\"Michael\",\"Rose\",\"\",\"40288\",\"RD\",\"M\", 4000),\n",
    "    (\"Robert\",\"\",\"Williams\",\"42114\",\"SRE\",\"M\", 4000),\n",
    "    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"SRE\",\"F\", 4000),\n",
    "    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",\"BACKEND\", -1)\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"firstname\",StringType(), True), # Nullable True\n",
    "    StructField(\"middlename\",StringType(), True),\n",
    "    StructField(\"lastname\",StringType(), True),\n",
    "    StructField(\"id\",StringType(), True),\n",
    "    StructField(\"deparment\",StringType(), True),\n",
    "    StructField(\"gender\",StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True)\n",
    "    ])\n",
    "df = spark.createDataFrame(data=data,schema=schema)\n",
    "\n",
    "df_grp_department = df.groupby(\"deparment\").agg(\n",
    "    F.sum(\"salary\").alias(\"sum_salary\"),\n",
    "    F.avg(\"salary\").alias(\"avg_salary\"),\n",
    "    F.max(\"salary\").alias(\"max_salary\"),\n",
    "    F.min(\"salary\").alias(\"min_salary\"),\n",
    "    F.count(\"salary\").alias(\"count_rows\"),\n",
    "    F.collect_list(\"salary\").alias(\"all_rows\")\n",
    ")\n",
    "\n",
    "df_grp_department.toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T09:57:33.146518Z",
     "start_time": "2021-06-19T09:57:32.604597Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+-----+---------+-------+------+\n",
      "|firstname|middlename|lastname|   id|deparment| gender|salary|\n",
      "+---------+----------+--------+-----+---------+-------+------+\n",
      "|      Jen|      Mary|   Brown|     |        F|BACKEND|    -1|\n",
      "|  Michael|      Rose|        |40288|       RD|      M|  8000|\n",
      "|    Maria|      Anne|   Jones|39192|      SRE|      F|  6000|\n",
      "+---------+----------+--------+-----+---------+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5 get first one row in each group\n",
    "# We use Window Function here\n",
    "# Key to think about this, we rank the data in each group, then \n",
    "# filtering\n",
    "# no nothing is groupby\n",
    "# which is different in pandas\n",
    "\n",
    "data = [\n",
    "    (\"James\",\"\",\"Smith\",\"36636\",\"RD\",\"M\", 3000),\n",
    "    (\"Michael\",\"Rose\",\"\",\"40288\",\"RD\",\"M\", 8000),\n",
    "    (\"Robert\",\"\",\"Williams\",\"42114\",\"SRE\",\"M\", 4000),\n",
    "    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"SRE\",\"F\", 6000),\n",
    "    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",\"BACKEND\", -1)\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"firstname\",StringType(), True), # Nullable True\n",
    "    StructField(\"middlename\",StringType(), True),\n",
    "    StructField(\"lastname\",StringType(), True),\n",
    "    StructField(\"id\",StringType(), True),\n",
    "    StructField(\"deparment\",StringType(), True),\n",
    "    StructField(\"gender\",StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True)\n",
    "    ])\n",
    "\n",
    "\n",
    "df = spark.createDataFrame(data=data,schema=schema)\n",
    "\n",
    "df = (\n",
    "    df.withColumn(\"rank_salary_by_deparment\",\n",
    "                  F.row_number().over(\n",
    "                  W.partitionBy(\"deparment\")\\\n",
    "                      .orderBy(F.desc(\"salary\"))\n",
    "                  )\n",
    "                 )\\\n",
    "    .filter(F.col(\"rank_salary_by_deparment\") == 1)\\\n",
    "    .drop('rank_salary_by_deparment')\n",
    ")\n",
    "\n",
    "df.show(n=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T09:57:33.627652Z",
     "start_time": "2021-06-19T09:57:33.148259Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+----------+----------+----------+----------+\n",
      "|deparment|sum_salary|avg_salary|max_salary|min_salary|count_rows|\n",
      "+---------+----------+----------+----------+----------+----------+\n",
      "|       RD|      7000|    3500.0|      4000|      3000|         2|\n",
      "|      SRE|      8000|    4000.0|      4000|      4000|         2|\n",
      "+---------+----------+----------+----------+----------+----------+\n",
      "\n",
      "root\n",
      " |-- deparment: string (nullable = true)\n",
      " |-- sum_salary: long (nullable = true)\n",
      " |-- avg_salary: double (nullable = true)\n",
      " |-- max_salary: integer (nullable = true)\n",
      " |-- min_salary: integer (nullable = true)\n",
      " |-- count_rows: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6 groupby and filtering\n",
    "\n",
    "data = [\n",
    "    (\"James\",\"\",\"Smith\",\"36636\",\"RD\",\"M\", 3000),\n",
    "    (\"Michael\",\"Rose\",\"\",\"40288\",\"RD\",\"M\", 4000),\n",
    "    (\"Robert\",\"\",\"Williams\",\"42114\",\"SRE\",\"M\", 4000),\n",
    "    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"SRE\",\"F\", 4000),\n",
    "    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",\"BACKEND\", -1)\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"firstname\",StringType(), True), # Nullable True\n",
    "    StructField(\"middlename\",StringType(), True),\n",
    "    StructField(\"lastname\",StringType(), True),\n",
    "    StructField(\"id\",StringType(), True),\n",
    "    StructField(\"deparment\",StringType(), True),\n",
    "    StructField(\"gender\",StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True)\n",
    "    ])\n",
    "df = spark.createDataFrame(data=data,schema=schema)\n",
    "\n",
    "df_grp_department = (\n",
    "    df.groupby(\"deparment\").agg(\n",
    "        F.sum(\"salary\").alias(\"sum_salary\"),\n",
    "        F.avg(\"salary\").alias(\"avg_salary\"),\n",
    "        F.max(\"salary\").alias(\"max_salary\"),\n",
    "        F.min(\"salary\").alias(\"min_salary\"),\n",
    "        F.count(\"salary\").alias(\"count_rows\"))\n",
    "    .filter(C(\"sum_salary\") > 0)\n",
    "    \n",
    ")\n",
    "\n",
    "df_grp_department.show(n=5)\n",
    "df_grp_department.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T09:57:34.402435Z",
     "start_time": "2021-06-19T09:57:33.630417Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|item|score|\n",
      "+----+-----+\n",
      "|   a|   10|\n",
      "|   a|   10|\n",
      "|   a|   20|\n",
      "+----+-----+\n",
      "\n",
      "+----+-----+----+----------+----------+\n",
      "|item|score|rank|dense_rank|row_number|\n",
      "+----+-----+----+----------+----------+\n",
      "|   a|   10|   1|         1|         1|\n",
      "|   a|   10|   1|         1|         2|\n",
      "|   a|   20|   3|         2|         3|\n",
      "+----+-----+----+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 7 rank, dense_rank, and row_number\n",
    "# https://stackoverflow.com/questions/44968912/difference-in-dense-rank-and-row-number-in-spark\n",
    "# The window functions\n",
    "\n",
    "data = [\n",
    "    (\"a\",10),\n",
    "    ('a',10),\n",
    "    ('a',20)\n",
    "]\n",
    "\n",
    "columns = ['item','score']\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show(n=5)\n",
    "\n",
    "window_spec = W.partitionBy(\"item\").orderBy(\"score\")\n",
    "df = (\n",
    "    df.withColumn(\"rank\", F.rank().over(window_spec))\\\n",
    "    .withColumn(\"dense_rank\", F.dense_rank().over(window_spec))\\\n",
    "    .withColumn(\"row_number\", F.row_number().over(window_spec))\n",
    ")\n",
    "\n",
    "df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T09:57:34.999814Z",
     "start_time": "2021-06-19T09:57:34.412049Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-----+--------------+\n",
      "|store_name|food_category|  url|food_cat_count|\n",
      "+----------+-------------+-----+--------------+\n",
      "|    branch|      Dessert|ulr_7|             1|\n",
      "|    hotpop|         Meat|url_1|             3|\n",
      "|    hotpop|         Meat|url_2|             3|\n",
      "|    hotpop|         Meat|url_3|             3|\n",
      "|    hotpop|    Vegetable|url_4|             2|\n",
      "|    hotpop|    Vegetable|url_5|             2|\n",
      "|    branch|   Fried food|url_6|             1|\n",
      "+----------+-------------+-----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 11 groupby and sum by a window function\n",
    "data = [\n",
    "    (\"hotpop\",\"Meat\",\"url_1\",),\n",
    "    (\"hotpop\",\"Meat\",\"url_2\"),\n",
    "    (\"hotpop\",\"Meat\",\"url_3\"),\n",
    "    (\"hotpop\",\"Vegetable\",\"url_4\"),\n",
    "    (\"hotpop\",\"Vegetable\",\"url_5\"),\n",
    "    (\"branch\",\"Fried food\",\"url_6\"),\n",
    "    (\"branch\",\"Dessert\",\"ulr_7\"),\n",
    "  ]\n",
    "\n",
    "columns = [\"store_name\",\"food_category\",\"url\"]\n",
    "window_spec = w.partitionBy(\"store_name\",\"food_category\")\n",
    "\n",
    "df = spark.createDataFrame(data = data, schema = columns)\n",
    "df = (\n",
    "    df.withColumn(\"food_cat_count\", F.count(\"food_category\").over(window_spec))\n",
    ")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T09:57:35.012041Z",
     "start_time": "2021-06-19T09:57:35.001973Z"
    }
   },
   "outputs": [],
   "source": [
    "# Difference between countdistinct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## melt operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T09:57:35.527819Z",
     "start_time": "2021-06-19T09:57:35.013664Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------------------------------------------------------------------------\n",
      " poi_name           | 丹丹漢堡                                                                   \n",
      " img_url            | https://pic.pimg.tw/ksdelicacy/1438450572-4270481322.jpg               \n",
      " food_cat_pop_score | 318                                                                    \n",
      " img_article_url    | http://ksdelicacy.pixnet.net/blog/post/55774905                        \n",
      " img_author_id      | ksdelicacy                                                             \n",
      " food_cat           | Fried food                                                             \n",
      "-RECORD 1------------------------------------------------------------------------------------\n",
      " poi_name           | 拿坡里                                                                    \n",
      " img_url            | https://rmfoodie.com/wp-content/uploads/rm/1446140572-1157042739_n.jpg \n",
      " food_cat_pop_score | 333                                                                    \n",
      " img_article_url    | http://rmlove30.pixnet.net/blog/post/61986505                          \n",
      " img_author_id      | RMlove30                                                               \n",
      " food_cat           | Bread                                                                  \n",
      "\n",
      "root\n",
      " |-- poi_name: string (nullable = true)\n",
      " |-- img_url: string (nullable = true)\n",
      " |-- food_cat_pop_score: string (nullable = true)\n",
      " |-- img_article_url: string (nullable = true)\n",
      " |-- img_author_id: string (nullable = true)\n",
      " |-- food_cat: string (nullable = true)\n",
      "\n",
      "-RECORD 0--------------------------\n",
      " poi_name   | 丹丹漢堡                 \n",
      " menu_key   | img_url              \n",
      " menu_value | https://pic.pimg.... \n",
      "-RECORD 1--------------------------\n",
      " poi_name   | 丹丹漢堡                 \n",
      " menu_key   | food_cat_pop_score   \n",
      " menu_value | 318                  \n",
      "-RECORD 2--------------------------\n",
      " poi_name   | 丹丹漢堡                 \n",
      " menu_key   | img_article_url      \n",
      " menu_value | http://ksdelicacy... \n",
      "-RECORD 3--------------------------\n",
      " poi_name   | 丹丹漢堡                 \n",
      " menu_key   | img_author_id        \n",
      " menu_value | ksdelicacy           \n",
      "-RECORD 4--------------------------\n",
      " poi_name   | 丹丹漢堡                 \n",
      " menu_key   | food_cat             \n",
      " menu_value | Fried food           \n",
      "-RECORD 5--------------------------\n",
      " poi_name   | 拿坡里                  \n",
      " menu_key   | img_url              \n",
      " menu_value | https://rmfoodie.... \n",
      "-RECORD 6--------------------------\n",
      " poi_name   | 拿坡里                  \n",
      " menu_key   | food_cat_pop_score   \n",
      " menu_value | 333                  \n",
      "-RECORD 7--------------------------\n",
      " poi_name   | 拿坡里                  \n",
      " menu_key   | img_article_url      \n",
      " menu_value | http://rmlove30.p... \n",
      "-RECORD 8--------------------------\n",
      " poi_name   | 拿坡里                  \n",
      " menu_key   | img_author_id        \n",
      " menu_value | RMlove30             \n",
      "-RECORD 9--------------------------\n",
      " poi_name   | 拿坡里                  \n",
      " menu_key   | food_cat             \n",
      " menu_value | Bread                \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 9 melt the dataframe (wide dataframe to long dataframe)\n",
    "from typing import Iterable\n",
    "import pandas as pd\n",
    "from IPython.core.display import display\n",
    "# https://stackoverflow.com/questions/41670103/how-to-melt-spark-dataframe\n",
    "\n",
    "\n",
    "def melt(\n",
    "        df: DataFrame, \n",
    "        id_vars: Iterable[str], value_vars: Iterable[str], \n",
    "        var_name: str=\"variable\", value_name: str=\"value\") -> DataFrame:\n",
    "    \"\"\"Convert :class:`DataFrame` from wide to long format.\"\"\"\n",
    "\n",
    "    value_names_dtype = dict(df.select(value_vars).dtypes)\n",
    "    unique_dtype = set(value_names_dtype.values())\n",
    "    assert len(unique_dtype) == 1, f\"value_vars should be the same dtype, your dype fo columns : {value_names_dtype}\"\n",
    "\n",
    "    # Create array<struct<variable: str, value: ...>>\n",
    "    _vars_and_vals = F.array(*(\n",
    "        F.struct(F.lit(c).alias(var_name), C(c).alias(value_name)) \n",
    "        for c in value_vars))\n",
    "\n",
    "    # Add to the DataFrame and explode\n",
    "    _tmp = df.withColumn(\"_vars_and_vals\", F.explode(_vars_and_vals))\n",
    "\n",
    "    cols = id_vars + [\n",
    "            C(\"_vars_and_vals\")[x].alias(x) for x in [var_name, value_name]]\n",
    "    return _tmp.select(*cols)\n",
    "\n",
    "# pdf = pd.DataFrame({'A': {0: 'a', 1: 'b', 2: 'c', 4 : 'a'},\n",
    "#                     'B': {0: 1, 1: 3, 2: 5, 4 : 11},\n",
    "#                     'C': {0: 2, 1: 4, 2: 6, 4 : 12}\n",
    "#                    })\n",
    "\n",
    "# pdf_result = pd.melt(pdf, id_vars=['A'], value_vars=['B', 'C']).sort_values(by=['A'])\n",
    "\n",
    "\n",
    "# display(\n",
    "#     \"Pandas\",\n",
    "#     pdf,\n",
    "#     pdf_result,\n",
    "#     \"PySpark\",\n",
    "#        )\n",
    "\n",
    "# # Case 1\n",
    "# # pdf['C'] = pdf['C'].astype(str) # then you can convert to spark df\n",
    "# sdf = spark.createDataFrame(pdf)\n",
    "# sdf.show()\n",
    "# sdf.printSchema()\n",
    "# melt(sdf, id_vars=['A'], value_vars=['B', 'C']).show()\n",
    "\n",
    "############### Case 2 ##############\n",
    "\n",
    "data = [\n",
    "    (\"丹丹漢堡\",                                                                \n",
    "     \"https://pic.pimg.tw/ksdelicacy/1438450572-4270481322.jpg\",               \n",
    "     \"318\",                                                                    \n",
    "     \"http://ksdelicacy.pixnet.net/blog/post/55774905\",\n",
    "     \"ksdelicacy\",                                                             \n",
    "     \"Fried food\"\n",
    "    ),\n",
    "    (\"拿坡里\"   ,                                                                 \n",
    "\"https://rmfoodie.com/wp-content/uploads/rm/1446140572-1157042739_n.jpg\",\n",
    "\"333\",                                                                    \n",
    "\"http://rmlove30.pixnet.net/blog/post/61986505\",                          \n",
    "\"RMlove30\",                                                               \n",
    "    \"Bread\")\n",
    "]\n",
    "\n",
    "cols = ['poi_name',\n",
    "        'img_url',\n",
    "        'food_cat_pop_score',\n",
    "        'img_article_url',\n",
    "        'img_author_id',\n",
    "        'food_cat']\n",
    "\n",
    "df = spark.createDataFrame(data, cols)\n",
    "df.show(vertical=True, truncate=False)\n",
    "df.printSchema()\n",
    "melt(\n",
    "        df,\n",
    "        id_vars=['poi_name'],\n",
    "        value_vars=[\"img_url\",\n",
    "                    'food_cat_pop_score',\n",
    "                    \"img_article_url\",\n",
    "                    \"img_author_id\",\n",
    "                    'food_cat'\n",
    "                    ],\n",
    "        var_name = 'menu_key',value_name = 'menu_value'\n",
    "        ).show(vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create complex type when aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T09:57:37.184432Z",
     "start_time": "2021-06-19T09:57:35.530003Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----+\n",
      "| id|key|value|\n",
      "+---+---+-----+\n",
      "|  1|  a|  123|\n",
      "|  1|  b|  234|\n",
      "|  1|  c|  345|\n",
      "|  2|  a|   12|\n",
      "|  2|  x|   23|\n",
      "+---+---+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- collections: array (nullable = true)\n",
      " |    |-- element: map (containsNull = true)\n",
      " |    |    |-- key: string\n",
      " |    |    |-- value: long (valueContainsNull = true)\n",
      "\n",
      "+---+------------------------------------+\n",
      "|id |collections                         |\n",
      "+---+------------------------------------+\n",
      "|1  |[[a -> 123], [b -> 234], [c -> 345]]|\n",
      "|2  |[[a -> 12], [x -> 23], [y -> 123]]  |\n",
      "+---+------------------------------------+\n",
      "\n",
      "[Row(id=1, collections=[{'a': 123}, {'b': 234}, {'c': 345}]), Row(id=2, collections=[{'a': 12}, {'x': 23}, {'y': 123}])]\n",
      "\n",
      "\n",
      "\n",
      "----------------- Case 2 -------------------\n",
      "\n",
      "\n",
      "\n",
      "+----------+-------------+------------------------+---------------+---------+\n",
      "|store_name|food_category|food_category_popularity|        img_url|author_id|\n",
      "+----------+-------------+------------------------+---------------+---------+\n",
      "|    hotpop|         Meat|                       3|https//:123.png|   rtyg11|\n",
      "|    hotpop|         Meat|                       3|https//:456.png|   rtyg11|\n",
      "|    hotpop|         Meat|                       3|https//:789.png|   rtyg11|\n",
      "|    hotpop|    Vegetable|                       2|https//:111.png|   rtyg11|\n",
      "|    hotpop|    Vegetable|                       2|https//:222.png|   rtyg11|\n",
      "|    branch|   Fried food|                       1|https//:333.png|     bvc1|\n",
      "|    branch|      Dessert|                       1|https//:444.png|     7854|\n",
      "+----------+-------------+------------------------+---------------+---------+\n",
      "\n",
      "+----------+--------------------+\n",
      "|store_name|                menu|\n",
      "+----------+--------------------+\n",
      "|    hotpop|[[food_category -...|\n",
      "|    branch|[[food_category -...|\n",
      "+----------+--------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(store_name='hotpop', menu=[{'food_category': 'Meat'}, {'food_category_popularity': '3'}, {'img_url': 'https//:123.png'}, {'author_id': 'rtyg11'}, {'food_category': 'Meat'}, {'food_category_popularity': '3'}, {'img_url': 'https//:456.png'}, {'author_id': 'rtyg11'}, {'food_category': 'Meat'}, {'food_category_popularity': '3'}, {'img_url': 'https//:789.png'}, {'author_id': 'rtyg11'}, {'food_category': 'Vegetable'}, {'food_category_popularity': '2'}, {'img_url': 'https//:111.png'}, {'author_id': 'rtyg11'}, {'food_category': 'Vegetable'}, {'food_category_popularity': '2'}, {'img_url': 'https//:222.png'}, {'author_id': 'rtyg11'}]),\n",
       " Row(store_name='branch', menu=[{'food_category': 'Fried food'}, {'food_category_popularity': '1'}, {'img_url': 'https//:333.png'}, {'author_id': 'bvc1'}, {'food_category': 'Dessert'}, {'food_category_popularity': '1'}, {'img_url': 'https//:444.png'}, {'author_id': '7854'}])]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 8 collect dict (map) with a group\n",
    "# https://stackoverflow.com/questions/55308482/pyspark-create-dictionary-within-groupby\n",
    "\n",
    "# collect_list : return a list of objects with duplicated\n",
    "# collect_set : return a set of objects without duplicated\n",
    "# struct : create a new struct column\n",
    "# ( > 2.4.0)map_from_entries : returns a map created from the given array of entries\n",
    "# create_map\n",
    "\n",
    "######### pyspark < 2.4.0\n",
    "data = [\n",
    "    (1,'a',123),\n",
    "    (1,'b',234),\n",
    "    (1,'c',345),\n",
    "    (2,'a',12),\n",
    "    (2,'x',23),\n",
    "    (2,'y',123)\n",
    "]\n",
    "\n",
    "columns = ['id','key','value']\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show(n=5)\n",
    "\n",
    "######## pyspark < 2.4.0\n",
    "\n",
    "df_agg = df.groupBy(\"id\").agg(\n",
    "    F.collect_list(F.create_map(C(\"key\"),C(\"value\"))).alias('collections')\n",
    ")\n",
    "\n",
    "df_agg.printSchema()\n",
    "df_agg.show(n=10, truncate=False)\n",
    "print(df_agg.collect())\n",
    "df_agg.toPandas().to_json('output/tmp.json',\n",
    "                          orient='records',\n",
    "                          force_ascii=False,\n",
    "                          lines=True)\n",
    "\n",
    "\n",
    "# to_json(join(SERVING_POI_FOOD_IMG_FOLDER,serving_fname),\n",
    "#                                        orient='records',\n",
    "#                                        force_ascii=False,\n",
    "#                                        lines=True)\n",
    "######### pyspark > 2.4.0\n",
    "# df.groupBy(\"id\").agg(\n",
    "#     F.map_from_entries(\n",
    "#         F.collect_list(\n",
    "#             F.struct(\"key\",\"value\"))).alias(\"key_value\")\n",
    "# ).show()\n",
    "\n",
    "############## Case 2 #######################\n",
    "for i in range(3):\n",
    "    print()\n",
    "print('----------------- Case 2 -------------------')\n",
    "for i in range(3):\n",
    "    print()\n",
    "    \n",
    "data = [\n",
    "    (\"hotpop\",\"Meat\",3,\"https//:123.png\",\"rtyg11\"),\n",
    "    (\"hotpop\",\"Meat\",3,\"https//:456.png\",\"rtyg11\"),\n",
    "    (\"hotpop\",\"Meat\",3,\"https//:789.png\",\"rtyg11\"),\n",
    "    (\"hotpop\",\"Vegetable\",2,\"https//:111.png\",\"rtyg11\"),\n",
    "    (\"hotpop\",\"Vegetable\",2,\"https//:222.png\",\"rtyg11\"),\n",
    "    (\"branch\",\"Fried food\",1,\"https//:333.png\",\"bvc1\"),\n",
    "    (\"branch\",\"Dessert\",1,\"https//:444.png\",\"7854\"),\n",
    "  ]\n",
    "\n",
    "columns = [\"store_name\",\"food_category\",\"food_category_popularity\",\"img_url\",\"author_id\"]\n",
    "df = spark.createDataFrame(data = data, schema = columns)\n",
    "\n",
    "df.show(n=10)\n",
    "\n",
    "\n",
    "# convert the melt column to string\n",
    "# because the column you wanna melt should be the same dtype\n",
    "df = (\n",
    "    df.withColumn(\"food_category_popularity\", C(\"food_category_popularity\").cast(StringType()))\n",
    ")\n",
    "df_complex = (\n",
    "    melt(df, id_vars=['store_name'],\n",
    "             value_vars=['food_category','food_category_popularity','img_url','author_id'],\n",
    "             var_name = 'menu_key',value_name = 'menu_value'\n",
    "        ).groupBy(\"store_name\").agg(\n",
    "        F.collect_list(F.create_map(C(\"menu_key\"), C(\"menu_value\"))).alias(\"menu\")\n",
    "    )\n",
    ")\n",
    "\n",
    "df_complex.show()\n",
    "df_complex.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T09:57:37.195172Z",
     "start_time": "2021-06-19T09:57:37.185790Z"
    }
   },
   "outputs": [],
   "source": [
    "# group by key, create a complexy json format like\n",
    "\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T09:57:37.757508Z",
     "start_time": "2021-06-19T09:57:37.196546Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- group: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- languagesAtSchool: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- currentState: string (nullable = true)\n",
      "\n",
      "+-----+----------------+------------------+------------+\n",
      "|group|name            |languagesAtSchool |currentState|\n",
      "+-----+----------------+------------------+------------+\n",
      "|1    |James,,Smith    |[Java, Scala, C++]|CA          |\n",
      "|1    |Michael,Rose,   |[Spark, Java, C++]|NJ          |\n",
      "|2    |Robert,,Williams|[CSharp, VB]      |NV          |\n",
      "|2    |Robert,,Williams|null              |NV          |\n",
      "+-----+----------------+------------------+------------+\n",
      "\n",
      "+-----+------------------------------------+\n",
      "|group|languagesAtSchool                   |\n",
      "+-----+------------------------------------+\n",
      "|1    |[Java, Scala, C++, Spark, Java, C++]|\n",
      "|2    |[CSharp, VB]                        |\n",
      "+-----+------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 12 groupby , concat the element in array\n",
    "\n",
    "# collect in array list, and flatern then\n",
    "\n",
    "columns = [\"group\",\"name\",\"languagesAtSchool\",\"currentState\"]\n",
    "data = [\n",
    "    ('1',\"James,,Smith\",[\"Java\",\"Scala\",\"C++\"],\"CA\"), \\\n",
    "    ('1',\"Michael,Rose,\",[\"Spark\",\"Java\",\"C++\"],\"NJ\"), \\\n",
    "    ('2',\"Robert,,Williams\",[\"CSharp\",\"VB\"],\"NV\"),\n",
    "    ('2',\"Robert,,Williams\",None,\"NV\")\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data=data,schema=columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n",
    "\n",
    "# collect list with collect list\n",
    "# we need to explode it first\n",
    "\n",
    "df_agg = (\n",
    "    df\n",
    "    .withColumn(\"flattern_tags\",F.explode(C(\"languagesAtSchool\")))\n",
    "    .drop('languagesAtSchool')\n",
    "    .groupBy(\"group\")\n",
    "    .agg(\n",
    "        F.collect_list(\"flattern_tags\").alias('languagesAtSchool')\n",
    "    )\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T09:57:38.521064Z",
     "start_time": "2021-06-19T09:57:37.759622Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- group: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- languagesAtSchool: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- currentState: string (nullable = true)\n",
      "\n",
      "+-----+----------------+------------------+------------+\n",
      "|group|name            |languagesAtSchool |currentState|\n",
      "+-----+----------------+------------------+------------+\n",
      "|1    |James,,Smith    |[Java, Scala, C++]|CA          |\n",
      "|1    |Michael,Rose,   |[Spark, Java, C++]|NJ          |\n",
      "|2    |Robert,,Williams|[CSharp, VB]      |NV          |\n",
      "|2    |Robert,,Williams|null              |NV          |\n",
      "+-----+----------------+------------------+------------+\n",
      "\n",
      "+-----+----------------------------------------+\n",
      "|group|tags                                    |\n",
      "+-----+----------------------------------------+\n",
      "|1    |[[Java, Scala, C++], [Spark, Java, C++]]|\n",
      "|2    |[[CSharp, VB]]                          |\n",
      "+-----+----------------------------------------+\n",
      "\n",
      "+-----+----------------------------------------+------------------+\n",
      "|group|tags                                    |explode_tags      |\n",
      "+-----+----------------------------------------+------------------+\n",
      "|1    |[[Java, Scala, C++], [Spark, Java, C++]]|[Java, Scala, C++]|\n",
      "|1    |[[Java, Scala, C++], [Spark, Java, C++]]|[Spark, Java, C++]|\n",
      "|2    |[[CSharp, VB]]                          |[CSharp, VB]      |\n",
      "+-----+----------------------------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 13 groupby and collect arrays into arrays\n",
    "# Yes we can\n",
    "\n",
    "columns = [\"group\",\"name\",\"languagesAtSchool\",\"currentState\"]\n",
    "data = [\n",
    "    ('1',\"James,,Smith\",[\"Java\",\"Scala\",\"C++\"],\"CA\"), \\\n",
    "    ('1',\"Michael,Rose,\",[\"Spark\",\"Java\",\"C++\"],\"NJ\"), \\\n",
    "    ('2',\"Robert,,Williams\",[\"CSharp\",\"VB\"],\"NV\"),\n",
    "    ('2',\"Robert,,Williams\",None,\"NV\")\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data=data,schema=columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n",
    "\n",
    "df_agg = (\n",
    " df.groupBy(\"group\")\n",
    "    .agg(\n",
    "        F.collect_list(C(\"languagesAtSchool\")).alias(\"tags\")\n",
    "    )\n",
    ")\n",
    "\n",
    "df_agg.show(truncate=False)\n",
    "\n",
    "\n",
    "(\n",
    "    df_agg\n",
    "    .withColumn(\"explode_tags\",F.explode(C(\"tags\")))\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T09:57:39.092826Z",
     "start_time": "2021-06-19T09:57:38.523659Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+---+\n",
      "|age|      dept| id|\n",
      "+---+----------+---+\n",
      "| 38|  medicine|  1|\n",
      "| 41|  medicine|  2|\n",
      "| 55|  medicine|  3|\n",
      "| 15|technology|  4|\n",
      "| 88|technology|  5|\n",
      "| 88|technology|  6|\n",
      "| 75|technology|  7|\n",
      "| 75|       mba|  8|\n",
      "| 75|       mba|  9|\n",
      "| 75|       mba| 10|\n",
      "+---+----------+---+\n",
      "\n",
      "This is wrong answer\n",
      "+---+----------+---+-------+------+\n",
      "|age|      dept| id|firstID|lastID|\n",
      "+---+----------+---+-------+------+\n",
      "| 75|       mba|  8|      8|    10|\n",
      "| 75|       mba|  9|      8|    10|\n",
      "| 75|       mba| 10|      8|    10|\n",
      "| 38|  medicine|  1|      1|     1|\n",
      "| 41|  medicine|  2|      1|     2|\n",
      "| 55|  medicine|  3|      1|     3|\n",
      "| 15|technology|  4|      4|     4|\n",
      "| 75|technology|  7|      4|     7|\n",
      "| 88|technology|  5|      4|     6|\n",
      "| 88|technology|  6|      4|     6|\n",
      "+---+----------+---+-------+------+\n",
      "\n",
      "Because we prodive a orderBy clause, default frame is \n",
      "RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW\n",
      "== Physical Plan ==\n",
      "Window [first(id#2459, false) windowspecdefinition(dept#2455, age#2454L ASC NULLS FIRST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS firstID#2476, last(id#2459, false) windowspecdefinition(dept#2455, age#2454L ASC NULLS FIRST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS lastID#2482], [dept#2455], [age#2454L ASC NULLS FIRST]\n",
      "+- *(3) Sort [dept#2455 ASC NULLS FIRST, age#2454L ASC NULLS FIRST], false, 0\n",
      "   +- *(3) Project [age#2454L, dept#2455, id#2459]\n",
      "      +- Window [row_number() windowspecdefinition(_w0#2460L ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS id#2459], [_w0#2460L ASC NULLS FIRST]\n",
      "         +- *(2) Sort [_w0#2460L ASC NULLS FIRST], false, 0\n",
      "            +- Exchange SinglePartition\n",
      "               +- *(1) Project [age#2454L, dept#2455, (monotonically_increasing_id() - 1) AS _w0#2460L]\n",
      "                  +- Scan ExistingRDD[age#2454L,dept#2455]\n",
      "So we need to re-define the wibndow\n",
      "== Physical Plan ==\n",
      "Window [first(id#2459, false) windowspecdefinition(dept#2455, age#2454L ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS firstID#2510, last(id#2459, false) windowspecdefinition(dept#2455, age#2454L ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS lastID#2516, row_number() windowspecdefinition(dept#2455, age#2454L ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS age_rank_in_dept#2523], [dept#2455], [age#2454L ASC NULLS FIRST]\n",
      "+- *(3) Sort [dept#2455 ASC NULLS FIRST, age#2454L ASC NULLS FIRST], false, 0\n",
      "   +- *(3) Project [age#2454L, dept#2455, id#2459]\n",
      "      +- Window [row_number() windowspecdefinition(_w0#2460L ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS id#2459], [_w0#2460L ASC NULLS FIRST]\n",
      "         +- *(2) Sort [_w0#2460L ASC NULLS FIRST], false, 0\n",
      "            +- Exchange SinglePartition\n",
      "               +- *(1) Project [age#2454L, dept#2455, (monotonically_increasing_id() - 1) AS _w0#2460L]\n",
      "                  +- Scan ExistingRDD[age#2454L,dept#2455]\n",
      "+---+----------+---+-------+------+----------------+\n",
      "|age|      dept| id|firstID|lastID|age_rank_in_dept|\n",
      "+---+----------+---+-------+------+----------------+\n",
      "| 75|       mba|  8|      8|    10|               1|\n",
      "| 75|       mba|  9|      8|    10|               2|\n",
      "| 75|       mba| 10|      8|    10|               3|\n",
      "| 38|  medicine|  1|      1|     3|               1|\n",
      "| 41|  medicine|  2|      1|     3|               2|\n",
      "| 55|  medicine|  3|      1|     3|               3|\n",
      "| 15|technology|  4|      4|     6|               1|\n",
      "| 75|technology|  7|      4|     6|               2|\n",
      "| 88|technology|  5|      4|     6|               3|\n",
      "| 88|technology|  6|      4|     6|               4|\n",
      "+---+----------+---+-------+------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 14 groupby column C1 , get first row and last row once, order by column C2\n",
    "\n",
    "\n",
    "data = [\n",
    "    (38,\"medicine\"),\n",
    "    (41,\"medicine\"),\n",
    "    (55,\"medicine\"),\n",
    "    (15,\"technology\"),\n",
    "    (88,\"technology\"),\n",
    "    (88,\"technology\"),\n",
    "    (75,\"technology\"),\n",
    "    (75,\"mba\"),\n",
    "    (75,\"mba\"),\n",
    "    (75,\"mba\")\n",
    "    ]\n",
    "\n",
    "\n",
    "columns = ['age','dept']\n",
    "\n",
    "df = (\n",
    "    spark.createDataFrame(data=data, schema=columns)\n",
    "    .withColumn(\"id\",\n",
    "                F.row_number()\n",
    "                .over(\n",
    "                    W.orderBy(F.monotonically_increasing_id() - 1)\n",
    "                ))\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "df.show()\n",
    "\n",
    "# https://stackoverflow.com/questions/52273186/pyspark-spark-window-function-first-last-issue\n",
    "print(\"This is wrong answer\")\n",
    "\n",
    "age_in_dept = W.partitionBy('dept').orderBy(\"age\")\n",
    "df_res = (\n",
    "    df\n",
    "    .withColumn(\"firstID\", F.first('id').over(age_in_dept))\n",
    "    .withColumn(\"lastID\", F.last('id').over(age_in_dept))\n",
    ")\n",
    "\n",
    "df_res.show()\n",
    "\n",
    "print('Because we prodive a orderBy clause, default frame is ')\n",
    "print(\"RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW\")\n",
    "\n",
    "df_res.explain()\n",
    "\n",
    "print(\"So we need to re-define the wibndow\")\n",
    "\n",
    "age_in_dept = (\n",
    "    W.partitionBy('dept')\n",
    "    .orderBy(\"age\")\n",
    "    .rowsBetween(W.unboundedPreceding, W.unboundedFollowing)\n",
    ")\n",
    "\n",
    "\n",
    "##### withcolumn, select gives you the same answer, you need to do your own filtering\n",
    "df_res = (\n",
    "    df\n",
    "    .withColumn(\"firstID\", F.first('id').over(age_in_dept))\n",
    "    .withColumn(\"lastID\", F.last('id').over(age_in_dept))\n",
    "    .withColumn(\"age_rank_in_dept\",\n",
    "                F.row_number()\n",
    "                .over(\n",
    "                W.partitionBy('dept')\n",
    "                 .orderBy(\"age\")\n",
    "                ))\n",
    ")\n",
    "\n",
    "df_res.explain()\n",
    "df_res.show()\n",
    "\n",
    "\n",
    "# df_res = (\n",
    "#     df.select(\n",
    "#         \"*\",\n",
    "#         F.first('id').over(age_in_dept).alias('first_id'),\n",
    "#         F.last('id').over(age_in_dept).alias('last_id'),\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# df_res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T09:57:39.937647Z",
     "start_time": "2021-06-19T09:57:39.095857Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|item|score|\n",
      "+----+-----+\n",
      "|   a|   10|\n",
      "|   a|   10|\n",
      "|   a|   20|\n",
      "|   b|    5|\n",
      "|   b|    3|\n",
      "+----+-----+\n",
      "\n",
      "-RECORD 0-------------------------------------\n",
      " item                          | b            \n",
      " score                         | 3            \n",
      " collect_score_list_to_current | [3]          \n",
      " collect_score_set_to_current  | [3]          \n",
      " collect_score_list            | [3, 5]       \n",
      " collect_score_set             | [5, 3]       \n",
      "-RECORD 1-------------------------------------\n",
      " item                          | b            \n",
      " score                         | 5            \n",
      " collect_score_list_to_current | [3, 5]       \n",
      " collect_score_set_to_current  | [5, 3]       \n",
      " collect_score_list            | [3, 5]       \n",
      " collect_score_set             | [5, 3]       \n",
      "-RECORD 2-------------------------------------\n",
      " item                          | a            \n",
      " score                         | 10           \n",
      " collect_score_list_to_current | [10, 10]     \n",
      " collect_score_set_to_current  | [10]         \n",
      " collect_score_list            | [10, 10, 20] \n",
      " collect_score_set             | [20, 10]     \n",
      "-RECORD 3-------------------------------------\n",
      " item                          | a            \n",
      " score                         | 10           \n",
      " collect_score_list_to_current | [10, 10]     \n",
      " collect_score_set_to_current  | [10]         \n",
      " collect_score_list            | [10, 10, 20] \n",
      " collect_score_set             | [20, 10]     \n",
      "-RECORD 4-------------------------------------\n",
      " item                          | a            \n",
      " score                         | 20           \n",
      " collect_score_list_to_current | [10, 10, 20] \n",
      " collect_score_set_to_current  | [20, 10]     \n",
      " collect_score_list            | [10, 10, 20] \n",
      " collect_score_set             | [20, 10]     \n",
      "\n",
      "== Physical Plan ==\n",
      "Window [collect_list(score#2555L, 0, 0) windowspecdefinition(item#2554, score#2555L ASC NULLS FIRST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS collect_score_list_to_current#2567, collect_set(score#2555L, 0, 0) windowspecdefinition(item#2554, score#2555L ASC NULLS FIRST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS collect_score_set_to_current#2572, collect_list(score#2555L, 0, 0) windowspecdefinition(item#2554, score#2555L ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS collect_score_list#2578, collect_set(score#2555L, 0, 0) windowspecdefinition(item#2554, score#2555L ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS collect_score_set#2585], [item#2554], [score#2555L ASC NULLS FIRST]\n",
      "+- *(1) Sort [item#2554 ASC NULLS FIRST, score#2555L ASC NULLS FIRST], false, 0\n",
      "   +- Exchange hashpartitioning(item#2554, 200)\n",
      "      +- Scan ExistingRDD[item#2554,score#2555L]\n"
     ]
    }
   ],
   "source": [
    "# 15. Perform collect_list, colect set in a wondow\n",
    "data = [\n",
    "    (\"a\",10),\n",
    "    ('a',10),\n",
    "    ('a',20),\n",
    "    ('b',5),\n",
    "    ('b',3)\n",
    "]\n",
    "\n",
    "columns = ['item','score']\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show(n=5)\n",
    "\n",
    "score_in_item_from_unbounded_to_curr = (\n",
    "                W.partitionBy(\"item\")\n",
    "                 .orderBy(\"score\"))\n",
    "score_in_item = (\n",
    "                W.partitionBy('item')\n",
    "                 .orderBy(\"score\")\n",
    "                 .rowsBetween(W.unboundedPreceding, W.unboundedFollowing)\n",
    ")\n",
    "df_window = (\n",
    "    df\n",
    "    .withColumn(\"collect_score_list_to_current\",\n",
    "                F.collect_list(\"score\").over(score_in_item_from_unbounded_to_curr))\n",
    "    .withColumn(\"collect_score_set_to_current\",\n",
    "                F.collect_set(\"score\").over(score_in_item_from_unbounded_to_curr))\n",
    "    .withColumn(\"collect_score_list\",\n",
    "                F.collect_list(\"score\").over(score_in_item)\n",
    "               )\n",
    "    .withColumn(\"collect_score_set\",\n",
    "                F.collect_set(\"score\").over(score_in_item)\n",
    "               )\n",
    ")\n",
    "\n",
    "df_window.show(n=5, vertical=True, truncate=False)\n",
    "\n",
    "df_window.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T09:59:02.938682Z",
     "start_time": "2021-06-19T09:59:02.233251Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|item|score|\n",
      "+----+-----+\n",
      "|   a|   10|\n",
      "|   a|    0|\n",
      "|   a|   20|\n",
      "|   b|    5|\n",
      "|   b|    3|\n",
      "+----+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "-RECORD 0----------------------\n",
      " item               | c        \n",
      " score              | 0        \n",
      " collect_score_list | []       \n",
      "-RECORD 1----------------------\n",
      " item               | b        \n",
      " score              | 5        \n",
      " collect_score_list | [5, 3]   \n",
      "-RECORD 2----------------------\n",
      " item               | b        \n",
      " score              | 3        \n",
      " collect_score_list | [5, 3]   \n",
      "-RECORD 3----------------------\n",
      " item               | a        \n",
      " score              | 10       \n",
      " collect_score_list | [10, 20] \n",
      "-RECORD 4----------------------\n",
      " item               | a        \n",
      " score              | 0        \n",
      " collect_score_list | [10, 20] \n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 15. Perform collect_list with a filter\n",
    "# collect only the score > 0\n",
    "# https://stackoverflow.com/questions/61468705/pyspark-using-collect-list-over-window-with-condition\n",
    "data = [\n",
    "    (\"a\",10),\n",
    "    ('a',0),\n",
    "    ('a',20),\n",
    "    ('b',5),\n",
    "    ('b',3),\n",
    "    ('c',0)\n",
    "]\n",
    "\n",
    "columns = ['item','score']\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show(n=5)\n",
    "\n",
    "item = (\n",
    "        W.partitionBy(\"item\")\n",
    ")\n",
    "\n",
    "\n",
    "df_window = (\n",
    "    df\n",
    "    .withColumn(\"collect_score_list\",\n",
    "                F.collect_list(\n",
    "                    F.when(C(\"score\") > 0, C(\"score\"))\n",
    "                     .otherwise(F.lit(None))\n",
    "                    ).over(item)\n",
    "               )\n",
    "\n",
    ")\n",
    "\n",
    "df_window.show(n=5, vertical=True, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# udf & pandas_udf (5+)\n",
    "\n",
    "* pandas_udf return maximum 2G\n",
    "\n",
    "* https://issues.apache.org/jira/browse/ARROW-1907\n",
    "\n",
    "* [pandas_udf in classmethod, you need to write a new wrapper! which is not easy](https://stackoverflow.com/questions/58170261/how-to-use-pandas-udf-in-class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T09:57:39.981127Z",
     "start_time": "2021-06-19T09:57:06.642Z"
    }
   },
   "outputs": [],
   "source": [
    "# 1. Use Pyspark to send request, get image and store as b64string \n",
    "# https://stackoverflow.com/questions/49353752/use-requests-module-and-return-response-to-pyspark-dataframe\n",
    "\n",
    "data = [\n",
    "    (14431,'https://pic.pimg.tw/happy78/1528543947-685380499_n.jpg'),\n",
    "    (14431,'https://pic.pimg.tw/happy78/1528543947-362759723_n.jpg'),\n",
    "    (14431,'https://pic.pimg.tw/happy78/1528543962-2265924582_n.jpg'),\n",
    "    (67789,'https://pic.pimg.tw/happy78/1528543962-4007835890_n.jpg'),\n",
    "    (67789,'https://pic.pimg.tw/happy78/1528543962-45890_n.jpg')\n",
    "]\n",
    "\n",
    "columns = ['article_id','img_url']\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=columns)\n",
    "print('before')\n",
    "df.show(n=5)\n",
    "\n",
    "@F.udf(returnType=StringType())\n",
    "def get_img_binary(url : str) -> str:\n",
    "    import requests\n",
    "    import base64\n",
    "    resp = requests.get(url)\n",
    "    if resp.status_code == 200:\n",
    "        return base64.encodestring(resp.content)\n",
    "    else:\n",
    "        return None\n",
    "df = (\n",
    "    df.withColumn(\"img_b64_str\", get_img_binary(C(\"img_url\")))\n",
    ")\n",
    "\n",
    "df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T09:57:39.982110Z",
     "start_time": "2021-06-19T09:57:06.647Z"
    }
   },
   "outputs": [],
   "source": [
    "# 2 udf return two column values, e.g. model prediction with label and probability\n",
    "data = [\n",
    "    (1,64),\n",
    "    (2,76),\n",
    "    (3,54),\n",
    "    (4,11),\n",
    "    (5,100),\n",
    "]\n",
    "columns = ['id','features']\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=columns)\n",
    "print(\"Before : \")\n",
    "df.show(n=5)\n",
    "\n",
    "############# sol #################\n",
    "# using Row object to return multiple column\n",
    "from pyspark.sql import Row\n",
    "\n",
    "model_pred = StructType([\n",
    "    StructField(\"category\", StringType(), False),\n",
    "    StructField(\"prob\", T.FloatType(), False)\n",
    "])\n",
    "\n",
    "@F.udf(returnType=model_pred)\n",
    "def model_pred(n):\n",
    "    import random\n",
    "    category = random.choice(['food','env','compose','drink'])\n",
    "    prob = random.random()\n",
    "    return Row('category', 'prob')(category, prob)\n",
    "\n",
    "\n",
    "\n",
    "newDF = df.withColumn(\"pred\", model_pred(df[\"features\"]))\n",
    "\n",
    "print(newDF.dtypes)\n",
    "\n",
    "# newDF = newDF.select(\"id\", \"features\", \"pred.*\")\n",
    "\n",
    "newDF.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T09:57:39.982963Z",
     "start_time": "2021-06-19T09:57:06.651Z"
    }
   },
   "outputs": [],
   "source": [
    "# 3. Use Pyspark to load a tf.keras model\n",
    "# serieslize the model and make prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T09:57:39.983831Z",
     "start_time": "2021-06-19T09:57:06.655Z"
    }
   },
   "outputs": [],
   "source": [
    "# 4. Pandas udf\n",
    "# documentation and concept\n",
    "# user defined function, but vectorlized by Arrow\n",
    "# 2.3.0\n",
    "# https://spark.apache.org/docs/2.3.0/sql-programming-guide.html#pandas-udfs-aka-vectorized-udfs\n",
    "# 3.0 support more!\n",
    "# https://spark.apache.org/docs/3.0.0/sql-pyspark-pandas-with-arrow.html#pandas-udfs-aka-vectorized-udfs\n",
    "\n",
    "\n",
    "# for 2.3.0\n",
    "#  Currently, there are two types of Pandas UDF: Scalar and Grouped Map.\n",
    "#  Input pd.Series, Output pd.Series\n",
    "\n",
    "# Scalar type\n",
    "@F.pandas_udf(returnType=T.LongType())\n",
    "def multiply_func(a, b):\n",
    "    return a * b\n",
    "\n",
    "\n",
    "x = pd.Series([1, 2, 3])\n",
    "\n",
    "df = spark.createDataFrame(pd.DataFrame(x), schema=[\"x\"])\n",
    "df.select(multiply_func(C(\"x\"), C(\"x\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T09:57:39.984705Z",
     "start_time": "2021-06-19T09:57:06.664Z"
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "\n",
    "# # Use pandas_udf to define a Pandas UDF\n",
    "# @pandas_udf('double', PandasUDFType.SCALAR)\n",
    "# # Input/output are both a pandas.Series of doubles\n",
    "\n",
    "# def pandas_plus_one(x):\n",
    "#     return x + 1\n",
    "\n",
    "# df.withColumn('v2', pandas_plus_one(df.x))\n",
    "# df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T09:57:39.985564Z",
     "start_time": "2021-06-19T09:57:06.669Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 5 pandasUDF return a dataframe\n",
    "# like a model prediction\n",
    "# # predict label and probability\n",
    "# A grouped map UDF defines transformation:\n",
    "# A pandas.DataFrame -> A pandas.DataFrame The returnType \n",
    "# should be a StructType describing the schema of the returned pandas.DataFrame.\n",
    "# The length of the returned pandas.DataFrame can be arbitrary and the columns must be indexed so that their position matches the corresponding field in the schema.\n",
    "# Grouped map UDFs are used with pyspark.sql.GroupedData.apply().\n",
    "# https://databricks.com/blog/2017/10/30/introducing-vectorized-udfs-for-pyspark.html\n",
    "\n",
    "# We can use BucketID for this ID\n",
    "\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "\n",
    "df = spark.createDataFrame(\n",
    "    [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)],\n",
    "    (\"ID\", \"v\"))\n",
    "\n",
    "df.show()\n",
    "########## case 1 ##############\n",
    "\n",
    "@pandas_udf(\"id long, v double, n_rows long\", PandasUDFType.GROUPED_MAP)\n",
    "def substract_mean(pdf):\n",
    "    # pdf is a pandas.DataFrame\n",
    "    n_rows = len(pdf)\n",
    "    v = pdf.v\n",
    "    return pdf.assign(\n",
    "        v=v - v.mean(),\n",
    "        n_rows=n_rows\n",
    "    )\n",
    "\n",
    "df.groupby(\"ID\").apply(substract_mean).show()\n",
    "\n",
    "######### case 2 ##############\n",
    "\n",
    "@pandas_udf(\"ID long, v double, new_col_1 double, new_col_2 double\", PandasUDFType.GROUPED_MAP)\n",
    "def get_more_col(pdf):\n",
    "    # pdf is a pandas.DataFrame\n",
    "    import random\n",
    "    n_counts = len(pdf)\n",
    "    \n",
    "    return pdf.assign(\n",
    "        new_col_1 = [i for i in range(n_counts)],\n",
    "        new_col_2 = [random.random() for i in range(n_counts)]\n",
    "    )\n",
    "# sdf groupby its bucket_id and apply\n",
    "\n",
    "df.groupby(\"ID\").apply(get_more_col).show()\n",
    "\n",
    "\n",
    "######## case 3 #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T09:57:39.986422Z",
     "start_time": "2021-06-19T09:57:06.674Z"
    }
   },
   "outputs": [],
   "source": [
    "s = pd.Series([5,5,7,7,8])\n",
    "for row in s:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T09:57:39.987298Z",
     "start_time": "2021-06-19T09:57:06.678Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 6 pandas udf return Null used for sending request to get images\n",
    "# return string for pandas_udf\n",
    "# https://stackoverflow.com/questions/65694026/spark-exception-error-using-pandas-udf-with-logical-statement\n",
    "from typing import Union\n",
    "import requests\n",
    "import base64\n",
    "\n",
    "\n",
    "@F.udf(returnType=StringType())\n",
    "def get_img_binary(url : str) -> str:\n",
    "    import requests\n",
    "    import base64\n",
    "    resp = requests.get(url, timeout=10)\n",
    "    if resp.status_code == 200:\n",
    "        return base64.encodestring(resp.content)\n",
    "    else:\n",
    "        return None\n",
    "# df = (\n",
    "#     df.withColumn(\"img_b64_str\", get_img_binary(C(\"img_url\")))\n",
    "# )\n",
    "\n",
    "\n",
    "# If you wanna dealing with a lot of data, use the rep\n",
    "# @F.pandas_udf('string', PandasUDFType.SCALAR)\n",
    "def download_img_to_b64_pd(img_url : pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    get image in the response and save to base64 string\n",
    "    data type flow : \n",
    "    resp.content - (bytes) \n",
    "    -> base64.encode - (bytes) \n",
    "    -> decode('utf-8') - str\n",
    "    \"\"\"\n",
    "    b64_str_list = []\n",
    "    for link in img_url:   \n",
    "        try:\n",
    "            resp = requests.get(link, timeout=10)\n",
    "            if resp.status_code == 200:\n",
    "                b64_str_list.append(base64.encodebytes(resp.content).decode('utf-8'))\n",
    "            else:\n",
    "                b64_str_list.append(None)\n",
    "        except Exception as e:\n",
    "            b64_str_list.append(None)\n",
    "    return pd.Series(b64_str_list)\n",
    "\n",
    "\n",
    "\n",
    "data = [\n",
    "    (14431,'https://pic.pimg.tw/happy78/1528543947-685380499_n.jpg'),\n",
    "    (14431,'https://pic.pimg.tw/happy78/1528543947-3623_n.jpg'), # This one will be not found\n",
    "    (14431,'https://pic.pimg.tw/happy78/1528543962-2265924582_n.jpg'),\n",
    "    (67789,'https://pic.pimg.tw/happy78/1528543962-4007835890_n.jpg'),\n",
    "    (67789,'https://pic.pimg.tw/happy78/1528543962-45890_n.jpg')\n",
    "]\n",
    "\n",
    "columns = ['article_id','img_url']\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=columns)\n",
    "# df.show(vertical=True, truncate=False)\n",
    "\n",
    "#             .withColumn(\n",
    "#                 \"bucket_id\",\n",
    "#                 F.udf(self.simple_random, returnType=\"integer\")(\"bucket_size\"),\n",
    "#             )\n",
    "\n",
    "df = (\n",
    "    df\\\n",
    "#     .withColumn(\"img_b64_str\", download_img_to_b64_pd(C(\"img_url\")))\n",
    "    .withColumn(\"img_b64_str\",F.pandas_udf(download_img_to_b64_pd, \"string\",PandasUDFType.SCALAR)(\"img_url\"))\n",
    "    .filter(C(\"img_b64_str\").isNotNull())\n",
    ")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T09:57:39.988158Z",
     "start_time": "2021-06-19T09:57:06.683Z"
    }
   },
   "outputs": [],
   "source": [
    "def my_range(n):\n",
    "    x = 0\n",
    "    while True:\n",
    "        if x < n:\n",
    "            yield x\n",
    "            x += 1 \n",
    "        else:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T09:57:39.988945Z",
     "start_time": "2021-06-19T09:57:06.697Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in my_range(10):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T09:57:39.989871Z",
     "start_time": "2021-06-19T09:57:06.715Z"
    }
   },
   "outputs": [],
   "source": [
    "# 7 pandas udf using generator\n",
    "# due to pd.Series return should under 2G\n",
    "# we're using another apporoach (GroupMap)\n",
    "\n",
    "def get_img_b64_generator(img_url : pd.Series):\n",
    "    '''\n",
    "    we can move this out of the udf\n",
    "    '''\n",
    "    for link in img_url:\n",
    "        try:\n",
    "            resp = requests.get(link, timeout=10)\n",
    "            if resp.status_code == 200:\n",
    "                yield base64.encodebytes(resp.content).decode('utf-8')\n",
    "            else:\n",
    "                yield None\n",
    "        except Exception as e:\n",
    "            yield None\n",
    "\n",
    "            \n",
    "return_type = \"article_id long, img_url string\"\n",
    "@F.pandas_udf(return_type,\n",
    "            F.PandasUDFType.GROUPED_MAP)\n",
    "def download_img_to_b64(df_with_url : pd.DataFrame) -> pd.DataFrame:\n",
    "    import requests\n",
    "    import base64\n",
    "    \n",
    "    img_url_series = df_with_img_b64.img_url\n",
    "    \n",
    "    # Using generator to avoid big series OOM/Serilization error\n",
    "    \n",
    "    return df_with_url.assign(\n",
    "        img_b64_str = img_type_series,\n",
    "        )\n",
    "\n",
    "data = [\n",
    "    (14431,'https://pic.pimg.tw/happy78/1528543947-685380499_n.jpg'),\n",
    "    (14431,'https://pic.pimg.tw/happy78/1528543947-3623_n.jpg'), # This one will be not found\n",
    "    (14431,'https://pic.pimg.tw/happy78/1528543962-2265924582_n.jpg'),\n",
    "    (67789,'https://pic.pimg.tw/happy78/1528543962-4007835890_n.jpg'),\n",
    "    (67789,'https://pic.pimg.tw/happy78/1528543962-45890_n.jpg')\n",
    "]\n",
    "\n",
    "columns = ['article_id','img_url']\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=columns)\n",
    "pdf = df.toPandas()\n",
    "pdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# broadcasting (2+)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T09:57:39.990697Z",
     "start_time": "2021-06-19T09:57:06.740Z"
    }
   },
   "outputs": [],
   "source": [
    "# 1\n",
    "# broadcast the dictionary to spark \n",
    "# (which is a way that enhance multi-processing cross machine using your python code)\n",
    "# the broadcast variable should be serializable\n",
    "\n",
    "states = {\"NY\":\"New York\", \"CA\":\"California\", \"FL\":\"Florida\"}\n",
    "broadcastStates = spark.sparkContext.broadcast(states)\n",
    "\n",
    "data = [(\"James\",\"Smith\",\"USA\",\"CA\"),\n",
    "    (\"Michael\",\"Rose\",\"USA\",\"NY\"),\n",
    "    (\"Robert\",\"Williams\",\"USA\",\"CA\"),\n",
    "    (\"Maria\",\"Jones\",\"USA\",\"FL\")\n",
    "  ]\n",
    "\n",
    "columns = [\"firstname\",\"lastname\",\"country\",\"state\"]\n",
    "df = spark.createDataFrame(data = data, schema = columns)\n",
    "df.show(n=5)\n",
    "\n",
    "def state_convert(code):\n",
    "    return broadcastStates.value[code]\n",
    "\n",
    "# case 1, using rdd\n",
    "result_rdd = df.rdd.map(lambda x: (x[0],x[1],x[2],state_convert(x[3]))).toDF(columns)\n",
    "result_rdd.show(n=5)\n",
    "\n",
    "\n",
    "# case 2, using pdf\n",
    "\n",
    "@F.udf(returnType=StringType())\n",
    "def state_convert_udf(code : str) -> str:\n",
    "    return broadcastStates.value[code]\n",
    "\n",
    "result_df = (\n",
    "    df.withColumn(\"converted_state\", state_convert_udf(C(\"state\")))\n",
    ")\n",
    "\n",
    "result_df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T09:57:39.991578Z",
     "start_time": "2021-06-19T09:57:06.751Z"
    }
   },
   "outputs": [],
   "source": [
    "# 2\n",
    "# Knowing broacsting object\n",
    "\n",
    "# broadcast the dictionary to spark \n",
    "# (which is a way that enhance multi-processing cross machine using your python code)\n",
    "# the broadcast variable should be serializable\n",
    "# https://spark.apache.org/docs/2.3.3/api/python/_modules/pyspark/broadcast.html\n",
    "states = {\"NY\":\"New York\", \"CA\":\"California\", \"FL\":\"Florida\"}\n",
    "broadcastStates = spark.sparkContext.broadcast(states)\n",
    "print(type(broadcastStates), dir(broadcastStates))\n",
    "\n",
    "# value to access the object\n",
    "broadcastStates.value, type(broadcastStates.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T09:57:39.992416Z",
     "start_time": "2021-06-19T09:57:06.760Z"
    }
   },
   "outputs": [],
   "source": [
    "# braordcast join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export dataframe(2+)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T09:57:39.993332Z",
     "start_time": "2021-06-19T09:57:06.770Z"
    }
   },
   "outputs": [],
   "source": [
    "# 1\n",
    "# write dataframe to jsonl format\n",
    "# https://stackoverflow.com/questions/43269244/pyspark-dataframe-write-to-single-json-file-with-specific-name\n",
    "# https://sparkbyexamples.com/pyspark/pyspark-read-json-file-into-dataframe/\n",
    "data = [\n",
    "    (14431,'https://pic.pimg.tw/happy78/1528543947-685380499_n.jpg'),\n",
    "    (14431,'https://pic.pimg.tw/happy78/1528543947-362759723_n.jpg'),\n",
    "    (14431,'https://pic.pimg.tw/happy78/1528543962-2265924582_n.jpg'),\n",
    "    (67789,'https://pic.pimg.tw/happy78/1528543962-4007835890_n.jpg'),\n",
    "    (67789,'https://pic.pimg.tw/happy78/1528543962-45890_n.jpg')\n",
    "]\n",
    "\n",
    "columns = ['article_id','img_url']\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "fname_folder = join('output','jsonl_format_folder.json')\n",
    "# This one will creat a folder contains part file for better multiple worker IO\n",
    "df.coalesce(1).write.format('json').save(fname_folder, mode='overwrite')\n",
    "df_new = spark.read.json(fname_folder)\n",
    "df_new.show(n=10)\n",
    "# However, if you wanna save it in a single file, use pandas\n",
    "fname = join('output','jsonl_format.json')\n",
    "# df.toPandas().to_json('path/file_name.json', orient='records', force_ascii=False, lines=True)\n",
    "df.toPandas().to_json(fname, orient='records',force_ascii=False,lines=True)\n",
    "df_new_pd = pd.read_json(fname,orient='records',lines=True)\n",
    "df_new_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T09:57:39.994211Z",
     "start_time": "2021-06-19T09:57:06.776Z"
    }
   },
   "outputs": [],
   "source": [
    "# 2 write parquet by date parittion\n",
    "\n",
    "\n",
    "# 1\n",
    "# write dataframe to jsonl format\n",
    "# https://sparkbyexamples.com/pyspark/pyspark-read-and-write-parquet-file/\n",
    "data_d1 = [\n",
    "    (14431,20210224,'https://pic.pimg.tw/happy78/1528543947-685380499_n.jpg'),\n",
    "    (14431,20210224,'https://pic.pimg.tw/happy78/1528543947-362759723_n.jpg'),\n",
    "    (14431,20210224,'https://pic.pimg.tw/happy78/1528543962-2265924582_n.jpg'),\n",
    "    (67789,20210224,'https://pic.pimg.tw/happy78/1528543962-4007835890_n.jpg'),\n",
    "    (67789,20210224,'https://pic.pimg.tw/happy78/1528543962-45890_n.jpg')\n",
    "]\n",
    "\n",
    "data_d2 = [\n",
    "    (86481,20210225,'https://pic.pimg.tw/happy78/1528543947-685380499_n.jpg'),\n",
    "    (45213,20210225,'https://pic.pimg.tw/happy78/1528543947-362759723_n.jpg'),\n",
    "    (24561,20210225,'https://pic.pimg.tw/happy78/1528543962-2265924582_n.jpg'),\n",
    "    (75371,20210225,'https://pic.pimg.tw/happy78/1528543962-4007835890_n.jpg'),\n",
    "    (25691,20210225,'https://pic.pimg.tw/happy78/1528543962-45890_n.jpg')\n",
    "]\n",
    "\n",
    "data_d3 = [\n",
    "    (7861,20210304,'https://pic.pimg.tw/happy78/1528543947-685380499_n.jpg'),\n",
    "    (45213,20210304,'https://pic.pimg.tw/happy78/1528543947-362759723_n.jpg'),\n",
    "    (1111,20210304,'https://pic.pimg.tw/happy78/1528543962-2265924582_n.jpg'),\n",
    "    (76661,20210304,'https://pic.pimg.tw/happy78/1528543962-4007835890_n.jpg'),\n",
    "    (8888,20210304,'https://pic.pimg.tw/happy78/1528543962-45890_n.jpg')\n",
    "]\n",
    "\n",
    "\n",
    "columns = ['article_id','date','img_url']\n",
    "\n",
    "df_d1 = spark.createDataFrame(data_d1, columns)\n",
    "df_d2 = spark.createDataFrame(data_d2, columns)\n",
    "df_d3 = spark.createDataFrame(data_d3, columns)\n",
    "\n",
    "for df in [df_d1, df_d2, df_d3]:\n",
    "    df.show(n=5)\n",
    "\n",
    "# save it\n",
    "parquet_fname = join(\"output\",\"save_by_date_partition.parquet\")\n",
    "for df in [df_d1, df_d2, df_d3]:\n",
    "    df.write.parquet(parquet_fname, mode=\"overwrite\", partitionBy=\"date\")\n",
    "\n",
    "# read by date range\n",
    "start_date = 20210224\n",
    "end_date = 20210301\n",
    "# support int and daterange, string might be problem\n",
    "\n",
    "new_df = spark.read.parquet(parquet_fname)\\\n",
    "         .where(C(\"date\").between(start_date, end_date))\n",
    "\n",
    "new_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T09:57:39.995082Z",
     "start_time": "2021-06-19T09:57:06.781Z"
    }
   },
   "outputs": [],
   "source": [
    "# read it\n",
    "start_date = 20210224\n",
    "end_date = 20210304\n",
    "\n",
    "# df.filter(df.year >= myYear)\n",
    "new_df = spark.read.parquet(parquet_fname)\n",
    "date_range_cond = (new_df.date >= start_date) & (new_df.date <= end_date)\n",
    "new_df.filter(date_range_cond).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other (3+)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T09:57:39.995990Z",
     "start_time": "2021-06-19T09:57:06.788Z"
    }
   },
   "outputs": [],
   "source": [
    "# 1\n",
    "# rank the food category popularity by store_name but crossed and rotated\n",
    "\n",
    "\n",
    "# create a mix ranking number\n",
    "# create a popularity_rank_score in each store_name\n",
    "    # Top popular in each store_name -> score 0\n",
    "    # Second popular in each store_name -> score 0.1\n",
    "# Add row number in each store_name, food_category -> cat_rank\n",
    "# Create category_popularity_mix_rank_score = cat_rank + popularity_rank_score\n",
    "# Sort the category_popularity_mix_rank_score by store_name\n",
    "\n",
    "data = [\n",
    "    (\"hotpop\",\"Meat\",3,),\n",
    "    (\"hotpop\",\"Meat\",3),\n",
    "    (\"hotpop\",\"Meat\",3),\n",
    "    (\"hotpop\",\"Vegetable\",2),\n",
    "    (\"hotpop\",\"Vegetable\",2),\n",
    "    (\"branch\",\"Fried food\",1),\n",
    "    (\"branch\",\"Dessert\",1),\n",
    "  ]\n",
    "\n",
    "columns = [\"store_name\",\"food_category\",\"food_category_popularity\"]\n",
    "df = spark.createDataFrame(data = data, schema = columns)\n",
    "\n",
    "print('before')\n",
    "df.show(n=10)\n",
    "\n",
    "################# sol #######################\n",
    "store_cat_pop_rank_score = F.when(C(\"store_cat_pop_rank\") == 1, 0)\\\n",
    "                            .when(C(\"store_cat_pop_rank\") == 2, 0.1)\n",
    "\n",
    "\n",
    "window_sotre_cat_pop = W.partitionBy('store_name').orderBy(C(\"food_category_popularity\").desc())\n",
    "window_sotre_cat = W.partitionBy(['store_name','food_category']).orderBy(C(\"food_category\"))\n",
    "window_sotr_cat_mix_rank = W.partitionBy(['store_name']).orderBy(C(\"mix_cat_pop_rank_score\"))\n",
    "df = (\n",
    "    df.withColumn(\"food_cat_pop_score\", 100 * C(\"food_category_popularity\") + 20 * F.randn(seed=42))\\\n",
    "      .withColumn(\"cat_idx\", F.row_number().over(window_sotre_cat))\\\n",
    "      .withColumn(\"store_cat_pop_rank\", F.dense_rank().over(window_sotre_cat_pop))\n",
    "      .withColumn(\"store_cat_pop_rank_score\", store_cat_pop_rank_score)\\\n",
    "      .withColumn(\"mix_cat_pop_rank_score\", C(\"cat_idx\") + C(\"store_cat_pop_rank_score\"))\\\n",
    "      .withColumn(\"mix_cat_pop_rank\", F.row_number().over(window_sotr_cat_mix_rank))\\\n",
    ")\n",
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T09:57:39.996885Z",
     "start_time": "2021-06-19T09:57:06.904Z"
    }
   },
   "outputs": [],
   "source": [
    "# 2\n",
    "# create a food category popularity score\n",
    "# rank the food category popularity score but crossed and rotated\n",
    "\n",
    "\n",
    "\n",
    "# create a mix ranking number\n",
    "# create a popularity_rank_score in each store_name\n",
    "    # Top popular in each store_name -> score 0\n",
    "    # Second popular in each store_name -> score 0.1\n",
    "# Add row number in each store_name, food_category -> cat_rank\n",
    "# Create category_popularity_mix_rank_score = cat_rank + popularity_rank_score\n",
    "# Sort the category_popularity_mix_rank_score by store_name\n",
    "\n",
    "data = [\n",
    "    (\"hotpop\",\"Meat\",3,),\n",
    "    (\"hotpop\",\"Meat\",3),\n",
    "    (\"hotpop\",\"Meat\",3),\n",
    "    (\"hotpop\",\"Vegetable\",2),\n",
    "    (\"hotpop\",\"Vegetable\",2),\n",
    "    (\"branch\",\"Fried food\",1),\n",
    "    (\"branch\",\"Dessert\",1),\n",
    "  ]\n",
    "\n",
    "columns = [\"store_name\",\"food_category\",\"food_category_popularity\"]\n",
    "df = spark.createDataFrame(data = data, schema = columns)\n",
    "\n",
    "print('before')\n",
    "df.show(n=10)\n",
    "\n",
    "################# sol 1 #######################\n",
    "store_cat_pop_rank_score = F.when(C(\"store_cat_pop_rank\") == 1, 0)\\\n",
    "                            .when(C(\"store_cat_pop_rank\") == 2, 0.1)\n",
    "\n",
    "\n",
    "window_sotre_cat_pop = W.partitionBy('store_name').orderBy(C(\"food_category_popularity\").desc())\n",
    "window_sotre_cat = W.partitionBy(['store_name','food_category']).orderBy(C(\"food_cat_pop_score\").desc())\n",
    "window_sotr_cat_mix_rank = W.partitionBy(['store_name']).orderBy(C(\"mix_cat_pop_rank_score\"))\n",
    "df = (\n",
    "    df.withColumn(\"food_cat_pop_score\",\n",
    "                  F.round(100 * C(\"food_category_popularity\") + 20 * F.randn(seed=42))\n",
    "                 )\\\n",
    "      .withColumn(\"cat_idx\", F.row_number().over(window_sotre_cat))\\\n",
    "      .withColumn(\"store_cat_pop_rank\", F.dense_rank().over(window_sotre_cat_pop))\n",
    "      .withColumn(\"store_cat_pop_rank_score\", store_cat_pop_rank_score)\\\n",
    "      .withColumn(\"mix_cat_pop_rank_score\", C(\"cat_idx\") + C(\"store_cat_pop_rank_score\"))\\\n",
    "      .withColumn(\"mix_cat_pop_rank\", F.row_number().over(window_sotr_cat_mix_rank))\\\n",
    ")\n",
    "\n",
    "print('sol - 1')\n",
    "display(df.toPandas())\n",
    "\n",
    "############### sol 2 ########################\n",
    "\n",
    "# Use another way to sort it\n",
    "# sort by cat_idx and store_cat_pop_rank\n",
    "\n",
    "\n",
    "\n",
    "category_id_in_poi = W.partitionBy(['store_name','food_category']).orderBy(C(\"food_category\"))\n",
    "category_popularity_rank_in_poi = W.partitionBy('store_name').orderBy(C(\"food_category_popularity\").desc())\n",
    "window_sotr_cat_mix_rank = W.partitionBy(['store_name']).orderBy(\n",
    "    C(\"category_id_in_poi\"),\n",
    "    C(\"category_popularity_rank_in_poi\")\n",
    ")\n",
    "df_sol_2 = (\n",
    "    df.withColumn(\"food_cat_pop_score\", 100 * C(\"food_category_popularity\") + 20 * F.randn(seed=42))\\\n",
    "      .withColumn(\"category_id_in_poi\", F.row_number().over(category_id_in_poi))\\\n",
    "      .withColumn(\"category_popularity_rank_in_poi\", F.dense_rank().over(category_popularity_rank_in_poi))\n",
    "      .withColumn(\"staggered_rank\", F.row_number().over(window_sotr_cat_mix_rank))\\\n",
    ")\n",
    "\n",
    "print('so1 - 2')\n",
    "\n",
    "display(df_sol_2.toPandas())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T09:57:39.997769Z",
     "start_time": "2021-06-19T09:57:06.914Z"
    }
   },
   "outputs": [],
   "source": [
    "# 3 Knowing the functions of dataframe operation \n",
    "\n",
    "dir(F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T09:57:39.998620Z",
     "start_time": "2021-06-19T09:57:06.920Z"
    }
   },
   "outputs": [],
   "source": [
    "# 4 explode_outer\n",
    "# https://spark.apache.org/docs/2.3.0/api/python/pyspark.sql.html#pyspark.sql.GroupedData.apply\n",
    "\n",
    "# return a new row for each element in the given array or map\n",
    "# Unlike explode, if the array/map is null or empty\n",
    "# the null is produced\n",
    "\n",
    "df = spark.createDataFrame(\n",
    "    [\n",
    "        (1, [\"foo\",\"bar\"], {\"x\" : 1.0}),\n",
    "        (2, [], {}),\n",
    "        (3, None, None)\n",
    "    ],\n",
    "    (\"id\", \"an_array\",\"a_map\")\n",
    ")\n",
    "\n",
    "df.show()\n",
    "\n",
    "df.select(\"id\", F.explode_outer(C(\"a_map\"))).show()\n",
    "\n",
    "df.select(\"id\", F.explode(C(\"a_map\"))).show() # null thing will be nothing\n",
    "\n",
    "df.select(\"id\", F.explode_outer(C(\"an_array\"))).show()\n",
    "\n",
    "df.select(\"id\", F.explode(C(\"an_array\"))).show() # null thing will be nothing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T09:57:39.999365Z",
     "start_time": "2021-06-19T09:57:06.932Z"
    }
   },
   "outputs": [],
   "source": [
    "# 4 iterate your dataframe row by row\n",
    "# you will not use it in production\n",
    "# but it is useful when debugging and develop your algorithm\n",
    "\n",
    "df = spark.createDataFrame(\n",
    "    [\n",
    "        (1, [\"foo\",\"bar\"], {\"x\" : 1.0}),\n",
    "        (2, [], {}),\n",
    "        (3, None, None)\n",
    "    ],\n",
    "    (\"id\", \"an_array\",\"a_map\")\n",
    ")\n",
    "\n",
    "\n",
    "print('default you will get row object')\n",
    "print('you can convert row into dataframe')\n",
    "print('sometimes you might inidcate schema')\n",
    "print('\\n\\n\\n')\n",
    "    \n",
    "for row_idx, row in enumerate(df.rdd.toLocalIterator()):\n",
    "    print(row_idx, type(row))\n",
    "    slice_sdf = spark.createDataFrame([row],schema=df.schema)\n",
    "    slice_sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T09:57:40.000164Z",
     "start_time": "2021-06-19T09:57:06.947Z"
    }
   },
   "outputs": [],
   "source": [
    "# cache and persistant\n",
    "\n",
    "# cache and persist, spark provides an optimization mechanism to astore thre indermediate compurtation \n",
    "# of a Spark DataFrame so they can be resued in subsequent actions\n",
    "\n",
    "# persist -> each node stores it's partitioned data in memory and reuse them in other actions on the datasaet\n",
    "\n",
    "# Cost efficient - Spark computations are very expensive hence reusing the computations are used to save cost\n",
    "\n",
    "# Time efficient - Reusing the repeated computations save lots of time.\n",
    "\n",
    "\n",
    "# SparkDataFrame.cache() storage level `MEMORY_AND_DISK`\n",
    "# RDD.cache() storage level `MEMORY_ONLY`\n",
    "\n",
    "\n",
    "data = [\n",
    "    (\"hotpop\",\"Meat\",3,),\n",
    "    (\"hotpop\",\"Meat\",3),\n",
    "    (\"hotpop\",\"Meat\",3),\n",
    "    (\"hotpop\",\"Vegetable\",2),\n",
    "    (\"hotpop\",\"Vegetable\",2),\n",
    "    (\"branch\",\"Fried food\",1),\n",
    "    (\"branch\",\"Dessert\",1),\n",
    "  ]\n",
    "\n",
    "columns = [\"store_name\",\"food_category\",\"food_category_popularity\"]\n",
    "\n",
    "# first df\n",
    "df = spark.createDataFrame(data = data, schema = columns)\n",
    "print('df1', df.explain())\n",
    "# second stage\n",
    "\n",
    "df2 = df.where(C(\"store_name\") == \"branch\").cache()\n",
    "\n",
    "print('df2', df2.explain())\n",
    "\n",
    "# third stage\n",
    "df3 = df2.where(\n",
    "    C(\"food_category\") == \"Dessert\"\n",
    ")\n",
    "\n",
    "print('df3', df3.explain())\n",
    "\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pixlake",
   "language": "python",
   "name": "pixlake"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "245px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
