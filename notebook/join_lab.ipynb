{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-03T01:44:04.154245Z",
     "start_time": "2022-03-03T01:44:04.132043Z"
    }
   },
   "outputs": [],
   "source": [
    "# env : pixlake\n",
    "# we focuing on pyspark dataframe processing\n",
    "# documentation https://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-03T01:44:04.609501Z",
     "start_time": "2022-03-03T01:44:04.590405Z"
    }
   },
   "outputs": [],
   "source": [
    "# make you auto compeletion faster\n",
    "# https://stackoverflow.com/questions/40536560/ipython-and-jupyter-autocomplete-not-working\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-03T01:44:05.200605Z",
     "start_time": "2022-03-03T01:44:05.183805Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have pyspark version :  ['spark-2.3', 'spark-3.1.2-bin-hadoop2.7', 'spark-3.0', 'spark-3.0.1-bin-hadoop2.7', 'spark-2.3.4-bin-hadoop2.7', 'spark-2.4.7-bin-hadoop2.7', 'spark-2.4', 'spark-3.1']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "def get_workstation_spark_path(where_are_you : str) -> str:\n",
    "    if where_are_you == 'titan':\n",
    "        return '/home/data/ryanchao2012/lib'\n",
    "    elif where_are_you == 'thor':\n",
    "        return '/opt/spark/versions'\n",
    "    else:\n",
    "        raise ValueError(\"wrong work station name\")\n",
    "\n",
    "spark_path = get_workstation_spark_path('thor')\n",
    "\n",
    "print('You have pyspark version : ', os.listdir(spark_path))\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "# spark-2.3, spark-2.4\n",
    "# os.environ['SPARK_HOME'] = os.path.join(spark_path,'spark-2.3')\n",
    "\n",
    "# use spakr 3.1\n",
    "\n",
    "os.environ['SPARK_HOME'] = '/opt/spark/versions/spark-3.1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-03T01:44:06.814702Z",
     "start_time": "2022-03-03T01:44:06.794686Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/opt/spark/versions/spark-3.1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ['SPARK_HOME']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-03T01:44:07.478252Z",
     "start_time": "2022-03-03T01:44:07.173327Z"
    }
   },
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession as Session\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark import SparkConf as Conf\n",
    "from pyspark.sql import functions as F, Window as W, types as T\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "C = F.col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-03T01:44:08.486383Z",
     "start_time": "2022-03-03T01:44:08.469917Z"
    }
   },
   "outputs": [],
   "source": [
    "conf = (Conf()\n",
    "    .set('spark.sql.sources.partitionOverwriteMode', 'dynamic')\n",
    "    .set('spark.driver.memory', '10g')\n",
    "    .set('spark.driver.maxResultSize', '8g')\n",
    "   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-03T01:44:12.282310Z",
     "start_time": "2022-03-03T01:44:08.771807Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/03 09:44:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = (Session\n",
    "     .builder\n",
    "     .appName('pyspark-challenge')\n",
    "     .master('local[10]')\n",
    "     .config(conf=conf)\n",
    "     .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-03T01:44:12.300335Z",
     "start_time": "2022-03-03T01:44:12.284831Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://thor:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[10]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-challenge</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f53713d1520>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Cross join within a partition (PN2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-26T10:29:22.397620Z",
     "start_time": "2022-02-26T10:29:17.683682Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0----------------------------\n",
      " addr       | 台北市信義區信義路20號 \n",
      " poi_name_A | 肉多多火鍋             \n",
      " poi_name_B | 文章牛肉湯台北信義店   \n",
      "-RECORD 1----------------------------\n",
      " addr       | 台北市信義區信義路20號 \n",
      " poi_name_A | 肉多多火鍋             \n",
      " poi_name_B | 7-11                   \n",
      "-RECORD 2----------------------------\n",
      " addr       | 台北市信義區信義路20號 \n",
      " poi_name_A | 文章牛肉湯台北信義店   \n",
      " poi_name_B | 肉多多火鍋             \n",
      "-RECORD 3----------------------------\n",
      " addr       | 台北市信義區信義路20號 \n",
      " poi_name_A | 文章牛肉湯台北信義店   \n",
      " poi_name_B | 7-11                   \n",
      "-RECORD 4----------------------------\n",
      " addr       | 台北市信義區信義路20號 \n",
      " poi_name_A | 7-11                   \n",
      " poi_name_B | 肉多多火鍋             \n",
      "-RECORD 5----------------------------\n",
      " addr       | 台北市信義區信義路20號 \n",
      " poi_name_A | 7-11                   \n",
      " poi_name_B | 文章牛肉湯台北信義店   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cross join within a partition (or create pairs in partition)\n",
    "# Aka Permutation N 2 in partition\n",
    "\n",
    "# perform cross join in a window?\n",
    "# just N vs N inner join =)\n",
    "# https://stackoverflow.com/questions/53630342/sparksql-pyspark-crossjoin-over-dimension-for-a-specific-window\n",
    "\n",
    "# Permutation (3 2) = 6\n",
    "\n",
    "data = [\n",
    "    ('台北市信義區信義路20號','肉多多火鍋'),\n",
    "    ('台北市信義區信義路20號','文章牛肉湯台北信義店'),\n",
    "    ('台北市信義區信義路20號','7-11'),\n",
    "    ('台南市安平區安平路590號','文章牛肉湯總店'),\n",
    "]\n",
    "\n",
    "poi_sdf = spark.createDataFrame(data, ['addr','poi_name'])\n",
    "\n",
    "(\n",
    "     poi_sdf\n",
    "    .withColumnRenamed('poi_name','poi_name_A')\n",
    "    .join(\n",
    "        poi_sdf\n",
    "        .select(\n",
    "            'addr',\n",
    "            C('poi_name').alias('poi_name_B'),\n",
    "        ),\n",
    "        on=['addr']\n",
    "    )\n",
    "    .where(C(\"poi_name_A\") != C(\"poi_name_B\"))\n",
    ").show(truncate=False, vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join at least match 2 tags\n",
    "\n",
    "* Need to avoid `CartesianProduct` - If you have a big / small sdf, you can use broadcast join to get `BroadcastNestedLoopJoin`\n",
    "\n",
    "* According to your join expression, spark will detect your join types(equal join, non-equal join)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `F.size(F.array_intersect(C(\"a_tags\"),C(\"b_tags\"))) >= 2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-26T10:29:22.595931Z",
     "start_time": "2022-02-26T10:29:22.400609Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "CartesianProduct (size(array_intersect(a_tags#33, b_tags#37), true) >= 2)\n",
      ":- *(1) Project [article_id#28 AS a_article_id#32, tags#29 AS a_tags#33]\n",
      ":  +- *(1) Scan ExistingRDD[article_id#28,tags#29]\n",
      "+- *(2) Project [article_id#28 AS b_article_id#36, tags#29 AS b_tags#37]\n",
      "   +- *(2) Scan ExistingRDD[article_id#28,tags#29]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cross join within a partition (or create pairs in partition)\n",
    "# Aka Permutation N 2 in partition\n",
    "\n",
    "# perform cross join in a window?\n",
    "# just N vs N inner join =)\n",
    "# https://stackoverflow.com/questions/53630342/sparksql-pyspark-crossjoin-over-dimension-for-a-specific-window\n",
    "\n",
    "# Permutation (3 2) = 6\n",
    "\n",
    "data = [\n",
    "    ('a1',['火鍋','台北','麻辣鍋']),\n",
    "    ('a2',['火鍋','台北','火烤兩吃']),\n",
    "    ('a3',['火烤兩吃','台北','鮮魚湯']),\n",
    "    ('a4',['台北','網美咖啡廳'])\n",
    "]\n",
    "\n",
    "feature_sdf = spark.createDataFrame(data, ['article_id','tags'])\n",
    "\n",
    "at_least_2_matches : C = (\n",
    "    F.size(F.array_intersect(C(\"a_tags\"),C(\"b_tags\"))) >= 2\n",
    ")\n",
    "    \n",
    "joined_sdf = (\n",
    "    feature_sdf\n",
    "    .select(\n",
    "        C(\"article_id\").alias('a_article_id'),\n",
    "        C(\"tags\").alias('a_tags')\n",
    "    )\n",
    "    .join(\n",
    "        (\n",
    "            feature_sdf\n",
    "            .select(\n",
    "            C(\"article_id\").alias('b_article_id'),\n",
    "            C(\"tags\").alias('b_tags')\n",
    "            )\n",
    "        ),\n",
    "        on=at_least_2_matches\n",
    "    )\n",
    ")\n",
    "\n",
    "joined_sdf.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-26T10:29:22.663967Z",
     "start_time": "2022-02-26T10:29:22.598131Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "BroadcastNestedLoopJoin BuildRight, Inner, (size(array_intersect(a_tags#49, b_tags#53), true) >= 2)\n",
      ":- *(1) Project [article_id#28 AS a_article_id#48, tags#29 AS a_tags#49]\n",
      ":  +- *(1) Scan ExistingRDD[article_id#28,tags#29]\n",
      "+- BroadcastExchange IdentityBroadcastMode, [id=#103]\n",
      "   +- *(2) Project [article_id#28 AS b_article_id#52, tags#29 AS b_tags#53]\n",
      "      +- *(2) Scan ExistingRDD[article_id#28,tags#29]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "joined_sdf = (\n",
    "    feature_sdf\n",
    "    .select(\n",
    "        C(\"article_id\").alias('a_article_id'),\n",
    "        C(\"tags\").alias('a_tags')\n",
    "    )\n",
    "    .join(\n",
    "        F.broadcast(\n",
    "            feature_sdf\n",
    "            .select(\n",
    "            C(\"article_id\").alias('b_article_id'),\n",
    "            C(\"tags\").alias('b_tags')\n",
    "            )\n",
    "        ),\n",
    "        on=at_least_2_matches\n",
    "    )\n",
    ")\n",
    "\n",
    "joined_sdf.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create hashes, apply join\n",
    "\n",
    "* in the same stage, no shuffle needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-26T10:47:30.464575Z",
     "start_time": "2022-02-26T10:47:30.443921Z"
    }
   },
   "outputs": [],
   "source": [
    "def fingerprint(text:str,joiner:str='') -> str:\n",
    "    return joiner.join(sorted(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-26T12:17:38.939967Z",
     "start_time": "2022-02-26T12:17:37.906483Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "== Physical Plan ==\n",
      "SortAggregate(key=[hashcode#994], functions=[first(article_id#974, false), first(t1#979, false), first(t2#984, false)])\n",
      "+- *(5) Sort [hashcode#994 ASC NULLS FIRST], false, 0\n",
      "   +- Exchange hashpartitioning(hashcode#994, 200), ENSURE_REQUIREMENTS, [id=#2153]\n",
      "      +- SortAggregate(key=[hashcode#994], functions=[partial_first(article_id#974, false), partial_first(t1#979, false), partial_first(t2#984, false)])\n",
      "         +- *(4) Sort [hashcode#994 ASC NULLS FIRST], false, 0\n",
      "            +- *(4) Project [article_id#974, t1#979, t2#984, pythonUDF0#1021 AS hashcode#994]\n",
      "               +- BatchEvalPython [fingerprint(concat_ws(, t1#979, t2#984))], [pythonUDF0#1021]\n",
      "                  +- *(3) Filter (isnotnull(t2#984) AND NOT (t1#979 = t2#984))\n",
      "                     +- Generate explode(tags#975), [article_id#974, t1#979], false, [t2#984]\n",
      "                        +- *(2) Filter isnotnull(t1#979)\n",
      "                           +- Generate explode(tags#975), [article_id#974, tags#975], false, [t1#979]\n",
      "                              +- *(1) Filter ((size(tags#975, true) > 0) AND isnotnull(tags#975))\n",
      "                                 +- *(1) Scan ExistingRDD[article_id#974,tags#975]\n",
      "\n",
      "\n",
      "+----------+--------+----------+--------------+\n",
      "|article_id|      t1|        t2|      hashcode|\n",
      "+----------+--------+----------+--------------+\n",
      "|        a3|    台北|    鮮魚湯|    北台湯魚鮮|\n",
      "|        a2|    台北|  火烤兩吃|  兩北台吃火烤|\n",
      "|        a1|    台北|    麻辣鍋|    北台辣鍋麻|\n",
      "|        a4|    台北|網美咖啡廳|北台咖啡廳網美|\n",
      "|        a1|    火鍋|      台北|      北台火鍋|\n",
      "|        a1|    火鍋|    麻辣鍋|    火辣鍋鍋麻|\n",
      "|        a2|    火鍋|  火烤兩吃|  兩吃火火烤鍋|\n",
      "|        a3|火烤兩吃|    鮮魚湯|兩吃湯火烤魚鮮|\n",
      "+----------+--------+----------+--------------+\n",
      "\n",
      "-RECORD 0--------------------\n",
      " hashcode   | 北台湯魚鮮     \n",
      " article_id | a3             \n",
      " t1         | 台北           \n",
      " t2         | 鮮魚湯         \n",
      " article_id | a3             \n",
      " t1         | 台北           \n",
      " t2         | 鮮魚湯         \n",
      "-RECORD 1--------------------\n",
      " hashcode   | 兩北台吃火烤   \n",
      " article_id | a2             \n",
      " t1         | 台北           \n",
      " t2         | 火烤兩吃       \n",
      " article_id | a2             \n",
      " t1         | 台北           \n",
      " t2         | 火烤兩吃       \n",
      "-RECORD 2--------------------\n",
      " hashcode   | 北台辣鍋麻     \n",
      " article_id | a1             \n",
      " t1         | 台北           \n",
      " t2         | 麻辣鍋         \n",
      " article_id | a1             \n",
      " t1         | 台北           \n",
      " t2         | 麻辣鍋         \n",
      "-RECORD 3--------------------\n",
      " hashcode   | 北台咖啡廳網美 \n",
      " article_id | a4             \n",
      " t1         | 台北           \n",
      " t2         | 網美咖啡廳     \n",
      " article_id | a4             \n",
      " t1         | 台北           \n",
      " t2         | 網美咖啡廳     \n",
      "-RECORD 4--------------------\n",
      " hashcode   | 北台火鍋       \n",
      " article_id | a1             \n",
      " t1         | 火鍋           \n",
      " t2         | 台北           \n",
      " article_id | a1             \n",
      " t1         | 火鍋           \n",
      " t2         | 台北           \n",
      "-RECORD 5--------------------\n",
      " hashcode   | 火辣鍋鍋麻     \n",
      " article_id | a1             \n",
      " t1         | 火鍋           \n",
      " t2         | 麻辣鍋         \n",
      " article_id | a1             \n",
      " t1         | 火鍋           \n",
      " t2         | 麻辣鍋         \n",
      "-RECORD 6--------------------\n",
      " hashcode   | 兩吃火火烤鍋   \n",
      " article_id | a2             \n",
      " t1         | 火鍋           \n",
      " t2         | 火烤兩吃       \n",
      " article_id | a2             \n",
      " t1         | 火鍋           \n",
      " t2         | 火烤兩吃       \n",
      "-RECORD 7--------------------\n",
      " hashcode   | 兩吃湯火烤魚鮮 \n",
      " article_id | a3             \n",
      " t1         | 火烤兩吃       \n",
      " t2         | 鮮魚湯         \n",
      " article_id | a3             \n",
      " t1         | 火烤兩吃       \n",
      " t2         | 鮮魚湯         \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = [\n",
    "    ('a1',['火鍋','台北','麻辣鍋']),\n",
    "    ('a2',['火鍋','台北','火烤兩吃']),\n",
    "    ('a3',['火烤兩吃','台北','鮮魚湯']),\n",
    "    ('a4',['台北','網美咖啡廳'])\n",
    "]\n",
    "\n",
    "feature_sdf = spark.createDataFrame(data, ['article_id','tags'])\n",
    "\n",
    "# approach 1\n",
    "\n",
    "\n",
    "\n",
    "cn2_sdf = (\n",
    "    feature_sdf\n",
    "    .withColumn('t1',F.explode('tags'))\n",
    "    .select(\n",
    "        'article_id',\n",
    "        't1',\n",
    "        F.explode('tags').alias('t2'),\n",
    "        \n",
    "    )\n",
    "    .where(C(\"t1\") != C(\"t2\"))\n",
    "    .withColumn('hashcode',F.concat_ws('',C(\"t1\"),C(\"t2\")))\n",
    "    .withColumn('hashcode',F.udf(fingerprint,'string')(C(\"hashcode\")))\n",
    "    .drop_duplicates(['hashcode'])\n",
    "    \n",
    ")\n",
    "\n",
    "print(cn2_sdf.count())\n",
    "cn2_sdf.explain()\n",
    "cn2_sdf.show()\n",
    "\n",
    "# apply self join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join at least match 3 tags\n",
    "\n",
    "if we wanna explode it to N, we need itertools maybe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-26T12:29:58.515450Z",
     "start_time": "2022-02-26T12:29:57.674922Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "== Physical Plan ==\n",
      "SortAggregate(key=[hashcode#1224], functions=[first(article_id#1191, false), first(t1#1196, false), first(t2#1201, false), first(t3#1207, false)])\n",
      "+- *(6) Sort [hashcode#1224 ASC NULLS FIRST], false, 0\n",
      "   +- Exchange hashpartitioning(hashcode#1224, 200), ENSURE_REQUIREMENTS, [id=#2623]\n",
      "      +- SortAggregate(key=[hashcode#1224], functions=[partial_first(article_id#1191, false), partial_first(t1#1196, false), partial_first(t2#1201, false), partial_first(t3#1207, false)])\n",
      "         +- *(5) Sort [hashcode#1224 ASC NULLS FIRST], false, 0\n",
      "            +- *(5) Project [article_id#1191, t1#1196, t2#1201, t3#1207, pythonUDF0#1257 AS hashcode#1224]\n",
      "               +- BatchEvalPython [fingerprint(concat_ws(, t1#1196, t2#1201, t3#1207))], [pythonUDF0#1257]\n",
      "                  +- *(4) Filter ((isnotnull(t3#1207) AND NOT (t1#1196 = t3#1207)) AND NOT (t2#1201 = t3#1207))\n",
      "                     +- Generate explode(tags#1192), [article_id#1191, t1#1196, t2#1201], false, [t3#1207]\n",
      "                        +- *(3) Filter (isnotnull(t2#1201) AND NOT (t1#1196 = t2#1201))\n",
      "                           +- Generate explode(tags#1192), [article_id#1191, tags#1192, t1#1196], false, [t2#1201]\n",
      "                              +- *(2) Filter isnotnull(t1#1196)\n",
      "                                 +- Generate explode(tags#1192), [article_id#1191, tags#1192], false, [t1#1196]\n",
      "                                    +- *(1) Filter ((size(tags#1192, true) > 0) AND isnotnull(tags#1192))\n",
      "                                       +- *(1) Scan ExistingRDD[article_id#1191,tags#1192]\n",
      "\n",
      "\n",
      "+----------+--------+--------+--------+------------------+\n",
      "|article_id|      t1|      t2|      t3|          hashcode|\n",
      "+----------+--------+--------+--------+------------------+\n",
      "|        a2|    火鍋|火烤兩吃|  麻辣鍋|兩吃火火烤辣鍋鍋麻|\n",
      "|        a1|    火鍋|    台北|  麻辣鍋|    北台火辣鍋鍋麻|\n",
      "|        a2|    台北|火烤兩吃|  麻辣鍋|兩北台吃火烤辣鍋麻|\n",
      "|        a2|    火鍋|    台北|火烤兩吃|  兩北台吃火火烤鍋|\n",
      "|        a3|火烤兩吃|    台北|  鮮魚湯|兩北台吃湯火烤魚鮮|\n",
      "+----------+--------+--------+--------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = [\n",
    "    ('a1',['火鍋','台北','麻辣鍋']),\n",
    "    ('a2',['火鍋','台北','火烤兩吃','麻辣鍋']),\n",
    "    ('a3',['火烤兩吃','台北','鮮魚湯']),\n",
    "    ('a4',['台北','網美咖啡廳'])\n",
    "]\n",
    "\n",
    "feature_sdf = spark.createDataFrame(data, ['article_id','tags'])\n",
    "\n",
    "# approach 1\n",
    "\n",
    "\n",
    "distinct_tag_combination = (\n",
    "      (C(\"t1\") != C(\"t2\"))\n",
    "    & (C(\"t1\") != C(\"t3\"))\n",
    "    & (C(\"t2\") != C(\"t3\"))\n",
    ")\n",
    "\n",
    "cn3_sdf = (\n",
    "    feature_sdf\n",
    "    .withColumn('t1',F.explode('tags'))\n",
    "    .withColumn('t2',F.explode('tags'))\n",
    "    .withColumn('t3',F.explode('tags'))\n",
    "    .select(\n",
    "        'article_id',\n",
    "        *['t1','t2','t3'],\n",
    "    )\n",
    "    .where(distinct_tag_combination)\n",
    "    .withColumn('hashcode',F.concat_ws('',*[C(col) for col in ['t1','t2','t3']]))\n",
    "    .withColumn('hashcode',F.udf(fingerprint,'string')(C(\"hashcode\")))\n",
    "    .drop_duplicates(['hashcode'])\n",
    ")\n",
    "\n",
    "print(cn3_sdf.count())\n",
    "cn3_sdf.explain()\n",
    "cn3_sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using broadcast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## analyze your data, then broadcast \n",
    "\n",
    "* daily updated poi (small) <---> historical articles (big)\n",
    "* popular tags articles(small) <---> long tail articles (big)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compress data size, then broadcast\n",
    "\n",
    "\n",
    "* numerical\n",
    "\n",
    "Numeric types\n",
    "\n",
    "ByteType: Represents 1-byte signed integer numbers. The range of numbers is from -128 to 127.\n",
    "\n",
    "ShortType: Represents 2-byte signed integer numbers. The range of numbers is from -32768 to 32767.\n",
    "\n",
    "IntegerType: Represents 4-byte signed integer numbers. The range of numbers is from -2147483648 to 2147483647.\n",
    "\n",
    "LongType: Represents 8-byte signed integer numbers. The range of numbers is from -9223372036854775808 to 9223372036854775807.\n",
    "\n",
    "FloatType: Represents 4-byte single-precision floating point numbers.\n",
    "\n",
    "DoubleType: Represents 8-byte double-precision floating point numbers.\n",
    "\n",
    "DecimalType: Represents arbitrary-precision signed decimal numbers. Backed internally by java.math.BigDecimal. A \n",
    "BigDecimal consists of an arbitrary precision integer unscaled value and a 32-bit integer scale.\n",
    "\n",
    "1. default `Double` --> `Float` size : 1/2\n",
    "2. if you using signed integer, you could make it smaller\n",
    "\n",
    "\n",
    "* string\n",
    "\n",
    "String type\n",
    "\n",
    "StringType: Represents character string values.\n",
    "\n",
    "VarcharType(length): A variant of StringType which has a length limitation. Data writing will fail if the input string exceeds the length limitation. Note: this type can only be used in table schema, not functions/operators.\n",
    "\n",
    "CharType(length): A variant of VarcharType(length) which is fixed length. Reading column of type CharType(n) always returns string values of length n. Char type column comparison will pad the short one to the longer length.\n",
    "\n",
    "1. hashing string to unsigned integer (smaller!)\n",
    "\n",
    "    * using `F.monotonically_increasing_id()` to get complete hashed result(with float64 data type) -\n",
    "    * using zipWithIndex from RDD to make hashes smaller\n",
    "    https://stackoverflow.com/questions/48209667/using-monotonically-increasing-id-for-assigning-row-number-to-pyspark-datafram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-26T10:29:59.532696Z",
     "start_time": "2022-02-26T10:29:32.408290Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/02/26 18:29:59 WARN TaskSetManager: Stage 17 contains a task of very large size (3001 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000\n",
      "root\n",
      " |-- vec_component_id: long (nullable = true)\n",
      " |-- tfidf: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# suppose 200M urls, with double by default\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "size= 2*1e6\n",
    "\n",
    "data = np.random.random(size=int(size))\n",
    "\n",
    "\n",
    "indexed_data_gen = (\n",
    "    (i, float(number)) \n",
    "    for i, number \n",
    "    in enumerate(data)\n",
    ")\n",
    "\n",
    "# print(list(indexed_data_gen))\n",
    "\n",
    "sdf = spark.createDataFrame(\n",
    "    list(indexed_data_gen),\n",
    "    schema=['vec_component_id','tfidf']\n",
    ")\n",
    "\n",
    "print(sdf.count())\n",
    "sdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-26T10:30:42.375753Z",
     "start_time": "2022-02-26T10:29:59.534271Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/02/26 18:30:36 WARN TaskSetManager: Stage 19 contains a task of very large size (9046 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/02/26 18:30:38 WARN TaskSetManager: Stage 20 contains a task of very large size (9046 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/02/26 18:30:38 WARN TaskSetManager: Stage 21 contains a task of very large size (9046 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/02/26 18:30:39 WARN TaskSetManager: Stage 22 contains a task of very large size (9046 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/02/26 18:30:39 WARN TaskSetManager: Stage 23 contains a task of very large size (9046 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "\n",
    "def get_link(n=30):\n",
    "    return 'http://' + ''.join(random.choices(string.ascii_uppercase + string.digits, k=n))\n",
    "\n",
    "save = True\n",
    "size= int(2*1e6)\n",
    "\n",
    "data = [get_link() for _ in range(size)]\n",
    "\n",
    "\n",
    "indexed_data_gen = (\n",
    "    (i, url) \n",
    "    for i, url \n",
    "    in enumerate(data)\n",
    ")\n",
    "\n",
    "sdf = spark.createDataFrame(\n",
    "    list(indexed_data_gen),\n",
    "    schema=['idx','url']\n",
    ")\n",
    "\n",
    "complete_hashed_sdf = (\n",
    "    sdf\n",
    "    .withColumn('hashes',F.monotonically_increasing_id())\n",
    "    .select('idx','hashes')\n",
    ")\n",
    "\n",
    "\n",
    "smaller_complete_hashed_sdf = (\n",
    "    sdf\n",
    "    .rdd\n",
    "    .zipWithIndex()\n",
    "    .toDF()\n",
    "    .select(\n",
    "    # T.IntegerType() - -20億 ~ 20億\n",
    "    C('_1.idx').cast(T.IntegerType()),\n",
    "    C('_2').cast(T.IntegerType()).alias('hashes'),    \n",
    "    )\n",
    ")\n",
    "\n",
    "if save:\n",
    "    sdf.write.json('2m_url.json',mode='overwrite')\n",
    "    complete_hashed_sdf.write.json('2m_url_hashed.json',mode='overwrite')\n",
    "    smaller_complete_hashed_sdf.write.json('2m_url_hashed_smaller.json',mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-26T10:30:42.517884Z",
     "start_time": "2022-02-26T10:30:42.377653Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119M\t2m_url.json\r\n"
     ]
    }
   ],
   "source": [
    "! du -sh 2m_url.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-26T10:30:42.727431Z",
     "start_time": "2022-02-26T10:30:42.519986Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69M\t2m_url_hashed.json\r\n"
     ]
    }
   ],
   "source": [
    "! du -sh 2m_url_hashed.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-26T10:30:42.875594Z",
     "start_time": "2022-02-26T10:30:42.730659Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62M\t2m_url_hashed_smaller.json\r\n"
     ]
    }
   ],
   "source": [
    "# 調整資料型態確實是有一些幫助\n",
    "! du -sh 2m_url_hashed_smaller.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using shuffle hash join\n",
    "\n",
    "\n",
    "https://www.hadoopinrealworld.com/how-does-shuffle-hash-join-work-in-spark/\n",
    "\n",
    "https://www.waitingforcode.com/apache-spark-sql/what-new-apache-spark-3-1-join-evolutions/read\n",
    "\n",
    "1. estimate the hash size (so you know what many memory needed)\n",
    "2. check the execution plan\n",
    "3. remember to deal the data skew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-03T01:45:20.896858Z",
     "start_time": "2022-03-03T01:45:16.601964Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+\n",
      "|article_id| tag|\n",
      "+----------+----+\n",
      "|        a1|火鍋|\n",
      "+----------+----+\n",
      "only showing top 1 row\n",
      "\n",
      "== Physical Plan ==\n",
      "*(3) Project [tag#1, article_id#0, article_id#13]\n",
      "+- *(3) ShuffledHashJoin [tag#1], [tag#14], Inner, BuildRight\n",
      "   :- Exchange hashpartitioning(tag#1, 200), ENSURE_REQUIREMENTS, [id=#37]\n",
      "   :  +- *(1) Filter isnotnull(tag#1)\n",
      "   :     +- *(1) Scan ExistingRDD[article_id#0,tag#1]\n",
      "   +- ReusedExchange [article_id#13, tag#14], Exchange hashpartitioning(tag#1, 200), ENSURE_REQUIREMENTS, [id=#37]\n",
      "\n",
      "\n",
      "+--------+----------+----------+\n",
      "|     tag|article_id|article_id|\n",
      "+--------+----------+----------+\n",
      "|火烤兩吃|        a3|        a3|\n",
      "|    台北|        a2|        a3|\n",
      "|    台北|        a2|        a2|\n",
      "|    台北|        a3|        a3|\n",
      "|    台北|        a3|        a2|\n",
      "|  鮮魚湯|        a3|        a3|\n",
      "|    火鍋|        a1|        a2|\n",
      "|    火鍋|        a1|        a1|\n",
      "|    火鍋|        a2|        a2|\n",
      "|    火鍋|        a2|        a1|\n",
      "+--------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    ('a1',\"火鍋\"),\n",
    "    ('a2','火鍋'),\n",
    "    ('a2','台北'),\n",
    "    ('a3','火烤兩吃'),\n",
    "    ('a3','台北'),\n",
    "    ('a3','鮮魚湯')\n",
    "]\n",
    "\n",
    "feature_sdf = (\n",
    "    spark.createDataFrame(data, ['article_id','tag'])\n",
    ")\n",
    "\n",
    "feature_sdf.show(n=1)\n",
    "\n",
    "joined_sdf = (\n",
    "    feature_sdf\n",
    "    .join(feature_sdf.hint(\"shuffle_hash\"),\n",
    "          on=['tag']\n",
    "         )\n",
    ")\n",
    "\n",
    "joined_sdf.explain()\n",
    "\n",
    "joined_sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pixlake_spark3",
   "language": "python",
   "name": "pixlake_spark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "213.965px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
